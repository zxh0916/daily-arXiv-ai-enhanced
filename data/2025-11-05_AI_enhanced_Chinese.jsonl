{"id": "2511.01999", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01999", "abs": "https://arxiv.org/abs/2511.01999", "authors": ["Sangyun Park", "Jin Kim", "Yuchen Cui", "Matthew S. Brown"], "title": "TRACE: Textual Reasoning for Affordance Coordinate Extraction", "comment": "ICCV 2025. *Equal contribution. {\\dag}Corresponding author", "summary": "Vision-Language Models (VLMs) struggle to translate high-level instructions\ninto the precise spatial affordances required for robotic manipulation. While\nvisual Chain-of-Thought (CoT) methods exist, they are often computationally\nintensive. In this work, we introduce TRACE (Textual Reasoning for Affordance\nCoordinate Extraction), a novel methodology that integrates a textual Chain of\nReasoning (CoR) into the affordance prediction process. We use this methodology\nto create the TRACE dataset, a large-scale collection created via an autonomous\npipeline that pairs instructions with explicit textual rationales. By\nfine-tuning a VLM on this data, our model learns to externalize its spatial\nreasoning before acting. Our experiments show that our TRACE-tuned model\nachieves state-of-the-art performance, reaching 48.1% accuracy on the primary\nWhere2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more\nchallenging W2P(h) subset. Crucially, an ablation study demonstrates that\nperformance scales directly with the amount of reasoning data used, confirming\nthe CoR's effectiveness. Furthermore, analysis of the model's attention maps\nreveals an interpretable reasoning process where focus shifts dynamically\nacross reasoning steps. This work shows that training VLMs to generate a\ntextual CoR is an effective and robust strategy for enhancing the precision,\nreliability, and interpretability of VLM-based robot control. Our dataset and\ncode are available at https://github.com/jink-ucla/TRACE", "AI": {"tldr": "TRACE\u65b9\u6cd5\u901a\u8fc7\u5c06\u6587\u672c\u63a8\u7406\u94fe\u96c6\u6210\u5230affordance\u9884\u6d4b\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u7cbe\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c06\u9ad8\u7ea7\u6307\u4ee4\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u6240\u9700\u7cbe\u786e\u7a7a\u95f4affordance\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u73b0\u6709\u89c6\u89c9\u601d\u7ef4\u94fe\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faTRACE\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u4e3b\u7ba1\u9053\u521b\u5efa\u5305\u542b\u6307\u4ee4\u548c\u663e\u5f0f\u6587\u672c\u63a8\u7406\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u5728\u8be5\u6570\u636e\u4e0a\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u884c\u52a8\u524d\u5916\u90e8\u5316\u7a7a\u95f4\u63a8\u7406\u3002", "result": "\u5728Where2Place\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523048.1%\u51c6\u786e\u7387\uff08\u76f8\u5bf9\u63d0\u53479.6%\uff09\uff0c\u5728\u66f4\u96be\u7684W2P(h)\u5b50\u96c6\u8fbe\u523055.0%\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u6027\u80fd\u4e0e\u63a8\u7406\u6570\u636e\u91cf\u76f4\u63a5\u76f8\u5173\u3002", "conclusion": "\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u63a8\u7406\u94fe\u662f\u63d0\u5347VLM\u673a\u5668\u4eba\u63a7\u5236\u7cbe\u5ea6\u3001\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2511.02015", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02015", "abs": "https://arxiv.org/abs/2511.02015", "authors": ["Jace Aldrich", "Odest Chadwicke Jenkins"], "title": "Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control", "comment": "8 pages, 6 figures", "summary": "This paper presents a novel method for Model Predictive Path Integral (MPPI)\ncontrol that optimizes sample generation towards an optimal trajectory through\nStein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on\nrandomly sampled trajectories, often by a Gaussian distribution. The result can\nlead to sample deprivation, under-representing the space of possible\ntrajectories, and yield suboptimal results. Through introducing SVGD updates in\nbetween MPPI environment steps, we present Stein-Optimized Path-Integral\nInference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise\ndistributions at runtime to shape a more optimal representation without an\nexcessive increase in computational requirements. We demonstrate the efficacy\nof our method systems ranging from a Cart-Pole to a two-dimensional bipedal\nwalking task, indicating improved performance above standard MPPI across a\nrange of hyper-parameters and demonstrate feasibility at lower particle counts.\nWe discuss the applicability of this MPPI/SVGD method to higher\ndegree-of-freedom systems, as well as its potential to new developments in\nstate-of-the-art differentiable simulators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u63a7\u5236\u4e0eStein\u53d8\u5206\u68af\u5ea6\u4e0b\u964d\u7684\u65b0\u65b9\u6cd5SOPPI\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u91c7\u6837\u5206\u5e03\u6765\u6539\u8fdb\u8f68\u8ff9\u751f\u6210\uff0c\u5728\u591a\u4e2a\u7cfb\u7edf\u4e2d\u5c55\u793a\u4e86\u4f18\u4e8e\u6807\u51c6MPPI\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfMPPI\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u8f68\u8ff9\uff0c\u5e38\u5bfc\u81f4\u6837\u672c\u4e0d\u8db3\u548c\u6b21\u4f18\u7ed3\u679c\uff0c\u9700\u8981\u6539\u8fdb\u91c7\u6837\u7b56\u7565\u4ee5\u63d0\u9ad8\u63a7\u5236\u6027\u80fd\u3002", "method": "\u5728MPPI\u73af\u5883\u6b65\u9aa4\u4e4b\u95f4\u5f15\u5165SVGD\u66f4\u65b0\uff0c\u52a8\u6001\u8c03\u6574\u566a\u58f0\u5206\u5e03\u4ee5\u5f62\u6210\u66f4\u4f18\u7684\u8f68\u8ff9\u8868\u793a\uff0c\u79f0\u4e3aStein\u4f18\u5316\u8def\u5f84\u79ef\u5206\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u5728Cart-Pole\u548c\u4e8c\u7ef4\u53cc\u8db3\u884c\u8d70\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u591a\u79cd\u8d85\u53c2\u6570\u4e0b\u5747\u4f18\u4e8e\u6807\u51c6MPPI\uff0c\u4e14\u5728\u8f83\u4f4e\u7c92\u5b50\u6570\u4e0b\u4ecd\u53ef\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u66f4\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\uff0c\u5e76\u6709\u6f5c\u529b\u63a8\u52a8\u53ef\u5fae\u5206\u6a21\u62df\u5668\u7684\u65b0\u53d1\u5c55\u3002"}}
{"id": "2511.02036", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02036", "abs": "https://arxiv.org/abs/2511.02036", "authors": ["Parsa Hosseininejad", "Kimia Khabiri", "Shishir Gopinath", "Soudabeh Mohammadhashemi", "Karthik Dantu", "Steven Y. Ko"], "title": "TurboMap: GPU-Accelerated Local Mapping for Visual SLAM", "comment": "Submitted to ICRA 2026", "summary": "This paper presents TurboMap, a GPU-accelerated and CPU-optimized local\nmapping module for visual SLAM systems. We identify key performance bottlenecks\nin the local mapping process for visual SLAM and address them through targeted\nGPU and CPU optimizations. Specifically, we offload map point triangulation and\nfusion to the GPU, accelerate redundant keyframe culling on the CPU, and\nintegrate a GPU-accelerated solver to speed up local bundle adjustment. Our\nimplementation is built on top of ORB-SLAM3 and leverages CUDA for GPU\nprogramming. The experimental results show that TurboMap achieves an average\nspeedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the\nlocal mapping module, on both desktop and embedded platforms, while maintaining\nthe accuracy of the original system.", "AI": {"tldr": "TurboMap\u662f\u4e00\u4e2aGPU\u52a0\u901f\u548cCPU\u4f18\u5316\u7684\u89c6\u89c9SLAM\u5c40\u90e8\u5efa\u56fe\u6a21\u5757\uff0c\u901a\u8fc7GPU\u548cCPU\u4f18\u5316\u89e3\u51b3\u5c40\u90e8\u5efa\u56fe\u6027\u80fd\u74f6\u9888\uff0c\u5728ORB-SLAM3\u57fa\u7840\u4e0a\u5b9e\u73b0\uff0c\u5728EuRoC\u548cTUM-VI\u6570\u636e\u96c6\u4e0a\u5206\u522b\u83b7\u5f971.3\u500d\u548c1.6\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u8bc6\u522b\u89c6\u89c9SLAM\u7cfb\u7edf\u4e2d\u5c40\u90e8\u5efa\u56fe\u8fc7\u7a0b\u7684\u5173\u952e\u6027\u80fd\u74f6\u9888\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u7684GPU\u548cCPU\u4f18\u5316\u6765\u63d0\u5347\u5efa\u56fe\u6548\u7387\u3002", "method": "\u5c06\u5730\u56fe\u70b9\u4e09\u89d2\u5316\u548c\u878d\u5408\u5378\u8f7d\u5230GPU\uff0c\u5728CPU\u4e0a\u52a0\u901f\u5197\u4f59\u5173\u952e\u5e27\u5254\u9664\uff0c\u96c6\u6210GPU\u52a0\u901f\u6c42\u89e3\u5668\u6765\u52a0\u901f\u5c40\u90e8\u675f\u8c03\u6574\uff0c\u57fa\u4e8eORB-SLAM3\u5e76\u4f7f\u7528CUDA\u8fdb\u884cGPU\u7f16\u7a0b\u3002", "result": "\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u52a0\u901f1.3\u500d\uff0c\u5728TUM-VI\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u52a0\u901f1.6\u500d\uff0c\u5728\u684c\u9762\u548c\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u5747\u6709\u6548\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7cfb\u7edf\u7684\u7cbe\u5ea6\u3002", "conclusion": "TurboMap\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u89c9SLAM\u5c40\u90e8\u5efa\u56fe\u7684\u6027\u80fd\u74f6\u9888\uff0c\u901a\u8fc7GPU\u548cCPU\u534f\u540c\u4f18\u5316\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7cfb\u7edf\u7cbe\u5ea6\u3002"}}
{"id": "2511.02060", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02060", "abs": "https://arxiv.org/abs/2511.02060", "authors": ["Hersh Sanghvi", "Spencer Folk", "Vijay Kumar", "Camillo Jose Taylor"], "title": "TACO: Trajectory-Aware Controller Optimization for Quadrotors", "comment": "8 pages, 6 figures. In submission to ICRA 2026", "summary": "Controller performance in quadrotor trajectory tracking depends heavily on\nparameter tuning, yet standard approaches often rely on fixed, manually tuned\nparameters that sacrifice task-specific performance. We present\nTrajectory-Aware Controller Optimization (TACO), a framework that adapts\ncontroller parameters online based on the upcoming reference trajectory and\ncurrent quadrotor state. TACO employs a learned predictive model and a\nlightweight optimization scheme to optimize controller gains in real time with\nrespect to a broad class of trajectories, and can also be used to adapt\ntrajectories to improve dynamic feasibility while respecting smoothness\nconstraints. To enable large-scale training, we also introduce a parallelized\nquadrotor simulator supporting fast data collection on diverse trajectories.\nExperiments on a variety of trajectory types show that TACO outperforms\nconventional, static parameter tuning while operating orders of magnitude\nfaster than black-box optimization baselines, enabling practical real-time\ndeployment on a physical quadrotor. Furthermore, we show that adapting\ntrajectories using TACO significantly reduces the tracking error obtained by\nthe quadrotor.", "AI": {"tldr": "TACO\u662f\u4e00\u4e2a\u5b9e\u65f6\u4f18\u5316\u56db\u65cb\u7ffc\u63a7\u5236\u5668\u53c2\u6570\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u4f18\u5316\u65b9\u6848\uff0c\u6839\u636e\u53c2\u8003\u8f68\u8ff9\u548c\u5f53\u524d\u72b6\u6001\u5728\u7ebf\u8c03\u6574\u63a7\u5236\u5668\u589e\u76ca\uff0c\u663e\u8457\u63d0\u5347\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u56db\u65cb\u7ffc\u63a7\u5236\u5668\u53c2\u6570\u901a\u5e38\u662f\u56fa\u5b9a\u624b\u52a8\u8c03\u4f18\u7684\uff0c\u727a\u7272\u4e86\u4efb\u52a1\u7279\u5b9a\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u8f68\u8ff9\u52a8\u6001\u8c03\u6574\u53c2\u6570\u7684\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u4f18\u5316\u65b9\u6848\u5b9e\u65f6\u4f18\u5316\u63a7\u5236\u5668\u589e\u76ca\uff0c\u652f\u6301\u8f68\u8ff9\u9002\u5e94\u4ee5\u63d0\u9ad8\u52a8\u6001\u53ef\u884c\u6027\uff0c\u5e76\u5f00\u53d1\u5e76\u884c\u5316\u6a21\u62df\u5668\u8fdb\u884c\u5927\u89c4\u6a21\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTACO\u5728\u591a\u79cd\u8f68\u8ff9\u7c7b\u578b\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u53c2\u6570\u8c03\u4f18\uff0c\u8fd0\u884c\u901f\u5ea6\u6bd4\u9ed1\u76d2\u4f18\u5316\u57fa\u7ebf\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5728\u7269\u7406\u56db\u65cb\u7ffc\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\uff0c\u8f68\u8ff9\u9002\u5e94\u663e\u8457\u964d\u4f4e\u8ddf\u8e2a\u8bef\u5dee\u3002", "conclusion": "TACO\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u56db\u65cb\u7ffc\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\uff0c\u901a\u8fc7\u5b9e\u65f6\u53c2\u6570\u4f18\u5316\u548c\u8f68\u8ff9\u9002\u5e94\u5b9e\u73b0\u66f4\u597d\u7684\u63a7\u5236\u6548\u679c\u3002"}}
{"id": "2511.02097", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.02097", "abs": "https://arxiv.org/abs/2511.02097", "authors": ["Peng-Fei Zhang", "Ying Cheng", "Xiaofan Sun", "Shijie Wang", "Lei Zhu", "Heng Tao Shen"], "title": "A Step Toward World Models: A Survey on Robotic Manipulation", "comment": "24 pages, 5 figures", "summary": "Autonomous agents are increasingly expected to operate in complex, dynamic,\nand uncertain environments, performing tasks such as manipulation, navigation,\nand decision-making. Achieving these capabilities requires agents to understand\nthe underlying mechanisms and dynamics of the world, moving beyond purely\nreactive control or simple replication of observed states. This motivates the\ndevelopment of world models as internal representations that encode\nenvironmental states, capture dynamics, and enable prediction, planning, and\nreasoning. Despite growing interest, the definition, scope, architectures, and\nessential capabilities of world models remain ambiguous. In this survey, rather\nthan directly imposing a fixed definition and limiting our scope to methods\nexplicitly labeled as world models, we examine approaches that exhibit the core\ncapabilities of world models through a review of methods in robotic\nmanipulation. We analyze their roles across perception, prediction, and\ncontrol, identify key challenges and solutions, and distill the core\ncomponents, capabilities, and functions that a real world model should possess.\nBuilding on this analysis, we aim to outline a roadmap for developing\ngeneralizable and practical world models for robotics.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7efc\u8ff0\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u4e16\u754c\u6a21\u578b\u7684\u6838\u5fc3\u80fd\u529b\u3001\u7ec4\u4ef6\u548c\u529f\u80fd\uff0c\u65e8\u5728\u4e3a\u5f00\u53d1\u901a\u7528\u5b9e\u7528\u7684\u673a\u5668\u4eba\u4e16\u754c\u6a21\u578b\u5236\u5b9a\u8def\u7ebf\u56fe\u3002", "motivation": "\u81ea\u4e3b\u667a\u80fd\u4f53\u9700\u8981\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u6267\u884c\u4efb\u52a1\uff0c\u8fd9\u8981\u6c42\u5b83\u4eec\u7406\u89e3\u4e16\u754c\u673a\u5236\u548c\u52a8\u6001\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u53cd\u5e94\u63a7\u5236\u6216\u72b6\u6001\u590d\u5236\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7f16\u7801\u73af\u5883\u72b6\u6001\u3001\u6355\u6349\u52a8\u6001\u5e76\u652f\u6301\u9884\u6d4b\u3001\u89c4\u5212\u548c\u63a8\u7406\u7684\u4e16\u754c\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u5177\u6709\u4e16\u754c\u6a21\u578b\u6838\u5fc3\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u8003\u5bdf\u5b83\u4eec\u5728\u611f\u77e5\u3001\u9884\u6d4b\u548c\u63a7\u5236\u4e2d\u7684\u4f5c\u7528\uff0c\u8bc6\u522b\u5173\u952e\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u63d0\u70bc\u4e86\u771f\u5b9e\u4e16\u754c\u6a21\u578b\u5e94\u5177\u5907\u7684\u6838\u5fc3\u7ec4\u4ef6\u3001\u80fd\u529b\u548c\u529f\u80fd\uff0c\u6f84\u6e05\u4e86\u4e16\u754c\u6a21\u578b\u7684\u5b9a\u4e49\u3001\u8303\u56f4\u3001\u67b6\u6784\u548c\u57fa\u672c\u80fd\u529b\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6790\u4e3a\u5f00\u53d1\u901a\u7528\u5b9e\u7528\u7684\u673a\u5668\u4eba\u4e16\u754c\u6a21\u578b\u5236\u5b9a\u4e86\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u4e16\u754c\u6a21\u578b\u5728\u5b9e\u73b0\u81ea\u4e3b\u667a\u80fd\u4f53\u590d\u6742\u4efb\u52a1\u80fd\u529b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2511.02147", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02147", "abs": "https://arxiv.org/abs/2511.02147", "authors": ["Tyler M. Paine", "Anastasia Bizyaeva", "Michael R. Benjamin"], "title": "Census-Based Population Autonomy For Distributed Robotic Teaming", "comment": "16 pages, 17 figures", "summary": "Collaborating teams of robots show promise due in their ability to complete\nmissions more efficiently and with improved robustness, attributes that are\nparticularly useful for systems operating in marine environments. A key issue\nis how to model, analyze, and design these multi-robot systems to realize the\nfull benefits of collaboration, a challenging task since the domain of\nmulti-robot autonomy encompasses both collective and individual behaviors. This\npaper introduces a layered model of multi-robot autonomy that uses the\nprinciple of census, or a weighted count of the inputs from neighbors, for\ncollective decision-making about teaming, coupled with multi-objective behavior\noptimization for individual decision-making about actions. The census component\nis expressed as a nonlinear opinion dynamics model and the multi-objective\nbehavior optimization is accomplished using interval programming. This model\ncan be reduced to recover foundational algorithms in distributed optimization\nand control, while the full model enables new types of collective behaviors\nthat are useful in real-world scenarios. To illustrate these points, a new\nmethod for distributed optimization of subgroup allocation is introduced where\nrobots use a gradient descent algorithm to minimize portions of the cost\nfunctions that are locally known, while being influenced by the opinion states\nfrom neighbors to account for the unobserved costs. With this method the group\ncan collectively use the information contained in the Hessian matrix of the\ntotal global cost. The utility of this model is experimentally validated in\nthree categorically different experiments with fleets of autonomous surface\nvehicles: an adaptive sampling scenario, a high value unit protection scenario,\nand a competitive game of capture the flag.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u7684\u5206\u5c42\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u90bb\u5c45\u8f93\u5165\u52a0\u6743\u8ba1\u6570\u7684\u96c6\u4f53\u51b3\u7b56\u548c\u57fa\u4e8e\u591a\u76ee\u6807\u884c\u4e3a\u4f18\u5316\u7684\u4e2a\u4f53\u51b3\u7b56\uff0c\u7528\u4e8e\u63d0\u9ad8\u673a\u5668\u4eba\u56e2\u961f\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6d77\u6d0b\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u6548\u5b8c\u6210\u4efb\u52a1\u548c\u589e\u5f3a\u9c81\u68d2\u6027\u7684\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u5efa\u6a21\u3001\u5206\u6790\u548c\u8bbe\u8ba1\u8fd9\u4e9b\u7cfb\u7edf\u4ee5\u5b9e\u73b0\u534f\u4f5c\u7684\u5168\u90e8\u6548\u76ca\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6a21\u578b\uff0c\u96c6\u4f53\u51b3\u7b56\u4f7f\u7528\u975e\u7ebf\u6027\u610f\u89c1\u52a8\u6001\u6a21\u578b\u8fdb\u884c\u52a0\u6743\u8ba1\u6570\uff0c\u4e2a\u4f53\u51b3\u7b56\u4f7f\u7528\u533a\u95f4\u89c4\u5212\u8fdb\u884c\u591a\u76ee\u6807\u884c\u4e3a\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u4e86\u5b50\u7fa4\u5206\u914d\u7684\u5206\u5e03\u5f0f\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u6062\u590d\u5206\u5e03\u5f0f\u4f18\u5316\u548c\u63a7\u5236\u7684\u57fa\u7840\u7b97\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u65b0\u7684\u96c6\u4f53\u884c\u4e3a\u7c7b\u578b\uff0c\u5728\u81ea\u9002\u5e94\u91c7\u6837\u3001\u9ad8\u4ef7\u503c\u5355\u5143\u4fdd\u62a4\u548c\u593a\u65d7\u6e38\u620f\u4e09\u4e2a\u5b9e\u9a8c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u6a21\u578b\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5efa\u6a21\u548c\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u7684\u534f\u4f5c\u884c\u4e3a\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.02162", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.02162", "abs": "https://arxiv.org/abs/2511.02162", "authors": ["Alexander Htet Kyaw", "Richa Gupta", "Dhruv Shah", "Anoop Sinha", "Kory Mathewson", "Stefanie Pender", "Sachin Chitta", "Yotto Koga", "Faez Ahmed", "Lawrence Sass", "Randall Davis"], "title": "Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models", "comment": "Accepted to NeurIPS 2025, Conference on Neural Information Processing\n  Systems, Creative AI Track", "summary": "Advances in 3D generative AI have enabled the creation of physical objects\nfrom text prompts, but challenges remain in creating objects involving multiple\ncomponent types. We present a pipeline that integrates 3D generative AI with\nvision-language models (VLMs) to enable the robotic assembly of multi-component\nobjects from natural language. Our method leverages VLMs for zero-shot,\nmulti-modal reasoning about geometry and functionality to decompose\nAI-generated meshes into multi-component 3D models using predefined structural\nand panel components. We demonstrate that a VLM is capable of determining which\nmesh regions need panel components in addition to structural components, based\non object functionality. Evaluation across test objects shows that users\npreferred the VLM-generated assignments 90.6% of the time, compared to 59.4%\nfor rule-based and 2.5% for random assignment. Lastly, the system allows users\nto refine component assignments through conversational feedback, enabling\ngreater human control and agency in making physical objects with generative AI\nand robotics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u751f\u6210AI\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u7ba1\u9053\uff0c\u7528\u4e8e\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u591a\u7ec4\u4ef63D\u6a21\u578b\u5e76\u5b9e\u73b0\u673a\u5668\u4eba\u7ec4\u88c5\u3002VLM\u901a\u8fc7\u96f6\u6837\u672c\u591a\u6a21\u6001\u63a8\u7406\u6765\u5206\u89e3AI\u751f\u6210\u7684\u7f51\u683c\uff0c\u786e\u5b9a\u7ed3\u6784\u7ec4\u4ef6\u548c\u9762\u677f\u7ec4\u4ef6\u7684\u5206\u914d\u3002", "motivation": "\u89e3\u51b33D\u751f\u6210AI\u5728\u521b\u5efa\u591a\u7ec4\u4ef6\u5bf9\u8c61\u65f6\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4ece\u6587\u672c\u63d0\u793a\u5230\u7269\u7406\u5bf9\u8c61\u673a\u5668\u4eba\u7ec4\u88c5\u7684\u5168\u6d41\u7a0b\u81ea\u52a8\u5316\u3002", "method": "\u96c6\u62103D\u751f\u6210AI\u4e0eVLM\uff0c\u5229\u7528VLM\u8fdb\u884c\u96f6\u6837\u672c\u591a\u6a21\u6001\u51e0\u4f55\u548c\u529f\u80fd\u63a8\u7406\uff0c\u5c06AI\u751f\u6210\u7684\u7f51\u683c\u5206\u89e3\u4e3a\u591a\u7ec4\u4ef63D\u6a21\u578b\uff0c\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u7ed3\u6784\u548c\u9762\u677f\u7ec4\u4ef6\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u7528\u623790.6%\u7684\u60c5\u51b5\u4e0b\u504f\u597dVLM\u751f\u6210\u7684\u7ec4\u4ef6\u5206\u914d\uff0c\u800c\u89c4\u5219\u65b9\u6cd5\u4e3a59.4%\uff0c\u968f\u673a\u5206\u914d\u4e3a2.5%\u3002\u7cfb\u7edf\u8fd8\u652f\u6301\u901a\u8fc7\u5bf9\u8bdd\u53cd\u9988\u6765\u4f18\u5316\u7ec4\u4ef6\u5206\u914d\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7VLM\u5b9e\u73b0\u4e86\u591a\u7ec4\u4ef6\u5bf9\u8c61\u7684\u667a\u80fd\u5206\u89e3\u548c\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u5bf9\u8bdd\u53cd\u9988\u673a\u5236\u589e\u5f3a\u4e86\u7528\u6237\u5728\u751f\u6210AI\u548c\u673a\u5668\u4eba\u5236\u9020\u8fc7\u7a0b\u4e2d\u7684\u63a7\u5236\u6743\u3002"}}
{"id": "2511.02167", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02167", "abs": "https://arxiv.org/abs/2511.02167", "authors": ["Tian Hao", "Tong Lu", "Che Chan"], "title": "Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery", "comment": null, "summary": "Robotic assistance in minimally invasive surgery can greatly enhance surgical\nprecision and reduce surgeon fatigue. This paper presents a focused\ninvestigation on the kinematic and ergonomic design principles for a\nlaparoscopic surgical robotic arm aimed at high-precision tasks. We propose a\n7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote\ncenter of motion (RCM) at the instrument insertion point and ergonomic\nconsiderations to improve surgeon interaction. The design is implemented on a\ngeneral-purpose robotic platform, and a series of simulated surgical tasks were\nperformed to evaluate targeting accuracy, task efficiency, and surgeon comfort\ncompared to conventional manual laparoscopy. Experimental results demonstrate\nthat the optimized robotic design achieves significantly improved targeting\naccuracy (error reduced by over 50%) and shorter task completion times, while\nsubstantially lowering operator muscle strain and discomfort. These findings\nvalidate the importance of kinematic optimization (such as added articulations\nand tremor filtering) and human-centered ergonomic design in enhancing the\nperformance of robot-assisted surgery. The insights from this work can guide\nthe development of next-generation surgical robots that improve surgical\noutcomes and ergonomics for the operating team.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u8179\u8154\u955c\u624b\u672f\u76847\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u81c2\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\u548c\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bbe\u8ba1\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\u548c\u5916\u79d1\u533b\u751f\u8212\u9002\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u76ee\u6807\u7cbe\u5ea6\u63d0\u9ad850%\u4ee5\u4e0a\u5e76\u51cf\u5c11\u808c\u8089\u52b3\u635f\u3002", "motivation": "\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u80fd\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\u5e76\u51cf\u5c11\u5916\u79d1\u533b\u751f\u75b2\u52b3\uff0c\u9700\u8981\u7814\u7a76\u8fd0\u52a8\u5b66\u548c\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bbe\u8ba1\u539f\u5219\u6765\u4f18\u5316\u8179\u8154\u955c\u624b\u672f\u673a\u5668\u4eba\u81c2\u7684\u6027\u80fd\u3002", "method": "\u8bbe\u8ba17\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u81c2\u7cfb\u7edf\uff0c\u96c6\u6210\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\u548c\u4eba\u4f53\u5de5\u7a0b\u5b66\u8003\u8651\uff0c\u5728\u901a\u7528\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\uff0c\u901a\u8fc7\u6a21\u62df\u624b\u672f\u4efb\u52a1\u8bc4\u4f30\u76ee\u6807\u7cbe\u5ea6\u3001\u4efb\u52a1\u6548\u7387\u548c\u5916\u79d1\u533b\u751f\u8212\u9002\u5ea6\u3002", "result": "\u4f18\u5316\u540e\u7684\u673a\u5668\u4eba\u8bbe\u8ba1\u663e\u8457\u63d0\u9ad8\u4e86\u76ee\u6807\u7cbe\u5ea6\uff08\u8bef\u5dee\u51cf\u5c11\u8d85\u8fc750%\uff09\uff0c\u7f29\u77ed\u4e86\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u64cd\u4f5c\u8005\u808c\u8089\u52b3\u635f\u548c\u4e0d\u9002\u611f\u3002", "conclusion": "\u8fd0\u52a8\u5b66\u4f18\u5316\u548c\u4ee5\u4eba\u4e3a\u672c\u7684\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bbe\u8ba1\u5bf9\u63d0\u5347\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u8fd9\u4e9b\u89c1\u89e3\u53ef\u6307\u5bfc\u4e0b\u4e00\u4ee3\u624b\u672f\u673a\u5668\u4eba\u7684\u5f00\u53d1\uff0c\u6539\u5584\u624b\u672f\u7ed3\u679c\u548c\u624b\u672f\u56e2\u961f\u7684\u4eba\u4f53\u5de5\u7a0b\u5b66\u6761\u4ef6\u3002"}}
{"id": "2511.02192", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02192", "abs": "https://arxiv.org/abs/2511.02192", "authors": ["Linxin Hou", "Qirui Wu", "Zhihang Qin", "Neil Banerjee", "Yongxin Guo", "Cecilia Laschi"], "title": "A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms", "comment": "7 pages, 4 figures, 2 tables, submitted to RoboSoft 2026", "summary": "This paper presents a quantitative comparison between centralised and\ndistributed multi-agent reinforcement learning (MARL) architectures for\ncontrolling a soft robotic arm modelled as a Cosserat rod in simulation. Using\nPyElastica and the OpenAI Gym interface, we train both a global Proximal Policy\nOptimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical\nbudgets. Both approaches are based on the arm having $n$ number of controlled\nsections. The study systematically varies $n$ and evaluates the performance of\nthe arm to reach a fixed target in three scenarios: default baseline condition,\nrecovery from external disturbance, and adaptation to actuator failure.\nQuantitative metrics used for the evaluation are mean action magnitude, mean\nfinal distance, mean episode length, and success rate. The results show that\nthere are no significant benefits of the distributed policy when the number of\ncontrolled sections $n\\le4$. In very simple systems, when $n\\le2$, the\ncentralised policy outperforms the distributed one. When $n$ increases to $4<\nn\\le 12$, the distributed policy shows a high sample efficiency. In these\nsystems, distributed policy promotes a stronger success rate, resilience, and\nrobustness under local observability and yields faster convergence given the\nsame sample size. However, centralised policies achieve much higher time\nefficiency during training as it takes much less time to train the same size of\nsamples. These findings highlight the trade-offs between centralised and\ndistributed policy in reinforcement learning-based control for soft robotic\nsystems and provide actionable design guidance for future sim-to-real transfer\nin soft rod-like manipulators.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u81c2\u63a7\u5236\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5f53\u63a7\u5236\u6bb5\u6570n\u22644\u65f6\u5206\u5e03\u5f0f\u7b56\u7565\u65e0\u663e\u8457\u4f18\u52bf\uff0c\u5f53n>4\u65f6\u5206\u5e03\u5f0f\u7b56\u7565\u5728\u6837\u672c\u6548\u7387\u3001\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u96c6\u4e2d\u5f0f\u7b56\u7565\u8bad\u7ec3\u65f6\u95f4\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u6bd4\u8f83\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u81c2\u63a7\u5236\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u63a7\u5236\u5668\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528PyElastica\u548cOpenAI Gym\u63a5\u53e3\uff0c\u5728\u76f8\u540c\u9884\u7b97\u4e0b\u8bad\u7ec3\u5168\u5c40PPO\u63a7\u5236\u5668\u548c\u591a\u667a\u80fd\u4f53PPO\uff0c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u63a7\u5236\u6bb5\u6570n\uff0c\u5728\u4e09\u79cd\u573a\u666f\u4e0b\u8bc4\u4f30\u6027\u80fd\uff1a\u57fa\u7ebf\u6761\u4ef6\u3001\u5916\u90e8\u5e72\u6270\u6062\u590d\u548c\u9a71\u52a8\u5668\u6545\u969c\u9002\u5e94\u3002", "result": "\u5f53n\u22644\u65f6\u5206\u5e03\u5f0f\u7b56\u7565\u65e0\u663e\u8457\u4f18\u52bf\uff1bn\u22642\u65f6\u96c6\u4e2d\u5f0f\u7b56\u7565\u8868\u73b0\u66f4\u597d\uff1b4<n\u226412\u65f6\u5206\u5e03\u5f0f\u7b56\u7565\u6837\u672c\u6548\u7387\u9ad8\u3001\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u66f4\u5f3a\uff1b\u4f46\u96c6\u4e2d\u5f0f\u7b56\u7565\u8bad\u7ec3\u65f6\u95f4\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u7b56\u7565\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u4e2d\u5b58\u5728\u6743\u8861\uff0c\u4e3a\u672a\u6765\u8f6f\u4f53\u6746\u72b6\u673a\u68b0\u81c2\u7684\u4eff\u771f\u5230\u771f\u5b9e\u8f6c\u79fb\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u6307\u5bfc\u3002"}}
{"id": "2511.02239", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.02239", "abs": "https://arxiv.org/abs/2511.02239", "authors": ["Youngjin Hong", "Houjian Yu", "Mingen Li", "Changhyun Choi"], "title": "LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation", "comment": "Preprint. Project page: https://vla2026.github.io/LACY/", "summary": "Learning generalizable policies for robotic manipulation increasingly relies\non large-scale models that map language instructions to actions (L2A). However,\nthis one-way paradigm often produces policies that execute tasks without deeper\ncontextual understanding, limiting their ability to generalize or explain their\nbehavior. We argue that the complementary skill of mapping actions back to\nlanguage (A2L) is essential for developing more holistic grounding. An agent\ncapable of both acting and explaining its actions can form richer internal\nrepresentations and unlock new paradigms for self-supervised learning. We\nintroduce LACY (Language-Action Cycle), a unified framework that learns such\nbidirectional mappings within a single vision-language model. LACY is jointly\ntrained on three synergistic tasks: generating parameterized actions from\nlanguage (L2A), explaining observed actions in language (A2L), and verifying\nsemantic consistency between two language descriptions (L2C). This enables a\nself-improving cycle that autonomously generates and filters new training data\nthrough an active augmentation strategy targeting low-confidence cases, thereby\nimproving the model without additional human labels. Experiments on\npick-and-place tasks in both simulation and the real world show that LACY\nimproves task success rates by 56.46% on average and yields more robust\nlanguage-action grounding for robotic manipulation. Project page:\nhttps://vla2026.github.io/LACY/", "AI": {"tldr": "LACY\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8bed\u8a00-\u52a8\u4f5c\u5faa\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u8bed\u8a00\u5230\u52a8\u4f5c(L2A)\u3001\u52a8\u4f5c\u5230\u8bed\u8a00(A2L)\u548c\u8bed\u8a00\u4e00\u81f4\u6027\u9a8c\u8bc1(L2C)\u4e09\u4e2a\u4efb\u52a1\uff0c\u5b9e\u73b0\u53cc\u5411\u6620\u5c04\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u589e\u5f3a\u7b56\u7565\u81ea\u4e3b\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u5230\u52a8\u4f5c(L2A)\u7684\u5927\u89c4\u6a21\u6a21\u578b\u867d\u7136\u80fd\u6267\u884c\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e0a\u4e0b\u6587\u7684\u6df1\u5ea6\u7406\u89e3\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u884c\u4e3a\u89e3\u91ca\u80fd\u529b\u3002\u4f5c\u8005\u8ba4\u4e3a\u52a8\u4f5c\u5230\u8bed\u8a00(A2L)\u7684\u4e92\u8865\u6280\u80fd\u5bf9\u4e8e\u53d1\u5c55\u66f4\u5168\u9762\u7684\u57fa\u7840\u81f3\u5173\u91cd\u8981\u3002", "method": "LACY\u6846\u67b6\u5728\u5355\u4e00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u8054\u5408\u8bad\u7ec3\u4e09\u4e2a\u534f\u540c\u4efb\u52a1\uff1a\u4ece\u8bed\u8a00\u751f\u6210\u53c2\u6570\u5316\u52a8\u4f5c(L2A)\u3001\u7528\u8bed\u8a00\u89e3\u91ca\u89c2\u5bdf\u5230\u7684\u52a8\u4f5c(A2L)\u3001\u9a8c\u8bc1\u4e24\u4e2a\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027(L2C)\u3002\u91c7\u7528\u4e3b\u52a8\u589e\u5f3a\u7b56\u7565\u9488\u5bf9\u4f4e\u7f6e\u4fe1\u5ea6\u6848\u4f8b\u81ea\u4e3b\u751f\u6210\u548c\u8fc7\u6ee4\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cLACY\u5e73\u5747\u63d0\u9ad8\u4e8656.46%\u7684\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u4ea7\u751f\u4e86\u66f4\u9c81\u68d2\u7684\u8bed\u8a00-\u52a8\u4f5c\u57fa\u7840\u3002", "conclusion": "\u53cc\u5411\u8bed\u8a00-\u52a8\u4f5c\u6620\u5c04\u5b66\u4e60\u80fd\u591f\u5f62\u6210\u66f4\u4e30\u5bcc\u7684\u5185\u90e8\u8868\u793a\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u89e3\u9501\u65b0\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff0c\u8bc1\u660e\u4e86LACY\u6846\u67b6\u5728\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u548c\u8bed\u8a00-\u52a8\u4f5c\u57fa\u7840\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.02294", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02294", "abs": "https://arxiv.org/abs/2511.02294", "authors": ["Ruiyong Yuan", "Jieji Ren", "Zhanxuan Peng", "Feifei Chen", "Guoying Gu"], "title": "SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction", "comment": null, "summary": "Suckers are significant for robots in picking, transferring, manipulation and\nlocomotion on diverse surfaces. However, most of the existing suckers lack\nhigh-fidelity perceptual and tactile sensing, which impedes them from resolving\nthe fine-grained geometric features and interaction status of the target\nsurface. This limits their robust performance with irregular objects and in\ncomplex, unstructured environments. Inspired by the adaptive structure and\nhigh-performance sensory capabilities of cephalopod suckers, in this paper, we\npropose a novel, intelligent sucker, named SuckTac, that integrates a\ncamera-based tactile sensor directly within its optimized structure to provide\nhigh-density perception and robust suction. Specifically, through joint\nstructure design and optimization and based on a multi-material integrated\ncasting technique, a camera and light source are embedded into the sucker,\nwhich enables in-situ, high-density perception of fine details like surface\nshape, texture and roughness. To further enhance robustness and adaptability,\nthe sucker's mechanical design is also optimized by refining its profile,\nadding a compliant lip, and incorporating surface microstructure. Extensive\nexperiments, including challenging tasks such as robotic cloth manipulation and\nsoft mobile robot inspection, demonstrate the superior performance and broad\napplicability of the proposed system.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSuckTac\u7684\u65b0\u578b\u667a\u80fd\u5438\u76d8\uff0c\u5c06\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u89e6\u89c9\u4f20\u611f\u5668\u96c6\u6210\u5230\u4f18\u5316\u7ed3\u6784\u4e2d\uff0c\u63d0\u4f9b\u9ad8\u5bc6\u5ea6\u611f\u77e5\u548c\u9c81\u68d2\u5438\u9644\u80fd\u529b\uff0c\u7075\u611f\u6765\u81ea\u5934\u8db3\u7c7b\u52a8\u7269\u7684\u81ea\u9002\u5e94\u7ed3\u6784\u548c\u611f\u5b98\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5438\u76d8\u7f3a\u4e4f\u9ad8\u4fdd\u771f\u611f\u77e5\u548c\u89e6\u89c9\u4f20\u611f\u80fd\u529b\uff0c\u65e0\u6cd5\u8bc6\u522b\u76ee\u6807\u8868\u9762\u7684\u7cbe\u7ec6\u51e0\u4f55\u7279\u5f81\u548c\u4ea4\u4e92\u72b6\u6001\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8054\u5408\u7ed3\u6784\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u57fa\u4e8e\u591a\u6750\u6599\u96c6\u6210\u94f8\u9020\u6280\u672f\uff0c\u5c06\u6444\u50cf\u5934\u548c\u5149\u6e90\u5d4c\u5165\u5438\u76d8\u5185\u90e8\uff0c\u5b9e\u73b0\u539f\u4f4d\u9ad8\u5bc6\u5ea6\u611f\u77e5\uff1b\u901a\u8fc7\u4f18\u5316\u5438\u76d8\u8f6e\u5ed3\u3001\u6dfb\u52a0\u67d4\u6027\u5507\u8fb9\u548c\u8868\u9762\u5fae\u7ed3\u6784\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "\u5728\u673a\u5668\u4eba\u5e03\u6599\u64cd\u4f5c\u548c\u8f6f\u4f53\u79fb\u52a8\u673a\u5668\u4eba\u68c0\u6d4b\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "SuckTac\u667a\u80fd\u5438\u76d8\u901a\u8fc7\u96c6\u6210\u89e6\u89c9\u611f\u77e5\u548c\u7ed3\u6784\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5438\u9644\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.02315", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.02315", "abs": "https://arxiv.org/abs/2511.02315", "authors": ["Zifei Wu", "Lijie Wang", "Zhe Yang", "Shijie Yang", "Liang Wang", "Haoran Fu", "Yinliang Cai", "Rong Xiong"], "title": "ZJUNlict Extended Team Description Paper 2025", "comment": null, "summary": "This paper presents the ZJUNlict team's work over the past year, covering\nboth hardware and software advancements. In the hardware domain, the\nintegration of an IMU into the v2023 robot was completed to enhance posture\naccuracy and angular velocity planning. On the software side, key modules were\noptimized, including the strategy and CUDA modules, with significant\nimprovements in decision making efficiency, ball pursuit prediction, and ball\npossession prediction to adapt to high-tempo game dynamics.", "AI": {"tldr": "ZJUNlict\u56e2\u961f\u5728\u8fc7\u53bb\u4e00\u5e74\u4e2d\u5728\u786c\u4ef6\u548c\u8f6f\u4ef6\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u786c\u4ef6\u4e0a\u4e3av2023\u673a\u5668\u4eba\u96c6\u6210\u4e86IMU\u4ee5\u63d0\u5347\u59ff\u6001\u7cbe\u5ea6\u548c\u89d2\u901f\u5ea6\u89c4\u5212\uff0c\u8f6f\u4ef6\u4e0a\u4f18\u5316\u4e86\u7b56\u7565\u548cCUDA\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u6548\u7387\u3001\u7403\u8ffd\u8e2a\u9884\u6d4b\u548c\u63a7\u7403\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u9002\u5e94\u9ad8\u8282\u594f\u7684\u6bd4\u8d5b\u52a8\u6001\uff0c\u9700\u8981\u63d0\u5347\u673a\u5668\u4eba\u7684\u59ff\u6001\u7cbe\u5ea6\u548c\u51b3\u7b56\u6548\u7387\uff0c\u56e0\u6b64\u56e2\u961f\u5728\u786c\u4ef6\u548c\u8f6f\u4ef6\u4e24\u65b9\u9762\u90fd\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "method": "\u786c\u4ef6\u65b9\u9762\u4e3av2023\u673a\u5668\u4eba\u96c6\u6210IMU\u4f20\u611f\u5668\uff1b\u8f6f\u4ef6\u65b9\u9762\u4f18\u5316\u7b56\u7565\u6a21\u5757\u548cCUDA\u6a21\u5757\uff0c\u6539\u8fdb\u51b3\u7b56\u7b97\u6cd5\u3001\u7403\u8ffd\u8e2a\u9884\u6d4b\u548c\u63a7\u7403\u9884\u6d4b\u3002", "result": "\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u59ff\u6001\u7cbe\u5ea6\u7684\u63d0\u5347\u3001\u89d2\u901f\u5ea6\u89c4\u5212\u7684\u6539\u8fdb\uff0c\u4ee5\u53ca\u51b3\u7b56\u6548\u7387\u3001\u7403\u8ffd\u8e2a\u9884\u6d4b\u548c\u63a7\u7403\u9884\u6d4b\u80fd\u529b\u7684\u663e\u8457\u589e\u5f3a\u3002", "conclusion": "\u901a\u8fc7\u786c\u4ef6\u548c\u8f6f\u4ef6\u7684\u7efc\u5408\u4f18\u5316\uff0cZJUNlict\u56e2\u961f\u6210\u529f\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u9ad8\u8282\u594f\u6bd4\u8d5b\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u548c\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2511.02504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02504", "abs": "https://arxiv.org/abs/2511.02504", "authors": ["Le Chen", "Yi Zhao", "Jan Schneider", "Quankai Gao", "Simon Guist", "Cheng Qian", "Juho Kannala", "Bernhard Sch\u00f6lkopf", "Joni Pajarinen", "Dieter B\u00fcchler"], "title": "Dexterous Robotic Piano Playing at Scale", "comment": null, "summary": "Endowing robot hands with human-level dexterity has been a long-standing goal\nin robotics. Bimanual robotic piano playing represents a particularly\nchallenging task: it is high-dimensional, contact-rich, and requires fast,\nprecise control. We present OmniPianist, the first agent capable of performing\nnearly one thousand music pieces via scalable, human-demonstration-free\nlearning. Our approach is built on three core components. First, we introduce\nan automatic fingering strategy based on Optimal Transport (OT), allowing the\nagent to autonomously discover efficient piano-playing strategies from scratch\nwithout demonstrations. Second, we conduct large-scale Reinforcement Learning\n(RL) by training more than 2,000 agents, each specialized in distinct music\npieces, and aggregate their experience into a dataset named RP1M++, consisting\nof over one million trajectories for robotic piano playing. Finally, we employ\na Flow Matching Transformer to leverage RP1M++ through large-scale imitation\nlearning, resulting in the OmniPianist agent capable of performing a wide range\nof musical pieces. Extensive experiments and ablation studies highlight the\neffectiveness and scalability of our approach, advancing dexterous robotic\npiano playing at scale.", "AI": {"tldr": "OmniPianist\u662f\u9996\u4e2a\u80fd\u591f\u901a\u8fc7\u53ef\u6269\u5c55\u3001\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u7684\u5b66\u4e60\u65b9\u5f0f\u6f14\u594f\u8fd1\u5343\u9996\u97f3\u4e50\u4f5c\u54c1\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u7075\u5de7\u673a\u5668\u4eba\u94a2\u7434\u6f14\u594f\u3002", "motivation": "\u8d4b\u4e88\u673a\u5668\u4eba\u624b\u4eba\u7c7b\u6c34\u5e73\u7684\u7075\u5de7\u6027\u4e00\u76f4\u662f\u673a\u5668\u4eba\u5b66\u7684\u957f\u671f\u76ee\u6807\u3002\u53cc\u624b\u673a\u5668\u4eba\u94a2\u7434\u6f14\u594f\u662f\u4e00\u4e2a\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff1a\u9ad8\u7ef4\u5ea6\u3001\u63a5\u89e6\u4e30\u5bcc\u4e14\u9700\u8981\u5feb\u901f\u7cbe\u786e\u7684\u63a7\u5236\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u81ea\u52a8\u6307\u6cd5\u7b56\u7565\uff0c\u8ba9\u667a\u80fd\u4f53\u80fd\u591f\u81ea\u4e3b\u53d1\u73b0\u9ad8\u6548\u7684\u94a2\u7434\u6f14\u594f\u7b56\u7565\uff1b2\uff09\u901a\u8fc7\u8bad\u7ec32000\u591a\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e0d\u540c\u97f3\u4e50\u4f5c\u54c1\u7684\u667a\u80fd\u4f53\u8fdb\u884c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5c06\u7ecf\u9a8c\u805a\u5408\u4e3a\u5305\u542b100\u591a\u4e07\u6761\u8f68\u8ff9\u7684RP1M++\u6570\u636e\u96c6\uff1b3\uff09\u4f7f\u7528\u6d41\u5339\u914d\u53d8\u6362\u5668\u901a\u8fc7\u5927\u89c4\u6a21\u6a21\u4eff\u5b66\u4e60\u5229\u7528RP1M++\u6570\u636e\u96c6\u3002", "result": "\u5f00\u53d1\u51fa\u4e86\u80fd\u591f\u6f14\u594f\u5e7f\u6cdb\u97f3\u4e50\u4f5c\u54c1\u7684OmniPianist\u667a\u80fd\u4f53\u3002\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u52a8\u4e86\u5927\u89c4\u6a21\u7075\u5de7\u673a\u5668\u4eba\u94a2\u7434\u6f14\u594f\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u5373\u53ef\u5b9e\u73b0\u590d\u6742\u6280\u80fd\u5b66\u4e60\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.02761", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02761", "abs": "https://arxiv.org/abs/2511.02761", "authors": ["Seth Stewart", "Joseph Pawelski", "Steve Ward", "Andrew J. Petruska"], "title": "Non-Contact Manipulation of Induced Magnetic Dipoles", "comment": null, "summary": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u901a\u8fc7\u632f\u8361\u78c1\u573a\u5bf9\u5bfc\u7535\u975e\u78c1\u6027\u7269\u4f53\u8fdb\u884c\u95ed\u73af\u4f4d\u7f6e\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u7a7a\u95f4\u788e\u7247\u56de\u6536\u5e94\u7528\u3002", "motivation": "\u5c06\u78c1\u64cd\u7eb5\u6269\u5c55\u5230\u5bfc\u7535\u975e\u78c1\u6027\u7269\u4f53\uff0c\u4e3a\u7a7a\u95f4\u788e\u7247\u56de\u6536\u7b49\u5e94\u7528\u5f00\u8f9f\u65b0\u9014\u5f84\uff0c\u5229\u7528\u611f\u5e94\u6da1\u6d41\u4ea7\u751f\u7684\u78c1\u77e9\u8fdb\u884c\u63a7\u5236\u3002", "method": "\u5229\u7528\u611f\u5e94\u6da1\u6d41\u4ea7\u751f\u7684\u53cd\u5411\u78c1\u5076\u6781\u77e9\uff0c\u5728\u5b9e\u9a8c\u5ba4\u6d4b\u8bd5\u4e2d\u5bf9\u534a\u6d6e\u94dd\u7403\u5b9e\u73b0\u95ed\u73af\u4f4d\u7f6e\u63a7\u5236\uff0c\u5e76\u63a2\u7d22\u4e86\u591a\u79cd\u529b\u53cd\u6f14\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u611f\u5e94\u78c1\u5076\u6781\u5b50\u76843\u81ea\u7531\u5ea6\u95ed\u73af\u4f4d\u7f6e\u63a7\u5236\uff0c\u8fd9\u662f\u8fc8\u5411\u66f4\u5e7f\u6cdb\u5e94\u7528\u7684\u5173\u952e\u7b2c\u4e00\u6b65\u3002", "conclusion": "\u95ed\u73af\u63a7\u5236\u65b9\u6cd5\u4ee3\u8868\u4e86\u5411\u611f\u5e94\u78c1\u5076\u6781\u5b503\u81ea\u7531\u5ea6\u4f4d\u7f6e\u63a7\u5236\u66f4\u5e7f\u6cdb\u5e94\u7528\u8fc8\u51fa\u7684\u91cd\u8981\u6b65\u9aa4\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7a7a\u95f4\u73af\u5883\u4e2d\u7684\u4f4e\u529b\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.02776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.02776", "abs": "https://arxiv.org/abs/2511.02776", "authors": ["Shichao Fan", "Kun Wu", "Zhengping Che", "Xinhua Wang", "Di Wu", "Fei Liao", "Ning Liu", "Yixue Zhang", "Zhen Zhao", "Zhiyuan Xu", "Meng Li", "Qingjie Liu", "Shanghang Zhang", "Min Wan", "Jian Tang"], "title": "XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations", "comment": null, "summary": "Recent progress in large-scale robotic datasets and vision-language models\n(VLMs) has advanced research on vision-language-action (VLA) models. However,\nexisting VLA models still face two fundamental challenges: (i) producing\nprecise low-level actions from high-dimensional observations, (ii) bridging\ndomain gaps across heterogeneous data sources, including diverse robot\nembodiments and human demonstrations. Existing methods often encode latent\nvariables from either visual dynamics or robotic actions to guide policy\nlearning, but they fail to fully exploit the complementary multi-modal\nknowledge present in large-scale, heterogeneous datasets. In this work, we\npresent X Robotic Model 1 (XR-1), a novel framework for versatile and scalable\nVLA learning across diverse robots, tasks, and environments. XR-1 introduces\nthe \\emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation\nlearned via a dual-branch VQ-VAE that jointly encodes visual dynamics and\nrobotic motion. UVMC addresses these challenges by (i) serving as an\nintermediate representation between the observations and actions, and (ii)\naligning multimodal dynamic information from heterogeneous data sources to\ncapture complementary knowledge. To effectively exploit UVMC, we propose a\nthree-stage training paradigm: (i) self-supervised UVMC learning, (ii)\nUVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and\n(iii) task-specific post-training. We validate XR-1 through extensive\nreal-world experiments with more than 14,000 rollouts on six different robot\nembodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently\noutperforms state-of-the-art baselines such as $\\pi_{0.5}$, $\\pi_0$, RDT,\nUniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel\nobjects, background variations, distractors, and illumination changes. Our\nproject is at https://xr-1-vla.github.io/.", "AI": {"tldr": "XR-1\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u89c6\u89c9-\u8fd0\u52a8\u7f16\u7801\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u5728\u7cbe\u786e\u4f4e\u7ea7\u52a8\u4f5c\u751f\u6210\u548c\u8de8\u5f02\u6784\u6570\u636e\u6e90\u9886\u57df\u5dee\u8ddd\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff1a(i)\u4ece\u9ad8\u7ef4\u89c2\u5bdf\u4e2d\u4ea7\u751f\u7cbe\u786e\u7684\u4f4e\u7ea7\u52a8\u4f5c\uff0c(ii)\u8de8\u8d8a\u5f02\u6784\u6570\u636e\u6e90\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u5305\u62ec\u4e0d\u540c\u7684\u673a\u5668\u4eba\u4f53\u73b0\u548c\u4eba\u7c7b\u6f14\u793a\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u4e92\u8865\u591a\u6a21\u6001\u77e5\u8bc6\u3002", "method": "XR-1\u5f15\u5165\u4e86\u7edf\u4e00\u89c6\u89c9-\u8fd0\u52a8\u7f16\u7801(UVMC)\uff0c\u901a\u8fc7\u53cc\u5206\u652fVQ-VAE\u8054\u5408\u7f16\u7801\u89c6\u89c9\u52a8\u6001\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u81ea\u76d1\u7763UVMC\u5b66\u4e60\u3001UVMC\u5f15\u5bfc\u7684\u5927\u89c4\u6a21\u8de8\u4f53\u73b0\u673a\u5668\u4eba\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u3001\u4efb\u52a1\u7279\u5b9a\u540e\u8bad\u7ec3\u3002", "result": "\u57286\u79cd\u4e0d\u540c\u673a\u5668\u4eba\u4f53\u73b0\u4e0a\u7684\u8d85\u8fc714,000\u6b21\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6db5\u76d6120\u591a\u4e2a\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u3002XR-1\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u65b0\u7269\u4f53\u3001\u80cc\u666f\u53d8\u5316\u3001\u5e72\u6270\u7269\u548c\u5149\u7167\u53d8\u5316\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "XR-1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u4e14\u53ef\u6269\u5c55\u7684VLA\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7UVMC\u8868\u793a\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u3001\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.02832", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.02832", "abs": "https://arxiv.org/abs/2511.02832", "authors": ["Yanjie Ze", "Siheng Zhao", "Weizhuo Wang", "Angjoo Kanazawa", "Rocky Duan", "Pieter Abbeel", "Guanya Shi", "Jiajun Wu", "C. Karen Liu"], "title": "TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System", "comment": "Website: https://yanjieze.com/TWIST2", "summary": "Large-scale data has driven breakthroughs in robotics, from language models\nto vision-language-action models in bimanual manipulation. However, humanoid\nrobotics lacks equally effective data collection frameworks. Existing humanoid\nteleoperation systems either use decoupled control or depend on expensive\nmotion capture setups. We introduce TWIST2, a portable, mocap-free humanoid\nteleoperation and data collection system that preserves full whole-body control\nwhile advancing scalability. Our system leverages PICO4U VR for obtaining\nreal-time whole-body human motions, with a custom 2-DoF robot neck (cost around\n$250) for egocentric vision, enabling holistic human-to-humanoid control. We\ndemonstrate long-horizon dexterous and mobile humanoid skills and we can\ncollect 100 demonstrations in 15 minutes with an almost 100% success rate.\nBuilding on this pipeline, we propose a hierarchical visuomotor policy\nframework that autonomously controls the full humanoid body based on egocentric\nvision. Our visuomotor policy successfully demonstrates whole-body dexterous\nmanipulation and dynamic kicking tasks. The entire system is fully reproducible\nand open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also\nopen-sourced at https://twist-data.github.io .", "AI": {"tldr": "TWIST2\u662f\u4e00\u4e2a\u65e0\u9700\u52a8\u4f5c\u6355\u6349\u7684\u53ef\u79fb\u690d\u4eba\u5f62\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u548c\u6570\u636e\u6536\u96c6\u7cfb\u7edf\uff0c\u901a\u8fc7VR\u8bbe\u5907\u5b9e\u73b0\u5168\u8eab\u63a7\u5236\uff0c\u80fd\u591f\u9ad8\u6548\u6536\u96c6\u6f14\u793a\u6570\u636e\u5e76\u8bad\u7ec3\u5206\u5c42\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u9886\u57df\u7f3a\u4e4f\u6709\u6548\u7684\u6570\u636e\u6536\u96c6\u6846\u67b6\uff0c\u73b0\u6709\u9065\u64cd\u4f5c\u7cfb\u7edf\u8981\u4e48\u4f7f\u7528\u89e3\u8026\u63a7\u5236\uff0c\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u52a8\u4f5c\u6355\u6349\u8bbe\u5907\u3002", "method": "\u4f7f\u7528PICO4U VR\u83b7\u53d6\u5b9e\u65f6\u5168\u8eab\u4eba\u4f53\u8fd0\u52a8\uff0c\u914d\u5408\u5b9a\u5236\u76842\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u9888\u90e8\u5b9e\u73b0\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\uff0c\u5b9e\u73b0\u6574\u4f53\u4eba-\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u3002", "result": "\u7cfb\u7edf\u80fd\u572815\u5206\u949f\u5185\u6536\u96c6100\u4e2a\u6f14\u793a\uff0c\u6210\u529f\u7387\u63a5\u8fd1100%\uff1b\u8bad\u7ec3\u7684\u5206\u5c42\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u80fd\u591f\u81ea\u4e3b\u63a7\u5236\u5b8c\u6574\u4eba\u5f62\u673a\u5668\u4eba\u8eab\u4f53\uff0c\u5b8c\u6210\u5168\u8eab\u7075\u5de7\u64cd\u4f5c\u548c\u52a8\u6001\u8e22\u7403\u4efb\u52a1\u3002", "conclusion": "TWIST2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u53ef\u590d\u73b0\u7684\u5f00\u6e90\u7cfb\u7edf\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u6536\u96c6\u548c\u7b56\u7565\u5b66\u4e60\u6846\u67b6\u3002"}}
