{"id": "2510.23763", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRoboOmni\u6846\u67b6\uff0c\u57fa\u4e8e\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u542c\u89c9\u548c\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u65f6\u7a7a\u63a8\u7406\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u4ece\u5bf9\u8bdd\u3001\u73af\u5883\u58f0\u97f3\u548c\u89c6\u89c9\u7ebf\u7d22\u4e2d\u4e3b\u52a8\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u800c\u4e0d\u4ec5\u4ec5\u4f9d\u8d56\u663e\u5f0f\u6307\u4ee4\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u6307\u4ee4\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u4e2d\u4eba\u673a\u534f\u4f5c\u9700\u8981\u673a\u5668\u4eba\u4e3b\u52a8\u63a8\u65ad\u7528\u6237\u610f\u56fe\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u4ece\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff08\u8bed\u97f3\u5bf9\u8bdd\u3001\u73af\u5883\u58f0\u97f3\u3001\u89c6\u89c9\u7ebf\u7d22\uff09\u4e2d\u8bc6\u522b\u610f\u56fe\u7684\u65b0\u95ee\u9898\u3002", "method": "\u63d0\u51faRoboOmni\u6846\u67b6\uff0c\u91c7\u7528\u611f\u77e5\u8005-\u601d\u8003\u8005-\u8bf4\u8bdd\u8005-\u6267\u884c\u8005\u67b6\u6784\uff0c\u57fa\u4e8e\u7aef\u5230\u7aef\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7edf\u4e00\u610f\u56fe\u8bc6\u522b\u3001\u4ea4\u4e92\u786e\u8ba4\u548c\u52a8\u4f5c\u6267\u884c\u3002\u6784\u5efaOmniAction\u6570\u636e\u96c6\uff0814\u4e07\u6761\u8bb0\u5f55\uff09\u7528\u4e8e\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cRoboOmni\u5728\u6210\u529f\u7387\u3001\u63a8\u7406\u901f\u5ea6\u3001\u610f\u56fe\u8bc6\u522b\u548c\u4e3b\u52a8\u534f\u52a9\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u548c\u8bed\u97f3\u8bc6\u522b\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RoboOmni\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u4e3b\u52a8\u610f\u56fe\u8bc6\u522b\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u7406\u89e3\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u7684\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2510.23860", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23860", "abs": "https://arxiv.org/abs/2510.23860", "authors": ["Hyung Chan Cho", "Go-Eum Cha", "Yanfu Liu", "Sooyeon Jeong"], "title": "Motivating Students' Self-study with Goal Reminder and Emotional Support", "comment": "RO-MAN 2025 accepted paper", "summary": "While the efficacy of social robots in supporting people in learning tasks\nhas been extensively investigated, their potential impact in assisting students\nin self-studying contexts has not been investigated much. This study explores\nhow a social robot can act as a peer study companion for college students\nduring self-study tasks by delivering task-oriented goal reminder and positive\nemotional support. We conducted an exploratory Wizard-of-Oz study to explore\nhow these robotic support behaviors impacted students' perceived focus,\nproductivity, and engagement in comparison to a robot that only provided\nphysical presence (control). Our study results suggest that participants in the\ngoal reminder and the emotional support conditions reported greater ease of\nuse, with the goal reminder condition additionally showing a higher willingness\nto use the robot in future study sessions. Participants' satisfaction with the\nrobot was correlated with their perception of the robot as a social other, and\nthis perception was found to be a predictor for their level of goal achievement\nin the self-study task. These findings highlight the potential of socially\nassistive robots to support self-study through both functional and emotional\nengagement.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u793e\u4ea4\u673a\u5668\u4eba\u4f5c\u4e3a\u540c\u4f34\u5b66\u4e60\u4f34\u4fa3\u5728\u5927\u5b66\u751f\u81ea\u4e3b\u5b66\u4e60\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u63d0\u4f9b\u76ee\u6807\u63d0\u9192\u548c\u79ef\u6781\u60c5\u611f\u652f\u6301\uff0c\u53d1\u73b0\u8fd9\u4e9b\u652f\u6301\u884c\u4e3a\u80fd\u63d0\u9ad8\u5b66\u751f\u7684\u4e13\u6ce8\u5ea6\u3001\u751f\u4ea7\u529b\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u867d\u7136\u793e\u4ea4\u673a\u5668\u4eba\u5728\u652f\u6301\u5b66\u4e60\u4efb\u52a1\u65b9\u9762\u7684\u6548\u679c\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5b83\u4eec\u5728\u534f\u52a9\u5b66\u751f\u81ea\u4e3b\u5b66\u4e60\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u63a2\u7d22\u6027\u7684Wizard-of-Oz\u7814\u7a76\uff0c\u6bd4\u8f83\u76ee\u6807\u63d0\u9192\u3001\u60c5\u611f\u652f\u6301\u4e0e\u4ec5\u63d0\u4f9b\u7269\u7406\u5b58\u5728\u7684\u63a7\u5236\u7ec4\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u76ee\u6807\u63d0\u9192\u548c\u60c5\u611f\u652f\u6301\u6761\u4ef6\u4e0b\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u4e86\u66f4\u9ad8\u7684\u6613\u7528\u6027\uff0c\u76ee\u6807\u63d0\u9192\u7ec4\u8fd8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u672a\u6765\u4f7f\u7528\u610f\u613f\u3002\u53c2\u4e0e\u8005\u5bf9\u673a\u5668\u4eba\u7684\u6ee1\u610f\u5ea6\u4e0e\u5176\u5c06\u673a\u5668\u4eba\u89c6\u4e3a\u793e\u4f1a\u5b58\u5728\u7684\u611f\u77e5\u76f8\u5173\uff0c\u8fd9\u79cd\u611f\u77e5\u662f\u81ea\u4e3b\u5b66\u4e60\u4efb\u52a1\u4e2d\u76ee\u6807\u8fbe\u6210\u6c34\u5e73\u7684\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u7a81\u663e\u4e86\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\u901a\u8fc7\u529f\u80fd\u548c\u60c5\u611f\u53c2\u4e0e\u652f\u6301\u81ea\u4e3b\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.23902", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23902", "abs": "https://arxiv.org/abs/2510.23902", "authors": ["Jans Solano", "Diego Quiroz"], "title": "Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped", "comment": "Accepted at the IROS 2025 Workshop on Wheeled-Legged Robots", "summary": "Wheeled-legged robots combine the efficiency of wheels with the obstacle\nnegotiation of legs, yet many state-of-the-art systems rely on costly actuators\nand sensors, and fall-recovery is seldom integrated, especially for\nwheeled-legged morphologies. This work presents a recovery-aware\nvisual-inertial navigation system on a low-cost wheeled quadruped. The proposed\nsystem leverages vision-based perception from a depth camera and deep\nreinforcement learning policies for robust locomotion and autonomous recovery\nfrom falls across diverse terrains. Simulation experiments show agile mobility\nwith low-torque actuators over irregular terrain and reliably recover from\nexternal perturbations and self-induced failures. We further show goal directed\nnavigation in structured indoor spaces with low-cost perception. Overall, this\napproach lowers the barrier to deploying autonomous navigation and robust\nlocomotion policies in budget-constrained robotic platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f4e\u6210\u672c\u8f6e\u5f0f\u56db\u8db3\u673a\u5668\u4eba\u7684\u6062\u590d\u611f\u77e5\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u6df1\u5ea6\u6444\u50cf\u5934\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u548c\u8dcc\u5012\u81ea\u4e3b\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u8f6e\u817f\u5f0f\u673a\u5668\u4eba\u901a\u5e38\u4f9d\u8d56\u6602\u8d35\u7684\u6267\u884c\u5668\u548c\u4f20\u611f\u5668\uff0c\u4e14\u5f88\u5c11\u96c6\u6210\u8dcc\u5012\u6062\u590d\u529f\u80fd\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8f6e\u817f\u5f62\u6001\u7684\u673a\u5668\u4eba\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u6444\u50cf\u5934\u7684\u89c6\u89c9\u611f\u77e5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u4f4e\u6210\u672c\u8f6e\u5f0f\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u548c\u81ea\u4e3b\u8dcc\u5012\u6062\u590d\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u5728\u975e\u89c4\u5219\u5730\u5f62\u4e0a\u5177\u6709\u654f\u6377\u79fb\u52a8\u6027\uff0c\u5e76\u80fd\u53ef\u9760\u5730\u4ece\u5916\u90e8\u6270\u52a8\u548c\u81ea\u8bf1\u5bfc\u6545\u969c\u4e2d\u6062\u590d\u3002\u5728\u7ed3\u6784\u5316\u5ba4\u5185\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u4e86\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u5728\u9884\u7b97\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\u81ea\u4e3b\u5bfc\u822a\u548c\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\u7684\u95e8\u69db\u3002"}}
{"id": "2510.23954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23954", "abs": "https://arxiv.org/abs/2510.23954", "authors": ["Pejman Kheradmand", "Behnam Moradkhani", "Raghavasimhan Sankaranarayanan", "Kent K. Yamamoto", "Tanner J. Zachem", "Patrick J. Codd", "Yash Chitalia", "Pierre E. Dupont"], "title": "A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons", "comment": null, "summary": "Tendon-actuated concentric tube mechanisms combine the advantages of\ntendon-driven continuum robots and concentric tube robots while addressing\ntheir respective limitations. They overcome the restricted degrees of freedom\noften seen in tendon-driven designs, and mitigate issues such as snapping\ninstability associated with concentric tube robots. However, a complete and\ngeneral mechanical model for these systems remains an open problem. In this\nwork, we propose a Cosserat rod-based framework for modeling the general case\nof $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \\{1,\n\\ldots, n\\}$. The model allows each tube to twist and elongate while enforcing\na shared centerline for bending. We validate the proposed framework through\nexperiments with two-tube and three tube assemblies under various tendon\nrouting configurations, achieving tip prediction errors $<4\\%$ of the robot's\ntotal length. We further demonstrate the model's generality by applying it to\nexisting robots in the field, where maximum tip deviations remain around $5\\%$\nof the total length. This model provides a foundation for accurate shape\nestimation and control of advanced tendon-actuated concentric tube robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8eCosserat\u6746\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62dfn\u4e2a\u540c\u5fc3\u7ba1\uff08\u6bcf\u4e2a\u7ba1\u7531m_i\u4e2a\u808c\u8171\u9a71\u52a8\uff09\u7684\u4e00\u822c\u60c5\u51b5\uff0c\u5b9e\u73b0\u4e86\u5c16\u7aef\u9884\u6d4b\u8bef\u5dee\u5c0f\u4e8e\u673a\u5668\u4eba\u603b\u957f\u5ea6\u76844%\u3002", "motivation": "\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u6784\u7ed3\u5408\u4e86\u808c\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u548c\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u4f18\u70b9\uff0c\u4f46\u7f3a\u4e4f\u5b8c\u6574\u901a\u7528\u7684\u529b\u5b66\u6a21\u578b\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528Cosserat\u6746\u7406\u8bba\u6846\u67b6\u5efa\u6a21n\u4e2a\u540c\u5fc3\u7ba1\uff0c\u6bcf\u4e2a\u7ba1\u7531m_i\u4e2a\u808c\u8171\u9a71\u52a8\uff0c\u5141\u8bb8\u6bcf\u4e2a\u7ba1\u626d\u8f6c\u548c\u4f38\u957f\uff0c\u540c\u65f6\u5f3a\u5236\u5f2f\u66f2\u5171\u4eab\u4e2d\u5fc3\u7ebf\u3002", "result": "\u901a\u8fc7\u53cc\u7ba1\u548c\u4e09\u7ba1\u7ec4\u4ef6\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5728\u5404\u79cd\u808c\u8171\u5e03\u7ebf\u914d\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u5c16\u7aef\u9884\u6d4b\u8bef\u5dee\u5c0f\u4e8e\u673a\u5668\u4eba\u603b\u957f\u5ea6\u76844%\u3002\u5e94\u7528\u4e8e\u73b0\u6709\u673a\u5668\u4eba\u65f6\uff0c\u6700\u5927\u5c16\u7aef\u504f\u5dee\u4fdd\u6301\u5728\u603b\u957f\u5ea6\u76845%\u5de6\u53f3\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u7cbe\u786e\u5f62\u72b6\u4f30\u8ba1\u548c\u5148\u8fdb\u808c\u8171\u9a71\u52a8\u540c\u5fc3\u7ba1\u673a\u5668\u4eba\u7684\u63a7\u5236\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.23963", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23963", "abs": "https://arxiv.org/abs/2510.23963", "authors": ["Hiroki Ishikawa", "Kyosuke Ishibashi", "Ko Yamamoto"], "title": "Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping", "comment": null, "summary": "This paper presents a soft robot finger capable of adaptive-twist deformation\nto grasp objects by wrapping them. For a soft hand to grasp and pick-up one\nobject from densely contained multiple objects, a soft finger requires the\nadaptive-twist deformation function in both in-plane and out-of-plane\ndirections. The function allows the finger to be inserted deeply into a limited\ngap among objects. Once inserted, the soft finger requires appropriate control\nof grasping force normal to contact surface, thereby maintaining the twisted\ndeformation. In this paper, we refer to this type of grasping as grasping by\nwrapping. To achieve these two functions by a single actuation source, we\npropose a variable stiffness mechanism that can adaptively change the stiffness\nas the pressure is higher. We conduct a finite element analysis (FEA) on the\nproposed mechanism and determine its design parameter based on the FEA result.\nUsing the developed soft finger, we report basic experimental results and\ndemonstrations on grasping various objects.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u80fd\u529b\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u624b\u6307\uff0c\u80fd\u591f\u901a\u8fc7\u5305\u88f9\u65b9\u5f0f\u6293\u53d6\u7269\u4f53\u3002\u8be5\u624b\u6307\u80fd\u591f\u5728\u5e73\u9762\u5185\u548c\u5e73\u9762\u5916\u65b9\u5411\u5b9e\u73b0\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\uff0c\u4ece\u800c\u6df1\u5165\u7269\u4f53\u95f4\u7684\u72ed\u7a84\u95f4\u9699\u8fdb\u884c\u6293\u53d6\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u8f6f\u4f53\u624b\u80fd\u591f\u5728\u5bc6\u96c6\u5806\u653e\u7684\u591a\u4e2a\u7269\u4f53\u4e2d\u6293\u53d6\u5355\u4e2a\u7269\u4f53\uff0c\u8f6f\u4f53\u624b\u6307\u9700\u8981\u5177\u5907\u5728\u5e73\u9762\u5185\u548c\u5e73\u9762\u5916\u65b9\u5411\u7684\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u529f\u80fd\uff0c\u4ee5\u4fbf\u6df1\u5165\u7269\u4f53\u95f4\u7684\u6709\u9650\u95f4\u9699\u8fdb\u884c\u6293\u53d6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u53d8\u521a\u5ea6\u673a\u5236\uff0c\u80fd\u591f\u968f\u7740\u538b\u529b\u589e\u52a0\u800c\u81ea\u9002\u5e94\u6539\u53d8\u521a\u5ea6\u3002\u901a\u8fc7\u6709\u9650\u5143\u5206\u6790\u786e\u5b9a\u8bbe\u8ba1\u53c2\u6570\uff0c\u5e76\u5f00\u53d1\u4e86\u8f6f\u4f53\u624b\u6307\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u5f00\u53d1\u7684\u8f6f\u4f53\u624b\u6307\u8fdb\u884c\u4e86\u57fa\u672c\u5b9e\u9a8c\u548c\u591a\u79cd\u7269\u4f53\u7684\u6293\u53d6\u6f14\u793a\uff0c\u9a8c\u8bc1\u4e86\u5176\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u548c\u5305\u88f9\u6293\u53d6\u7684\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53ef\u53d8\u521a\u5ea6\u673a\u5236\u8f6f\u4f53\u624b\u6307\u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u626d\u8f6c\u53d8\u5f62\u548c\u5305\u88f9\u6293\u53d6\u529f\u80fd\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u7269\u4f53\u6293\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.23988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.23988", "abs": "https://arxiv.org/abs/2510.23988", "authors": ["Phuc Nguyen Xuan", "Thanh Nguyen Canh", "Huu-Hung Nguyen", "Nak Young Chong", "Xiem HoangVan"], "title": "A Survey on Collaborative SLAM with 3D Gaussian Splatting", "comment": null, "summary": "This survey comprehensively reviews the evolving field of multi-robot\ncollaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian\nSplatting (3DGS). As an explicit scene representation, 3DGS has enabled\nunprecedented real-time, high-fidelity render- ing, ideal for robotics.\nHowever, its use in multi-robot systems introduces significant challenges in\nmaintaining global consistency, managing communication, and fusing data from\nheterogeneous sources. We systematically categorize approaches by their\narchitecture-centralized, distributed- and analyze core components like\nmulti-agent consistency and alignment, communication- efficient, Gaussian\nrepresentation, semantic distillation, fusion and pose optimization, and real-\ntime scalability. In addition, a summary of critical datasets and evaluation\nmetrics is provided to contextualize performance. Finally, we identify key open\nchallenges and chart future research directions, including lifelong mapping,\nsemantic association and mapping, multi-model for robustness, and bridging the\nSim2Real gap.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u7684\u591a\u673a\u5668\u4eba\u534f\u540cSLAM\u6280\u672f\uff0c\u5206\u6790\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u67b6\u6784\uff0c\u8ba8\u8bba\u4e86\u591a\u667a\u80fd\u4f53\u4e00\u81f4\u6027\u3001\u901a\u4fe1\u6548\u7387\u3001\u9ad8\u65af\u8868\u793a\u7b49\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5e76\u6307\u51fa\u4e86\u7ec8\u8eab\u5efa\u56fe\u3001\u8bed\u4e49\u5173\u8054\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u4f5c\u4e3a\u663e\u5f0f\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u975e\u5e38\u9002\u5408\u673a\u5668\u4eba\u5e94\u7528\u3002\u4f46\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u4fdd\u6301\u5168\u5c40\u4e00\u81f4\u6027\u3001\u7ba1\u7406\u901a\u4fe1\u548c\u878d\u5408\u5f02\u6784\u6570\u636e\u6e90\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u6309\u67b6\u6784\uff08\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\uff09\u5bf9\u65b9\u6cd5\u8fdb\u884c\u5206\u7c7b\uff0c\u5206\u6790\u591a\u667a\u80fd\u4f53\u4e00\u81f4\u6027\u5bf9\u9f50\u3001\u901a\u4fe1\u6548\u7387\u3001\u9ad8\u65af\u8868\u793a\u3001\u8bed\u4e49\u84b8\u998f\u3001\u878d\u5408\u4e0e\u4f4d\u59ff\u4f18\u5316\u4ee5\u53ca\u5b9e\u65f6\u53ef\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u7ec4\u4ef6\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u952e\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\u7684\u603b\u7ed3\uff0c\u4ee5\u8bc4\u4f30\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u8bc6\u522b\u4e86\u5173\u952e\u5f00\u653e\u6311\u6218\u5e76\u89c4\u5212\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u7ec8\u8eab\u5efa\u56fe\u3001\u8bed\u4e49\u5173\u8054\u4e0e\u5efa\u56fe\u3001\u591a\u6a21\u578b\u9c81\u68d2\u6027\u4ee5\u53ca\u5f25\u5408Sim2Real\u5dee\u8ddd\u3002"}}
{"id": "2510.23997", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.23997", "abs": "https://arxiv.org/abs/2510.23997", "authors": ["Stanley Wu", "Mohamad H. Danesh", "Simon Li", "Hanna Yurchyk", "Amin Abyaneh", "Anas El Houssaini", "David Meger", "Hsiu-Chin Lin"], "title": "VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion", "comment": "Accepted in IEEE Robotics and Automation Letters (RAL), 2025. 8\n  pages, 9 figures", "summary": "Recent advancements in legged robot locomotion have facilitated traversal\nover increasingly complex terrains. Despite this progress, many existing\napproaches rely on end-to-end deep reinforcement learning (DRL), which poses\nlimitations in terms of safety and interpretability, especially when\ngeneralizing to novel terrains. To overcome these challenges, we introduce\nVOCALoco, a modular skill-selection framework that dynamically adapts\nlocomotion strategies based on perceptual input. Given a set of pre-trained\nlocomotion policies, VOCALoco evaluates their viability and energy-consumption\nby predicting both the safety of execution and the anticipated cost of\ntransport over a fixed planning horizon. This joint assessment enables the\nselection of policies that are both safe and energy-efficient, given the\nobserved local terrain. We evaluate our approach on staircase locomotion tasks,\ndemonstrating its performance in both simulated and real-world scenarios using\na quadrupedal robot. Empirical results show that VOCALoco achieves improved\nrobustness and safety during stair ascent and descent compared to a\nconventional end-to-end DRL policy", "AI": {"tldr": "VOCALoco\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bc4\u4f30\u9884\u8bad\u7ec3\u8fd0\u52a8\u7b56\u7565\u7684\u5b89\u5168\u6027\u548c\u80fd\u8017\u6765\u9009\u62e9\u6700\u4f18\u7b56\u7565\uff0c\u5728\u697c\u68af\u8fd0\u52a8\u4efb\u52a1\u4e2d\u6bd4\u7aef\u5230\u7aefDRL\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u817f\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u7aef\u5230\u7aef\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u65b0\u5730\u5f62\u65f6\u3002\u9700\u8981\u5f00\u53d1\u66f4\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u6765\u9002\u5e94\u590d\u6742\u5730\u5f62\u3002", "method": "\u63d0\u51faVOCALoco\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6846\u67b6\uff0c\u57fa\u4e8e\u611f\u77e5\u8f93\u5165\u52a8\u6001\u8bc4\u4f30\u9884\u8bad\u7ec3\u8fd0\u52a8\u7b56\u7565\u7684\u53ef\u884c\u6027\u548c\u80fd\u8017\uff0c\u901a\u8fc7\u9884\u6d4b\u6267\u884c\u5b89\u5168\u6027\u548c\u9884\u671f\u8fd0\u8f93\u6210\u672c\u6765\u9009\u62e9\u5b89\u5168\u4e14\u8282\u80fd\u7684\u7b56\u7565\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u697c\u68af\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0cVOCALoco\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u90fd\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u7aef\u5230\u7aefDRL\u7b56\u7565\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "VOCALoco\u6846\u67b6\u901a\u8fc7\u6a21\u5757\u5316\u6280\u80fd\u9009\u62e9\u6709\u6548\u89e3\u51b3\u4e86\u7aef\u5230\u7aefDRL\u65b9\u6cd5\u5728\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u5c40\u9650\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24029", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "q-bio.NC", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24029", "abs": "https://arxiv.org/abs/2510.24029", "authors": ["Andrew Gerstenslager", "Bekarys Dukenbaev", "Ali A. Minai"], "title": "Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model", "comment": "8 pages, 9 figures, Presented at the 2025 International Joint\n  Conference on Neural Networks, Rome, July 2025", "summary": "Boundary Vector Cells (BVCs) are a class of neurons in the brains of\nvertebrates that encode environmental boundaries at specific distances and\nallocentric directions, playing a central role in forming place fields in the\nhippocampus. Most computational BVC models are restricted to two-dimensional\n(2D) environments, making them prone to spatial ambiguities in the presence of\nhorizontal symmetries in the environment. To address this limitation, we\nincorporate vertical angular sensitivity into the BVC framework, thereby\nenabling robust boundary detection in three dimensions, and leading to\nsignificantly more accurate spatial localization in a biologically-inspired\nrobot model.\n  The proposed model processes LiDAR data to capture vertical contours, thereby\ndisambiguating locations that would be indistinguishable under a purely 2D\nrepresentation. Experimental results show that in environments with minimal\nvertical variation, the proposed 3D model matches the performance of a 2D\nbaseline; yet, as 3D complexity increases, it yields substantially more\ndistinct place fields and markedly reduces spatial aliasing. These findings\nshow that adding a vertical dimension to BVC-based localization can\nsignificantly enhance navigation and mapping in real-world 3D spaces while\nretaining performance parity in simpler, near-planar scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\u7684\u4e09\u7ef4\u8fb9\u754c\u5411\u91cf\u7ec6\u80de\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e8c\u7ef4BVC\u6a21\u578b\u5728\u6c34\u5e73\u5bf9\u79f0\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u7a7a\u95f4\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u7269\u542f\u53d1\u7684\u673a\u5668\u4eba\u6a21\u578b\u4e2d\u7684\u7a7a\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8fb9\u754c\u5411\u91cf\u7ec6\u80de\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u4e8c\u7ef4\u73af\u5883\uff0c\u5728\u5b58\u5728\u6c34\u5e73\u5bf9\u79f0\u6027\u7684\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u7a7a\u95f4\u6a21\u7cca\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u548c\u5efa\u56fe\u80fd\u529b\u3002", "method": "\u5c06\u5782\u76f4\u89d2\u5ea6\u654f\u611f\u6027\u6574\u5408\u5230BVC\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u5904\u7406LiDAR\u6570\u636e\u6355\u6349\u5782\u76f4\u8f6e\u5ed3\uff0c\u4ece\u800c\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u8fb9\u754c\u68c0\u6d4b\u3002", "result": "\u5728\u5782\u76f4\u53d8\u5316\u6700\u5c0f\u7684\u73af\u5883\u4e2d\uff0c\u63d0\u51fa\u76843D\u6a21\u578b\u4e0e2D\u57fa\u7ebf\u6027\u80fd\u76f8\u5f53\uff1b\u4f46\u968f\u77403D\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u8be5\u6a21\u578b\u4ea7\u751f\u663e\u8457\u66f4\u72ec\u7279\u7684\u573a\u6240\u573a\uff0c\u5e76\u660e\u663e\u51cf\u5c11\u4e86\u7a7a\u95f4\u6df7\u53e0\u3002", "conclusion": "\u5728BVC\u5b9a\u4f4d\u4e2d\u52a0\u5165\u5782\u76f4\u7ef4\u5ea6\u53ef\u4ee5\u663e\u8457\u589e\u5f3a\u771f\u5b9e\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u548c\u5efa\u56fe\u80fd\u529b\uff0c\u540c\u65f6\u5728\u8f83\u7b80\u5355\u7684\u8fd1\u5e73\u9762\u573a\u666f\u4e2d\u4fdd\u6301\u6027\u80fd\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.24055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24055", "abs": "https://arxiv.org/abs/2510.24055", "authors": ["Xiucheng Zhang", "Yang Jiang", "Hongwei Qing", "Jiashuo Bai"], "title": "Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation", "comment": "8 pages", "summary": "Perceptual ambiguity and task conflict limit multitask robotic manipulation\nvia imitation learning. We propose a framework combining a Language-Conditioned\nVisual Representation (LCVR) module and a Language-conditioned\nMixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual\nambiguities by grounding visual features with language instructions, enabling\ndifferentiation between visually similar tasks. To mitigate task conflict,\nLMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal\naction distributions, stabilized by gradient modulation. On real-robot\nbenchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion\nPolicy (DP) success rates by 33.75% and 25%, respectively. The full framework\nachieves a 79% average success, outperforming the advanced baseline by 21%. Our\nwork shows that combining semantic grounding and expert specialization enables\nrobust, efficient multi-task manipulation", "AI": {"tldr": "\u63d0\u51faLCVR\u548cLMoE-DP\u6846\u67b6\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u611f\u77e5\u6a21\u7cca\u548c\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0cLCVR\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u589e\u5f3a\u89c6\u89c9\u7279\u5f81\u533a\u5206\u76f8\u4f3c\u4efb\u52a1\uff0cLMoE-DP\u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u67b6\u6784\u4e13\u95e8\u5904\u7406\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u611f\u77e5\u6a21\u7cca\u548c\u4efb\u52a1\u51b2\u7a81\u9650\u5236\u4e86\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u5b9e\u73b0\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u80fd\u529b\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89c9\u76f8\u4f3c\u4efb\u52a1\u7684\u533a\u5206\u95ee\u9898\u548c\u591a\u4efb\u52a1\u95f4\u7684\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u89c6\u89c9\u8868\u793a(LCVR)\u6a21\u5757\u548c\u8bed\u8a00\u6761\u4ef6\u6df7\u5408\u4e13\u5bb6\u5bc6\u5ea6\u7b56\u7565(LMoE-DP)\uff0cLCVR\u901a\u8fc7\u8bed\u8a00\u6307\u4ee4\u589e\u5f3a\u89c6\u89c9\u7279\u5f81\uff0cLMoE-DP\u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u67b6\u6784\u5904\u7406\u591a\u6a21\u6001\u52a8\u4f5c\u5206\u5e03\u5e76\u91c7\u7528\u68af\u5ea6\u8c03\u5236\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLCVR\u5c06ACT\u548cDP\u7684\u6210\u529f\u7387\u5206\u522b\u63d0\u534733.75%\u548c25%\uff0c\u5b8c\u6574\u6846\u67b6\u8fbe\u523079%\u5e73\u5747\u6210\u529f\u7387\uff0c\u6bd4\u5148\u8fdb\u57fa\u7ebf\u63d0\u534721%\u3002", "conclusion": "\u7ed3\u5408\u8bed\u4e49\u57fa\u7840\u548c\u4e13\u5bb6\u4e13\u4e1a\u5316\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u64cd\u4f5c\u3002"}}
{"id": "2510.24067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24067", "abs": "https://arxiv.org/abs/2510.24067", "authors": ["Tianyi Ding", "Ronghao Zheng", "Senlin Zhang", "Meiqin Liu"], "title": "Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition", "comment": null, "summary": "This work addresses the collaborative multi-robot autonomous online\nexploration problem, particularly focusing on distributed exploration planning\nfor dynamically balanced exploration area partition and task allocation among a\nteam of mobile robots operating in obstacle-dense non-convex environments.\n  We present a novel topological map structure that simultaneously\ncharacterizes both spatial connectivity and global exploration completeness of\nthe environment. The topological map is updated incrementally to utilize known\nspatial information for updating reachable spaces, while exploration targets\nare planned in a receding horizon fashion under global coverage guidance.\n  A distributed weighted topological graph Voronoi algorithm is introduced\nimplementing balanced graph space partitions of the fused topological maps.\nTheoretical guarantees are provided for distributed consensus convergence and\nequitable graph space partitions with constant bounds.\n  A local planner optimizes the visitation sequence of exploration targets\nwithin the balanced partitioned graph space to minimize travel distance, while\ngenerating safe, smooth, and dynamically feasible motion trajectories.\n  Comprehensive benchmarking against state-of-the-art methods demonstrates\nsignificant improvements in exploration efficiency, completeness, and workload\nbalance across the robot team.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u81ea\u4e3b\u5728\u7ebf\u63a2\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\u5b9e\u73b0\u5e73\u8861\u7684\u533a\u57df\u5212\u5206\u548c\u4efb\u52a1\u5206\u914d\uff0c\u5728\u969c\u788d\u5bc6\u96c6\u7684\u975e\u51f8\u73af\u5883\u4e2d\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u548c\u8d1f\u8f7d\u5747\u8861\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u969c\u788d\u5bc6\u96c6\u975e\u51f8\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u63a2\u7d22\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u52a8\u6001\u5e73\u8861\u7684\u63a2\u7d22\u533a\u57df\u5212\u5206\u548c\u4efb\u52a1\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u6574\u4f53\u63a2\u7d22\u6548\u7387\u548c\u56e2\u961f\u8d1f\u8f7d\u5747\u8861\u3002", "method": "\u4f7f\u7528\u589e\u91cf\u66f4\u65b0\u7684\u62d3\u6251\u56fe\u7ed3\u6784\u8868\u5f81\u7a7a\u95f4\u8fde\u901a\u6027\u548c\u5168\u5c40\u63a2\u7d22\u5b8c\u6574\u6027\uff1b\u63d0\u51fa\u5206\u5e03\u5f0f\u52a0\u6743\u62d3\u6251\u56feVoronoi\u7b97\u6cd5\u5b9e\u73b0\u5e73\u8861\u7684\u56fe\u7a7a\u95f4\u5212\u5206\uff1b\u5c40\u90e8\u89c4\u5212\u5668\u4f18\u5316\u63a2\u7d22\u76ee\u6807\u8bbf\u95ee\u5e8f\u5217\u5e76\u751f\u6210\u5b89\u5168\u5e73\u6ed1\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u63a2\u7d22\u6548\u7387\u3001\u5b8c\u6574\u6027\u548c\u673a\u5668\u4eba\u56e2\u961f\u5de5\u4f5c\u8d1f\u8f7d\u5e73\u8861\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u534f\u4f5c\u63a2\u7d22\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u6536\u655b\u548c\u5747\u8861\u56fe\u7a7a\u95f4\u5212\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u6027\u80fd\u3002"}}
{"id": "2510.24069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f4d\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u8db3\u5f0f\u673a\u5668\u4eba\u7684\u53ef\u9760\u8fd0\u52a8\uff0c\u901a\u8fc7\u89e3\u8026\u63a5\u89e6\u70b9\u52a8\u529b\u5b66\u548c\u5229\u7528\u8d1d\u585e\u5c14\u591a\u9879\u5f0f\u7279\u6027\uff0c\u786e\u4fdd\u6574\u4e2a\u8f68\u8ff9\u4e2d\u5e73\u79fb\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4e86\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u751f\u6210\u8db3\u5f0f\u673a\u5668\u4eba\u7684\u53ef\u9760\u8fd0\u52a8\uff0c\u9700\u8981\u540c\u65f6\u8ba1\u7b97\u673a\u5668\u4eba\u7684\u8def\u5f84\u548c\u63a5\u89e6\u5e8f\u5217\uff0c\u5e76\u5728\u95ee\u9898\u8868\u8ff0\u4e2d\u51c6\u786e\u8003\u8651\u52a8\u529b\u5b66\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u786e\u4fdd\u5e73\u79fb\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\u7684\u53ef\u884c\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u5229\u7528\u7ebf\u6027\u5fae\u5206\u65b9\u7a0b\u7684\u53e0\u52a0\u7279\u6027\u89e3\u8026\u6bcf\u4e2a\u63a5\u89e6\u70b9\u7684\u5e73\u79fb\u52a8\u529b\u5b66\uff1b\u4f7f\u7528\u8d1d\u585e\u5c14\u591a\u9879\u5f0f\u7684\u5fae\u5206\u77e9\u9635\u63a8\u5bfc\u4f4d\u7f6e\u4e0e\u529b\u4e4b\u95f4\u7684\u89e3\u6790\u5173\u7cfb\uff1b\u5229\u7528\u8d1d\u585e\u5c14\u591a\u9879\u5f0f\u7684\u51f8\u5305\u7279\u6027\u786e\u4fdd\u6469\u64e6\u9525\u7ea6\u675f\u7684\u6ee1\u8db3\u3002", "result": "\u6240\u63d0\u51fa\u7684\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u80fd\u591f\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u751f\u6210\u5177\u6709\u5404\u79cd\u6b65\u6001\u5e8f\u5217\u7684\u52a8\u6001\u53ef\u9760\u8fd0\u52a8\u3002\u901a\u8fc7\u56db\u8db3\u673a\u5668\u4eba\u6a21\u578b\u9a8c\u8bc1\u4e86\u52a8\u529b\u5b66\u548c\u8fd0\u52a8\u751f\u6210\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u6ee1\u8db3\u5e73\u79fb\u52a8\u529b\u5b66\u548c\u6469\u64e6\u9525\u7ea6\u675f\u7684\u53ef\u9760\u8db3\u5f0f\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u4e3a\u5404\u79cd\u6b65\u6001\u5e8f\u5217\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8f68\u8ff9\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24108", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.", "AI": {"tldr": "ZTRS\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u4ece\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u5b66\u4e60\u9a7e\u9a76\u7b56\u7565\uff0c\u65e0\u9700\u6a21\u4eff\u5b66\u4e60\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u4f46\u53d7\u9650\u4e8e\u4e13\u5bb6\u6f14\u793a\u8d28\u91cf\u548c\u90e8\u7f72\u65f6\u7684\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1b\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u5229\u7528\u4eff\u771f\u6269\u5c55\uff0c\u4f46\u901a\u5e38\u9650\u4e8e\u4f4e\u7ef4\u7b26\u53f7\u8f93\u5165\uff0c\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "method": "\u63d0\u51faZTRS\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u63d0\u51fa\u7684\u7a77\u4e3e\u7b56\u7565\u4f18\u5316(EPO)\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u53ef\u679a\u4e3e\u52a8\u4f5c\u548c\u5956\u52b1\u7a7a\u95f4\u4e0a\u8fdb\u884c\u7b56\u7565\u68af\u5ea6\u4f18\u5316\uff0c\u4ece\u539f\u59cb\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u5b66\u4e60\u9a7e\u9a76\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aNavtest\uff08\u901a\u7528\u73b0\u5b9e\u4e16\u754c\u5f00\u73af\u89c4\u5212\uff09\u3001Navhard\uff08\u6311\u6218\u6027\u73b0\u5b9e\u4e16\u754c\u548c\u5408\u6210\u573a\u666f\u5f00\u73af\u89c4\u5212\uff09\u548cHUGSIM\uff08\u4eff\u771f\u95ed\u73af\u9a7e\u9a76\uff09\uff0c\u5728Navhard\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728HUGSIM\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ZTRS\u662f\u7b2c\u4e00\u4e2a\u5b8c\u5168\u6d88\u9664\u6a21\u4eff\u5b66\u4e60\u3001\u4ec5\u4ece\u5956\u52b1\u5b66\u4e60\u5e76\u76f4\u63a5\u5904\u7406\u9ad8\u7ef4\u4f20\u611f\u5668\u6570\u636e\u7684\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24109", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24109", "abs": "https://arxiv.org/abs/2510.24109", "authors": ["Wenbin Ding", "Jun Chen", "Mingjia Chen", "Fei Xie", "Qi Mao", "Philip Dames"], "title": "PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has marked a\nsignificant breakthrough in Artificial Intelligence (AI), ushering in a new era\nof Human-centered Artificial Intelligence (HAI). HAI aims to better serve human\nwelfare and needs, thereby placing higher demands on the intelligence level of\nrobots, particularly in aspects such as natural language interaction, complex\ntask planning, and execution. Intelligent agents powered by LLMs have opened up\nnew pathways for realizing HAI. However, existing LLM-based embodied agents\noften lack the ability to plan and execute complex natural language control\ntasks online. This paper explores the implementation of intelligent robotic\nmanipulating agents based on Vision-Language Models (VLMs) in the physical\nworld. We propose a novel embodied agent framework for robots, which comprises\na human-robot voice interaction module, a vision-language agent module and an\naction execution module. The vision-language agent itself includes a\nvision-based task planner, a natural language instruction converter, and a task\nperformance feedback evaluator. Experimental results demonstrate that our agent\nachieves a 28\\% higher average task success rate in both simulated and real\nenvironments compared to approaches relying solely on LLM+CLIP, significantly\nimproving the execution success rate of high-level natural language instruction\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u673a\u5668\u4eba\u5177\u8eab\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5305\u542b\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u3001\u89c6\u89c9\u8bed\u8a00\u667a\u80fd\u4f53\u548c\u52a8\u4f5c\u6267\u884c\u4e09\u5927\u6a21\u5757\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6bd4\u7eafLLM+CLIP\u65b9\u6cd5\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8628%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u4f46\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7f3a\u4e4f\u5728\u7ebf\u89c4\u5212\u548c\u6267\u884c\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u9700\u8981\u63d0\u5347\u673a\u5668\u4eba\u5728\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3001\u590d\u6742\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\u65b9\u9762\u7684\u667a\u80fd\u6c34\u5e73\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4eba\u673a\u8bed\u97f3\u4ea4\u4e92\u6a21\u5757\u3001\u89c6\u89c9\u8bed\u8a00\u667a\u80fd\u4f53\u6a21\u5757\u548c\u52a8\u4f5c\u6267\u884c\u6a21\u5757\u7684\u673a\u5668\u4eba\u5177\u8eab\u667a\u80fd\u4f53\u6846\u67b6\u3002\u89c6\u89c9\u8bed\u8a00\u667a\u80fd\u4f53\u672c\u8eab\u5305\u62ec\u57fa\u4e8e\u89c6\u89c9\u7684\u4efb\u52a1\u89c4\u5212\u5668\u3001\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u5668\u548c\u4efb\u52a1\u6027\u80fd\u53cd\u9988\u8bc4\u4f30\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u667a\u80fd\u4f53\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6bd4\u4ec5\u4f9d\u8d56LLM+CLIP\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e8628%\u66f4\u9ad8\u7684\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u7ea7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u3002", "conclusion": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u673a\u5668\u4eba\u64cd\u4f5c\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u590d\u6742\u81ea\u7136\u8bed\u8a00\u63a7\u5236\u4efb\u52a1\u7684\u5728\u7ebf\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.24118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo", "AI": {"tldr": "LagMemo\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u7684\u89c6\u89c9\u5bfc\u822a\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u548c\u591a\u76ee\u6807\u5bfc\u822a\uff0c\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u76843D\u8bed\u8a00\u8bb0\u5fc6\u548c\u5c40\u90e8\u611f\u77e5\u9a8c\u8bc1\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u76ee\u6807\u3001\u5355\u6a21\u6001\u548c\u5c01\u95ed\u96c6\u76ee\u6807\u8bbe\u7f6e\u7684\u4e0d\u8db3\uff0c\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u3001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u67e5\u8be2\u548c\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u7684\u9700\u6c42\u3002", "method": "\u5728\u63a2\u7d22\u9636\u6bb5\u6784\u5efa\u7edf\u4e00\u76843D\u8bed\u8a00\u8bb0\u5fc6\uff1b\u63a5\u6536\u4efb\u52a1\u76ee\u6807\u65f6\u67e5\u8be2\u8bb0\u5fc6\u5e76\u9884\u6d4b\u5019\u9009\u76ee\u6807\u4f4d\u7f6e\uff1b\u96c6\u6210\u57fa\u4e8e\u5c40\u90e8\u611f\u77e5\u7684\u9a8c\u8bc1\u673a\u5236\uff0c\u5728\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5339\u914d\u548c\u9a8c\u8bc1\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eLagMemo\u7684\u8bb0\u5fc6\u6a21\u5757\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5b9a\u4f4d\uff0c\u5e76\u4e14\u5728\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "LagMemo\u7cfb\u7edf\u901a\u8fc7\u8bed\u8a003D\u9ad8\u65af\u6cfc\u6e85\u8bb0\u5fc6\u548c\u52a8\u6001\u9a8c\u8bc1\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5f00\u653e\u8bcd\u6c47\u591a\u76ee\u6807\u89c6\u89c9\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2510.24194", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24194", "abs": "https://arxiv.org/abs/2510.24194", "authors": ["Ev Zisselman", "Mirco Mutti", "Shelly Francis-Meretzki", "Elisei Shafer", "Aviv Tamar"], "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames", "comment": null, "summary": "Behavioral cloning is a simple yet effective technique for learning\nsequential decision-making from demonstrations. Recently, it has gained\nprominence as the core of foundation models for the physical world, where\nachieving generalization requires countless demonstrations of a multitude of\ntasks. Typically, a human expert with full information on the task demonstrates\na (nearly) optimal behavior. In this paper, we propose to hide some of the\ntask's information from the demonstrator. This ``blindfolded'' expert is\ncompelled to employ non-trivial exploration to solve the task. We show that\ncloning the blindfolded expert generalizes better to unseen tasks than its\nfully-informed counterpart. We conduct experiments of real-world robot peg\ninsertion tasks with (limited) human demonstrations, alongside videogames from\nthe Procgen benchmark. Additionally, we support our findings with theoretical\nanalysis, which confirms that the generalization error scales with\n$\\sqrt{I/m}$, where $I$ measures the amount of task information available to\nthe demonstrator, and $m$ is the number of demonstrated tasks. Both theory and\npractice indicate that cloning blindfolded experts generalizes better with\nfewer demonstrated tasks. Project page with videos and code:\nhttps://sites.google.com/view/blindfoldedexperts/home", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\"\u8499\u773c\u4e13\u5bb6\"\u7684\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u85cf\u90e8\u5206\u4efb\u52a1\u4fe1\u606f\u8feb\u4f7f\u4e13\u5bb6\u8fdb\u884c\u975e\u5e73\u51e1\u63a2\u7d22\uff0c\u4ece\u800c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u884c\u4e3a\u514b\u9686\u9700\u8981\u4eba\u7c7b\u4e13\u5bb6\u5728\u5b8c\u5168\u4e86\u89e3\u4efb\u52a1\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u6700\u4f18\u884c\u4e3a\u6f14\u793a\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5728\u9700\u8981\u6cdb\u5316\u5230\u591a\u79cd\u4efb\u52a1\u65f6\u6548\u679c\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u9650\u5236\u4e13\u5bb6\u53ef\u83b7\u5f97\u7684\u4fe1\u606f\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\"\u8499\u773c\u4e13\u5bb6\"\u6982\u5ff5\uff0c\u9690\u85cf\u90e8\u5206\u4efb\u52a1\u4fe1\u606f\u8feb\u4f7f\u4e13\u5bb6\u8fdb\u884c\u63a2\u7d22\u6027\u884c\u4e3a\uff0c\u7136\u540e\u514b\u9686\u8fd9\u79cd\u63a2\u7d22\u6027\u884c\u4e3a\u3002\u5728\u771f\u5b9e\u673a\u5668\u4eba\u63d2\u5165\u4efb\u52a1\u548cProcgen\u57fa\u51c6\u89c6\u9891\u6e38\u620f\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u514b\u9686\u8499\u773c\u4e13\u5bb6\u7684\u884c\u4e3a\u6bd4\u514b\u9686\u5b8c\u5168\u77e5\u60c5\u4e13\u5bb6\u7684\u884c\u4e3a\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u6cdb\u5316\u6548\u679c\u66f4\u597d\u3002\u7406\u8bba\u5206\u6790\u8bc1\u5b9e\u6cdb\u5316\u8bef\u5dee\u4e0e\u221a(I/m)\u6210\u6b63\u6bd4\uff0c\u5176\u4e2dI\u662f\u4e13\u5bb6\u53ef\u83b7\u5f97\u7684\u4efb\u52a1\u4fe1\u606f\u91cf\uff0cm\u662f\u6f14\u793a\u4efb\u52a1\u6570\u91cf\u3002", "conclusion": "\u514b\u9686\u8499\u773c\u4e13\u5bb6\u80fd\u591f\u7528\u66f4\u5c11\u7684\u6f14\u793a\u4efb\u52a1\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u7269\u7406\u4e16\u754c\u57fa\u7840\u6a21\u578b\u7684\u884c\u4e3a\u514b\u9686\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.24261", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24261", "abs": "https://arxiv.org/abs/2510.24261", "authors": ["Jingyi Tian", "Le Wang", "Sanping Zhou", "Sen Wang", "Jiayi Li", "Gang Hua"], "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation", "comment": "Accepted to NeurIPS 2025", "summary": "Learning generalizable robotic manipulation policies remains a key challenge\ndue to the scarcity of diverse real-world training data. While recent\napproaches have attempted to mitigate this through self-supervised\nrepresentation learning, most either rely on 2D vision pretraining paradigms\nsuch as masked image modeling, which primarily focus on static semantics or\nscene geometry, or utilize large-scale video prediction models that emphasize\n2D dynamics, thus failing to jointly learn the geometry, semantics, and\ndynamics required for effective manipulation. In this paper, we present\nDynaRend, a representation learning framework that learns 3D-aware and\ndynamics-informed triplane features via masked reconstruction and future\nprediction using differentiable volumetric rendering. By pretraining on\nmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, future\ndynamics, and task semantics in a unified triplane representation. The learned\nrepresentations can be effectively transferred to downstream robotic\nmanipulation tasks via action value map prediction. We evaluate DynaRend on two\nchallenging benchmarks, RLBench and Colosseum, as well as in real-world robotic\nexperiments, demonstrating substantial improvements in policy success rate,\ngeneralization to environmental perturbations, and real-world applicability\nacross diverse manipulation tasks.", "AI": {"tldr": "DynaRend\u662f\u4e00\u4e2a\u901a\u8fc7\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u5b66\u4e603D\u611f\u77e5\u548c\u52a8\u6001\u611f\u77e5\u7684\u4e09\u5e73\u9762\u7279\u5f81\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d562D\u89c6\u89c9\u9884\u8bad\u7ec3\u8303\u5f0f\u6216\u5927\u89c4\u6a21\u89c6\u9891\u9884\u6d4b\u6a21\u578b\uff0c\u65e0\u6cd5\u8054\u5408\u5b66\u4e60\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u52a8\u6001\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u63a9\u7801\u91cd\u5efa\u548c\u672a\u6765\u9884\u6d4b\uff0c\u4f7f\u7528\u53ef\u5fae\u5206\u4f53\u79ef\u6e32\u67d3\u5728\u591a\u89c6\u89d2RGB-D\u89c6\u9891\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u5b66\u4e60\u7edf\u4e00\u7684\u4e09\u5e73\u9762\u8868\u793a\u3002", "result": "\u5728RLBench\u548cColosseum\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cDynaRend\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u6210\u529f\u7387\u3001\u5bf9\u73af\u5883\u6270\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u8de8\u4efb\u52a1\u7684\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "DynaRend\u80fd\u591f\u6709\u6548\u5b66\u4e60\u5305\u542b\u7a7a\u95f4\u51e0\u4f55\u3001\u672a\u6765\u52a8\u6001\u548c\u4efb\u52a1\u8bed\u4e49\u7684\u7edf\u4e00\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24315", "abs": "https://arxiv.org/abs/2510.24315", "authors": ["Baozhe Zhang", "Xinwei Chen", "Qingcheng Chen", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation", "comment": null, "summary": "CoNi-MPC provides an efficient framework for UAV control in air-ground\ncooperative tasks by relying exclusively on relative states, eliminating the\nneed for global state estimation. However, its lack of environmental\ninformation poses significant challenges for obstacle avoidance. To address\nthis issue, we propose a novel obstacle avoidance algorithm, Cooperative\nNon-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for\nUAV-UGV cooperative scenarios without reliance on global state estimation or\nobstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data\nfrom the UAV to generate a modulation matrix, which directly adjusts the\nquadrotor's velocity to achieve obstacle avoidance. This modulation-based\nmethod enables real-time generation of collision-free trajectories within the\nUGV's non-inertial frame, significantly reducing computational demands (less\nthan 5 ms per iteration) while maintaining safety in dynamic and unpredictable\nenvironments. The key contributions of this work include: (1) a\nmodulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV\ncooperation in non-inertial frames without global states; (2) rapid, real-time\ntrajectory generation based solely on single-frame LiDAR data, removing the\nneed for obstacle modeling or prediction; and (3) adaptability to both static\nand dynamic environments, thus extending applicability to featureless or\nunknown scenarios.", "AI": {"tldr": "\u63d0\u51faCoNi-OA\u7b97\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u65e0\u4eba\u673a-\u5730\u9762\u8f66\u534f\u540c\u573a\u666f\u4e2d\u7684\u969c\u788d\u7269\u907f\u8ba9\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u9884\u6d4b\uff0c\u4ec5\u4f7f\u7528\u5355\u5e27LiDAR\u6570\u636e\u5b9e\u65f6\u751f\u6210\u907f\u969c\u8f68\u8ff9\u3002", "motivation": "CoNi-MPC\u6846\u67b6\u867d\u7136\u80fd\u6709\u6548\u8fdb\u884c\u65e0\u4eba\u673a\u63a7\u5236\uff0c\u4f46\u7f3a\u4e4f\u73af\u5883\u4fe1\u606f\u5bfc\u81f4\u969c\u788d\u7269\u907f\u8ba9\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u7684\u907f\u969c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u65e0\u4eba\u673a\u5355\u5e27\u539f\u59cbLiDAR\u6570\u636e\u751f\u6210\u8c03\u5236\u77e9\u9635\uff0c\u76f4\u63a5\u8c03\u6574\u56db\u65cb\u7ffc\u98de\u884c\u5668\u901f\u5ea6\u5b9e\u73b0\u907f\u969c\uff0c\u5728\u975e\u60ef\u6027\u6846\u67b6\u5185\u5b9e\u65f6\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "result": "\u7b97\u6cd5\u8ba1\u7b97\u6548\u7387\u9ad8\uff08\u6bcf\u6b21\u8fed\u4ee3\u5c0f\u4e8e5\u6beb\u79d2\uff09\uff0c\u5728\u52a8\u6001\u548c\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u4fdd\u6301\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u3002", "conclusion": "CoNi-OA\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u4eba\u673a-\u5730\u9762\u8f66\u534f\u540c\u4efb\u52a1\u4e2d\u7684\u969c\u788d\u7269\u907f\u8ba9\u95ee\u9898\uff0c\u65e0\u9700\u5168\u5c40\u72b6\u6001\u4f30\u8ba1\u6216\u969c\u788d\u7269\u5efa\u6a21\uff0c\u5177\u6709\u5b9e\u65f6\u6027\u548c\u9002\u5e94\u6027\u5f3a\u7684\u7279\u70b9\u3002"}}
{"id": "2510.24335", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24335", "abs": "https://arxiv.org/abs/2510.24335", "authors": ["Mingyu Jeong", "Eunsung Kim", "Sehun Park", "Andrew Jaeyong Choi"], "title": "NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation", "comment": "9 pages, 10 figures", "summary": "We present NVSim, a framework that automatically constructs large-scale,\nnavigable indoor simulators from only common image sequences, overcoming the\ncost and scalability limitations of traditional 3D scanning. Our approach\nadapts 3D Gaussian Splatting to address visual artifacts on sparsely observed\nfloors a common issue in robotic traversal data. We introduce Floor-Aware\nGaussian Splatting to ensure a clean, navigable ground plane, and a novel\nmesh-free traversability checking algorithm that constructs a topological graph\nby directly analyzing rendered views. We demonstrate our system's ability to\ngenerate valid, large-scale navigation graphs from real-world data. A video\ndemonstration is avilable at https://youtu.be/tTiIQt6nXC8", "AI": {"tldr": "NVSim\u662f\u4e00\u4e2a\u4ece\u666e\u901a\u56fe\u50cf\u5e8f\u5217\u81ea\u52a8\u6784\u5efa\u5927\u89c4\u6a21\u53ef\u5bfc\u822a\u5ba4\u5185\u6a21\u62df\u5668\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf3D\u626b\u63cf\u7684\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6539\u8fdb3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u6765\u5904\u7406\u7a00\u758f\u89c2\u6d4b\u5730\u9762\u4e0a\u7684\u89c6\u89c9\u4f2a\u5f71\uff0c\u5e76\u5f15\u5165\u65e0\u7f51\u683c\u53ef\u7a7f\u8d8a\u6027\u68c0\u67e5\u7b97\u6cd5\u6765\u6784\u5efa\u62d3\u6251\u56fe\u3002", "motivation": "\u4f20\u7edf3D\u626b\u63cf\u65b9\u6cd5\u5728\u6784\u5efa\u5927\u89c4\u6a21\u5ba4\u5185\u6a21\u62df\u5668\u65f6\u9762\u4e34\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u800c\u673a\u5668\u4eba\u904d\u5386\u6570\u636e\u901a\u5e38\u5b58\u5728\u5730\u9762\u7a00\u758f\u89c2\u6d4b\u5bfc\u81f4\u7684\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u76843D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff08Floor-Aware Gaussian Splatting\uff09\u786e\u4fdd\u6e05\u6d01\u53ef\u5bfc\u822a\u7684\u5730\u9762\u5e73\u9762\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6e32\u67d3\u89c6\u56fe\u76f4\u63a5\u5206\u6790\u7684\u65e0\u7f51\u683c\u53ef\u7a7f\u8d8a\u6027\u68c0\u67e5\u7b97\u6cd5\u6765\u6784\u5efa\u62d3\u6251\u56fe\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4ece\u771f\u5b9e\u4e16\u754c\u6570\u636e\u751f\u6210\u6709\u6548\u7684\u5927\u89c4\u6a21\u5bfc\u822a\u56fe\uff0c\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u3002", "conclusion": "NVSim\u6846\u67b6\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u4e3a\u5927\u89c4\u6a21\u5ba4\u5185\u5bfc\u822a\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.24457", "abs": "https://arxiv.org/abs/2510.24457", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance", "comment": "8 pages, 11 figures", "summary": "This paper presents an optimal trajectory generation method for 3D overhead\ncranes by leveraging differential flatness. This framework enables the direct\ninclusion of complex physical and dynamic constraints, such as nonlinear\nfriction and collision avoidance for both payload and rope. Our approach allows\nfor aggressive movements by constraining payload swing only at the final point.\nA comparative simulation study validates our approach, demonstrating that\nneglecting dry friction leads to actuator saturation and collisions. The\nresults show that friction modeling is a fundamental requirement for fast and\nsafe crane trajectories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u5206\u5e73\u5766\u6027\u76843D\u6865\u5f0f\u8d77\u91cd\u673a\u6700\u4f18\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u5904\u7406\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\u7b49\u590d\u6742\u7269\u7406\u7ea6\u675f\uff0c\u5b9e\u73b0\u4ec5\u7ea6\u675f\u6700\u7ec8\u70b9\u8f7d\u8377\u6446\u52a8\u7684\u6fc0\u8fdb\u8fd0\u52a8\u3002", "motivation": "\u73b0\u6709\u7684\u8d77\u91cd\u673a\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u7269\u7406\u548c\u52a8\u6001\u7ea6\u675f\uff0c\u5982\u975e\u7ebf\u6027\u6469\u64e6\u548c\u78b0\u649e\u907f\u514d\uff0c\u8fd9\u9650\u5236\u4e86\u8d77\u91cd\u673a\u7684\u8fd0\u52a8\u901f\u5ea6\u548c\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528\u5fae\u5206\u5e73\u5766\u6027\u6846\u67b6\uff0c\u76f4\u63a5\u5305\u542b\u975e\u7ebf\u6027\u6469\u64e6\u548c\u8f7d\u8377\u3001\u7ef3\u7d22\u78b0\u649e\u907f\u514d\u7b49\u590d\u6742\u7269\u7406\u7ea6\u675f\uff0c\u4ec5\u7ea6\u675f\u6700\u7ec8\u70b9\u7684\u8f7d\u8377\u6446\u52a8\u4ee5\u5b9e\u73b0\u6fc0\u8fdb\u8fd0\u52a8\u3002", "result": "\u5bf9\u6bd4\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u5ffd\u7565\u5e72\u6469\u64e6\u4f1a\u5bfc\u81f4\u6267\u884c\u5668\u9971\u548c\u548c\u78b0\u649e\uff0c\u800c\u6469\u64e6\u5efa\u6a21\u5bf9\u4e8e\u5feb\u901f\u5b89\u5168\u7684\u8d77\u91cd\u673a\u8f68\u8ff9\u662f\u57fa\u672c\u8981\u6c42\u3002", "conclusion": "\u6469\u64e6\u5efa\u6a21\u662f\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u8d77\u91cd\u673a\u8f68\u8ff9\u7684\u57fa\u672c\u8981\u6c42\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u7269\u7406\u7ea6\u675f\uff0c\u5b9e\u73b0\u6fc0\u8fdb\u4e14\u5b89\u5168\u7684\u8fd0\u52a8\u3002"}}
{"id": "2510.24508", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24508", "abs": "https://arxiv.org/abs/2510.24508", "authors": ["Haoying Li", "Yifan Peng", "Junfeng Wu"], "title": "Supervisory Measurement-Guided Noise Covariance Estimation", "comment": null, "summary": "Reliable state estimation hinges on accurate specification of sensor noise\ncovariances, which weigh heterogeneous measurements. In practice, these\ncovariances are difficult to identify due to environmental variability,\nfront-end preprocessing, and other reasons. We address this by formulating\nnoise covariance estimation as a bilevel optimization that, from a Bayesian\nperspective, factorizes the joint likelihood of so-called odometry and\nsupervisory measurements, thereby balancing information utilization with\ncomputational efficiency. The factorization converts the nested Bayesian\ndependency into a chain structure, enabling efficient parallel computation: at\nthe lower level, an invariant extended Kalman filter with state augmentation\nestimates trajectories, while a derivative filter computes analytical gradients\nin parallel for upper-level gradient updates. The upper level refines the\ncovariance to guide the lower-level estimation. Experiments on synthetic and\nreal-world datasets show that our method achieves higher efficiency over\nexisting baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u8868\u8ff0\u4e3a\u8d1d\u53f6\u65af\u95ee\u9898\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u5e76\u884c\u8ba1\u7b97\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u66f4\u9ad8\u6548\u3002", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\u4f20\u611f\u5668\u566a\u58f0\u534f\u65b9\u5dee\u96be\u4ee5\u51c6\u786e\u786e\u5b9a\uff0c\u56e0\u4e3a\u53d7\u5230\u73af\u5883\u53d8\u5316\u3001\u524d\u7aef\u9884\u5904\u7406\u7b49\u56e0\u7d20\u5f71\u54cd\uff0c\u8fd9\u4f1a\u5f71\u54cd\u72b6\u6001\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002", "method": "\u5c06\u566a\u58f0\u534f\u65b9\u5dee\u4f30\u8ba1\u6784\u5efa\u4e3a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff1a\u4e0b\u5c42\u4f7f\u7528\u4e0d\u53d8\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fdb\u884c\u8f68\u8ff9\u4f30\u8ba1\uff0c\u4e0a\u5c42\u901a\u8fc7\u68af\u5ea6\u66f4\u65b0\u4f18\u5316\u534f\u65b9\u5dee\uff1b\u901a\u8fc7\u56e0\u5b50\u5316\u5206\u89e3\u5c06\u5d4c\u5957\u8d1d\u53f6\u65af\u4f9d\u8d56\u8f6c\u6362\u4e3a\u94fe\u5f0f\u7ed3\u6784\uff0c\u5b9e\u73b0\u5e76\u884c\u8ba1\u7b97\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u53cc\u5c42\u4f18\u5316\u6846\u67b6\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u566a\u58f0\u534f\u65b9\u5dee\uff0c\u5e73\u8861\u4fe1\u606f\u5229\u7528\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u72b6\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.24533", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24533", "abs": "https://arxiv.org/abs/2510.24533", "authors": ["Yuan Shen", "Yuze Hong", "Guangyang Zeng", "Tengfei Zhang", "Pui Yi Chui", "Ziyang Hong", "Junfeng Wu"], "title": "GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots", "comment": null, "summary": "Accurate visual inertial simultaneous localization and mapping (VI SLAM) for\nunderwater robots remains a significant challenge due to frequent visual\ndegeneracy and insufficient inertial measurement unit (IMU) motion excitation.\nIn this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system\ndesigned to address these issues. By leveraging the stereo camera's direct\ndepth estimation ability, we eliminate the need to estimate scale during IMU\ninitialization, enabling stable operation even under low acceleration dynamics.\nWith precise gravity initialization, we decouple the pitch and roll from the\npose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point\n(PnP) problem for pose tracking. This allows the use of a minimal 3-point\nsolver, which significantly reduces computational time to reject outliers\nwithin a Random Sample Consensus framework. We further propose a\nbias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the\nrelative pose converges to the true value as the feature number increases. To\nhandle dynamic motion, we refine the full 6-DOF pose while jointly estimating\nthe IMU covariance, enabling adaptive weighting of the gravity prior. Extensive\nexperiments on simulated and real-world data demonstrate that GeVI-SLAM\nachieves higher accuracy and greater stability compared to state-of-the-art\nmethods.", "AI": {"tldr": "GeVI-SLAM\u662f\u4e00\u79cd\u91cd\u529b\u589e\u5f3a\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u7acb\u4f53\u76f8\u673a\u6df1\u5ea6\u4f30\u8ba1\u3001\u91cd\u529b\u589e\u5f3a\u548c4\u81ea\u7531\u5ea6PnP\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u89c6\u89c9\u9000\u5316\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4eba\u89c6\u89c9\u60ef\u6027SLAM\u9762\u4e34\u89c6\u89c9\u9891\u7e41\u9000\u5316\u548cIMU\u8fd0\u52a8\u6fc0\u52b1\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5b9a\u548c\u51c6\u786e\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u7cfb\u7edf\u3002", "method": "\u5229\u7528\u7acb\u4f53\u76f8\u673a\u76f4\u63a5\u6df1\u5ea6\u4f30\u8ba1\u6d88\u9664IMU\u521d\u59cb\u5316\u4e2d\u7684\u5c3a\u5ea6\u4f30\u8ba1\u9700\u6c42\uff1b\u901a\u8fc7\u7cbe\u786e\u91cd\u529b\u521d\u59cb\u5316\u5c06\u4fef\u4ef0\u548c\u6a2a\u6eda\u4ece\u4f4d\u59ff\u4f30\u8ba1\u4e2d\u89e3\u8026\uff0c\u4f7f\u75284\u81ea\u7531\u5ea6PnP\u95ee\u9898\u6c42\u89e3\uff1b\u63d0\u51fa\u53ef\u8bc1\u660e\u4e00\u81f4\u6027\u7684\u504f\u7f6e\u6d88\u96644-DOF PnP\u4f30\u8ba1\u5668\uff1b\u5904\u7406\u52a8\u6001\u8fd0\u52a8\u65f6\u8054\u5408\u4f30\u8ba1IMU\u534f\u65b9\u5dee\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u91cd\u529b\u5148\u9a8c\u6743\u91cd\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGeVI-SLAM\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u66f4\u597d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "GeVI-SLAM\u901a\u8fc7\u91cd\u529b\u589e\u5f3a\u548c4\u81ea\u7531\u5ea6PnP\u6c42\u89e3\u5668\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0bSLAM\u7684\u6311\u6218\uff0c\u4e3a\u6c34\u4e0b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24554", "abs": "https://arxiv.org/abs/2510.24554", "authors": ["Vignesh Kottayam Viswanathan", "Yifan Bai", "Scott Fredriksson", "Sumeet Satpute", "Christoforos Kanellakis", "George Nikolakopoulos"], "title": "An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments", "comment": "Submitted for ICRA 2026", "summary": "In this work, we present a hierarchical framework designed to support robotic\ninspection under environment uncertainty. By leveraging a known environment\nmodel, existing methods plan and safely track inspection routes to visit points\nof interest. However, discrepancies between the model and actual site\nconditions, caused by either natural or human activities, can alter the surface\nmorphology or introduce path obstructions. To address this challenge, the\nproposed framework divides the inspection task into: (a) generating the initial\nglobal view-plan for region of interests based on a historical map and (b)\nlocal view replanning to adapt to the current morphology of the inspection\nscene. The proposed hierarchy preserves global coverage objectives while\nenabling reactive adaptation to the local surface morphology. This enables the\nlocal autonomy to remain robust against environment uncertainty and complete\nthe inspection tasks. We validate the approach through deployments in\nreal-world subterranean mines using quadrupedal robot.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u6846\u67b6\u89e3\u51b3\u673a\u5668\u4eba\u5de1\u68c0\u4e2d\u7684\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u5168\u5c40\u89c6\u56fe\u89c4\u5212\u548c\u5c40\u90e8\u89c6\u56fe\u91cd\u89c4\u5212\u6765\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u5728\u771f\u5b9e\u5730\u4e0b\u77ff\u4e95\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5df2\u77e5\u73af\u5883\u6a21\u578b\u89c4\u5212\u5de1\u68c0\u8def\u7ebf\uff0c\u4f46\u6a21\u578b\u4e0e\u5b9e\u9645\u73af\u5883\u5dee\u5f02\uff08\u81ea\u7136\u6216\u4eba\u4e3a\u6d3b\u52a8\u5bfc\u81f4\uff09\u4f1a\u6539\u53d8\u8868\u9762\u5f62\u6001\u6216\u5f15\u5165\u8def\u5f84\u969c\u788d\uff0c\u9700\u8981\u9002\u5e94\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u5c42\u6846\u67b6\u5c06\u5de1\u68c0\u4efb\u52a1\u5206\u4e3a\uff1a(a)\u57fa\u4e8e\u5386\u53f2\u5730\u56fe\u751f\u6210\u611f\u5174\u8da3\u533a\u57df\u7684\u521d\u59cb\u5168\u5c40\u89c6\u56fe\u89c4\u5212\uff1b(b)\u5c40\u90e8\u89c6\u56fe\u91cd\u89c4\u5212\u4ee5\u9002\u5e94\u5f53\u524d\u68c0\u67e5\u573a\u666f\u7684\u8868\u9762\u5f62\u6001\u3002", "result": "\u8be5\u5c42\u6b21\u7ed3\u6784\u4fdd\u6301\u5168\u5c40\u8986\u76d6\u76ee\u6807\uff0c\u540c\u65f6\u5b9e\u73b0\u5bf9\u5c40\u90e8\u8868\u9762\u5f62\u6001\u7684\u54cd\u5e94\u5f0f\u9002\u5e94\uff0c\u4f7f\u5c40\u90e8\u81ea\u6cbb\u5728\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u5e76\u5b8c\u6210\u5de1\u68c0\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u5728\u5730\u4e0b\u77ff\u4e95\u4e2d\u4f7f\u7528\u56db\u8db3\u673a\u5668\u4eba\u7684\u5b9e\u9645\u90e8\u7f72\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24571", "abs": "https://arxiv.org/abs/2510.24571", "authors": ["Hongxu Zhao", "Guangyang Zeng", "Yunling Shao", "Tengfei Zhang", "Junfeng Wu"], "title": "Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots", "comment": null, "summary": "The calibration of extrinsic parameters and clock offsets between sensors for\nhigh-accuracy performance in underwater SLAM systems remains insufficiently\nexplored. Existing methods for Doppler Velocity Log (DVL) calibration are\neither constrained to specific sensor configurations or rely on oversimplified\nassumptions, and none jointly estimate translational extrinsics and time\noffsets. We propose a Unified Iterative Calibration (UIC) framework for general\nDVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a\nGaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC\nalternates between efficient GP-based motion state updates and gradient-based\ncalibration variable updates, supported by a provably statistically consistent\nsequential initialization scheme. The proposed UIC can be applied to IMU,\ncameras and other modalities as co-sensors. We release an open-source\nDVL-camera calibration toolbox. Beyond underwater applications, several aspects\nof UIC-such as the integration of GP priors for MAP-based calibration and the\ndesign of provably reliable initialization procedures-are broadly applicable to\nother multi-sensor calibration problems. Finally, simulations and real-world\ntests validate our approach.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u8fed\u4ee3\u6821\u51c6(UIC)\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u7684\u5916\u53c2\u548c\u65f6\u95f4\u504f\u79fb\u8054\u5408\u4f30\u8ba1\uff0c\u7279\u522b\u9488\u5bf9\u6c34\u4e0bSLAM\u4e2d\u7684DVL\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709DVL\u6821\u51c6\u65b9\u6cd5\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u4f20\u611f\u5668\u914d\u7f6e\uff0c\u8981\u4e48\u4f9d\u8d56\u8fc7\u5ea6\u7b80\u5316\u7684\u5047\u8bbe\uff0c\u4e14\u6ca1\u6709\u540c\u65f6\u4f30\u8ba1\u5e73\u79fb\u5916\u53c2\u548c\u65f6\u95f4\u504f\u79fb\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6700\u5927\u540e\u9a8c\u6982\u7387\u4f30\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u8fd0\u52a8\u5148\u9a8c\u8fdb\u884c\u9ad8\u4fdd\u771f\u8fd0\u52a8\u63d2\u503c\uff0c\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884cGP\u8fd0\u52a8\u72b6\u6001\u66f4\u65b0\u548c\u68af\u5ea6\u6821\u51c6\u53d8\u91cf\u66f4\u65b0\uff0c\u5e76\u8bbe\u8ba1\u7edf\u8ba1\u4e00\u81f4\u7684\u5e8f\u5217\u521d\u59cb\u5316\u65b9\u6848\u3002", "result": "UIC\u6846\u67b6\u53ef\u5e94\u7528\u4e8eIMU\u3001\u76f8\u673a\u7b49\u591a\u79cd\u4f20\u611f\u5668\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90DVL-\u76f8\u673a\u6821\u51c6\u5de5\u5177\u7bb1\u3002\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u6c34\u4e0b\u5e94\u7528\uff0c\u5176GP\u5148\u9a8c\u96c6\u6210\u548c\u53ef\u9760\u521d\u59cb\u5316\u7a0b\u5e8f\u7b49\u8bbe\u8ba1\u4e5f\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5176\u4ed6\u591a\u4f20\u611f\u5668\u6821\u51c6\u95ee\u9898\u3002"}}
{"id": "2510.24584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24584", "abs": "https://arxiv.org/abs/2510.24584", "authors": ["J\u00f8rgen Anker Olsen", "Lars R\u00f8nhaug Pettersen", "Kostas Alexis"], "title": "Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning", "comment": "8 pages", "summary": "This paper presents a curriculum-based reinforcement learning framework for\ntraining precise and high-performance jumping policies for the robot `Olympus'.\nSeparate policies are developed for vertical and horizontal jumps, leveraging a\nsimple yet effective strategy. First, we densify the inherently sparse jumping\nreward using the laws of projectile motion. Next, a reference state\ninitialization scheme is employed to accelerate the exploration of dynamic\njumping behaviors without reliance on reference trajectories. We also present a\nwalking policy that, when combined with the jumping policies, unlocks versatile\nand dynamic locomotion capabilities. Comprehensive testing validates walking on\nvaried terrain surfaces and jumping performance that exceeds previous works,\neffectively crossing the Sim2Real gap. Experimental validation demonstrates\nhorizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to\n1.0 m. Additionally, we show that with only minor modifications, the proposed\nmethod can be used to learn omnidirectional jumping.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u4eba'Olympus'\u7684\u7cbe\u786e\u9ad8\u6027\u80fd\u8df3\u8dc3\u7b56\u7565\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u79bb\u5782\u76f4\u548c\u6c34\u5e73\u8df3\u8dc3\u7b56\u7565\uff0c\u5229\u7528\u5f39\u9053\u8fd0\u52a8\u5b9a\u5f8b\u7a20\u5bc6\u5316\u7a00\u758f\u5956\u52b1\uff0c\u5e76\u91c7\u7528\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\u65b9\u6848\u52a0\u901f\u63a2\u7d22\u3002\u7ed3\u5408\u884c\u8d70\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u591a\u529f\u80fd\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u591a\u79cd\u5730\u5f62\u4e0a\u7684\u884c\u8d70\u548c\u8d85\u8d8a\u5148\u524d\u5de5\u4f5c\u7684\u8df3\u8dc3\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u9ad8\u6027\u80fd\u8df3\u8dc3\u7684\u673a\u5668\u4eba\u7b56\u7565\uff0c\u89e3\u51b3\u7a00\u758f\u5956\u52b1\u95ee\u9898\u548c\u52a8\u6001\u8df3\u8dc3\u884c\u4e3a\u7684\u63a2\u7d22\u6311\u6218\uff0c\u540c\u65f6\u5b9e\u73b0\u884c\u8d70\u4e0e\u8df3\u8dc3\u7684\u534f\u540c\u8fd0\u52a8\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5206\u79bb\u5782\u76f4\u548c\u6c34\u5e73\u8df3\u8dc3\u7b56\u7565\uff1b\u5229\u7528\u5f39\u9053\u8fd0\u52a8\u5b9a\u5f8b\u7a20\u5bc6\u5316\u7a00\u758f\u5956\u52b1\uff1b\u4f7f\u7528\u53c2\u8003\u72b6\u6001\u521d\u59cb\u5316\u65b9\u6848\u52a0\u901f\u52a8\u6001\u8df3\u8dc3\u884c\u4e3a\u63a2\u7d22\uff1b\u7ed3\u5408\u884c\u8d70\u7b56\u7565\u5b9e\u73b0\u591a\u529f\u80fd\u8fd0\u52a8\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u591a\u79cd\u5730\u5f62\u4e0a\u7684\u884c\u8d70\u80fd\u529b\uff0c\u6c34\u5e73\u8df3\u8dc3\u53ef\u8fbe1.25\u7c73\u4e14\u5177\u6709\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u5782\u76f4\u8df3\u8dc3\u53ef\u8fbe1.0\u7c73\u3002\u4ec5\u9700\u5c11\u91cf\u4fee\u6539\u5373\u53ef\u5b66\u4e60\u5168\u5411\u8df3\u8dc3\uff0c\u6709\u6548\u8de8\u8d8aSim2Real\u5dee\u8ddd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u8bad\u7ec3\u51fa\u7cbe\u786e\u9ad8\u6027\u80fd\u7684\u8df3\u8dc3\u7b56\u7565\uff0c\u7ed3\u5408\u884c\u8d70\u7b56\u7565\u5b9e\u73b0\u4e86\u591a\u529f\u80fd\u52a8\u6001\u8fd0\u52a8\u80fd\u529b\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5353\u8d8a\u7684\u8df3\u8dc3\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u52a8\u6001\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.24680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24680", "abs": "https://arxiv.org/abs/2510.24680", "authors": ["Zishuo Wang", "Joel Loo", "David Hsu"], "title": "Fare: Failure Resilience in Learned Visual Navigation Control", "comment": null, "summary": "While imitation learning (IL) enables effective visual navigation, IL\npolicies are prone to unpredictable failures in out-of-distribution (OOD)\nscenarios. We advance the notion of failure-resilient policies, which not only\ndetect failures but also recover from them automatically. Failure recognition\nthat identifies the factors causing failure is key to informing recovery: e.g.\npinpointing image regions triggering failure detections can provide cues to\nguide recovery. We present Fare, a framework to construct failure-resilient IL\npolicies, embedding OOD-detection and recognition in them without using\nexplicit failure data, and pairing them with recovery heuristics. Real-world\nexperiments show that Fare enables failure recovery across two different policy\narchitectures, enabling robust long-range navigation in complex environments.", "AI": {"tldr": "Fare\u6846\u67b6\u6784\u5efa\u4e86\u6545\u969c\u6062\u590d\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7OOD\u68c0\u6d4b\u548c\u8bc6\u522b\u5b9e\u73b0\u6545\u969c\u81ea\u52a8\u6062\u590d\uff0c\u65e0\u9700\u4f7f\u7528\u663e\u5f0f\u6545\u969c\u6570\u636e\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u5bb9\u6613\u53d1\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u6545\u969c\uff0c\u9700\u8981\u80fd\u591f\u68c0\u6d4b\u6545\u969c\u5e76\u81ea\u52a8\u6062\u590d\u7684\u6545\u969c\u6062\u590d\u7b56\u7565\u3002", "method": "\u63d0\u51faFare\u6846\u67b6\uff0c\u5728\u7b56\u7565\u4e2d\u5d4c\u5165OOD\u68c0\u6d4b\u548c\u8bc6\u522b\u529f\u80fd\uff0c\u65e0\u9700\u663e\u5f0f\u6545\u969c\u6570\u636e\uff0c\u5e76\u914d\u5907\u6062\u590d\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cFare\u80fd\u591f\u5728\u4e24\u79cd\u4e0d\u540c\u7b56\u7565\u67b6\u6784\u4e0a\u5b9e\u73b0\u6545\u969c\u6062\u590d\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u957f\u8ddd\u79bb\u5bfc\u822a\u3002", "conclusion": "Fare\u6846\u67b6\u6210\u529f\u6784\u5efa\u4e86\u6545\u969c\u6062\u590d\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5bfc\u822a\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24683", "abs": "https://arxiv.org/abs/2510.24683", "authors": ["Caleb Escobedo", "Nataliya Nechyporenko", "Shreyas Kadekodi", "Alessandro Roncone"], "title": "A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers", "comment": null, "summary": "Real-time control is an essential aspect of safe robot operation in the real\nworld with dynamic objects. We present a framework for the analysis of\nobject-aware controllers, methods for altering a robot's motion to anticipate\nand avoid possible collisions. This framework is focused on three design\nconsiderations: kinematics, motion profiles, and virtual constraints.\nAdditionally, the analysis in this work relies on verification of robot\nbehaviors using fundamental robot-obstacle experimental scenarios. To showcase\nthe effectiveness of our method we compare three representative object-aware\ncontrollers. The comparison uses metrics originating from the design\nconsiderations. From the analysis, we find that the design of object-aware\ncontrollers often lacks kinematic considerations, continuity of control points,\nand stability in movement profiles. We conclude that this framework can be used\nin the future to design, compare, and benchmark obstacle avoidance methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u66f2\u7ebf\u548c\u865a\u62df\u7ea6\u675f\u4e09\u4e2a\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u4eba-\u969c\u788d\u7269\u5b9e\u9a8c\u573a\u666f\u9a8c\u8bc1\u673a\u5668\u4eba\u884c\u4e3a\u3002", "motivation": "\u5b9e\u65f6\u63a7\u5236\u662f\u673a\u5668\u4eba\u5728\u52a8\u6001\u7269\u4f53\u73af\u5883\u4e2d\u5b89\u5168\u64cd\u4f5c\u7684\u5173\u952e\u65b9\u9762\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9884\u6d4b\u548c\u907f\u514d\u78b0\u649e\u7684\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u3002", "method": "\u5efa\u7acb\u5206\u6790\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u66f2\u7ebf\u548c\u865a\u62df\u7ea6\u675f\u4e09\u4e2a\u8bbe\u8ba1\u56e0\u7d20\uff0c\u4f7f\u7528\u57fa\u7840\u673a\u5668\u4eba-\u969c\u788d\u7269\u5b9e\u9a8c\u573a\u666f\u8fdb\u884c\u884c\u4e3a\u9a8c\u8bc1\uff0c\u5e76\u6bd4\u8f83\u4e09\u4e2a\u4ee3\u8868\u6027\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u3002", "result": "\u5206\u6790\u53d1\u73b0\u7269\u4f53\u611f\u77e5\u63a7\u5236\u5668\u7684\u8bbe\u8ba1\u901a\u5e38\u7f3a\u4e4f\u8fd0\u52a8\u5b66\u8003\u8651\u3001\u63a7\u5236\u70b9\u8fde\u7eed\u6027\u548c\u8fd0\u52a8\u66f2\u7ebf\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u7528\u4e8e\u672a\u6765\u8bbe\u8ba1\u3001\u6bd4\u8f83\u548c\u57fa\u51c6\u6d4b\u8bd5\u907f\u969c\u65b9\u6cd5\u3002"}}
{"id": "2510.24692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24692", "abs": "https://arxiv.org/abs/2510.24692", "authors": ["Jun Wang", "Ziyang Zhou", "Ardalan Kahak", "Suyi Li"], "title": "Embodying Physical Computing into Soft Robots", "comment": null, "summary": "Softening and onboarding computers and controllers is one of the final\nfrontiers in soft robotics towards their robustness and intelligence for\neveryday use. In this regard, embodying soft and physical computing presents\nexciting potential. Physical computing seeks to encode inputs into a mechanical\ncomputing kernel and leverage the internal interactions among this kernel's\nconstituent elements to compute the output. Moreover, such input-to-output\nevolution can be re-programmable. This perspective paper proposes a framework\nfor embodying physical computing into soft robots and discusses three unique\nstrategies in the literature: analog oscillators, physical reservoir computing,\nand physical algorithmic computing. These embodied computers enable the soft\nrobot to perform complex behaviors that would otherwise require CMOS-based\nelectronics -- including coordinated locomotion with obstacle avoidance,\npayload weight and orientation classification, and programmable operation based\non logical rules. This paper will detail the working principles of these\nembodied physical computing methods, survey the current state-of-the-art, and\npresent a perspective for future development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u7269\u7406\u8ba1\u7b97\u5d4c\u5165\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u63a2\u8ba8\u4e86\u4e09\u79cd\u5b9e\u73b0\u7b56\u7565\uff1a\u6a21\u62df\u632f\u8361\u5668\u3001\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\u548c\u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\uff0c\u4f7f\u8f6f\u4f53\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u884c\u4e3a\u800c\u65e0\u9700\u4f20\u7edf\u7535\u5b50\u5143\u4ef6\u3002", "motivation": "\u8f6f\u5316\u548c\u96c6\u6210\u8ba1\u7b97\u673a\u4e0e\u63a7\u5236\u5668\u662f\u8f6f\u4f53\u673a\u5668\u4eba\u5b9e\u73b0\u65e5\u5e38\u4f7f\u7528\u9c81\u68d2\u6027\u548c\u667a\u80fd\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u7269\u7406\u8ba1\u7b97\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002", "method": "\u63d0\u51fa\u7269\u7406\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u79cd\u5177\u4f53\u7b56\u7565\uff1a1) \u6a21\u62df\u632f\u8361\u5668\uff1b2) \u7269\u7406\u50a8\u5c42\u8ba1\u7b97\uff1b3) \u7269\u7406\u7b97\u6cd5\u8ba1\u7b97\uff0c\u5c06\u8f93\u5165\u7f16\u7801\u5230\u673a\u68b0\u8ba1\u7b97\u5185\u6838\u5e76\u5229\u7528\u5185\u90e8\u76f8\u4e92\u4f5c\u7528\u8ba1\u7b97\u8f93\u51fa\u3002", "result": "\u8fd9\u4e9b\u5d4c\u5165\u5f0f\u8ba1\u7b97\u673a\u4f7f\u8f6f\u4f53\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u590d\u6742\u884c\u4e3a\uff0c\u5305\u62ec\u5e26\u969c\u788d\u7269\u89c4\u907f\u7684\u534f\u8c03\u8fd0\u52a8\u3001\u6709\u6548\u8f7d\u8377\u91cd\u91cf\u548c\u65b9\u5411\u5206\u7c7b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u903b\u8f91\u89c4\u5219\u7684\u53ef\u7f16\u7a0b\u64cd\u4f5c\u3002", "conclusion": "\u672c\u6587\u8be6\u7ec6\u9610\u8ff0\u4e86\u8fd9\u4e9b\u5d4c\u5165\u5f0f\u7269\u7406\u8ba1\u7b97\u65b9\u6cd5\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u7efc\u8ff0\u4e86\u5f53\u524d\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u5c55\u671b\u3002"}}
