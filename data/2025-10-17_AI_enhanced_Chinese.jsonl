{"id": "2510.14000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14000", "abs": "https://arxiv.org/abs/2510.14000", "authors": ["Mingyang Jiang", "Yueyuan Li", "Jiaru Zhang", "Songan Zhang", "Ming Yang"], "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking", "comment": null, "summary": "The growing demand for parking has increased the need for automated parking\nplanning methods that can operate reliably in confined spaces. In restricted\nand complex environments, high-precision maneuvers are required to achieve a\nhigh success rate in planning, yet existing approaches often rely on explicit\naction modeling, which faces challenges when accurately modeling the optimal\naction distribution. In this paper, we propose DRIP, a diffusion-refined\nplanner anchored in reinforcement learning (RL) prior action distribution, in\nwhich an RL-pretrained policy provides prior action distributions to regularize\nthe diffusion training process. During the inference phase the denoising\nprocess refines these coarse priors into more precise action distributions. By\nsteering the denoising trajectory through the reinforcement learning prior\ndistribution during training, the diffusion model inherits a well-informed\ninitialization, resulting in more accurate action modeling, a higher planning\nsuccess rate, and reduced inference steps. We evaluate our approach across\nparking scenarios with varying degrees of spatial constraints. Experimental\nresults demonstrate that our method significantly improves planning performance\nin confined-space parking environments while maintaining strong generalization\nin common scenarios.", "AI": {"tldr": "\u63d0\u51faDRIP\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5148\u9a8c\u5206\u5e03\u548c\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u53d7\u9650\u7a7a\u95f4\u505c\u8f66\u89c4\u5212\uff0c\u63d0\u9ad8\u89c4\u5212\u6210\u529f\u7387\u548c\u63a8\u7406\u6548\u7387", "motivation": "\u505c\u8f66\u9700\u6c42\u589e\u957f\u9700\u8981\u81ea\u52a8\u5316\u505c\u8f66\u89c4\u5212\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u53d7\u9650\u590d\u6742\u73af\u5883\u4e2d\u9762\u4e34\u7cbe\u786e\u52a8\u4f5c\u5efa\u6a21\u7684\u6311\u6218", "method": "\u4f7f\u7528RL\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u4f9b\u5148\u9a8c\u52a8\u4f5c\u5206\u5e03\u6765\u6b63\u5219\u5316\u6269\u6563\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63a8\u7406\u65f6\u901a\u8fc7\u53bb\u566a\u8fc7\u7a0b\u5c06\u7c97\u7565\u5148\u9a8c\u7ec6\u5316\u4e3a\u7cbe\u786e\u52a8\u4f5c\u5206\u5e03", "result": "\u5728\u5177\u6709\u4e0d\u540c\u7a7a\u95f4\u7ea6\u675f\u7684\u505c\u8f66\u573a\u666f\u4e2d\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u53d7\u9650\u7a7a\u95f4\u505c\u8f66\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u89c4\u5212\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5e38\u89c1\u573a\u666f\u4e2d\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "DRIP\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408RL\u5148\u9a8c\u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u52a8\u4f5c\u5efa\u6a21\u3001\u66f4\u9ad8\u7684\u89c4\u5212\u6210\u529f\u7387\u548c\u66f4\u5c11\u7684\u63a8\u7406\u6b65\u9aa4"}}
{"id": "2510.14018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14018", "abs": "https://arxiv.org/abs/2510.14018", "authors": ["Adam Morris", "Timothy Pelham", "Edmund R. Hunt"], "title": "Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms", "comment": null, "summary": "This paper introduces a method for designing spatially intelligent robot\nswarm behaviors to localize concealed radio emitters. We use differential\nevolution to generate geometric patrol routes that localize unknown signals\nindependently of emitter parameters, a key challenge in electromagnetic\nsurveillance. Patrol shape and antenna type are shown to influence information\ngain, which in turn determines the effective triangulation coverage. We\nsimulate a four-robot swarm across eight configurations, assigning\npre-generated patrol routes based on a specified patrol shape and sensing\ncapability (antenna type: omnidirectional or directional). An emitter is placed\nwithin the map for each trial, with randomized position, transmission power and\nfrequency. Results show that omnidirectional localization success rates are\ndriven primarily by source location rather than signal properties, with\nfailures occurring most often when sources are placed in peripheral areas of\nthe map. Directional antennas are able to overcome this limitation due to their\nhigher gain and directivity, with an average detection success rate of 98.75%\ncompared to 80.25% for omnidirectional. Average localization errors range from\n1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional\nsensing; while directional sensing also benefits from shorter patrol edges.\nThese results demonstrate that a swarm's ability to predict electromagnetic\nphenomena is directly dependent on its physical interaction with the\nenvironment. Consequently, spatial intelligence, realized here through\noptimized patrol routes and antenna selection, is a critical design\nconsideration for effective robotic surveillance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u7a7a\u95f4\u667a\u80fd\u673a\u5668\u4eba\u7fa4\u4f53\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u4f4d\u9690\u85cf\u7684\u65e0\u7ebf\u7535\u53d1\u5c04\u5668\u3002\u901a\u8fc7\u5dee\u5206\u8fdb\u5316\u751f\u6210\u51e0\u4f55\u5de1\u903b\u8def\u7ebf\uff0c\u72ec\u7acb\u4e8e\u53d1\u5c04\u5668\u53c2\u6570\u8fdb\u884c\u4fe1\u53f7\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u7535\u78c1\u76d1\u89c6\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u7535\u78c1\u76d1\u89c6\u4e2d\u5b9a\u4f4d\u672a\u77e5\u4fe1\u53f7\u7684\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u53d1\u5c04\u5668\u53c2\u6570\u672a\u77e5\uff0c\u9700\u8981\u5f00\u53d1\u72ec\u7acb\u4e8e\u8fd9\u4e9b\u53c2\u6570\u7684\u5b9a\u4f4d\u65b9\u6cd5\u3002\u673a\u5668\u4eba\u7fa4\u4f53\u901a\u8fc7\u4f18\u5316\u5de1\u903b\u8def\u7ebf\u548c\u5929\u7ebf\u9009\u62e9\u6765\u5b9e\u73b0\u7a7a\u95f4\u667a\u80fd\uff0c\u63d0\u9ad8\u5b9a\u4f4d\u6548\u679c\u3002", "method": "\u4f7f\u7528\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\u751f\u6210\u51e0\u4f55\u5de1\u903b\u8def\u7ebf\uff0c\u6a21\u62df\u56db\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u516b\u79cd\u914d\u7f6e\u4e0b\u7684\u884c\u4e3a\u3002\u6839\u636e\u6307\u5b9a\u7684\u5de1\u903b\u5f62\u72b6\u548c\u611f\u77e5\u80fd\u529b\uff08\u5168\u5411\u6216\u5b9a\u5411\u5929\u7ebf\uff09\u5206\u914d\u9884\u751f\u6210\u7684\u5de1\u903b\u8def\u7ebf\u3002\u53d1\u5c04\u5668\u4f4d\u7f6e\u3001\u53d1\u5c04\u529f\u7387\u548c\u9891\u7387\u968f\u673a\u5316\u3002", "result": "\u5168\u5411\u5929\u7ebf\u5b9a\u4f4d\u6210\u529f\u7387\u4e3b\u8981\u53d7\u6e90\u4f4d\u7f6e\u5f71\u54cd\uff0c\u5916\u56f4\u533a\u57df\u5931\u8d25\u7387\u8f83\u9ad8\uff08\u6210\u529f\u738780.25%\uff09\u3002\u5b9a\u5411\u5929\u7ebf\u7531\u4e8e\u589e\u76ca\u548c\u65b9\u5411\u6027\u66f4\u9ad8\uff0c\u5e73\u5747\u68c0\u6d4b\u6210\u529f\u7387\u8fbe98.75%\u3002\u5b9a\u5411\u611f\u77e5\u7684\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\u4e3a1.01-1.30\u7c73\uff0c\u5168\u5411\u611f\u77e5\u4e3a1.67-1.90\u7c73\u3002", "conclusion": "\u7fa4\u4f53\u9884\u6d4b\u7535\u78c1\u73b0\u8c61\u7684\u80fd\u529b\u76f4\u63a5\u4f9d\u8d56\u4e8e\u5176\u4e0e\u73af\u5883\u7684\u7269\u7406\u4ea4\u4e92\u3002\u901a\u8fc7\u4f18\u5316\u5de1\u903b\u8def\u7ebf\u548c\u5929\u7ebf\u9009\u62e9\u5b9e\u73b0\u7684\u7a7a\u95f4\u667a\u80fd\u662f\u6709\u6548\u673a\u5668\u4eba\u76d1\u89c6\u7684\u5173\u952e\u8bbe\u8ba1\u8003\u8651\u56e0\u7d20\u3002"}}
{"id": "2510.14063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14063", "abs": "https://arxiv.org/abs/2510.14063", "authors": ["Nan Li", "Jiming Ren", "Haris Miller", "Samuel Coogan", "Karen M. Feigh", "Ye Zhao"], "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming", "comment": "16 pages, 11 figures, 4 tables", "summary": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing\nattention but remains challenging in terms of scalability, spatial reasoning,\nand adaptability in obstacle-rich environments. To address these challenges, we\npropose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for\nHeterogeneous Robot Teaming, which advances MATP by introducing a novel\nobstacle-aware strategy for task assignment. First, we develop an adaptive\nHalton sequence map, the first known application of Halton sampling with\nobstacle-aware adaptation in MATP, which adjusts sampling density based on\nobstacle distribution. Second, we propose a cluster-auction-selection framework\nthat integrates obstacle-aware clustering with weighted auctions and\nintra-cluster task selection. These mechanisms jointly enable effective\ncoordination among heterogeneous robots while maintaining scalability and\nnear-optimal allocation performance. In addition, our framework leverages an\nLLM to interpret human instructions and directly guide the planner in real\ntime. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in\ntask assignment quality, scalability, adaptability to dynamic changes, and\noverall execution performance compared to state-of-the-art MATP baselines. A\nproject website is available at https://llm-oath.github.io/.", "AI": {"tldr": "OATH\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u969c\u788d\u611f\u77e5\u4efb\u52a1\u5206\u914d\u548c\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7Halton\u5e8f\u5217\u5730\u56fe\u548c\u96c6\u7fa4-\u62cd\u5356-\u9009\u62e9\u6846\u67b6\uff0c\u5728\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u4e2d\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u63a5\u8fd1\u6700\u4f18\u7684\u4efb\u52a1\u5206\u914d\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u5206\u914d\u548c\u89c4\u5212\u5728\u53ef\u6269\u5c55\u6027\u3001\u7a7a\u95f4\u63a8\u7406\u548c\u969c\u788d\u4e30\u5bcc\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u81ea\u9002\u5e94Halton\u5e8f\u5217\u5730\u56fe\uff08\u9996\u6b21\u5728MATP\u4e2d\u5e94\u7528Halton\u91c7\u6837\u548c\u969c\u788d\u611f\u77e5\u9002\u5e94\uff09\uff0c\u63d0\u51fa\u96c6\u7fa4-\u62cd\u5356-\u9009\u62e9\u6846\u67b6\uff0c\u7ed3\u5408\u969c\u788d\u611f\u77e5\u805a\u7c7b\u3001\u52a0\u6743\u62cd\u5356\u548c\u96c6\u7fa4\u5185\u4efb\u52a1\u9009\u62e9\uff0c\u5e76\u5229\u7528LLM\u5b9e\u65f6\u89e3\u91ca\u4eba\u7c7b\u6307\u4ee4\u6307\u5bfc\u89c4\u5212\u5668\u3002", "result": "\u5728NVIDIA Isaac Sim\u4e2d\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684MATP\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4efb\u52a1\u5206\u914d\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u6027\u3001\u52a8\u6001\u53d8\u5316\u9002\u5e94\u6027\u548c\u6574\u4f53\u6267\u884c\u6027\u80fd\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "OATH\u901a\u8fc7\u521b\u65b0\u7684\u969c\u788d\u611f\u77e5\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u534f\u8c03\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u7684\u4efb\u52a1\u5206\u914d\u4e0e\u89c4\u5212\u3002"}}
{"id": "2510.14065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14065", "abs": "https://arxiv.org/abs/2510.14065", "authors": ["Gaoyuan Liu", "Joris de Winter", "Yuri Durodie", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "title": "Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning", "comment": null, "summary": "Task and motion planning (TAMP) for robotics manipulation necessitates\nlong-horizon reasoning involving versatile actions and skills. While\ndeterministic actions can be crafted by sampling or optimizing with certain\nconstraints, planning actions with uncertainty, i.e., probabilistic actions,\nremains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)\nexcels in acquiring versatile, yet short-horizon, manipulation skills that are\nrobust with uncertainties. In this letter, we design a method that integrates\nRL skills into TAMP pipelines. Besides the policy, a RL skill is defined with\ndata-driven logical components that enable the skill to be deployed by symbolic\nplanning. A plan refinement sub-routine is designed to further tackle the\ninevitable effect uncertainties. In the experiments, we compare our method with\nbaseline hierarchical planning from both TAMP and RL fields and illustrate the\nstrength of the method. The results show that by embedding RL skills, we extend\nthe capability of TAMP to domains with probabilistic skills, and improve the\nplanning efficiency compared to the previous methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5f3a\u5316\u5b66\u4e60\u6280\u80fd\u96c6\u6210\u5230\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u7ba1\u9053\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u903b\u8f91\u7ec4\u4ef6\u5b9a\u4e49RL\u6280\u80fd\uff0c\u4f7f\u5176\u80fd\u591f\u88ab\u7b26\u53f7\u89c4\u5212\u90e8\u7f72\uff0c\u5e76\u8bbe\u8ba1\u8ba1\u5212\u7ec6\u5316\u5b50\u7a0b\u5e8f\u6765\u5904\u7406\u6548\u679c\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u9700\u8981\u957f\u65f6\u7a0b\u63a8\u7406\uff0c\u4f46\u5904\u7406\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u6982\u7387\u52a8\u4f5c\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u800c\u5f3a\u5316\u5b66\u4e60\u64c5\u957f\u83b7\u53d6\u9c81\u68d2\u7684\u77ed\u65f6\u7a0b\u64cd\u4f5c\u6280\u80fd\uff0c\u4f46\u7f3a\u4e4f\u957f\u65f6\u7a0b\u89c4\u5212\u80fd\u529b\u3002\u56e0\u6b64\u9700\u8981\u5c06\u4e24\u8005\u7684\u4f18\u52bf\u7ed3\u5408\u8d77\u6765\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5c06RL\u6280\u80fd\u96c6\u6210\u5230TAMP\u7ba1\u9053\u7684\u65b9\u6cd5\uff1a1\uff09\u5b9a\u4e49RL\u6280\u80fd\u65f6\u5305\u542b\u6570\u636e\u9a71\u52a8\u7684\u903b\u8f91\u7ec4\u4ef6\uff0c\u4f7f\u6280\u80fd\u80fd\u591f\u88ab\u7b26\u53f7\u89c4\u5212\u90e8\u7f72\uff1b2\uff09\u8bbe\u8ba1\u8ba1\u5212\u7ec6\u5316\u5b50\u7a0b\u5e8f\u6765\u5904\u7406\u4e0d\u53ef\u907f\u514d\u7684\u6548\u679c\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5d4c\u5165RL\u6280\u80fd\uff0c\u6269\u5c55\u4e86TAMP\u5728\u6982\u7387\u6280\u80fd\u9886\u57df\u7684\u80fd\u529b\uff0c\u5e76\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad8\u4e86\u89c4\u5212\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u5f3a\u5316\u5b66\u4e60\u6280\u80fd\u4e0e\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u6982\u7387\u52a8\u4f5c\u89c4\u5212\u96be\u9898\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u89c4\u5212\u7684\u6548\u7387\u548c\u80fd\u529b\u3002"}}
{"id": "2510.14072", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14072", "abs": "https://arxiv.org/abs/2510.14072", "authors": ["Hemjyoti Das", "Christian Ott"], "title": "Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load", "comment": "Accepted for IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "In this work, we present a novel control approach based on partial feedback\nlinearization (PFL) for the stabilization of a suspended aerial platform with\nan attached load. Such systems are envisioned for various applications in\nconstruction sites involving cranes, such as the holding and transportation of\nheavy objects. Our proposed control approach considers the underactuation of\nthe whole system while utilizing its coupled dynamics for stabilization. We\ndemonstrate using numerical stability analysis that these coupled terms are\ncrucial for the stabilization of the complete system. We also carried out\nrobustness analysis of the proposed approach in the presence of external wind\ndisturbances, sensor noise, and uncertainties in system dynamics. As our\nenvisioned target application involves cranes in outdoor construction sites,\nour control approaches rely on only onboard sensors, thus making it suitable\nfor such applications. We carried out extensive simulation studies and\nexperimental tests to validate our proposed control approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u53cd\u9988\u7ebf\u6027\u5316\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a33\u5b9a\u5e26\u6709\u8d1f\u8f7d\u7684\u60ac\u6302\u5f0f\u7a7a\u4e2d\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u5efa\u7b51\u5de5\u5730\u7684\u8d77\u91cd\u673a\u5e94\u7528\u3002", "motivation": "\u5f00\u53d1\u9002\u7528\u4e8e\u5efa\u7b51\u5de5\u5730\u8d77\u91cd\u673a\u5e94\u7528\u7684\u7a33\u5b9a\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u91cd\u7269\u642c\u8fd0\u548c\u4fdd\u6301\uff0c\u540c\u65f6\u8003\u8651\u7cfb\u7edf\u6b20\u9a71\u52a8\u7279\u6027\u548c\u8026\u5408\u52a8\u529b\u5b66\u3002", "method": "\u4f7f\u7528\u90e8\u5206\u53cd\u9988\u7ebf\u6027\u5316\u63a7\u5236\u65b9\u6cd5\uff0c\u8003\u8651\u6574\u4e2a\u7cfb\u7edf\u7684\u6b20\u9a71\u52a8\u7279\u6027\uff0c\u5e76\u5229\u7528\u8026\u5408\u52a8\u529b\u5b66\u8fdb\u884c\u7a33\u5b9a\u63a7\u5236\u3002", "result": "\u6570\u503c\u7a33\u5b9a\u6027\u5206\u6790\u8868\u660e\u8026\u5408\u9879\u5bf9\u7cfb\u7edf\u7a33\u5b9a\u81f3\u5173\u91cd\u8981\uff0c\u5728\u5916\u90e8\u98ce\u6270\u52a8\u3001\u4f20\u611f\u5668\u566a\u58f0\u548c\u7cfb\u7edf\u52a8\u529b\u5b66\u4e0d\u786e\u5b9a\u6027\u4e0b\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u673a\u8f7d\u4f20\u611f\u5668\uff0c\u9002\u7528\u4e8e\u6237\u5916\u5efa\u7b51\u5de5\u5730\u5e94\u7528\uff0c\u901a\u8fc7\u5e7f\u6cdb\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u63a7\u5236\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.14234", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14234", "abs": "https://arxiv.org/abs/2510.14234", "authors": ["Ning Han", "Gu Gong", "Bin Zhang", "Yuexuan Xu", "Bohan Yang", "Yunhui Liu", "David Navarro-Alarcon"], "title": "Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space", "comment": null, "summary": "Manipulating three-dimensional (3D) deformable objects presents significant\nchallenges for robotic systems due to their infinite-dimensional state space\nand complex deformable dynamics. This paper proposes a novel model-free\napproach for shape control with constraints imposed on key points. Unlike\nexisting methods that rely on feature dimensionality reduction, the proposed\ncontroller leverages the coordinates of key points as the feature vector, which\nare extracted from the deformable object's point cloud using deep learning\nmethods. This approach not only reduces the dimensionality of the feature space\nbut also retains the spatial information of the object. By extracting key\npoints, the manipulation of deformable objects is simplified into a visual\nservoing problem, where the shape dynamics are described using a deformation\nJacobian matrix. To enhance control accuracy, a prescribed performance control\nmethod is developed by integrating barrier Lyapunov functions (BLF) to enforce\nconstraints on the key points. The stability of the closed-loop system is\nrigorously analyzed and verified using the Lyapunov method. Experimental\nresults further demonstrate the effectiveness and robustness of the proposed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u952e\u70b9\u7ea6\u675f\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e09\u7ef4\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u5f62\u72b6\u63a7\u5236\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u63d0\u53d6\u5173\u952e\u70b9\u5750\u6807\u4f5c\u4e3a\u7279\u5f81\u5411\u91cf\uff0c\u5c06\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u7eb5\u7b80\u5316\u4e3a\u89c6\u89c9\u4f3a\u670d\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u89c4\u5b9a\u6027\u80fd\u63a7\u5236\u65b9\u6cd5\u63d0\u9ad8\u63a7\u5236\u7cbe\u5ea6\u3002", "motivation": "\u4e09\u7ef4\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u64cd\u7eb5\u9762\u4e34\u65e0\u9650\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u590d\u6742\u53d8\u5f62\u52a8\u529b\u5b66\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7279\u5f81\u964d\u7ef4\u4f46\u53ef\u80fd\u4e22\u5931\u7a7a\u95f4\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u7559\u7a7a\u95f4\u4fe1\u606f\u540c\u65f6\u7b80\u5316\u63a7\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u4ece\u70b9\u4e91\u4e2d\u63d0\u53d6\u5173\u952e\u70b9\u5750\u6807\u4f5c\u4e3a\u7279\u5f81\u5411\u91cf\uff0c\u6784\u5efa\u53d8\u5f62\u96c5\u53ef\u6bd4\u77e9\u9635\u63cf\u8ff0\u5f62\u72b6\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u969c\u788d\u674e\u96c5\u666e\u8bfa\u592b\u51fd\u6570\u5f00\u53d1\u89c4\u5b9a\u6027\u80fd\u63a7\u5236\u65b9\u6cd5\uff0c\u786e\u4fdd\u5173\u952e\u70b9\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u674e\u96c5\u666e\u8bfa\u592b\u65b9\u6cd5\u4e25\u683c\u5206\u6790\u548c\u9a8c\u8bc1\u4e86\u95ed\u73af\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u7b80\u5316\u4e86\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u64cd\u7eb5\u95ee\u9898\uff0c\u5728\u4fdd\u7559\u7a7a\u95f4\u4fe1\u606f\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u5f62\u72b6\u63a7\u5236\uff0c\u4e3a\u4e09\u7ef4\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.14293", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14293", "abs": "https://arxiv.org/abs/2510.14293", "authors": ["Yushi Du", "Yixuan Li", "Baoxiong Jia", "Yutang Lin", "Pei Zhou", "Wei Liang", "Yanchao Yang", "Siyuan Huang"], "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying", "comment": null, "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5COLA\uff0c\u7528\u4e8e\u5b9e\u73b0\u4eba\u4e0e\u4eff\u4eba\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u987a\u4ece\u534f\u4f5c\u642c\u8fd0\uff0c\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u6216\u590d\u6742\u4ea4\u4e92\u6a21\u578b\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u3001\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u867d\u7136\u673a\u5668\u4eba\u624b\u81c2\u7684\u987a\u4ece\u4eba\u673a\u534f\u4f5c\u5df2\u5e7f\u6cdb\u53d1\u5c55\uff0c\u4f46\u7531\u4e8e\u4eff\u4eba\u673a\u5668\u4eba\u590d\u6742\u7684\u5168\u8eab\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u987a\u4ece\u7684\u4eba-\u4eff\u4eba\u673a\u5668\u4eba\u534f\u4f5c\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u4ec5\u672c\u4f53\u611f\u89c9\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5COLA\uff0c\u5728\u5355\u4e2a\u7b56\u7565\u4e2d\u7ed3\u5408\u9886\u5bfc\u8005\u548c\u8ddf\u968f\u8005\u884c\u4e3a\uff0c\u5728\u95ed\u73af\u73af\u5883\u4e2d\u8bad\u7ec3\u4ee5\u9690\u5f0f\u9884\u6d4b\u7269\u4f53\u8fd0\u52a8\u6a21\u5f0f\u548c\u4eba\u7c7b\u610f\u56fe\uff0c\u901a\u8fc7\u534f\u8c03\u8f68\u8ff9\u89c4\u5212\u5b9e\u73b0\u987a\u4ece\u534f\u4f5c\u4ee5\u4fdd\u6301\u8d1f\u8f7d\u5e73\u8861\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u51cf\u5c11\u4eba\u7c7b\u52aa\u529b24.7%\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u4f53\u7a33\u5b9a\u6027\uff1b\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u4e0d\u540c\u7269\u4f53\u7c7b\u578b\u548c\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u9c81\u68d2\u534f\u4f5c\u642c\u8fd0\uff1b23\u540d\u53c2\u4e0e\u8005\u7684\u4eba\u4f53\u7814\u7a76\u786e\u8ba4\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5e73\u5747\u63d0\u534727.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u6216\u590d\u6742\u4ea4\u4e92\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u987a\u4ece\u7684\u4eba-\u4eff\u4eba\u673a\u5668\u4eba\u534f\u4f5c\u642c\u8fd0\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.14300", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14300", "abs": "https://arxiv.org/abs/2510.14300", "authors": ["Weijie Shen", "Yitian Liu", "Yuhao Wu", "Zhixuan Liang", "Sijia Gu", "Dehui Wang", "Tian Nian", "Lei Xu", "Yusen Qin", "Jiangmiao Pang", "Xinping Guan", "Xiaokang Yang", "Yao Mu"], "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning", "comment": null, "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.", "AI": {"tldr": "AdaMoE\u662f\u4e00\u79cd\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u901a\u8fc7\u7ee7\u627f\u9884\u8bad\u7ec3\u7684\u5bc6\u96c6VLA\u6a21\u578b\u6743\u91cd\uff0c\u5e76\u5c06\u524d\u9988\u5c42\u66ff\u6362\u4e3a\u7a00\u758f\u6fc0\u6d3b\u7684MoE\u5c42\u6765\u6269\u5c55\u52a8\u4f5c\u4e13\u5bb6\u3002\u5b83\u91c7\u7528\u89e3\u8026\u6280\u672f\uff0c\u901a\u8fc7\u72ec\u7acb\u7684\u6bd4\u4f8b\u9002\u914d\u5668\u5c06\u4e13\u5bb6\u9009\u62e9\u4e0e\u4e13\u5bb6\u6743\u91cd\u5206\u79bb\uff0c\u5b9e\u73b0\u534f\u4f5c\u5f0f\u4e13\u5bb6\u5229\u7528\u800c\u975e\u8d62\u5bb6\u901a\u5403\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u6269\u5c55\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a(1)\u4ece\u5934\u8bad\u7ec3\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u96c6\uff0c\u800c\u673a\u5668\u4eba\u6570\u636e\u7a00\u7f3a\uff0c\u9700\u8981\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6743\u91cd\uff1b(2)\u5b9e\u65f6\u63a7\u5236\u9700\u8981\u5728\u6a21\u578b\u5bb9\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5e73\u8861\u3002", "method": "\u63d0\u51faAdaMoE\u67b6\u6784\uff0c\u7ee7\u627f\u9884\u8bad\u7ec3VLA\u6a21\u578b\u6743\u91cd\uff0c\u5c06\u524d\u9988\u5c42\u66ff\u6362\u4e3a\u7a00\u758f\u6fc0\u6d3b\u7684MoE\u5c42\uff0c\u4f7f\u7528\u89e3\u8026\u6280\u672f\u901a\u8fc7\u72ec\u7acb\u6bd4\u4f8b\u9002\u914d\u5668\u5206\u79bb\u4e13\u5bb6\u9009\u62e9\u548c\u6743\u91cd\u5206\u914d\u3002", "result": "\u5728\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff1aLIBERO\u4e0a\u63d0\u53471.8%\uff0cRoboTwin\u4e0a\u63d0\u53479.3%\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u534721.5%\u3002", "conclusion": "\u901a\u8fc7\u534f\u4f5c\u5f0f\u4e13\u5bb6\u5229\u7528\u800c\u975e\u4e13\u5bb6\u5784\u65ad\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86AdaMoE\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6709\u6548\u6027\u3002"}}
{"id": "2510.14338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14338", "abs": "https://arxiv.org/abs/2510.14338", "authors": ["Yuanhong Zeng", "Anushri Dixit"], "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion", "comment": null, "summary": "In this work, we study risk-aware reinforcement learning for quadrupedal\nlocomotion. Our approach trains a family of risk-conditioned policies using a\nConditional Value-at-Risk (CVaR) constrained policy optimization technique that\nprovides improved stability and sample efficiency. At deployment, we adaptively\nselect the best performing policy from the family of policies using a\nmulti-armed bandit framework that uses only observed episodic returns, without\nany privileged environment information, and adapts to unknown conditions on the\nfly. Hence, we train quadrupedal locomotion policies at various levels of\nrobustness using CVaR and adaptively select the desired level of robustness\nonline to ensure performance in unknown environments. We evaluate our method in\nsimulation across eight unseen settings (by changing dynamics, contacts,\nsensing noise, and terrain) and on a Unitree Go2 robot in previously unseen\nterrains. Our risk-aware policy attains nearly twice the mean and tail\nperformance in unseen environments compared to other baselines and our\nbandit-based adaptation selects the best-performing risk-aware policy in\nunknown terrain within two minutes of operation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u98ce\u9669\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7CVaR\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\u98ce\u9669\u6761\u4ef6\u7b56\u7565\u65cf\uff0c\u5e76\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u5728\u7ebf\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u7b56\u7565\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8fd0\u52a8\u65f6\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u98ce\u9669\u4ef7\u503c(CVaR)\u7ea6\u675f\u7b56\u7565\u4f18\u5316\u6280\u672f\u8bad\u7ec3\u98ce\u9669\u6761\u4ef6\u7b56\u7565\u65cf\uff0c\u90e8\u7f72\u65f6\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u56de\u5408\u56de\u62a5\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f73\u7b56\u7565\u3002", "result": "\u57288\u4e2a\u672a\u89c1\u8fc7\u7684\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u98ce\u9669\u611f\u77e5\u7b56\u7565\u7684\u5e73\u5747\u6027\u80fd\u548c\u5c3e\u90e8\u6027\u80fd\u6bd4\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u8fd1\u4e24\u500d\uff1b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\uff0c\u8001\u864e\u673a\u81ea\u9002\u5e94\u65b9\u6cd5\u80fd\u57282\u5206\u949f\u5185\u9009\u62e9\u51fa\u5728\u672a\u77e5\u5730\u5f62\u4e2d\u8868\u73b0\u6700\u4f73\u7684\u98ce\u9669\u611f\u77e5\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u5728\u7ebf\u7b56\u7565\u9009\u62e9\u5b9e\u73b0\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2510.14357", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14357", "abs": "https://arxiv.org/abs/2510.14357", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation", "comment": null, "summary": "Agricultural robots are emerging as powerful assistants across a wide range\nof agricultural tasks, nevertheless, still heavily rely on manual operation or\nfixed rail systems for movement. The AgriVLN method and the A2A benchmark\npioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural\ndomain, enabling robots to navigate to the target positions following the\nnatural language instructions. In practical agricultural scenarios, navigation\ninstructions often repeatedly occur, yet AgriVLN treat each instruction as an\nindependent episode, overlooking the potential of past experiences to provide\nspatial context for subsequent ones. To bridge this gap, we propose the method\nof Spatial Understanding Memory for Agricultural Vision-and-Language Navigation\n(SUM-AgriVLN), in which the SUM module employs spatial understanding and save\nspatial memory through 3D reconstruction and representation. When evaluated on\nthe A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47\nto 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,\ndemonstrating the state-of-the-art performance in the agricultural domain.\nCode: https://github.com/AlexTraveling/SUM-AgriVLN.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSUM-AgriVLN\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u7406\u89e3\u8bb0\u5fc6\u6a21\u5757\u6539\u8fdb\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u8fbe\u52300.54\uff0c\u5bfc\u822a\u8bef\u5dee2.93\u7c73\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\uff08\u5982AgriVLN\uff09\u5c06\u6bcf\u4e2a\u5bfc\u822a\u6307\u4ee4\u89c6\u4e3a\u72ec\u7acb\u4e8b\u4ef6\uff0c\u5ffd\u7565\u4e86\u91cd\u590d\u51fa\u73b0\u7684\u5bfc\u822a\u6307\u4ee4\u53ef\u4ee5\u5229\u7528\u8fc7\u5f80\u7ecf\u9a8c\u63d0\u4f9b\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff0c\u8fd9\u9650\u5236\u4e86\u5bfc\u822a\u6027\u80fd\u7684\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u7406\u89e3\u8bb0\u5fc6\u6a21\u5757\uff08SUM\uff09\uff0c\u901a\u8fc73D\u91cd\u5efa\u548c\u8868\u793a\u6765\u4fdd\u5b58\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u5229\u7528\u8fc7\u5f80\u5bfc\u822a\u7ecf\u9a8c\u4e3a\u540e\u7eed\u6307\u4ee4\u63d0\u4f9b\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728A2A\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6210\u529f\u7387\u8fbe\u52300.54\uff08\u76f8\u6bd4\u57fa\u7ebf0.47\u63d0\u5347\uff09\uff0c\u5bfc\u822a\u8bef\u5dee\u4e3a2.93\u7c73\uff08\u76f8\u6bd4\u57fa\u7ebf2.91\u7c73\u7565\u6709\u589e\u52a0\uff09\uff0c\u5728\u519c\u4e1a\u9886\u57df\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u7a7a\u95f4\u7406\u89e3\u8bb0\u5fc6\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u519c\u4e1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u8fc7\u5f80\u5bfc\u822a\u7ecf\u9a8c\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.14414", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14414", "abs": "https://arxiv.org/abs/2510.14414", "authors": ["Baris Baysal", "Omid Arfaie", "Ramazan Unal"], "title": "RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit", "comment": null, "summary": "This study presents a powered transtibial prosthesis with complete push-off\nassistance, RoboANKLE. The design aims to fulfill specific requirements, such\nas a sufficient range of motion (RoM) while providing the necessary torque for\nachieving natural ankle motion in daily activities. Addressing the challenges\nfaced in designing active transtibial prostheses, such as maintaining energetic\nautonomy and minimizing weight, is vital for the study. With this aim, we try\nto imitate the human ankle by providing extensive push-off assistance to\nachieve a natural-like torque profile. Thus, Energy Store and Extended Release\nmechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.\nKinematic and kinetic analyses are carried out to determine the design\nparameters and assess the design performance. Subsequently, a Computer-Aided\nDesign (CAD) model is built and used in comprehensive dynamic and structural\nanalyses. These analyses are used for the design performance evaluation and\ndetermine the forces and torques applied to the prosthesis, which aids in\noptimizing the design for minimal weight via structural analysis and topology\noptimization. The design of the prototype is then finalized and manufactured\nfor experimental evaluation to validate the design and functionality. The\nprototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.\nThe Functional evaluations of the RoboANKLE revealed that it is capable of\nachieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,\nThanks to the implemented mechanisms, the results show that RoboANKLE can\ngenerate 57% higher than the required torque for natural walking. The result of\nthe power generation capacity of the RoboANKLE is 10% more than the natural\npower during the gait cycle.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aRoboANKLE\u7684\u4e3b\u52a8\u5f0f\u80eb\u9aa8\u5047\u80a2\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u80fd\u91cf\u5b58\u50a8\u548c\u91ca\u653e\u673a\u5236\uff08ESER\uff09\u4ee5\u53ca\u989d\u5916\u80fd\u91cf\u5b58\u50a8\uff08EES\uff09\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u63a8\u8fdb\u8f85\u52a9\u529f\u80fd\u3002\u8be5\u5047\u80a2\u91cd\u91cf\u4e3a1.92kg\uff0c\u80fd\u591f\u4ee595%\u7684\u51c6\u786e\u5ea6\u8fbe\u5230\u81ea\u7136\u6700\u5927\u80cc\u5c48\u89d2\u5ea6\uff0c\u4ea7\u751f\u6bd4\u81ea\u7136\u884c\u8d70\u6240\u9700\u626d\u77e9\u9ad857%\u7684\u626d\u77e9\uff0c\u529f\u7387\u751f\u6210\u80fd\u529b\u6bd4\u81ea\u7136\u6b65\u6001\u5468\u671f\u9ad810%\u3002", "motivation": "\u8bbe\u8ba1\u4e3b\u52a8\u5f0f\u80eb\u9aa8\u5047\u80a2\u9762\u4e34\u4fdd\u6301\u80fd\u91cf\u81ea\u4e3b\u6027\u548c\u6700\u5c0f\u5316\u91cd\u91cf\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8e1d\u5173\u8282\uff0c\u63d0\u4f9b\u5e7f\u6cdb\u7684\u63a8\u8fdb\u8f85\u52a9\u6765\u5b9e\u73b0\u81ea\u7136\u7684\u626d\u77e9\u5206\u5e03\uff0c\u4ece\u800c\u89e3\u51b3\u8fd9\u4e9b\u8bbe\u8ba1\u96be\u9898\u3002", "method": "\u91c7\u7528\u80fd\u91cf\u5b58\u50a8\u548c\u6269\u5c55\u91ca\u653e\u673a\u5236\uff08ESER\uff09\u4e0e\u65b0\u578b\u989d\u5916\u80fd\u91cf\u5b58\u50a8\uff08EES\uff09\u673a\u5236\u3002\u8fdb\u884c\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u5206\u6790\u786e\u5b9a\u8bbe\u8ba1\u53c2\u6570\uff0c\u5efa\u7acbCAD\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u548c\u7ed3\u6784\u5206\u6790\uff0c\u901a\u8fc7\u7ed3\u6784\u5206\u6790\u548c\u62d3\u6251\u4f18\u5316\u5b9e\u73b0\u6700\u5c0f\u91cd\u91cf\u8bbe\u8ba1\uff0c\u6700\u7ec8\u5236\u9020\u539f\u578b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "RoboANKLE\u539f\u578b\u91cd\u91cf\u4e3a1.92kg\uff0c\u5c3a\u5bf8\u4e3a261x107x420mm\u3002\u529f\u80fd\u8bc4\u4f30\u663e\u793a\uff1a\u80fd\u591f\u4ee595%\u7684\u51c6\u786e\u5ea6\u8fbe\u5230\u81ea\u7136\u6700\u5927\u80cc\u5c48\u89d2\u5ea6\uff1b\u80fd\u591f\u4ea7\u751f\u6bd4\u81ea\u7136\u884c\u8d70\u6240\u9700\u626d\u77e9\u9ad857%\u7684\u626d\u77e9\uff1b\u529f\u7387\u751f\u6210\u80fd\u529b\u6bd4\u81ea\u7136\u6b65\u6001\u5468\u671f\u9ad810%\u3002", "conclusion": "RoboANKLE\u6210\u529f\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u63a8\u8fdb\u8f85\u52a9\u529f\u80fd\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u80fd\u91cf\u5b58\u50a8\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5047\u80a2\u7684\u6027\u80fd\uff0c\u5728\u8fd0\u52a8\u8303\u56f4\u3001\u626d\u77e9\u8f93\u51fa\u548c\u529f\u7387\u751f\u6210\u65b9\u9762\u90fd\u4f18\u4e8e\u81ea\u7136\u8e1d\u5173\u8282\u7684\u8981\u6c42\uff0c\u4e3a\u4e3b\u52a8\u5f0f\u80eb\u9aa8\u5047\u80a2\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.14454", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.14454", "abs": "https://arxiv.org/abs/2510.14454", "authors": ["Tao Huang", "Huayi Wang", "Junli Ren", "Kangning Yin", "Zirui Wang", "Xiao Chen", "Feiyu Jia", "Wentao Zhang", "Junfeng Long", "Jingbo Wang", "Jiangmiao Pang"], "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking", "comment": "9 pages", "summary": "Humanoid robots are envisioned to adapt demonstrated motions to diverse\nreal-world conditions while accurately preserving motion patterns. Existing\nmotion prior approaches enable well adaptability with a few motions but often\nsacrifice imitation accuracy, whereas motion-tracking methods achieve accurate\nimitation yet require many training motions and a test-time target motion to\nadapt. To combine their strengths, we introduce AdaMimic, a novel motion\ntracking algorithm that enables adaptable humanoid control from a single\nreference motion. To reduce data dependence while ensuring adaptability, our\nmethod first creates an augmented dataset by sparsifying the single reference\nmotion into keyframes and applying light editing with minimal physical\nassumptions. A policy is then initialized by tracking these sparse keyframes to\ngenerate dense intermediate motions, and adapters are subsequently trained to\nadjust tracking speed and refine low-level actions based on the adjustment,\nenabling flexible time warping that further improves imitation accuracy and\nadaptability. We validate these significant improvements in our approach in\nboth simulation and the real-world Unitree G1 humanoid robot in multiple tasks\nacross a wide range of adaptation conditions. Videos and code are available at\nhttps://taohuang13.github.io/adamimic.github.io/.", "AI": {"tldr": "AdaMimic\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8fd0\u52a8\u8ddf\u8e2a\u7b97\u6cd5\uff0c\u80fd\u591f\u4ece\u5355\u4e00\u53c2\u8003\u8fd0\u52a8\u5b9e\u73b0\u53ef\u9002\u5e94\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\uff0c\u901a\u8fc7\u7a00\u758f\u5173\u952e\u5e27\u548c\u8f7b\u91cf\u7f16\u8f91\u589e\u5f3a\u6570\u636e\uff0c\u7ed3\u5408\u65f6\u95f4\u626d\u66f2\u6280\u672f\u63d0\u9ad8\u6a21\u4eff\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8fd0\u52a8\u5148\u9a8c\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u6a21\u4eff\u7cbe\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u7ed3\u5408\u8fd0\u52a8\u8ddf\u8e2a\u65b9\u6cd5\u7684\u7cbe\u786e\u6a21\u4eff\u548c\u8fd0\u52a8\u5148\u9a8c\u65b9\u6cd5\u7684\u9002\u5e94\u6027\u4f18\u52bf\u3002", "method": "\u9996\u5148\u901a\u8fc7\u7a00\u758f\u5316\u5355\u4e00\u53c2\u8003\u8fd0\u52a8\u521b\u5efa\u5173\u952e\u5e27\u6570\u636e\u96c6\uff0c\u5e94\u7528\u8f7b\u91cf\u7f16\u8f91\uff1b\u7136\u540e\u8bad\u7ec3\u7b56\u7565\u8ddf\u8e2a\u7a00\u758f\u5173\u952e\u5e27\u751f\u6210\u5bc6\u96c6\u4e2d\u95f4\u8fd0\u52a8\uff1b\u6700\u540e\u8bad\u7ec3\u9002\u914d\u5668\u8c03\u6574\u8ddf\u8e2a\u901f\u5ea6\u548c\u7ec6\u5316\u4f4e\u7ea7\u52a8\u4f5c\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u65f6\u95f4\u626d\u66f2\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754cUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0c\u5728\u5e7f\u6cdb\u7684\u9002\u5e94\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "AdaMimic\u80fd\u591f\u4ece\u5355\u4e00\u53c2\u8003\u8fd0\u52a8\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u8fd0\u52a8\u6a21\u4eff\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u591a\u6837\u5316\u73b0\u5b9e\u6761\u4ef6\u7684\u826f\u597d\u9002\u5e94\u6027\u3002"}}
{"id": "2510.14467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14467", "abs": "https://arxiv.org/abs/2510.14467", "authors": ["Shang-Fu Chen", "Co Yong", "Shao-Hua Sun"], "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion Models", "comment": "Published in IEEE Transactions on Neural Networks and Learning\n  Systems (TNNLS)", "summary": "Imitation learning (IL) aims to learn a policy from expert demonstrations and\nhas been applied to various applications. By learning from the expert policy,\nIL methods do not require environmental interactions or reward signals.\nHowever, most existing imitation learning algorithms assume perfect expert\ndemonstrations, but expert demonstrations often contain imperfections caused by\nerrors from human experts or sensor/control system inaccuracies. To address the\nabove problems, this work proposes a filter-and-restore framework to best\nleverage expert demonstrations with inherent noise. Our proposed method first\nfilters clean samples from the demonstrations and then learns conditional\ndiffusion models to recover the noisy ones. We evaluate our proposed framework\nand existing methods in various domains, including robot arm manipulation,\ndexterous manipulation, and locomotion. The experiment results show that our\nproposed framework consistently outperforms existing methods across all the\ntasks. Ablation studies further validate the effectiveness of each component\nand demonstrate the framework's robustness to different noise types and levels.\nThese results confirm the practical applicability of our framework to noisy\noffline demonstration data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fc7\u6ee4-\u6062\u590d\u6846\u67b6\u6765\u5904\u7406\u6a21\u4eff\u5b66\u4e60\u4e2d\u5b58\u5728\u566a\u58f0\u7684\u4e13\u5bb6\u6f14\u793a\u6570\u636e\uff0c\u901a\u8fc7\u5148\u8fc7\u6ee4\u5e72\u51c0\u6837\u672c\u518d\u5b66\u4e60\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6765\u6062\u590d\u566a\u58f0\u6837\u672c\uff0c\u5728\u591a\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u4e13\u5bb6\u6f14\u793a\u662f\u5b8c\u7f8e\u7684\uff0c\u4f46\u5b9e\u9645\u4e2d\u4e13\u5bb6\u6f14\u793a\u5f80\u5f80\u5305\u542b\u4eba\u7c7b\u9519\u8bef\u6216\u4f20\u611f\u5668/\u63a7\u5236\u7cfb\u7edf\u4e0d\u51c6\u786e\u5bfc\u81f4\u7684\u566a\u58f0\uff0c\u9700\u8981\u5904\u7406\u4e0d\u5b8c\u7f8e\u6f14\u793a\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9996\u5148\u4ece\u6f14\u793a\u6570\u636e\u4e2d\u8fc7\u6ee4\u51fa\u5e72\u51c0\u6837\u672c\uff0c\u7136\u540e\u5b66\u4e60\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6765\u6062\u590d\u566a\u58f0\u6837\u672c\u3002", "result": "\u5728\u673a\u5668\u4eba\u624b\u81c2\u64cd\u4f5c\u3001\u7075\u5de7\u64cd\u4f5c\u548c\u8fd0\u52a8\u7b49\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u6709\u6548\u6027\u53ca\u5bf9\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u548c\u6c34\u5e73\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5bf9\u566a\u58f0\u79bb\u7ebf\u6f14\u793a\u6570\u636e\u5177\u6709\u5b9e\u9645\u9002\u7528\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u4e0d\u5b8c\u7f8e\u7684\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u3002"}}
{"id": "2510.14546", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14546", "abs": "https://arxiv.org/abs/2510.14546", "authors": ["Matti Pekkanen", "Francesco Verdoja", "Ville Kyrki"], "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps", "comment": "Submitted to ICRA 2026", "summary": "Embeddings from Visual-Language Models are increasingly utilized to represent\nsemantics in robotic maps, offering an open-vocabulary scene understanding that\nsurpasses traditional, limited labels. Embeddings enable on-demand querying by\ncomparing embedded user text prompts to map embeddings via a similarity metric.\nThe key challenge in performing the task indicated in a query is that the robot\nmust determine the parts of the environment relevant to the query.\n  This paper proposes a solution to this challenge. We leverage\nnatural-language synonyms and antonyms associated with the query within the\nembedding space, applying heuristics to estimate the language space relevant to\nthe query, and use that to train a classifier to partition the environment into\nmatches and non-matches. We evaluate our method through extensive experiments,\nquerying both maps and standard image benchmarks. The results demonstrate\nincreased queryability of maps and images. Our querying technique is agnostic\nto the representation and encoder used, and requires limited training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u8fdb\u884c\u673a\u5668\u4eba\u5730\u56fe\u8bed\u4e49\u67e5\u8be2\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u540c\u4e49\u8bcd\u548c\u53cd\u4e49\u8bcd\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5173\u7cfb\u6765\u4f30\u8ba1\u67e5\u8be2\u76f8\u5173\u7684\u8bed\u8a00\u7a7a\u95f4\uff0c\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\u5c06\u73af\u5883\u5212\u5206\u4e3a\u5339\u914d\u548c\u975e\u5339\u914d\u533a\u57df\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u63d0\u4f9b\u4e86\u8d85\u8d8a\u4f20\u7edf\u6709\u9650\u6807\u7b7e\u7684\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u7406\u89e3\uff0c\u4f46\u673a\u5668\u4eba\u9700\u8981\u786e\u5b9a\u73af\u5883\u4e2d\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u90e8\u5206\uff0c\u8fd9\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u5229\u7528\u67e5\u8be2\u76f8\u5173\u7684\u81ea\u7136\u8bed\u8a00\u540c\u4e49\u8bcd\u548c\u53cd\u4e49\u8bcd\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5173\u7cfb\uff0c\u5e94\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u4f30\u8ba1\u67e5\u8be2\u76f8\u5173\u7684\u8bed\u8a00\u7a7a\u95f4\uff0c\u5e76\u8bad\u7ec3\u5206\u7c7b\u5668\u6765\u5212\u5206\u73af\u5883\u533a\u57df\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5728\u67e5\u8be2\u5730\u56fe\u548c\u6807\u51c6\u56fe\u50cf\u57fa\u51c6\u65f6\uff0c\u7ed3\u679c\u663e\u793a\u5730\u56fe\u548c\u56fe\u50cf\u7684\u53ef\u67e5\u8be2\u6027\u5f97\u5230\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u67e5\u8be2\u6280\u672f\u5bf9\u8868\u793a\u548c\u7f16\u7801\u5668\u5177\u6709\u4e0d\u53ef\u77e5\u6027\uff0c\u4e14\u9700\u8981\u6709\u9650\u7684\u8bad\u7ec3\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5730\u56fe\u7684\u8bed\u4e49\u67e5\u8be2\u80fd\u529b\u3002"}}
{"id": "2510.14584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14584", "abs": "https://arxiv.org/abs/2510.14584", "authors": ["Benno Wingender", "Nils Dengler", "Rohit Menon", "Sicong Pan", "Maren Bennewitz"], "title": "A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning", "comment": null, "summary": "To reliably pick and place unknown objects under real-world sensing noise\nremains a challenging task, as existing methods rely on strong object priors\n(e.g., CAD models), or planar-support assumptions, limiting generalization and\nunified reasoning between grasping and placing. In this work, we introduce a\ngeneralized placeability metric that evaluates placement poses directly from\nnoisy point clouds, without any shape priors. The metric jointly scores\nstability, graspability, and clearance. From raw geometry, we extract the\nsupport surfaces of the object to generate diverse candidates for\nmulti-orientation placement and sample contacts that satisfy collision and\nstability constraints. By conditioning grasp scores on each candidate\nplacement, our proposed method enables model-free unified pick-and-place\nreasoning and selects grasp-place pairs that lead to stable, collision-free\nplacements. On unseen real objects and non-planar object supports, our metric\ndelivers CAD-comparable accuracy in predicting stability loss and generally\nproduces more physically plausible placements than learning-based predictors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u566a\u58f0\u70b9\u4e91\u7684\u5e7f\u4e49\u53ef\u653e\u7f6e\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u65e0\u9700\u7269\u4f53\u5148\u9a8c\u77e5\u8bc6\u5373\u53ef\u8bc4\u4f30\u653e\u7f6e\u59ff\u6001\uff0c\u5b9e\u73b0\u4e86\u65e0\u6a21\u578b\u7edf\u4e00\u7684\u6293\u53d6-\u653e\u7f6e\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5f3a\u7269\u4f53\u5148\u9a8c\u6216\u5e73\u9762\u652f\u6491\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6293\u53d6\u653e\u7f6e\u7684\u7edf\u4e00\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u672a\u77e5\u7269\u4f53\u548c\u771f\u5b9e\u4e16\u754c\u611f\u77e5\u566a\u58f0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4ece\u539f\u59cb\u51e0\u4f55\u4e2d\u63d0\u53d6\u7269\u4f53\u652f\u6491\u8868\u9762\uff0c\u751f\u6210\u591a\u65b9\u5411\u653e\u7f6e\u5019\u9009\u4f4d\u59ff\uff0c\u91c7\u6837\u6ee1\u8db3\u78b0\u649e\u548c\u7a33\u5b9a\u6027\u7ea6\u675f\u7684\u63a5\u89e6\u70b9\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u6293\u53d6\u8bc4\u5206\u5b9e\u73b0\u7edf\u4e00\u7684\u6293\u53d6-\u653e\u7f6e\u63a8\u7406\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u7269\u4f53\u548c\u975e\u5e73\u9762\u652f\u6491\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7a33\u5b9a\u6027\u635f\u5931\u65b9\u9762\u8fbe\u5230\u4e0eCAD\u6a21\u578b\u76f8\u5f53\u7684\u7cbe\u5ea6\uff0c\u5e76\u6bd4\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4ea7\u751f\u66f4\u7269\u7406\u5408\u7406\u7684\u653e\u7f6e\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u4ece\u566a\u58f0\u70b9\u4e91\u4e2d\u8bc4\u4f30\u653e\u7f6e\u59ff\u6001\uff0c\u65e0\u9700\u5f62\u72b6\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u3001\u65e0\u78b0\u649e\u7684\u653e\u7f6e\uff0c\u5e76\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.14615", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14615", "abs": "https://arxiv.org/abs/2510.14615", "authors": ["Edward Sandra", "Lander Vanroye", "Dries Dirckx", "Ruben Cartuyvels", "Jan Swevers", "Wilm Decr\u00e9"], "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models", "comment": "This paper has been submitted and has not yet been peer reviewed or\n  accepted for publication", "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods.", "AI": {"tldr": "\u63d0\u51faCAMPD\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u5206\u7c7b\u5668\u81ea\u7531\u53bb\u566a\u7684\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u4f20\u611f\u5668\u65e0\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u80fd\u591f\u5728\u672a\u89c1\u73af\u5883\u4e2d\u6cdb\u5316\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8f68\u8ff9\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u9ad8\u7ef4\u72b6\u6001\u7a7a\u95f4\u548c\u590d\u6742\u73af\u5883\u4e2d\u6269\u5c55\u6027\u5dee\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u5927\u591a\u9488\u5bf9\u5355\u4e00\u73af\u5883\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u7279\u5b9a\u4f20\u611f\u5668\u63d0\u4f9b\u73af\u5883\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u5206\u7c7b\u5668\u81ea\u7531\u53bb\u566a\u6982\u7387\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u5728U-Net\u67b6\u6784\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u4f20\u611f\u5668\u65e0\u5173\u7684\u65b9\u5f0f\u6761\u4ef6\u5316\u4efb\u610f\u6570\u91cf\u7684\u4e0a\u4e0b\u6587\u53c2\u6570\u3002", "result": "\u57287\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cCAMPD\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u73af\u5883\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u8f68\u8ff9\uff0c\u4e14\u6240\u9700\u65f6\u95f4\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "CAMPD\u5c55\u793a\u4e86\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u6709\u6548\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.14627", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14627", "abs": "https://arxiv.org/abs/2510.14627", "authors": ["Yao Zhong", "Hanzhi Chen", "Simon Schaefer", "Anran Zhang", "Stefan Leutenegger"], "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement", "comment": null, "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.", "AI": {"tldr": "GOPLA\u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u7684\u4eba\u7c7b\u6f14\u793a\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u7269\u4f53\u653e\u7f6e\uff0c\u7ed3\u5408\u8bed\u4e49\u504f\u597d\u548c\u51e0\u4f55\u53ef\u884c\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u653e\u7f6e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u4f5c\u4e3a\u667a\u80fd\u52a9\u624b\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u653e\u7f6e\u7684\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u8bed\u4e49\u504f\u597d\uff08\u5982\u5e38\u8bc6\u6027\u7269\u4f53\u5173\u7cfb\uff09\u548c\u51e0\u4f55\u53ef\u884c\u6027\uff08\u5982\u78b0\u649e\u907f\u514d\uff09\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u4eba\u7c7b\u6307\u4ee4\u548c\u89c6\u89c9\u8f93\u5165\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8ba1\u5212\uff0c\u901a\u8fc7\u7a7a\u95f4\u6620\u5c04\u5668\u751f\u62103D\u53ef\u64cd\u4f5c\u5730\u56fe\uff0c\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u5668\u751f\u6210\u653e\u7f6e\u59ff\u6001\uff0c\u5e76\u5f15\u5165\u53ef\u6269\u5c55\u7684\u6570\u636e\u589e\u5f3a\u7ba1\u9053\u3002", "result": "\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\u8bc4\u4f30\u4e2d\uff0c\u6bd4\u7b2c\u4e8c\u540d\u65b9\u6cd5\u63d0\u9ad8\u4e8630.04\u4e2a\u767e\u5206\u70b9\u7684\u653e\u7f6e\u6210\u529f\u7387\uff0c\u5728\u5e7f\u6cdb\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u653e\u7f6e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GOPLA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c\u51e0\u4f55\u89c4\u5212\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7269\u4f53\u653e\u7f6e\u95ee\u9898\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u5f3a\u6cdb\u5316\u3002"}}
{"id": "2510.14643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14643", "abs": "https://arxiv.org/abs/2510.14643", "authors": ["Lara Bruderm\u00fcller", "Brandon Hung", "Xinghao Zhu", "Jiuguang Wang", "Nick Hawes", "Preston Culbertson", "Simon Le Cleac'h"], "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation", "comment": "9 pages, 5 figures", "summary": "We present a generative predictive control (GPC) framework that amortizes\nsampling-based Model Predictive Control (SPC) by bootstrapping it with\nconditional flow-matching models trained on SPC control sequences collected in\nsimulation. Unlike prior work relying on iterative refinement or gradient-based\nsolvers, we show that meaningful proposal distributions can be learned directly\nfrom noisy SPC data, enabling more efficient and informed sampling during\nonline planning. We further demonstrate, for the first time, the application of\nthis approach to real-world contact-rich loco-manipulation with a quadruped\nrobot. Extensive experiments in simulation and on hardware show that our method\nimproves sample efficiency, reduces planning horizon requirements, and\ngeneralizes robustly across task variations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u5728\u6a21\u62df\u4e2d\u6536\u96c6\u7684\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u5e8f\u5217\u8bad\u7ec3\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\uff0c\u6765\u644a\u9500\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u3002\u8be5\u65b9\u6cd5\u76f4\u63a5\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u5b66\u4e60\u6709\u610f\u4e49\u7684\u63d0\u8bae\u5206\u5e03\uff0c\u63d0\u9ad8\u4e86\u5728\u7ebf\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u5927\u91cf\u8fed\u4ee3\u4f18\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u751f\u6210\u5f0f\u6a21\u578b\u76f4\u63a5\u5b66\u4e60\u63a7\u5236\u7b56\u7565\uff0c\u51cf\u5c11\u5728\u7ebf\u89c4\u5212\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u7279\u522b\u662f\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u57fa\u4e8e\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u6536\u96c6\u7684\u63a7\u5236\u5e8f\u5217\uff0c\u5b66\u4e60\u6709\u610f\u4e49\u7684\u63d0\u8bae\u5206\u5e03\uff0c\u4ece\u800c\u5728\u5728\u7ebf\u89c4\u5212\u65f6\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u89c4\u5212\u89c6\u91ce\u8981\u6c42\uff0c\u5e76\u5728\u4efb\u52a1\u53d8\u5316\u4e2d\u5c55\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9996\u6b21\u6210\u529f\u5e94\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u771f\u5b9e\u4e16\u754c\u63a5\u89e6\u4e30\u5bcc\u8fd0\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "\u751f\u6210\u5f0f\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u80fd\u591f\u6709\u6548\u644a\u9500\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u901a\u8fc7\u76f4\u63a5\u4ece\u566a\u58f0\u6570\u636e\u5b66\u4e60\u63d0\u8bae\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u5728\u7ebf\u89c4\u5212\uff0c\u5e76\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.14647", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14647", "abs": "https://arxiv.org/abs/2510.14647", "authors": ["Jialei Huang", "Yang Ye", "Yuanqing Gong", "Xuezhou Zhu", "Yang Gao", "Kaifeng Zhang"], "title": "Spatially anchored Tactile Awareness for Robust Dexterous Manipulation", "comment": "8 pages", "summary": "Dexterous manipulation requires precise geometric reasoning, yet existing\nvisuo-tactile learning methods struggle with sub-millimeter precision tasks\nthat are routine for traditional model-based approaches. We identify a key\nlimitation: while tactile sensors provide rich contact information, current\nlearning frameworks fail to effectively leverage both the perceptual richness\nof tactile signals and their spatial relationship with hand kinematics. We\nbelieve an ideal tactile representation should explicitly ground contact\nmeasurements in a stable reference frame while preserving detailed sensory\ninformation, enabling policies to not only detect contact occurrence but also\nprecisely infer object geometry in the hand's coordinate system. We introduce\nSaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an\nend-to-end policy framework that explicitly anchors tactile features to the\nhand's kinematic frame through forward kinematics, enabling accurate geometric\nreasoning without requiring object models or explicit pose estimation. Our key\ninsight is that spatially grounded tactile representations allow policies to\nnot only detect contact occurrence but also precisely infer object geometry in\nthe hand's coordinate system. We validate SaTA on challenging dexterous\nmanipulation tasks, including bimanual USB-C mating in free space, a task\ndemanding sub-millimeter alignment precision, as well as light bulb\ninstallation requiring precise thread engagement and rotational control, and\ncard sliding that demands delicate force modulation and angular precision.\nThese tasks represent significant challenges for learning-based methods due to\ntheir stringent precision requirements. Across multiple benchmarks, SaTA\nsignificantly outperforms strong visuo-tactile baselines, improving success\nrates by up to 30 percentage while reducing task completion times by 27\npercentage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSaTA\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89e6\u89c9\u7279\u5f81\u951a\u5b9a\u5230\u624b\u7684\u8fd0\u52a8\u5b66\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e9a\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u89e6\u89c9\u5b66\u4e60\u65b9\u6cd5\u5728\u4e9a\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u89e6\u89c9\u4fe1\u53f7\u7684\u611f\u77e5\u4e30\u5bcc\u6027\u548c\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "SaTA\u6846\u67b6\u901a\u8fc7\u524d\u5411\u8fd0\u52a8\u5b66\u5c06\u89e6\u89c9\u7279\u5f81\u663e\u5f0f\u951a\u5b9a\u5230\u624b\u7684\u8fd0\u52a8\u5b66\u6846\u67b6\u4e2d\uff0c\u65e0\u9700\u7269\u4f53\u6a21\u578b\u6216\u663e\u5f0f\u59ff\u6001\u4f30\u8ba1\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u51e0\u4f55\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSaTA\u663e\u8457\u4f18\u4e8e\u5f3a\u89c6\u89c9\u89e6\u89c9\u57fa\u7ebf\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u8fbe30\u4e2a\u767e\u5206\u70b9\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1127\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u7a7a\u95f4\u951a\u5b9a\u7684\u89e6\u89c9\u8868\u793a\u4f7f\u7b56\u7565\u4e0d\u4ec5\u80fd\u68c0\u6d4b\u63a5\u89e6\u53d1\u751f\uff0c\u8fd8\u80fd\u7cbe\u786e\u63a8\u65ad\u624b\u5750\u6807\u7cfb\u4e2d\u7684\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.14768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14768", "abs": "https://arxiv.org/abs/2510.14768", "authors": ["Fan Yang", "Zixuan Huang", "Abhinav Kumar", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery", "comment": null, "summary": "Real-world dexterous manipulation often encounters unexpected errors and\ndisturbances, which can lead to catastrophic failures, such as dropping the\nmanipulated object. To address this challenge, we focus on the problem of\ncatching a falling object while it remains within grasping range and,\nimportantly, resetting the system to a configuration favorable for resuming the\nprimary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a\nreinforcement learning framework that incorporates a Neural Descriptor Field\n(NDF)-inspired module to extract implicit contact features. Compared to methods\nthat rely solely on object pose or point cloud input, NDFs can directly reason\nabout finger-object correspondence and adapt to different object geometries.\nOur experiments show that incorporating contact features improves training\nefficiency, enhances convergence performance for RL training, and ultimately\nleads to more successful recoveries. Additionally, we demonstrate that CADRE\ncan generalize zero-shot to unseen objects with different geometries.", "AI": {"tldr": "\u63d0\u51faCADRE\u6846\u67b6\uff0c\u5229\u7528\u795e\u7ecf\u63cf\u8ff0\u7b26\u573a\u63d0\u53d6\u63a5\u89e6\u7279\u5f81\uff0c\u5b9e\u73b0\u7075\u5de7\u64cd\u4f5c\u4e2d\u6389\u843d\u7269\u4f53\u7684\u52a8\u6001\u6062\u590d\u548c\u7cfb\u7edf\u91cd\u7f6e\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u6062\u590d\u6210\u529f\u7387\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u7269\u4f53\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7075\u5de7\u64cd\u4f5c\u5e38\u9047\u5230\u610f\u5916\u9519\u8bef\u548c\u5e72\u6270\uff0c\u5bfc\u81f4\u7269\u4f53\u6389\u843d\u7b49\u707e\u96be\u6027\u5931\u8d25\u3002\u9700\u8981\u89e3\u51b3\u5728\u6293\u53d6\u8303\u56f4\u5185\u63a5\u4f4f\u6389\u843d\u7269\u4f53\u5e76\u91cd\u7f6e\u7cfb\u7edf\u4ee5\u6062\u590d\u4e3b\u8981\u64cd\u4f5c\u4efb\u52a1\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faContact-Aware Dynamic Recovery (CADRE)\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u795e\u7ecf\u63cf\u8ff0\u7b26\u573a(NDF)\u6a21\u5757\u63d0\u53d6\u9690\u5f0f\u63a5\u89e6\u7279\u5f81\uff0c\u76f4\u63a5\u63a8\u7406\u624b\u6307-\u7269\u4f53\u5bf9\u5e94\u5173\u7cfb\u5e76\u9002\u5e94\u4e0d\u540c\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u878d\u5165\u63a5\u89e6\u7279\u5f81\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u589e\u5f3a\u4e86RL\u8bad\u7ec3\u7684\u6536\u655b\u6027\u80fd\uff0c\u6700\u7ec8\u5e26\u6765\u66f4\u6210\u529f\u7684\u6062\u590d\u3002CADRE\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u7684\u672a\u89c1\u7269\u4f53\u3002", "conclusion": "CADRE\u6846\u67b6\u901a\u8fc7\u63a5\u89e6\u611f\u77e5\u7684\u52a8\u6001\u6062\u590d\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u7269\u4f53\u6389\u843d\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u63a5\u89e6\u7279\u5f81\u5728\u63d0\u9ad8\u6062\u590d\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.14771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14771", "abs": "https://arxiv.org/abs/2510.14771", "authors": ["Xu Chi", "Chao Zhang", "Yang Su", "Lingfeng Dou", "Fujia Yang", "Jiakuo Zhao", "Haoyu Zhou", "Xiaoyou Jia", "Yong Zhou", "Shan An"], "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation", "comment": "17 pages", "summary": "Accurate and high-fidelity demonstration data acquisition is a critical\nbottleneck for deploying robot Imitation Learning (IL) systems, particularly\nwhen dealing with heterogeneous robotic platforms. Existing teleoperation\nsystems often fail to guarantee high-precision data collection across diverse\ntypes of teleoperation devices. To address this, we developed Open TeleDex, a\nunified teleoperation framework engineered for demonstration data collection.\nOpen TeleDex specifically tackles the TripleAny challenge, seamlessly\nsupporting any robotic arm, any dexterous hand, and any external input device.\nFurthermore, we propose a novel hand pose retargeting algorithm that\nsignificantly boosts the interoperability of Open TeleDex, enabling robust and\naccurate compatibility with an even wider spectrum of heterogeneous master and\nslave equipment. Open TeleDex establishes a foundational, high-quality, and\npublicly available platform for accelerating both academic research and\nindustry development in complex robotic manipulation and IL.", "AI": {"tldr": "Open TeleDex\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u83b7\u53d6\u7684\u74f6\u9888\u95ee\u9898\uff0c\u652f\u6301\u4efb\u4f55\u673a\u68b0\u81c2\u3001\u7075\u5de7\u624b\u548c\u5916\u90e8\u8f93\u5165\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u5e73\u53f0\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u83b7\u53d6\u7684\u74f6\u9888\u95ee\u9898\uff0c\u73b0\u6709\u9065\u64cd\u4f5c\u7cfb\u7edf\u65e0\u6cd5\u4fdd\u8bc1\u8de8\u4e0d\u540c\u7c7b\u578b\u9065\u64cd\u4f5c\u8bbe\u5907\u7684\u9ad8\u7cbe\u5ea6\u6570\u636e\u6536\u96c6\u3002", "method": "\u5f00\u53d1\u4e86Open TeleDex\u7edf\u4e00\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u63d0\u51fa\u65b0\u7684\u624b\u90e8\u59ff\u6001\u91cd\u5b9a\u5411\u7b97\u6cd5\uff0c\u63d0\u5347\u7cfb\u7edf\u4e0e\u5f02\u6784\u4e3b\u4ece\u8bbe\u5907\u7684\u4e92\u64cd\u4f5c\u6027\u3002", "result": "Open TeleDex\u5b9e\u73b0\u4e86\u5bf9\u4efb\u4f55\u673a\u68b0\u81c2\u3001\u7075\u5de7\u624b\u548c\u5916\u90e8\u8f93\u5165\u8bbe\u5907\u7684\u65e0\u7f1d\u652f\u6301\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6f14\u793a\u6570\u636e\u6536\u96c6\u7684\u51c6\u786e\u6027\u548c\u4fdd\u771f\u5ea6\u3002", "conclusion": "Open TeleDex\u4e3a\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u6a21\u4eff\u5b66\u4e60\u9886\u57df\u5efa\u7acb\u4e86\u4e00\u4e2a\u57fa\u7840\u6027\u3001\u9ad8\u8d28\u91cf\u4e14\u516c\u5f00\u53ef\u7528\u7684\u5e73\u53f0\uff0c\u52a0\u901f\u5b66\u672f\u7814\u7a76\u548c\u5de5\u4e1a\u53d1\u5c55\u3002"}}
{"id": "2510.14783", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14783", "abs": "https://arxiv.org/abs/2510.14783", "authors": ["Aderik Verraest", "Stavrow Bahnam", "Robin Ferede", "Guido de Croon", "Christophe De Wagter"], "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning", "comment": null, "summary": "Autonomous drone racing (ADR) systems have recently achieved champion-level\nperformance, yet remain highly specific to drone racing. While end-to-end\nvision-based methods promise broader applicability, no system to date\nsimultaneously achieves full sim-to-real transfer, onboard execution, and\nchampion-level performance. In this work, we present SkyDreamer, to the best of\nour knowledge, the first end-to-end vision-based ADR policy that maps directly\nfrom pixel-level representations to motor commands. SkyDreamer builds on\ninformed Dreamer, a model-based reinforcement learning approach where the world\nmodel decodes to privileged information only available during training. By\nextending this concept to end-to-end vision-based ADR, the world model\neffectively functions as an implicit state and parameter estimator, greatly\nimproving interpretability. SkyDreamer runs fully onboard without external aid,\nresolves visual ambiguities by tracking progress using the state decoded from\nthe world model's hidden state, and requires no extrinsic camera calibration,\nenabling rapid deployment across different drones without retraining.\nReal-world experiments show that SkyDreamer achieves robust, high-speed flight,\nexecuting tight maneuvers such as an inverted loop, a split-S and a ladder,\nreaching speeds of up to 21 m/s and accelerations of up to 6 g. It further\ndemonstrates a non-trivial visual sim-to-real transfer by operating on\npoor-quality segmentation masks, and exhibits robustness to battery depletion\nby accurately estimating the maximum attainable motor RPM and adjusting its\nflight path in real-time. These results highlight SkyDreamer's adaptability to\nimportant aspects of the reality gap, bringing robustness while still achieving\nextremely high-speed, agile flight.", "AI": {"tldr": "SkyDreamer\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\u7b56\u7565\uff0c\u76f4\u63a5\u5c06\u50cf\u7d20\u8868\u793a\u6620\u5c04\u5230\u7535\u673a\u547d\u4ee4\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u3001\u673a\u8f7d\u6267\u884c\u548c\u51a0\u519b\u7ea7\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\u7cfb\u7edf\u9ad8\u5ea6\u7279\u5b9a\u4e8e\u7ade\u901f\u4efb\u52a1\uff0c\u800c\u7aef\u5230\u7aef\u89c6\u89c9\u65b9\u6cd5\u867d\u7136\u627f\u8bfa\u66f4\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4f46\u5c1a\u65e0\u7cfb\u7edf\u80fd\u540c\u65f6\u5b9e\u73b0\u5b8c\u6574\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\u3001\u673a\u8f7d\u6267\u884c\u548c\u51a0\u519b\u7ea7\u6027\u80fd\u3002", "method": "\u57fa\u4e8einformed Dreamer\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e16\u754c\u6a21\u578b\u89e3\u7801\u4e3a\u4ec5\u5728\u8bad\u7ec3\u65f6\u53ef\u7528\u7684\u7279\u6743\u4fe1\u606f\uff0c\u4f5c\u4e3a\u9690\u5f0f\u72b6\u6001\u548c\u53c2\u6570\u4f30\u8ba1\u5668\uff0c\u65e0\u9700\u5916\u90e8\u76f8\u673a\u6807\u5b9a\uff0c\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u9690\u85cf\u72b6\u6001\u89e3\u7801\u7684\u72b6\u6001\u8ddf\u8e2a\u8fdb\u5ea6\u89e3\u51b3\u89c6\u89c9\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u7a33\u5065\u9ad8\u901f\u98de\u884c\uff0c\u6267\u884c\u5012\u7f6e\u5faa\u73af\u3001Split-S\u548c\u68af\u5b50\u7b49\u590d\u6742\u673a\u52a8\uff0c\u901f\u5ea6\u8fbe21 m/s\uff0c\u52a0\u901f\u5ea6\u8fbe6g\uff0c\u5728\u4f4e\u8d28\u91cf\u5206\u5272\u63a9\u7801\u4e0a\u5b9e\u73b0\u975e\u5e73\u51e1\u7684\u89c6\u89c9\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u79fb\uff0c\u5e76\u80fd\u5b9e\u65f6\u4f30\u8ba1\u6700\u5927\u53ef\u8fbe\u7535\u673aRPM\u5e76\u8c03\u6574\u98de\u884c\u8def\u5f84\u4ee5\u5e94\u5bf9\u7535\u6c60\u6d88\u8017\u3002", "conclusion": "SkyDreamer\u80fd\u591f\u9002\u5e94\u73b0\u5b9e\u5dee\u8ddd\u7684\u91cd\u8981\u65b9\u9762\uff0c\u5728\u4fdd\u6301\u6781\u9ad8\u901f\u5ea6\u548c\u654f\u6377\u98de\u884c\u7684\u540c\u65f6\u5b9e\u73b0\u7a33\u5065\u6027\uff0c\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u89c6\u89c9\u81ea\u4e3b\u65e0\u4eba\u673a\u7ade\u901f\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.14827", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14827", "abs": "https://arxiv.org/abs/2510.14827", "authors": ["Yufei Zhu", "Shih-Min Yang", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping", "comment": null, "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u51fd\u6570\u7684\u8fde\u7eed\u65f6\u7a7a\u52a8\u6001\u5730\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u590d\u6742\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u5728\u590d\u6742\u4eba\u7c7b\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u826f\u597d\u7684\u7279\u5b9a\u573a\u666f\u8fd0\u52a8\u6a21\u5f0f\u6a21\u578b\u3002\u73b0\u6709\u52a8\u6001\u5730\u56fe\u65b9\u6cd5\u4f7f\u7528\u79bb\u6563\u7a7a\u95f4\u91c7\u6837\uff0c\u901a\u5e38\u9700\u8981\u6602\u8d35\u7684\u79bb\u7ebf\u6784\u5efa\u8fc7\u7a0b\u3002", "method": "\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u51fd\u6570\u7684\u8fde\u7eed\u65f6\u7a7a\u52a8\u6001\u5730\u56fe\u8868\u793a\uff0c\u76f4\u63a5\u5c06\u5750\u6807\u6620\u5c04\u5230\u534a\u5305\u88f9\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u53c2\u6570\uff0c\u65e0\u9700\u79bb\u6563\u5316\u548c\u7a00\u758f\u533a\u57df\u63d2\u503c\uff0c\u5b9e\u73b0\u8de8\u65f6\u7a7a\u7684\u5e73\u6ed1\u6cdb\u5316\u3002", "result": "\u5728\u5305\u542b\u957f\u671f\u771f\u5b9e\u4e16\u754c\u4eba\u5458\u8ddf\u8e2a\u6570\u636e\u7684\u5927\u578b\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8fd0\u52a8\u8868\u793a\u51c6\u786e\u6027\u548c\u7a00\u758f\u533a\u57df\u901f\u5ea6\u5206\u5e03\u5e73\u6ed1\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5efa\u6a21\u590d\u6742\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u800c\u9ad8\u6548\u7684\u9014\u5f84\u3002"}}
{"id": "2510.14830", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14830", "abs": "https://arxiv.org/abs/2510.14830", "authors": ["Kun Lei", "Huanyu Li", "Dongjie Yu", "Zhenyu Wei", "Lingxiao Guo", "Zhennan Jiang", "Ziyu Wang", "Shiyu Liang", "Huazhe Xu"], "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning", "comment": "https://lei-kun.github.io/RL-100/", "summary": "Real-world robotic manipulation in homes and factories demands reliability,\nefficiency, and robustness that approach or surpass skilled human operators. We\npresent RL-100, a real-world reinforcement learning training framework built on\ndiffusion visuomotor policies trained bu supervised learning. RL-100 introduces\na three-stage pipeline. First, imitation learning leverages human priors.\nSecond, iterative offline reinforcement learning uses an Offline Policy\nEvaluation procedure, abbreviated OPE, to gate PPO-style updates that are\napplied in the denoising process for conservative and reliable improvement.\nThird, online reinforcement learning eliminates residual failure modes. An\nadditional lightweight consistency distillation head compresses the multi-step\nsampling process in diffusion into a single-step policy, enabling\nhigh-frequency control with an order-of-magnitude reduction in latency while\npreserving task performance. The framework is task-, embodiment-, and\nrepresentation-agnostic and supports both 3D point clouds and 2D RGB inputs, a\nvariety of robot platforms, and both single-step and action-chunk policies. We\nevaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,\nsuch as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth\nfolding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100\nattains 100\\% success across evaluated trials for a total of 900 out of 900\nepisodes, including up to 250 out of 250 consecutive trials on one task. The\nmethod achieves near-human teleoperation or better time efficiency and\ndemonstrates multi-hour robustness with uninterrupted operation lasting up to\ntwo hours.", "AI": {"tldr": "RL-100\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u771f\u5b9e\u4e16\u754c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u5b9e\u73b0100%\u4efb\u52a1\u6210\u529f\u7387\uff0c\u652f\u6301\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u548c\u8f93\u5165\u8868\u793a\u3002", "motivation": "\u5bb6\u5ead\u548c\u5de5\u5382\u4e2d\u7684\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u63a5\u8fd1\u6216\u8d85\u8fc7\u719f\u7ec3\u4eba\u7c7b\u64cd\u4f5c\u5458\u7684\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u6a21\u4eff\u5b66\u4e60\u5229\u7528\u4eba\u7c7b\u5148\u9a8c\uff1b2\uff09\u8fed\u4ee3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u6765\u95e8\u63a7PPO\u5f0f\u66f4\u65b0\uff1b3\uff09\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6d88\u9664\u6b8b\u4f59\u6545\u969c\u6a21\u5f0f\u3002\u989d\u5916\u7684\u4e00\u81f4\u6027\u84b8\u998f\u5934\u5c06\u591a\u6b65\u91c7\u6837\u538b\u7f29\u4e3a\u5355\u6b65\u7b56\u7565\u3002", "result": "\u57287\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b9e\u73b0100%\u6210\u529f\u7387\uff08900/900\u6b21\u8bd5\u9a8c\uff09\uff0c\u5305\u62ec\u5355\u4efb\u52a1250\u6b21\u8fde\u7eed\u8bd5\u9a8c\u3002\u8fbe\u5230\u63a5\u8fd1\u4eba\u7c7b\u9065\u64cd\u4f5c\u6216\u66f4\u597d\u7684\u65f6\u95f4\u6548\u7387\uff0c\u6700\u957f\u4e0d\u95f4\u65ad\u8fd0\u884c\u8fbe2\u5c0f\u65f6\u3002", "conclusion": "RL-100\u6846\u67b6\u5177\u6709\u4efb\u52a1\u3001\u4f53\u73b0\u548c\u8868\u793a\u65e0\u5173\u6027\uff0c\u652f\u63013D\u70b9\u4e91\u548c2D RGB\u8f93\u5165\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.14849", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14849", "abs": "https://arxiv.org/abs/2510.14849", "authors": ["Marcello Sorge", "Nicola Cigarini", "Riccardo Lorigiola", "Giulia Michieletto", "Andrea Masiero", "Angelo Cenedese", "Alberto Guarnieri"], "title": "Multi Agent Switching Mode Controller for Sound Source localization", "comment": null, "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u58f0\u5b66\u76ee\u6807\u5b9a\u4f4d\u7684\u591a\u667a\u80fd\u4f53\u5207\u6362\u6a21\u5f0f\u63a7\u5236\u7b56\u7565\uff0c\u5305\u62ec\u5355\u6e90\u5b9a\u4f4d\u548c\u591a\u6e90\u5b9a\u4f4d\u4e24\u79cd\u573a\u666f\u3002", "motivation": "\u58f0\u5b66\u4f20\u611f\u5668\u80fd\u591f\u5728\u65e0\u6cd5\u5efa\u7acb\u76f4\u63a5\u89c6\u7ebf\u7684\u60c5\u51b5\u4e0b\u5b9a\u4f4d\u76ee\u6807\uff0c\u8fd9\u5728\u673a\u5668\u4eba\u7814\u7a76\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u667a\u80fd\u4f53\u5207\u6362\u6a21\u5f0f\u63a7\u5236\u7b56\u7565\uff0c\u5728\u5355\u6e90\u573a\u666f\u4e2d\u4fdd\u6301\u521a\u6027\u7f16\u961f\u5411\u76ee\u6807\u79fb\u52a8\uff0c\u5728\u591a\u6e90\u573a\u666f\u4e2d\u5404\u667a\u80fd\u4f53\u72ec\u7acb\u641c\u7d22\u76ee\u6807\u3002", "result": "\u8be5\u7b56\u7565\u80fd\u591f\u6709\u6548\u5904\u7406\u58f0\u5b66\u76ee\u6807\u5b9a\u4f4d\u95ee\u9898\uff0c\u9002\u5e94\u4e0d\u540c\u7684\u5b9a\u4f4d\u573a\u666f\u9700\u6c42\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u667a\u80fd\u4f53\u5207\u6362\u63a7\u5236\u65b9\u6cd5\u4e3a\u58f0\u5b66\u76ee\u6807\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u89c6\u7ebf\u53d7\u9650\u7684\u73af\u5883\u3002"}}
{"id": "2510.14851", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.14851", "abs": "https://arxiv.org/abs/2510.14851", "authors": ["Jakob Bichler", "Andreu Matoses Gimenez", "Javier Alonso-Mora"], "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time", "comment": "7 pages, 5 figures. 2025 IEEE Int. Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS 2025). Website and Code:\n  https://autonomousrobots.nl/paper_websites/sadcher_MRTA/", "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/", "AI": {"tldr": "Sadcher\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u591a\u673a\u5668\u4eba\u56e2\u961f\u7684\u5b9e\u65f6\u4efb\u52a1\u5206\u914d\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u8054\u76df\u5f62\u6210\u548c\u4efb\u52a1\u4f18\u5148\u7ea7\u7ea6\u675f\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u548ctransformer\u9884\u6d4b\u673a\u5668\u4eba\u4e0e\u4efb\u52a1\u4e4b\u95f4\u7684\u5206\u914d\u5956\u52b1\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53ef\u6267\u884c\u8c03\u5ea6\u65b9\u6848\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u591a\u673a\u5668\u4eba\u56e2\u961f\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u5206\u914d\u95ee\u9898\uff0c\u9700\u8981\u5904\u7406\u52a8\u6001\u8054\u76df\u5f62\u6210\u3001\u4efb\u52a1\u4f18\u5148\u7ea7\u7ea6\u675f\u4ee5\u53ca\u65f6\u7a7a\u63a8\u7406\u7b49\u590d\u6742\u56e0\u7d20\uff0c\u540c\u65f6\u4fdd\u8bc1\u8ba1\u7b97\u6548\u7387\u9002\u5408\u5b9e\u65f6\u64cd\u4f5c\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u548ctransformer\u67b6\u6784\u9884\u6d4b\u5206\u914d\u5956\u52b1\uff0c\u901a\u8fc7\u677e\u5f1b\u4e8c\u5206\u5339\u914d\u751f\u6210\u8c03\u5ea6\u65b9\u6848\uff0c\u663e\u5f0f\u5efa\u6a21\u673a\u5668\u4eba\u548c\u4efb\u52a1\u4f4d\u7f6e\u3001\u4efb\u52a1\u6301\u7eed\u65f6\u95f4\u3001\u673a\u5668\u4eba\u5269\u4f59\u5904\u7406\u65f6\u95f4\u7b49\u65f6\u7a7a\u56e0\u7d20\u3002", "result": "\u5728\u968f\u673a\u672a\u89c1\u95ee\u9898\u4e0a\uff0cSadcher\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u5b66\u4e60\u548c\u542f\u53d1\u5f0f\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8ba1\u7b97\u65f6\u95f4\u9002\u5408\u5b9e\u65f6\u64cd\u4f5c\uff0c\u80fd\u591f\u6269\u5c55\u5230\u66f4\u5927\u7684\u4efb\u52a1\u96c6\u548c\u56e2\u961f\u89c4\u6a21\u3002", "conclusion": "Sadcher\u6846\u67b6\u5728\u5f02\u6784\u591a\u673a\u5668\u4eba\u4efb\u52a1\u5206\u914d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u5305\u542b25\u4e07\u4e2a\u6700\u4f18\u8c03\u5ea6\u65b9\u6848\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2510.14893", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.14893", "abs": "https://arxiv.org/abs/2510.14893", "authors": ["Helene J. Levy", "Brett T. Lopez"], "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search", "comment": null, "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners.", "AI": {"tldr": "STITCHER\u662f\u4e00\u79cd\u65e0\u4f18\u5316\u7684\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u77ed\u8f68\u8ff9\u6bb5\u62fc\u63a5\u7ed3\u5408\u56fe\u641c\u7d22\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8ba1\u7b97\u957f\u8ddd\u79bb\u3001\u8868\u8fbe\u6027\u5f3a\u4e14\u63a5\u8fd1\u6700\u4f18\u7684\u8f68\u8ff9\uff0c\u5728\u9ad8\u901f\u81ea\u4e3b\u5bfc\u822a\u4e2d\u4f18\u4e8e\u73b0\u4ee3\u57fa\u4e8e\u4f18\u5316\u7684\u89c4\u5212\u5668\u3002", "motivation": "\u9ad8\u901f\u81ea\u4e3b\u5bfc\u822a\u9700\u8981\u5b9e\u65f6\u751f\u6210\u52a8\u6001\u53ef\u884c\u3001\u65e0\u78b0\u649e\u4e14\u6ee1\u8db3\u72b6\u6001\u6216\u6267\u884c\u5668\u7ea6\u675f\u7684\u654f\u6377\u8f68\u8ff9\u3002\u57fa\u4e8e\u4f18\u5316\u7684\u89c4\u5212\u5668\u867d\u7136\u80fd\u8ba1\u7b97\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u4f46\u8ba1\u7b97\u65f6\u95f4\u8981\u6c42\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u98ce\u9669\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faSTITCHER\u6846\u67b6\uff0c\u91c7\u7528\u65e0\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u641c\u7d22\u5c06\u77ed\u8f68\u8ff9\u6bb5\u62fc\u63a5\uff0c\u7ed3\u5408\u521b\u65b0\u7684\u89c4\u5212\u67b6\u6784\u548c\u7b97\u6cd5\u6539\u8fdb\u5b9e\u73b0\u5b9e\u65f6\u89c4\u5212\u3002", "result": "\u4eff\u771f\u6d4b\u8bd5\u663e\u793a\uff0c\u572850m\u00d750m\u73af\u5883\u4e2d\uff0c\u51e0\u6beb\u79d2\u5185\u5373\u53ef\u751f\u6210\u5b89\u5168\u8f68\u8ff9\u3002\u786c\u4ef6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86STITCHER\u80fd\u5b9e\u65f6\u751f\u6210\u53ef\u8ddf\u8e2a\u8def\u5f84\uff0c\u5e76\u5904\u7406\u975e\u51f8\u7ea6\u675f\u5982\u503e\u659c\u89d2\u548c\u7535\u673a\u529b\u9650\u5236\u3002", "conclusion": "STITCHER\u6846\u67b6\u5728\u5b9e\u65f6\u6027\u3001\u8f68\u8ff9\u8d28\u91cf\u548c\u7ea6\u675f\u5904\u7406\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u4f18\u5316\u7684\u89c4\u5212\u5668\uff0c\u9002\u7528\u4e8e\u9ad8\u901f\u81ea\u4e3b\u5bfc\u822a\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\u3002"}}
{"id": "2510.14930", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.14930", "abs": "https://arxiv.org/abs/2510.14930", "authors": ["Binghao Huang", "Jie Xu", "Iretiayo Akinola", "Wei Yang", "Balakumar Sundaralingam", "Rowland O'Flaherty", "Dieter Fox", "Xiaolong Wang", "Arsalan Mousavian", "Yu-Wei Chao", "Yunzhu Li"], "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin", "comment": "Accepted by 9th Conference on Robot Learning (CoRL 2025); Website:\n  https://binghao-huang.github.io/vt_refine/", "summary": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\n-- a capability that remains difficult to replicate in robots through\nbehavioral cloning alone, due to the suboptimality and limited diversity of\nhuman demonstrations. In this work, we present VT-Refine, a visuo-tactile\npolicy learning framework that combines real-world demonstrations,\nhigh-fidelity tactile simulation, and reinforcement learning to tackle precise,\ncontact-rich bimanual assembly. We begin by training a diffusion policy on a\nsmall set of demonstrations using synchronized visual and tactile inputs. This\npolicy is then transferred to a simulated digital twin equipped with simulated\ntactile sensors and further refined via large-scale reinforcement learning to\nenhance robustness and generalization. To enable accurate sim-to-real transfer,\nwe leverage high-resolution piezoresistive tactile sensors that provide normal\nforce signals and can be realistically modeled in parallel using\nGPU-accelerated simulation. Experimental results show that VT-Refine improves\nassembly performance in both simulation and the real world by increasing data\ndiversity and enabling more effective policy fine-tuning. Our project page is\navailable at https://binghao-huang.github.io/vt_refine/.", "AI": {"tldr": "VT-Refine\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u89e6\u89c9\u7684\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u3001\u9ad8\u4fdd\u771f\u89e6\u89c9\u6a21\u62df\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u7cbe\u786e\u7684\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u3002", "motivation": "\u4eba\u7c7b\u64c5\u957f\u901a\u8fc7\u4e30\u5bcc\u7684\u89e6\u89c9\u53cd\u9988\u9002\u5e94\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\uff0c\u4f46\u4ec5\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u96be\u4ee5\u5728\u673a\u5668\u4eba\u4e2d\u590d\u5236\u8fd9\u79cd\u80fd\u529b\uff0c\u56e0\u4e3a\u4eba\u7c7b\u6f14\u793a\u5b58\u5728\u6b21\u4f18\u6027\u548c\u6709\u9650\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "\u9996\u5148\u4f7f\u7528\u540c\u6b65\u89c6\u89c9\u548c\u89e6\u89c9\u8f93\u5165\u5728\u5c0f\u89c4\u6a21\u6f14\u793a\u96c6\u4e0a\u8bad\u7ec3\u6269\u6563\u7b56\u7565\uff0c\u7136\u540e\u5c06\u7b56\u7565\u8f6c\u79fb\u5230\u914d\u5907\u6a21\u62df\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6570\u5b57\u5b6a\u751f\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7ec6\u5316\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u538b\u963b\u89e6\u89c9\u4f20\u611f\u5668\u63d0\u4f9b\u6cd5\u5411\u529b\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7GPU\u52a0\u901f\u6a21\u62df\u8fdb\u884c\u771f\u5b9e\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVT-Refine\u901a\u8fc7\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u548c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u7b56\u7565\u5fae\u8c03\uff0c\u63d0\u9ad8\u4e86\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u88c5\u914d\u6027\u80fd\u3002", "conclusion": "VT-Refine\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7cbe\u786e\u63a5\u89e6\u4e30\u5bcc\u7684\u53cc\u624b\u673a\u5668\u4eba\u88c5\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u771f\u5b9e\u6f14\u793a\u3001\u89e6\u89c9\u6a21\u62df\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u6709\u6548\u7684sim-to-real\u8fc1\u79fb\u3002"}}
{"id": "2510.14952", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.14952", "abs": "https://arxiv.org/abs/2510.14952", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Yibo Peng", "Tao Huang", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang", "Chang Xu"], "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance", "comment": null, "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.", "AI": {"tldr": "RoboGhost\u662f\u4e00\u4e2a\u514d\u91cd\u5b9a\u5411\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4eba\u5f62\u673a\u5668\u4eba\u7b56\u7565\u76f4\u63a5\u5efa\u7acb\u5728\u8bed\u8a00\u57fa\u7840\u7684\u8fd0\u52a8\u6f5c\u5728\u8868\u793a\u4e0a\uff0c\u6d88\u9664\u4e86\u4f20\u7edf\u591a\u9636\u6bb5\u6d41\u7a0b\u4e2d\u7684\u4e2d\u95f4\u73af\u8282\uff0c\u5b9e\u73b0\u4e86\u4ece\u8bed\u8a00\u5230\u52a8\u4f5c\u7684\u76f4\u63a5\u6620\u5c04\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5f15\u5bfc\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6d41\u7a0b\u5b58\u5728\u591a\u9636\u6bb5\u5904\u7406\u5bfc\u81f4\u7684\u7d2f\u79ef\u8bef\u5dee\u3001\u9ad8\u5ef6\u8fdf\u4ee5\u53ca\u8bed\u4e49\u4e0e\u63a7\u5236\u5f31\u8026\u5408\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u76f4\u63a5\u7684\u4ece\u8bed\u8a00\u5230\u52a8\u4f5c\u7684\u8def\u5f84\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u76f4\u63a5\u4ece\u566a\u58f0\u4e2d\u751f\u6210\u53ef\u6267\u884c\u52a8\u4f5c\uff0c\u4f7f\u7528\u6df7\u5408\u56e0\u679c\u53d8\u6362\u5668-\u6269\u6563\u8fd0\u52a8\u751f\u6210\u5668\u786e\u4fdd\u957f\u65f6\u7a0b\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRoboGhost\u663e\u8457\u964d\u4f4e\u4e86\u90e8\u7f72\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5728\u771f\u5b9e\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u4ea7\u751f\u4e86\u5e73\u6ed1\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u4eba\u5f62\u7cfb\u7edf\u63d0\u4f9b\u4e86\u901a\u7528\u57fa\u7840\uff0c\u53ef\u81ea\u7136\u6269\u5c55\u5230\u56fe\u50cf\u3001\u97f3\u9891\u548c\u97f3\u4e50\u7b49\u5176\u4ed6\u6a21\u6001\u3002"}}
{"id": "2510.14959", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14959", "abs": "https://arxiv.org/abs/2510.14959", "authors": ["Lizhi Yang", "Blake Werner", "Massimiliano de Sa Aaron D. Ames"], "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions", "comment": "8 pages", "summary": "Reinforcement learning (RL), while powerful and expressive, can often\nprioritize performance at the expense of safety. Yet safety violations can lead\nto catastrophic outcomes in real-world deployments. Control Barrier Functions\n(CBFs) offer a principled method to enforce dynamic safety -- traditionally\ndeployed \\emph{online} via safety filters. While the result is safe behavior,\nthe fact that the RL policy does not have knowledge of the CBF can lead to\nconservative behaviors. This paper proposes CBF-RL, a framework for generating\nsafe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key\nattributes: (1) minimally modifying a nominal RL policy to encode safety\nconstraints via a CBF term, (2) and safety filtering of the policy rollouts in\ntraining. Theoretically, we prove that continuous-time safety filters can be\ndeployed via closed-form expressions on discrete-time roll-outs. Practically,\nwe demonstrate that CBF-RL internalizes the safety constraints in the learned\npolicy -- both enforcing safer actions and biasing towards safer rewards --\nenabling safe deployment without the need for an online safety filter. We\nvalidate our framework through ablation studies on navigation tasks and on the\nUnitree G1 humanoid robot, where CBF-RL enables safer exploration, faster\nconvergence, and robust performance under uncertainty, enabling the humanoid\nrobot to avoid obstacles and climb stairs safely in real-world settings without\na runtime safety filter.", "AI": {"tldr": "CBF-RL\u6846\u67b6\u901a\u8fc7\u5728\u8bad\u7ec3\u4e2d\u5f3a\u5236\u6267\u884c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u6765\u751f\u6210\u5b89\u5168\u7684\u5f3a\u5316\u5b66\u4e60\u884c\u4e3a\uff0c\u65e0\u9700\u5728\u7ebf\u5b89\u5168\u8fc7\u6ee4\u5668\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u90e8\u7f72\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5f80\u5f80\u4ee5\u727a\u7272\u5b89\u5168\u6027\u4e3a\u4ee3\u4ef7\u6765\u4f18\u5148\u8003\u8651\u6027\u80fd\uff0c\u800c\u5b89\u5168\u8fdd\u89c4\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53ef\u80fd\u5bfc\u81f4\u707e\u96be\u6027\u540e\u679c\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528\u5728\u7ebf\u5b89\u5168\u8fc7\u6ee4\u5668\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4RL\u7b56\u7565\u4e0d\u4e86\u89e3CBF\u800c\u4ea7\u751f\u4fdd\u5b88\u884c\u4e3a\u3002", "method": "CBF-RL\u6846\u67b6\u6709\u4e24\u4e2a\u5173\u952e\u5c5e\u6027\uff1a(1) \u901a\u8fc7CBF\u9879\u6700\u5c0f\u5316\u4fee\u6539\u540d\u4e49RL\u7b56\u7565\u6765\u7f16\u7801\u5b89\u5168\u7ea6\u675f\uff1b(2) \u5728\u8bad\u7ec3\u4e2d\u5bf9\u7b56\u7565rollouts\u8fdb\u884c\u5b89\u5168\u8fc7\u6ee4\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8fde\u7eed\u65f6\u95f4\u5b89\u5168\u8fc7\u6ee4\u5668\u53ef\u4ee5\u901a\u8fc7\u95ed\u5f0f\u8868\u8fbe\u5f0f\u5728\u79bb\u6563\u65f6\u95f4rollouts\u4e0a\u90e8\u7f72\u3002", "result": "CBF-RL\u5728\u5bfc\u822a\u4efb\u52a1\u548cUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u63a2\u7d22\u3001\u66f4\u5feb\u7684\u6536\u655b\u4ee5\u53ca\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u9c81\u68d2\u6027\u80fd\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u5728\u6ca1\u6709\u8fd0\u884c\u65f6\u5b89\u5168\u8fc7\u6ee4\u5668\u7684\u60c5\u51b5\u4e0b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b89\u5168\u907f\u5f00\u969c\u788d\u7269\u548c\u722c\u697c\u68af\u3002", "conclusion": "CBF-RL\u901a\u8fc7\u5b66\u4e60\u7b56\u7565\u5185\u90e8\u5316\u5b89\u5168\u7ea6\u675f\uff0c\u65e2\u80fd\u5f3a\u5236\u6267\u884c\u66f4\u5b89\u5168\u7684\u52a8\u4f5c\uff0c\u53c8\u80fd\u504f\u5411\u66f4\u5b89\u5168\u7684\u5956\u52b1\uff0c\u4ece\u800c\u65e0\u9700\u5728\u7ebf\u5b89\u5168\u8fc7\u6ee4\u5668\u5373\u53ef\u5b9e\u73b0\u5b89\u5168\u90e8\u7f72\u3002"}}
{"id": "2510.14968", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.14968", "abs": "https://arxiv.org/abs/2510.14968", "authors": ["Mingxuan Yan", "Yuping Wang", "Zechun Liu", "Jiachen Li"], "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025); Project Website: rdd-neurips.github.io", "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u7684\u6f14\u793a\u5206\u89e3\u5668\uff08RDD\uff09\uff0c\u901a\u8fc7\u5c06\u5206\u89e3\u7684\u5b50\u4efb\u52a1\u95f4\u9694\u7684\u89c6\u89c9\u7279\u5f81\u4e0e\u4f4e\u7ea7\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u8bad\u7ec3\u6570\u636e\u5bf9\u9f50\uff0c\u81ea\u52a8\u5c06\u6f14\u793a\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u542f\u53d1\u5f0f\u5b50\u4efb\u52a1\u4e0e\u8bad\u7ec3\u6570\u636e\u504f\u5dee\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edfVLM\u89c4\u5212\u5668\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u6216\u542f\u53d1\u5f0f\u89c4\u5219\u5206\u89e3\u6f14\u793a\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u7531\u6b64\u5bfc\u81f4\u7684\u5b50\u4efb\u52a1\u4e0e\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u8bad\u7ec3\u6570\u636e\u4e0d\u5339\u914d\u800c\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u68c0\u7d22\u5f0f\u6f14\u793a\u5206\u89e3\u5668\uff08RDD\uff09\uff0c\u901a\u8fc7\u5c06\u5206\u89e3\u7684\u5b50\u4efb\u52a1\u95f4\u9694\u7684\u89c6\u89c9\u7279\u5f81\u4e0e\u4f4e\u7ea7\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5bf9\u9f50\uff0c\u5b9e\u73b0\u81ea\u52a8\u6f14\u793a\u5206\u89e3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5b50\u4efb\u52a1\u5206\u89e3\u5668\uff0c\u5e76\u5728\u591a\u6837\u5316\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "RDD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6f14\u793a\u5206\u89e3\u4e2d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
