{"id": "2510.17948", "categories": ["cs.RO", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.17948", "abs": "https://arxiv.org/abs/2510.17948", "authors": ["Christopher A McClurg", "Alan R Wagner"], "title": "Studying the Effects of Robot Intervention on School Shooters in Virtual Reality", "comment": "Preprint under review for conference publication. 10 pages, 9\n  figures, 3 tables (including 1-page appendix)", "summary": "We advance the understanding of robotic intervention in high-risk scenarios\nby examining their potential to distract and impede a school shooter. To\nevaluate this concept, we conducted a virtual reality study with 150 university\nparticipants role-playing as a school shooter. Within the simulation, an\nautonomous robot predicted the shooter's movements and positioned itself\nstrategically to interfere and distract. The strategy the robot used to\napproach the shooter was manipulated -- either moving directly in front of the\nshooter (aggressive) or maintaining distance (passive) -- and the distraction\nmethod, ranging from no additional cues (low), to siren and lights (medium), to\nsiren, lights, and smoke to impair visibility (high). An aggressive,\nhigh-distraction robot reduced the number of victims by 46.6% relative to a\nno-robot control. This outcome underscores both the potential of robotic\nintervention to enhance safety and the pressing ethical questions surrounding\ntheir use in school environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u865a\u62df\u73b0\u5b9e\u5b9e\u9a8c\u8bc4\u4f30\u673a\u5668\u4eba\u5e72\u9884\u5bf9\u6821\u56ed\u67aa\u51fb\u4e8b\u4ef6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9ad8\u5e72\u6270\u6027\u7684\u6fc0\u8fdb\u673a\u5668\u4eba\u53ef\u5c06\u53d7\u5bb3\u8005\u6570\u91cf\u51cf\u5c1146.6%\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u901a\u8fc7\u5e72\u6270\u548c\u963b\u788d\u6821\u56ed\u67aa\u624b\u6765\u589e\u5f3a\u5b89\u5168\u6027\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528\u865a\u62df\u73b0\u5b9e\u6280\u672f\uff0c150\u540d\u5927\u5b66\u751f\u626e\u6f14\u6821\u56ed\u67aa\u624b\u89d2\u8272\uff0c\u6d4b\u8bd5\u4e0d\u540c\u673a\u5668\u4eba\u7b56\u7565\uff08\u6fc0\u8fdbvs\u88ab\u52a8\uff09\u548c\u5e72\u6270\u65b9\u6cd5\uff08\u4f4e\u3001\u4e2d\u3001\u9ad8\uff09\u7684\u6548\u679c\u3002", "result": "\u6fc0\u8fdb\u4e14\u9ad8\u5e72\u6270\u7684\u673a\u5668\u4eba\uff08\u4f7f\u7528\u8b66\u62a5\u3001\u706f\u5149\u548c\u70df\u96fe\uff09\u76f8\u5bf9\u4e8e\u65e0\u673a\u5668\u4eba\u63a7\u5236\u7ec4\uff0c\u53d7\u5bb3\u8005\u6570\u91cf\u51cf\u5c11\u4e8646.6%\u3002", "conclusion": "\u673a\u5668\u4eba\u5e72\u9884\u5728\u589e\u5f3a\u5b89\u5168\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u540c\u65f6\u4e5f\u5f15\u53d1\u4e86\u5728\u5b66\u6821\u73af\u5883\u4e2d\u4f7f\u7528\u673a\u5668\u4eba\u7684\u7d27\u8feb\u4f26\u7406\u95ee\u9898\u3002"}}
{"id": "2510.17950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17950", "abs": "https://arxiv.org/abs/2510.17950", "authors": ["Adina Yakefu", "Bin Xie", "Chongyang Xu", "Enwen Zhang", "Erjin Zhou", "Fan Jia", "Haitao Yang", "Haoqiang Fan", "Haowei Zhang", "Hongyang Peng", "Jing Tan", "Junwen Huang", "Kai Liu", "Kaixin Liu", "Kefan Gu", "Qinglun Zhang", "Ruitao Zhang", "Saike Huang", "Shen Cheng", "Shuaicheng Liu", "Tiancai Wang", "Tiezhen Wang", "Wei Sun", "Wenbin Tang", "Yajun Wei", "Yang Chen", "Youqiang Gui", "Yucheng Zhao", "Yunchao Ma", "Yunfei Wei", "Yunhuan Yang", "Yutong Guo", "Ze Chen", "Zhengyuan Du", "Ziheng Zhang", "Ziming Liu", "Ziwei Yan"], "title": "RoboChallenge: Large-scale Real-robot Evaluation of Embodied Policies", "comment": "Authors are listed in alphabetical order. The official website is\n  located at https://robochallenge.ai", "summary": "Testing on real machines is indispensable for robotic control algorithms. In\nthe context of learning-based algorithms, especially VLA models, demand for\nlarge-scale evaluation, i.e. testing a large number of models on a large number\nof tasks, is becoming increasingly urgent. However, doing this right is highly\nnon-trivial, especially when scalability and reproducibility is taken into\naccount. In this report, we describe our methodology for constructing\nRoboChallenge, an online evaluation system to test robotic control algorithms,\nand our survey of recent state-of-the-art VLA models using our initial\nbenchmark Table30.", "AI": {"tldr": "\u6784\u5efaRoboChallenge\u5728\u7ebf\u8bc4\u4f30\u7cfb\u7edf\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6d4b\u8bd5\u673a\u5668\u4eba\u63a7\u5236\u7b97\u6cd5\uff0c\u7279\u522b\u662fVLA\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u521d\u59cb\u57fa\u51c6Table30\u5bf9\u6700\u65b0VLA\u6a21\u578b\u8fdb\u884c\u8c03\u7814\u3002", "motivation": "\u5b66\u4e60\u578b\u7b97\u6cd5\uff08\u5c24\u5176\u662fVLA\u6a21\u578b\uff09\u9700\u8981\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u5373\u5728\u5927\u91cf\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u5927\u91cf\u6a21\u578b\uff0c\u4f46\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u548c\u53ef\u590d\u73b0\u6027\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6784\u5efaRoboChallenge\u5728\u7ebf\u8bc4\u4f30\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u521d\u59cb\u57fa\u51c6Table30\uff0c\u7528\u4e8e\u6d4b\u8bd5\u673a\u5668\u4eba\u63a7\u5236\u7b97\u6cd5\u3002", "result": "\u5f00\u53d1\u4e86\u5728\u7ebf\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5e76\u5bf9\u6700\u65b0VLA\u6a21\u578b\u8fdb\u884c\u4e86\u8c03\u7814\u3002", "conclusion": "RoboChallenge\u7cfb\u7edf\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u7b97\u6cd5\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8VLA\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.18002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18002", "abs": "https://arxiv.org/abs/2510.18002", "authors": ["Junli Ren", "Junfeng Long", "Tao Huang", "Huayi Wang", "Zirui Wang", "Feiyu Jia", "Wentao Zhang", "Jingbo Wang", "Ping Luo", "Jiangmiao Pang"], "title": "Humanoid Goalkeeper: Learning from Position Conditioned Task-Motion Constraints", "comment": null, "summary": "We present a reinforcement learning framework for autonomous goalkeeping with\nhumanoid robots in real-world scenarios. While prior work has demonstrated\nsimilar capabilities on quadrupedal platforms, humanoid goalkeeping introduces\ntwo critical challenges: (1) generating natural, human-like whole-body motions,\nand (2) covering a wider guarding range with an equivalent response time.\nUnlike existing approaches that rely on separate teleoperation or fixed motion\ntracking for whole-body control, our method learns a single end-to-end RL\npolicy, enabling fully autonomous, highly dynamic, and human-like robot-object\ninteractions. To achieve this, we integrate multiple human motion priors\nconditioned on perceptual inputs into the RL training via an adversarial\nscheme. We demonstrate the effectiveness of our method through real-world\nexperiments, where the humanoid robot successfully performs agile, autonomous,\nand naturalistic interceptions of fast-moving balls. In addition to\ngoalkeeping, we demonstrate the generalization of our approach through tasks\nsuch as ball escaping and grabbing. Our work presents a practical and scalable\nsolution for enabling highly dynamic interactions between robots and moving\nobjects, advancing the field toward more adaptive and lifelike robotic\nbehaviors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u81ea\u4e3b\u5b88\u95e8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u81ea\u7136\u3001\u52a8\u6001\u7684\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5b88\u95e8\u9762\u4e34\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u751f\u6210\u81ea\u7136\u7684\u7c7b\u4eba\u5168\u8eab\u8fd0\u52a8\uff0c\u4ee5\u53ca\u5728\u76f8\u540c\u54cd\u5e94\u65f6\u95f4\u5185\u8986\u76d6\u66f4\u5e7f\u7684\u9632\u5b88\u8303\u56f4\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9065\u64cd\u4f5c\u6216\u56fa\u5b9a\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5b8c\u5168\u81ea\u4e3b\u7684\u52a8\u6001\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u65b9\u6848\u5c06\u57fa\u4e8e\u611f\u77e5\u8f93\u5165\u7684\u591a\u4e2a\u4eba\u7c7b\u8fd0\u52a8\u5148\u9a8c\u6574\u5408\u5230RL\u8bad\u7ec3\u4e2d\u3002", "result": "\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u6210\u529f\u6267\u884c\u654f\u6377\u3001\u81ea\u4e3b\u4e14\u81ea\u7136\u7684\u5feb\u901f\u79fb\u52a8\u7403\u62e6\u622a\u3002\u65b9\u6cd5\u8fd8\u6cdb\u5316\u5230\u7403\u9003\u8131\u548c\u6293\u53d6\u7b49\u4efb\u52a1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5b9e\u73b0\u673a\u5668\u4eba\u4e0e\u79fb\u52a8\u7269\u4f53\u4e4b\u95f4\u7684\u9ad8\u5ea6\u52a8\u6001\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u884c\u4e3a\u5411\u66f4\u5177\u9002\u5e94\u6027\u548c\u903c\u771f\u6027\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2510.18063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18063", "abs": "https://arxiv.org/abs/2510.18063", "authors": ["Bin-Bin Hu", "Weijia Yao", "Ming Cao"], "title": "MOFM-Nav: On-Manifold Ordering-Flexible Multi-Robot Navigation", "comment": null, "summary": "This paper addresses the problem of multi-robot navigation where robots\nmaneuver on a desired \\(m\\)-dimensional (i.e., \\(m\\)-D) manifold in the\n$n$-dimensional Euclidean space, and maintain a {\\it flexible spatial\nordering}. We consider $ m\\geq 2$, and the multi-robot coordination is achieved\nvia non-Euclidean metrics. However, since the $m$-D manifold can be\ncharacterized by the zero-level sets of $n$ implicit functions, the last $m$\nentries of the GVF propagation term become {\\it strongly coupled} with the\npartial derivatives of these functions if the auxiliary vectors are not\nappropriately chosen. These couplings not only influence the on-manifold\nmaneuvering of robots, but also pose significant challenges to the further\ndesign of the ordering-flexible coordination via non-Euclidean metrics.\n  To tackle this issue, we first identify a feasible solution of auxiliary\nvectors such that the last $m$ entries of the propagation term are effectively\ndecoupled to be the same constant. Then, we redesign the coordinated GVF (CGVF)\nalgorithm to {\\it boost} the advantages of singularities elimination and global\nconvergence by treating $m$ manifold parameters as additional $m$ virtual\ncoordinates. Furthermore, we enable the on-manifold ordering-flexible motion\ncoordination by allowing each robot to share $m$ virtual coordinates with its\ntime-varying neighbors and a virtual target robot, which {\\it circumvents} the\npossible complex calculation if Euclidean metrics were used instead. Finally,\nwe showcase the proposed algorithm's flexibility, adaptability, and robustness\nthrough extensive simulations with different initial positions,\nhigher-dimensional manifolds, and robot breakdown, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u5728n\u7ef4\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u7684m\u7ef4\u6d41\u5f62\u4e0a\u8fd0\u52a8\uff0c\u5e76\u4fdd\u6301\u7075\u6d3b\u7684\u7a7a\u95f4\u6392\u5e8f\u3002\u901a\u8fc7\u975e\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u5b9e\u73b0\u534f\u8c03\uff0c\u89e3\u51b3\u4e86GVF\u4f20\u64ad\u9879\u4e2d\u5f3a\u8026\u5408\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e86\u534f\u8c03GVF\u7b97\u6cd5\u6765\u6d88\u9664\u5947\u70b9\u5e76\u5b9e\u73b0\u5168\u5c40\u6536\u655b\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728m\u7ef4\u6d41\u5f62\u4e0a\u5bfc\u822a\u65f6\uff0c\u7531\u4e8e\u6d41\u5f62\u7531n\u4e2a\u9690\u51fd\u6570\u7684\u96f6\u7ea7\u96c6\u8868\u5f81\uff0c\u5bfc\u81f4GVF\u4f20\u64ad\u9879\u4e2d\u6700\u540em\u4e2a\u6761\u76ee\u4e0e\u8fd9\u4e9b\u51fd\u6570\u7684\u504f\u5bfc\u6570\u5f3a\u8026\u5408\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u8026\u5408\u4e0d\u4ec5\u5f71\u54cd\u673a\u5668\u4eba\u5728\u6d41\u5f62\u4e0a\u7684\u673a\u52a8\uff0c\u8fd8\u5bf9\u4f7f\u7528\u975e\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u7684\u7075\u6d3b\u6392\u5e8f\u534f\u8c03\u8bbe\u8ba1\u6784\u6210\u6311\u6218\u3002", "method": "\u9996\u5148\u8bc6\u522b\u8f85\u52a9\u5411\u91cf\u7684\u53ef\u884c\u89e3\uff0c\u4f7f\u4f20\u64ad\u9879\u7684\u6700\u540em\u4e2a\u6761\u76ee\u6709\u6548\u89e3\u8026\u4e3a\u76f8\u540c\u5e38\u6570\uff1b\u7136\u540e\u91cd\u65b0\u8bbe\u8ba1\u534f\u8c03GVF\u7b97\u6cd5\uff0c\u5c06m\u4e2a\u6d41\u5f62\u53c2\u6570\u89c6\u4e3a\u989d\u5916\u7684m\u4e2a\u865a\u62df\u5750\u6807\uff0c\u4ee5\u589e\u5f3a\u5947\u70b9\u6d88\u9664\u548c\u5168\u5c40\u6536\u655b\u7684\u4f18\u52bf\uff1b\u6700\u540e\u901a\u8fc7\u8ba9\u6bcf\u4e2a\u673a\u5668\u4eba\u4e0e\u5176\u65f6\u53d8\u90bb\u5c45\u548c\u865a\u62df\u76ee\u6807\u673a\u5668\u4eba\u5171\u4eabm\u4e2a\u865a\u62df\u5750\u6807\uff0c\u5b9e\u73b0\u6d41\u5f62\u4e0a\u7684\u7075\u6d3b\u6392\u5e8f\u8fd0\u52a8\u534f\u8c03\u3002", "result": "\u901a\u8fc7\u4e0d\u540c\u521d\u59cb\u4f4d\u7f6e\u3001\u9ad8\u7ef4\u6d41\u5f62\u548c\u673a\u5668\u4eba\u6545\u969c\u7684\u5e7f\u6cdb\u4eff\u771f\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u7075\u6d3b\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u534f\u8c03GVF\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u5728\u6d41\u5f62\u4e0a\u5bfc\u822a\u65f6\u7684\u8026\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u7a7a\u95f4\u6392\u5e8f\u534f\u8c03\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.18085", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.18085", "abs": "https://arxiv.org/abs/2510.18085", "authors": ["Connor Mattson", "Varun Raveendra", "Ellen Novoseller", "Nicholas Waytowich", "Vernon J. Lawhern", "Daniel S. Brown"], "title": "R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations", "comment": "9 pages, 6 figures", "summary": "Imitation Learning (IL) is a natural way for humans to teach robots,\nparticularly when high-quality demonstrations are easy to obtain. While IL has\nbeen widely applied to single-robot settings, relatively few studies have\naddressed the extension of these methods to multi-agent systems, especially in\nsettings where a single human must provide demonstrations to a team of\ncollaborating robots. In this paper, we introduce and study Round-Robin\nBehavior Cloning (R2BC), a method that enables a single human operator to\neffectively train multi-robot systems through sequential, single-agent\ndemonstrations. Our approach allows the human to teleoperate one agent at a\ntime and incrementally teach multi-agent behavior to the entire system, without\nrequiring demonstrations in the joint multi-agent action space. We show that\nR2BC methods match, and in some cases surpass, the performance of an oracle\nbehavior cloning approach trained on privileged synchronized demonstrations\nacross four multi-agent simulated tasks. Finally, we deploy R2BC on two\nphysical robot tasks trained using real human demonstrations.", "AI": {"tldr": "\u63d0\u51faR2BC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4eba\u7c7b\u64cd\u4f5c\u8005\u6309\u987a\u5e8f\u6f14\u793a\u8bad\u7ec3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u65e0\u9700\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u6f14\u793a\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u5355\u673a\u5668\u4eba\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7814\u7a76\u8f83\u5c11\uff0c\u7279\u522b\u662f\u9700\u8981\u5355\u4e2a\u4eba\u7c7b\u4e3a\u534f\u4f5c\u673a\u5668\u4eba\u56e2\u961f\u63d0\u4f9b\u6f14\u793a\u7684\u573a\u666f\u3002", "method": "R2BC\u65b9\u6cd5\u5141\u8bb8\u4eba\u7c7b\u6309\u8f6e\u6362\u65b9\u5f0f\u4f9d\u6b21\u9065\u63a7\u5355\u4e2a\u667a\u80fd\u4f53\uff0c\u9010\u6b65\u6559\u6388\u591a\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u907f\u514d\u5728\u8054\u5408\u591a\u667a\u80fd\u4f53\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6f14\u793a\u3002", "result": "\u5728\u56db\u4e2a\u591a\u667a\u80fd\u4f53\u6a21\u62df\u4efb\u52a1\u4e2d\uff0cR2BC\u65b9\u6cd5\u6027\u80fd\u5339\u914d\u751a\u81f3\u8d85\u8fc7\u57fa\u4e8e\u7279\u6743\u540c\u6b65\u6f14\u793a\u7684oracle\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\uff0c\u5e76\u5728\u4e24\u4e2a\u7269\u7406\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u6210\u529f\u90e8\u7f72\u3002", "conclusion": "R2BC\u662f\u4e00\u79cd\u6709\u6548\u7684\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u987a\u5e8f\u5355\u667a\u80fd\u4f53\u6f14\u793a\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u5b66\u4e60\u3002"}}
{"id": "2510.18127", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18127", "abs": "https://arxiv.org/abs/2510.18127", "authors": ["Dharmik Patel", "Antonio Rafael Vazquez Pantoja", "Jiuzhou Lei", "Kiju Lee", "Xiao Liang", "Minghui Zheng"], "title": "ANGEL: A Novel Gripper for Versatile and Light-touch Fruit Harvesting", "comment": null, "summary": "Fruit harvesting remains predominantly a labor-intensive process, motivating\nthe development of research for robotic grippers. Conventional rigid or\nvacuum-driven grippers require complex mechanical design or high energy\nconsumption. Current enveloping-based fruit harvesting grippers lack\nadaptability to fruits of different sizes. This paper introduces a\ndrawstring-inspired, cable-driven soft gripper for versatile and gentle fruit\nharvesting. The design employs 3D-printed Thermoplastic Polyurethane (TPU)\npockets with integrated steel wires that constrict around the fruit when\nactuated, distributing pressure uniformly to minimize bruising and allow\nversatility to fruits of varying sizes. The lightweight structure, which\nrequires few components, reduces mechanical complexity and cost compared to\nother grippers. Actuation is achieved through servo-driven cable control, while\nmotor feedback provides autonomous grip adjustment with tunable grip strength.\nExperimental validation shows that, for tomatoes within the gripper's effective\nsize range, harvesting was achieved with a 0% immediate damage rate and a\nbruising rate of less than 9% after five days, reinforcing the gripper's\nsuitability for fruit harvesting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u62c9\u7ef3\u542f\u53d1\u7684\u7ebf\u9a71\u52a8\u8f6f\u5939\u5177\uff0c\u7528\u4e8e\u6c34\u679c\u91c7\u6458\u3002\u8be5\u8bbe\u8ba1\u4f7f\u75283D\u6253\u5370TPU\u53e3\u888b\u548c\u96c6\u6210\u94a2\u4e1d\uff0c\u901a\u8fc7\u4f3a\u670d\u9a71\u52a8\u63a7\u5236\u5b9e\u73b0\u81ea\u9002\u5e94\u6293\u53d6\uff0c\u80fd\u9002\u5e94\u4e0d\u540c\u5927\u5c0f\u7684\u6c34\u679c\u5e76\u51cf\u5c11\u635f\u4f24\u3002", "motivation": "\u6c34\u679c\u91c7\u6458\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56\u4eba\u5de5\u52b3\u52a8\uff0c\u4f20\u7edf\u521a\u6027\u6216\u771f\u7a7a\u9a71\u52a8\u5939\u5177\u9700\u8981\u590d\u6742\u673a\u68b0\u8bbe\u8ba1\u6216\u9ad8\u80fd\u8017\uff0c\u73b0\u6709\u5305\u88f9\u5f0f\u6c34\u679c\u91c7\u6458\u5939\u5177\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u5927\u5c0f\u6c34\u679c\u7684\u9002\u5e94\u6027\u3002", "method": "\u91c7\u75283D\u6253\u5370\u70ed\u5851\u6027\u805a\u6c28\u916f(TPU)\u53e3\u888b\uff0c\u96c6\u6210\u94a2\u4e1d\uff0c\u901a\u8fc7\u4f3a\u670d\u9a71\u52a8\u7ebf\u7f06\u63a7\u5236\u5b9e\u73b0\u6293\u53d6\u52a8\u4f5c\uff0c\u7535\u673a\u53cd\u9988\u63d0\u4f9b\u81ea\u4e3b\u6293\u53d6\u8c03\u6574\u548c\u53ef\u8c03\u6293\u53d6\u529b\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u5bf9\u4e8e\u5939\u5177\u6709\u6548\u5c3a\u5bf8\u8303\u56f4\u5185\u7684\u756a\u8304\uff0c\u91c7\u6458\u6210\u529f\u7387\u8fbe\u5230100%\u5373\u65f6\u635f\u4f24\u7387\uff0c5\u5929\u540e\u7600\u4f24\u7387\u4f4e\u4e8e9%\u3002", "conclusion": "\u8be5\u7ebf\u9a71\u52a8\u8f6f\u5939\u5177\u8bbe\u8ba1\u7b80\u5355\u3001\u6210\u672c\u4f4e\uff0c\u80fd\u591f\u8f7b\u67d4\u5730\u91c7\u6458\u4e0d\u540c\u5927\u5c0f\u7684\u6c34\u679c\uff0c\u9002\u5408\u6c34\u679c\u91c7\u6458\u5e94\u7528\u3002"}}
{"id": "2510.18137", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18137", "abs": "https://arxiv.org/abs/2510.18137", "authors": ["Hrishikesh Sathyanarayan", "Victor Vantilborgh", "Ian Abraham"], "title": "Quality Over Quantity: Curating Contact-Based Robot Datasets Improves Learning", "comment": null, "summary": "In this paper, we investigate the utility of datasets and whether more data\nor the 'right' data is advantageous for robot learning. In particular, we are\ninterested on quantifying the utility of contact-based data as contact holds\nsignificant information for robot learning. Our approach derives a\ncontact-aware objective function for learning object dynamics and shape from\npose and contact data. We show that the contact-aware Fisher-information metric\ncan be used to rank and curate contact-data based on how informative data is\nfor learning. In addition, we find that selecting a reduced dataset based on\nthis ranking improves the learning task while also making learning a\ndeterministic process. Interestingly, our results show that more data is not\nnecessarily advantageous, and rather, less but informative data can accelerate\nlearning, especially depending on the contact interactions. Last, we show how\nour metric can be used to provide initial guidance on data curation for\ncontact-based robot learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u96c6\u7684\u6548\u7528\uff0c\u63a2\u8ba8\u66f4\u591a\u6570\u636e\u8fd8\u662f'\u6b63\u786e'\u7684\u6570\u636e\u66f4\u6709\u4f18\u52bf\u3002\u91cd\u70b9\u91cf\u5316\u57fa\u4e8e\u63a5\u89e6\u7684\u6570\u636e\u7684\u6548\u7528\uff0c\u63d0\u51fa\u63a5\u89e6\u611f\u77e5\u7684Fisher\u4fe1\u606f\u5ea6\u91cf\u6765\u8bc4\u4f30\u548c\u7b5b\u9009\u63a5\u89e6\u6570\u636e\uff0c\u53d1\u73b0\u9009\u62e9\u4fe1\u606f\u91cf\u5927\u7684\u5c11\u91cf\u6570\u636e\u6bd4\u5927\u91cf\u6570\u636e\u66f4\u6709\u6548\u3002", "motivation": "\u7814\u7a76\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6570\u636e\u96c6\u7684\u6548\u7528\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u63a5\u89e6\u6570\u636e\u7684\u91cd\u8981\u6027\uff0c\u56e0\u4e3a\u63a5\u89e6\u4fe1\u606f\u5bf9\u673a\u5668\u4eba\u5b66\u4e60\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u63a5\u89e6\u611f\u77e5\u7684\u76ee\u6807\u51fd\u6570\uff0c\u7528\u4e8e\u4ece\u59ff\u6001\u548c\u63a5\u89e6\u6570\u636e\u4e2d\u5b66\u4e60\u7269\u4f53\u52a8\u529b\u5b66\u548c\u5f62\u72b6\u3002\u4f7f\u7528\u63a5\u89e6\u611f\u77e5\u7684Fisher\u4fe1\u606f\u5ea6\u91cf\u6765\u5bf9\u63a5\u89e6\u6570\u636e\u8fdb\u884c\u6392\u5e8f\u548c\u7b5b\u9009\u3002", "result": "\u57fa\u4e8e\u4fe1\u606f\u5ea6\u91cf\u7684\u6392\u540d\u9009\u62e9\u51cf\u5c11\u7684\u6570\u636e\u96c6\u80fd\u6539\u5584\u5b66\u4e60\u4efb\u52a1\u5e76\u4f7f\u5b66\u4e60\u8fc7\u7a0b\u786e\u5b9a\u6027\u5316\u3002\u7ed3\u679c\u663e\u793a\u66f4\u591a\u6570\u636e\u4e0d\u4e00\u5b9a\u66f4\u597d\uff0c\u5c11\u91cf\u4f46\u4fe1\u606f\u91cf\u5927\u7684\u6570\u636e\u80fd\u52a0\u901f\u5b66\u4e60\uff0c\u7279\u522b\u662f\u53d6\u51b3\u4e8e\u63a5\u89e6\u4ea4\u4e92\u7684\u7c7b\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u5ea6\u91cf\u65b9\u6cd5\u53ef\u4e3a\u57fa\u4e8e\u63a5\u89e6\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u6570\u636e\u7b5b\u9009\u7684\u521d\u6b65\u6307\u5bfc\uff0c\u5f3a\u8c03\u6570\u636e\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u91cd\u8981\u3002"}}
{"id": "2510.18316", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18316", "abs": "https://arxiv.org/abs/2510.18316", "authors": ["Chengshu Li", "Mengdi Xu", "Arpit Bahety", "Hang Yin", "Yunfan Jiang", "Huang Huang", "Josiah Wong", "Sujay Garlanka", "Cem Gokmen", "Ruohan Zhang", "Weiyu Liu", "Jiajun Wu", "Roberto Mart\u00edn-Mart\u00edn", "Li Fei-Fei"], "title": "MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation", "comment": "Project website: momagen.github.io. The first four authors contribute\n  equally", "summary": "Imitation learning from large-scale, diverse human demonstrations has proven\neffective for training robots, but collecting such data is costly and\ntime-consuming. This challenge is amplified for multi-step bimanual mobile\nmanipulation, where humans must teleoperate both a mobile base and two\nhigh-degree-of-freedom arms. Prior automated data generation frameworks have\naddressed static bimanual manipulation by augmenting a few human demonstrations\nin simulation, but they fall short for mobile settings due to two key\nchallenges: (1) determining base placement to ensure reachability, and (2)\npositioning the camera to provide sufficient visibility for visuomotor\npolicies. To address these issues, we introduce MoMaGen, which formulates data\ngeneration as a constrained optimization problem that enforces hard constraints\n(e.g., reachability) while balancing soft constraints (e.g., visibility during\nnavigation). This formulation generalizes prior approaches and provides a\nprincipled foundation for future methods. We evaluate MoMaGen on four\nmulti-step bimanual mobile manipulation tasks and show that it generates\nsignificantly more diverse datasets than existing methods. Leveraging this\ndiversity, MoMaGen can train successful imitation learning policies from a\nsingle source demonstration, and these policies can be fine-tuned with as few\nas 40 real-world demonstrations to achieve deployment on physical robotic\nhardware. More details are available at our project page: momagen.github.io.", "AI": {"tldr": "MoMaGen\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u6b65\u9aa4\u53cc\u624b\u79fb\u52a8\u64cd\u4f5c\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u89e3\u51b3\u57fa\u5ea7\u653e\u7f6e\u548c\u76f8\u673a\u5b9a\u4f4d\u95ee\u9898\uff0c\u80fd\u591f\u4ece\u5355\u4e00\u6e90\u6f14\u793a\u751f\u6210\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u90e8\u7f72\u3002", "motivation": "\u4ece\u5927\u89c4\u6a21\u591a\u6837\u5316\u4eba\u7c7b\u6f14\u793a\u4e2d\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\u5bf9\u8bad\u7ec3\u673a\u5668\u4eba\u6709\u6548\uff0c\u4f46\u6536\u96c6\u6b64\u7c7b\u6570\u636e\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u9aa4\u53cc\u624b\u79fb\u52a8\u64cd\u4f5c\u573a\u666f\u4e2d\uff0c\u9700\u8981\u540c\u65f6\u8fdc\u7a0b\u64cd\u4f5c\u79fb\u52a8\u57fa\u5ea7\u548c\u4e24\u4e2a\u9ad8\u81ea\u7531\u5ea6\u624b\u81c2\u3002", "method": "MoMaGen\u5c06\u6570\u636e\u751f\u6210\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5f3a\u5236\u6267\u884c\u786c\u7ea6\u675f\uff08\u5982\u53ef\u8fbe\u6027\uff09\u540c\u65f6\u5e73\u8861\u8f6f\u7ea6\u675f\uff08\u5982\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u53ef\u89c1\u6027\uff09\uff0c\u4e3a\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u5e76\u652f\u6301\u672a\u6765\u65b9\u6cd5\u53d1\u5c55\u3002", "result": "\u5728\u56db\u4e2a\u591a\u6b65\u9aa4\u53cc\u624b\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMoMaGen\u751f\u6210\u7684\u6570\u636e\u96c6\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u66f4\u591a\u6837\u5316\uff0c\u80fd\u591f\u4ece\u5355\u4e00\u6e90\u6f14\u793a\u8bad\u7ec3\u6210\u529f\u7684\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u4ec540\u4e2a\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u8fdb\u884c\u5fae\u8c03\u5373\u53ef\u5728\u7269\u7406\u673a\u5668\u4eba\u786c\u4ef6\u4e0a\u90e8\u7f72\u3002", "conclusion": "MoMaGen\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u6570\u636e\u6536\u96c6\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4ece\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6210\u529f\u8fc1\u79fb\u3002"}}
{"id": "2510.18337", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18337", "abs": "https://arxiv.org/abs/2510.18337", "authors": ["Wenhui Huang", "Changhe Chen", "Han Qi", "Chen Lv", "Yilun Du", "Heng Yang"], "title": "MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning", "comment": null, "summary": "Integrating visual-language instructions into visuomotor policies is gaining\nmomentum in robot learning for enhancing open-world generalization. Despite\npromising advances, existing approaches face two challenges: limited language\nsteerability when no generated reasoning is used as a condition, or significant\ninference latency when reasoning is incorporated.In this work, we introduce\nMoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)\nmodel that integrates fast-slow unified reasoning with behavior policy\nlearning. MoTVLA preserves the general intelligence of pre-trained VLMs\n(serving as the generalist) for tasks such as perception, scene understanding,\nand semantic planning, while incorporating a domain expert, a second\ntransformer that shares knowledge with the pretrained VLM, to generate\ndomain-specific fast reasoning (e.g., robot motion decomposition), thereby\nimproving policy execution efficiency. By conditioning the action expert on\ndecomposed motion instructions, MoTVLA can learn diverse behaviors and\nsubstantially improve language steerability. Extensive evaluations across\nnatural language processing benchmarks, robotic simulation environments, and\nreal-world experiments confirm the superiority of MoTVLA in both fast-slow\nreasoning and manipulation task performance.", "AI": {"tldr": "MoTVLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df7\u5408Transformer\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u5feb\u6162\u7edf\u4e00\u63a8\u7406\u4e0e\u884c\u4e3a\u7b56\u7565\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u8a00\u53ef\u63a7\u6027\u548c\u63a8\u7406\u5ef6\u8fdf\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u5f53\u4e0d\u4f7f\u7528\u751f\u6210\u63a8\u7406\u4f5c\u4e3a\u6761\u4ef6\u65f6\u8bed\u8a00\u53ef\u63a7\u6027\u6709\u9650\uff0c\u6216\u8005\u5f53\u52a0\u5165\u63a8\u7406\u65f6\u63a8\u7406\u5ef6\u8fdf\u663e\u8457\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u7528\u667a\u80fd\uff0c\u53c8\u80fd\u63d0\u9ad8\u7b56\u7565\u6267\u884c\u6548\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\uff0c\u5305\u542b\u901a\u7528\u4e13\u5bb6\uff08\u9884\u8bad\u7ec3VLM\uff09\u548c\u9886\u57df\u4e13\u5bb6\uff08\u7b2c\u4e8c\u4e2aTransformer\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u77e5\u8bc6\u751f\u6210\u9886\u57df\u7279\u5b9a\u7684\u5feb\u901f\u63a8\u7406\uff08\u5982\u673a\u5668\u4eba\u8fd0\u52a8\u5206\u89e3\uff09\uff0c\u5e76\u5c06\u52a8\u4f5c\u4e13\u5bb6\u57fa\u4e8e\u5206\u89e3\u7684\u8fd0\u52a8\u6307\u4ee4\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u57fa\u51c6\u6d4b\u8bd5\u3001\u673a\u5668\u4eba\u4eff\u771f\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cMoTVLA\u5728\u5feb\u6162\u63a8\u7406\u548c\u64cd\u4f5c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "MoTVLA\u80fd\u591f\u5b66\u4e60\u591a\u6837\u5316\u884c\u4e3a\u5e76\u663e\u8457\u63d0\u9ad8\u8bed\u8a00\u53ef\u63a7\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3VLM\u7684\u901a\u7528\u667a\u80fd\uff0c\u5728\u63a8\u7406\u6548\u7387\u548c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2510.18347", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2510.18347", "abs": "https://arxiv.org/abs/2510.18347", "authors": ["Muhammad Hanif", "Reiji Terunuma", "Takumi Sumino", "Kelvin Cheng", "Takeshi Hatanaka"], "title": "Coverage-Recon: Coordinated Multi-Drone Image Sampling with Online Map Feedback", "comment": "Submitted to IEEE Transactions on Control Systems Technology (under\n  review). Project page: https://htnk-lab.github.io/coverage-recon/", "summary": "This article addresses collaborative 3D map reconstruction using multiple\ndrones. Achieving high-quality reconstruction requires capturing images of\nkeypoints within the target scene from diverse viewing angles, and coverage\ncontrol offers an effective framework to meet this requirement. Meanwhile,\nrecent advances in real-time 3D reconstruction algorithms make it possible to\nrender an evolving map during flight, enabling immediate feedback to guide\ndrone motion. Building on this, we present Coverage-Recon, a novel coordinated\nimage sampling algorithm that integrates online map feedback to improve\nreconstruction quality on-the-fly. In Coverage-Recon, the coordinated motion of\ndrones is governed by a Quadratic Programming (QP)-based angle-aware coverage\ncontroller, which ensures multi-viewpoint image capture while enforcing safety\nconstraints. The captured images are processed in real time by the NeuralRecon\nalgorithm to generate an evolving 3D mesh. Mesh changes across the scene are\ninterpreted as indicators of reconstruction uncertainty and serve as feedback\nto update the importance index of the coverage control as the map evolves. The\neffectiveness of Coverage-Recon is validated through simulation and\nexperiments, demonstrating both qualitatively and quantitatively that\nincorporating online map feedback yields more complete and accurate 3D\nreconstructions than conventional methods. Project page:\nhttps://htnk-lab.github.io/coverage-recon/", "AI": {"tldr": "\u63d0\u51faCoverage-Recon\u7b97\u6cd5\uff0c\u5c06\u5728\u7ebf\u5730\u56fe\u53cd\u9988\u96c6\u6210\u5230\u591a\u65e0\u4eba\u673a\u534f\u4f5c3D\u5730\u56fe\u91cd\u5efa\u4e2d\uff0c\u901a\u8fc7\u89d2\u5ea6\u611f\u77e5\u8986\u76d6\u63a7\u5236\u548c\u5b9e\u65f6\u7f51\u683c\u53d8\u5316\u5206\u6790\uff0c\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u5b9e\u73b0\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u9700\u8981\u4ece\u4e0d\u540c\u89c6\u89d2\u6355\u83b7\u5173\u952e\u70b9\u56fe\u50cf\uff0c\u800c\u8986\u76d6\u63a7\u5236\u6846\u67b6\u80fd\u6709\u6548\u6ee1\u8db3\u8fd9\u4e00\u8981\u6c42\u3002\u5b9e\u65f63D\u91cd\u5efa\u7b97\u6cd5\u7684\u8fdb\u6b65\u4f7f\u5f97\u5728\u98de\u884c\u671f\u95f4\u6e32\u67d3\u6f14\u5316\u5730\u56fe\u6210\u4e3a\u53ef\u80fd\uff0c\u4e3a\u65e0\u4eba\u673a\u8fd0\u52a8\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u4e8c\u6b21\u89c4\u5212\u7684\u89d2\u611f\u77e5\u8986\u76d6\u63a7\u5236\u5668\u534f\u8c03\u65e0\u4eba\u673a\u8fd0\u52a8\uff0c\u786e\u4fdd\u591a\u89c6\u89d2\u56fe\u50cf\u6355\u83b7\u5e76\u5f3a\u5236\u6267\u884c\u5b89\u5168\u7ea6\u675f\u3002\u6355\u83b7\u56fe\u50cf\u901a\u8fc7NeuralRecon\u7b97\u6cd5\u5b9e\u65f6\u5904\u7406\u751f\u6210\u6f14\u53163D\u7f51\u683c\uff0c\u5c06\u7f51\u683c\u53d8\u5316\u89e3\u91ca\u4e3a\u91cd\u5efa\u4e0d\u786e\u5b9a\u6027\u6307\u6807\uff0c\u5e76\u4f5c\u4e3a\u53cd\u9988\u66f4\u65b0\u8986\u76d6\u63a7\u5236\u7684\u91cd\u8981\u6027\u6307\u6570\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Coverage-Recon\u7684\u6709\u6548\u6027\uff0c\u5b9a\u6027\u548c\u5b9a\u91cf\u5206\u6790\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u96c6\u6210\u5728\u7ebf\u5730\u56fe\u53cd\u9988\u80fd\u4ea7\u751f\u66f4\u5b8c\u6574\u548c\u51c6\u786e\u76843D\u91cd\u5efa\u3002", "conclusion": "Coverage-Recon\u7b97\u6cd5\u6210\u529f\u5730\u5c06\u5728\u7ebf\u5730\u56fe\u53cd\u9988\u96c6\u6210\u5230\u534f\u4f5c3D\u5730\u56fe\u91cd\u5efa\u4e2d\uff0c\u901a\u8fc7\u5b9e\u65f6\u7f51\u683c\u53d8\u5316\u5206\u6790\u548c\u8986\u76d6\u63a7\u5236\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u5b8c\u6574\u6027\u3002"}}
{"id": "2510.18348", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18348", "abs": "https://arxiv.org/abs/2510.18348", "authors": ["Alexandros Ntagkas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion", "comment": "9 pages, 9 figures, 2 tables", "summary": "State-of-the-art perceptive Reinforcement Learning controllers for legged\nrobots either (i) impose oscillator or IK-based gait priors that constrain the\naction space, add bias to the policy optimization and reduce adaptability\nacross robot morphologies, or (ii) operate \"blind\", which struggle to\nanticipate hind-leg terrain, and are brittle to noise. In this paper, we\npropose Phase-Guided Terrain Traversal (PGTT), a perception-aware deep-RL\napproach that overcomes these limitations by enforcing gait structure purely\nthrough reward shaping, thereby reducing inductive bias in policy learning\ncompared to oscillator/IK-conditioned action priors. PGTT encodes per-leg phase\nas a cubic Hermite spline that adapts swing height to local heightmap\nstatistics and adds a swing- phase contact penalty, while the policy acts\ndirectly in joint space supporting morphology-agnostic deployment. Trained in\nMuJoCo (MJX) on procedurally generated stair-like terrains with curriculum and\ndomain randomization, PGTT achieves the highest success under push disturbances\n(median +7.5% vs. the next best method) and on discrete obstacles (+9%), with\ncomparable velocity tracking, and converging to an effective policy roughly 2x\nfaster than strong end-to-end baselines. We validate PGTT on a Unitree Go2\nusing a real-time LiDAR elevation-to-heightmap pipeline, and we report\npreliminary results on ANYmal-C obtained with the same hyperparameters. These\nfindings indicate that terrain-adaptive, phase-guided reward shaping is a\nsimple and general mechanism for robust perceptive locomotion across platforms.", "AI": {"tldr": "PGTT\u662f\u4e00\u79cd\u611f\u77e5\u611f\u77e5\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u5851\u5f62\u800c\u4e0d\u662f\u52a8\u4f5c\u5148\u9a8c\u6765\u5f3a\u5236\u6267\u884c\u6b65\u6001\u7ed3\u6784\uff0c\u5728\u5173\u8282\u7a7a\u95f4\u4e2d\u76f4\u63a5\u64cd\u4f5c\uff0c\u652f\u6301\u5f62\u6001\u65e0\u5173\u7684\u90e8\u7f72\uff0c\u5728\u6270\u52a8\u548c\u79bb\u6563\u969c\u788d\u7269\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u8981\u4e48\u65bd\u52a0\u632f\u8361\u5668\u6216\u57fa\u4e8e\u9006\u8fd0\u52a8\u5b66\u7684\u6b65\u6001\u5148\u9a8c\uff0c\u9650\u5236\u4e86\u52a8\u4f5c\u7a7a\u95f4\u5e76\u964d\u4f4e\u4e86\u8de8\u673a\u5668\u4eba\u5f62\u6001\u7684\u9002\u5e94\u6027\uff1b\u8981\u4e48\u662f\"\u76f2\"\u64cd\u4f5c\uff0c\u96be\u4ee5\u9884\u6d4b\u540e\u817f\u5730\u5f62\u4e14\u5bf9\u566a\u58f0\u8106\u5f31\u3002", "method": "PGTT\u901a\u8fc7\u5956\u52b1\u5851\u5f62\u5f3a\u5236\u6267\u884c\u6b65\u6001\u7ed3\u6784\uff0c\u4f7f\u7528\u4e09\u6b21Hermite\u6837\u6761\u7f16\u7801\u6bcf\u817f\u76f8\u4f4d\uff0c\u6839\u636e\u5c40\u90e8\u9ad8\u5ea6\u56fe\u7edf\u8ba1\u8c03\u6574\u6446\u52a8\u9ad8\u5ea6\uff0c\u5e76\u6dfb\u52a0\u6446\u52a8\u76f8\u4f4d\u63a5\u89e6\u60e9\u7f5a\uff0c\u7b56\u7565\u76f4\u63a5\u5728\u5173\u8282\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u3002", "result": "\u5728MuJoCo\u4e2d\u8bad\u7ec3\uff0cPGTT\u5728\u63a8\u52a8\u6270\u52a8\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u6210\u529f\u7387\uff08\u4e2d\u4f4d\u6570+7.5%\uff09\uff0c\u5728\u79bb\u6563\u969c\u788d\u7269\u4e0a+9%\uff0c\u901f\u5ea6\u8ddf\u8e2a\u6027\u80fd\u76f8\u5f53\uff0c\u6536\u655b\u901f\u5ea6\u6bd4\u5f3a\u7aef\u5230\u7aef\u57fa\u7ebf\u5feb\u7ea62\u500d\u3002", "conclusion": "\u5730\u5f62\u81ea\u9002\u5e94\u3001\u76f8\u4f4d\u5f15\u5bfc\u7684\u5956\u52b1\u5851\u5f62\u662f\u5b9e\u73b0\u8de8\u5e73\u53f0\u9c81\u68d2\u611f\u77e5\u8fd0\u52a8\u7684\u7b80\u5355\u901a\u7528\u673a\u5236\u3002"}}
{"id": "2510.18371", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18371", "abs": "https://arxiv.org/abs/2510.18371", "authors": ["Mingxin Li", "Haibo Hu", "Jinghuai Deng", "Yuchen Xi", "Xinhong Chen", "Jianping Wang"], "title": "MMRHP: A Miniature Mixed-Reality HIL Platform for Auditable Closed-Loop Evaluation", "comment": null, "summary": "Validation of autonomous driving systems requires a trade-off between test\nfidelity, cost, and scalability. While miniaturized hardware-in-the-loop (HIL)\nplatforms have emerged as a promising solution, a systematic framework\nsupporting rigorous quantitative analysis is generally lacking, limiting their\nvalue as scientific evaluation tools. To address this challenge, we propose\nMMRHP, a miniature mixed-reality HIL platform that elevates miniaturized\ntesting from functional demonstration to rigorous, reproducible quantitative\nanalysis. The core contributions are threefold. First, we propose a systematic\nthree-phase testing process oriented toward the Safety of the Intended\nFunctionality(SOTIF)standard, providing actionable guidance for identifying the\nperformance limits and triggering conditions of otherwise correctly functioning\nsystems. Second, we design and implement a HIL platform centered around a\nunified spatiotemporal measurement core to support this process, ensuring\nconsistent and traceable quantification of physical motion and system timing.\nFinally, we demonstrate the effectiveness of this solution through\ncomprehensive experiments. The platform itself was first validated, achieving a\nspatial accuracy of 10.27 mm RMSE and a stable closed-loop latency baseline of\napproximately 45 ms. Subsequently, an in-depth Autoware case study leveraged\nthis validated platform to quantify its performance baseline and identify a\ncritical performance cliff at an injected latency of 40 ms. This work shows\nthat a structured process, combined with a platform offering a unified\nspatio-temporal benchmark, enables reproducible, interpretable, and\nquantitative closed-loop evaluation of autonomous driving systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86MMRHP\u5e73\u53f0\uff0c\u5c06\u5c0f\u578b\u5316\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u4ece\u529f\u80fd\u6f14\u793a\u63d0\u5347\u5230\u4e25\u683c\u7684\u5b9a\u91cf\u5206\u6790\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d4b\u8bd5\u6d41\u7a0b\u548c\u7edf\u4e00\u7684\u65f6\u7a7a\u6d4b\u91cf\u6838\u5fc3\uff0c\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u91cd\u590d\u3001\u53ef\u89e3\u91ca\u7684\u95ed\u73af\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9a8c\u8bc1\u4e2d\u6d4b\u8bd5\u4fdd\u771f\u5ea6\u3001\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5f25\u8865\u73b0\u6709\u5c0f\u578b\u5316HIL\u5e73\u53f0\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5b9a\u91cf\u5206\u6790\u6846\u67b6\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u9636\u6bb5\u6d4b\u8bd5\u6d41\u7a0b\uff08\u9762\u5411SOTIF\u6807\u51c6\uff09\uff0c\u6784\u5efa\u4e86\u4ee5\u7edf\u4e00\u65f6\u7a7a\u6d4b\u91cf\u6838\u5fc3\u4e3a\u4e2d\u5fc3\u7684HIL\u5e73\u53f0\uff0c\u786e\u4fdd\u7269\u7406\u8fd0\u52a8\u548c\u7cfb\u7edf\u65f6\u5e8f\u7684\u4e00\u81f4\u53ef\u8ffd\u6eaf\u91cf\u5316\u3002", "result": "\u5e73\u53f0\u9a8c\u8bc1\u663e\u793a\u7a7a\u95f4\u7cbe\u5ea6\u8fbe\u523010.27 mm RMSE\uff0c\u95ed\u73af\u5ef6\u8fdf\u57fa\u7ebf\u7ea645 ms\uff1bAutoware\u6848\u4f8b\u7814\u7a76\u53d1\u73b040 ms\u6ce8\u5165\u5ef6\u8fdf\u65f6\u51fa\u73b0\u5173\u952e\u6027\u80fd\u60ac\u5d16\u3002", "conclusion": "\u7ed3\u6784\u5316\u6d41\u7a0b\u4e0e\u63d0\u4f9b\u7edf\u4e00\u65f6\u7a7a\u57fa\u51c6\u7684\u5e73\u53f0\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u53ef\u91cd\u590d\u3001\u53ef\u89e3\u91ca\u548c\u5b9a\u91cf\u95ed\u73af\u8bc4\u4f30\u3002"}}
{"id": "2510.18402", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18402", "abs": "https://arxiv.org/abs/2510.18402", "authors": ["Matthias Lorenzen", "Teodoro Alamo", "Martina Mammarella", "Fabrizio Dabbene"], "title": "MPC-based motion planning for non-holonomic systems in non-convex domains", "comment": "Preprint of ECC 2025 submission", "summary": "Motivated by the application of using model predictive control (MPC) for\nmotion planning of autonomous mobile robots, a form of output tracking MPC for\nnon- holonomic systems and with non-convex constraints is studied. Although the\nadvantages of using MPC for motion planning have been demonstrated in several\npapers, in most of the available fundamental literature on output tracking MPC\nit is assumed, often implicitly, that the model is holonomic and generally the\nstate or output constraints must be convex. Thus, in application-oriented\npublications, empirical results dominate and the topic of proving completeness,\nin particular under which assumptions the target is always reached, has\nreceived comparatively little attention. To address this gap, we present a\nnovel MPC formulation that guarantees convergence to the desired target under\nrealistic assumptions, which can be verified in relevant real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7528\u4e8e\u975e\u5b8c\u6574\u7cfb\u7edf\u548c\u975e\u51f8\u7ea6\u675f\u7684\u8f93\u51fa\u8ddf\u8e2a\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u65e8\u5728\u4e3a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709MPC\u6587\u732e\u5927\u591a\u5047\u8bbe\u7cfb\u7edf\u662f\u5b8c\u6574\u7684\u4e14\u7ea6\u675f\u4e3a\u51f8\u7684\uff0c\u800c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff09\uff0c\u7cfb\u7edf\u5f80\u5f80\u662f\u975e\u5b8c\u6574\u7684\u4e14\u7ea6\u675f\u662f\u975e\u51f8\u7684\u3002\u5e94\u7528\u5bfc\u5411\u7684\u8bba\u6587\u4e3b\u8981\u4f9d\u8d56\u7ecf\u9a8c\u7ed3\u679c\uff0c\u7f3a\u4e4f\u5bf9\u76ee\u6807\u53ef\u8fbe\u6027\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684MPC\u516c\u5f0f\uff0c\u80fd\u591f\u5728\u73b0\u5b9e\u5047\u8bbe\u4e0b\u4fdd\u8bc1\u6536\u655b\u5230\u671f\u671b\u76ee\u6807\uff0c\u8fd9\u4e9b\u5047\u8bbe\u5728\u76f8\u5173\u5b9e\u9645\u573a\u666f\u4e2d\u662f\u53ef\u9a8c\u8bc1\u7684\u3002", "result": "\u8be5MPC\u516c\u5f0f\u80fd\u591f\u4fdd\u8bc1\u5728\u975e\u5b8c\u6574\u7cfb\u7edf\u548c\u975e\u51f8\u7ea6\u675f\u6761\u4ef6\u4e0b\u6536\u655b\u5230\u76ee\u6807\u4f4d\u7f6e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684MPC\u65b9\u6cd5\u586b\u8865\u4e86\u7406\u8bba\u4fdd\u8bc1\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u3002"}}
{"id": "2510.18518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18518", "abs": "https://arxiv.org/abs/2510.18518", "authors": ["Fang Nan", "Hao Ma", "Qinghua Guan", "Josie Hughes", "Michael Muehlebach", "Marco Hutter"], "title": "Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning", "comment": null, "summary": "We present an online model-based reinforcement learning algorithm suitable\nfor controlling complex robotic systems directly in the real world. Unlike\nprevailing sim-to-real pipelines that rely on extensive offline simulation and\nmodel-free policy optimization, our method builds a dynamics model from\nreal-time interaction data and performs policy updates guided by the learned\ndynamics model. This efficient model-based reinforcement learning scheme\nsignificantly reduces the number of samples to train control policies, enabling\ndirect training on real-world rollout data. This significantly reduces the\ninfluence of bias in the simulated data, and facilitates the search for\nhigh-performance control policies. We adopt online learning analysis to derive\nsublinear regret bounds under standard stochastic online optimization\nassumptions, providing formal guarantees on performance improvement as more\ninteraction data are collected. Experimental evaluations were performed on a\nhydraulic excavator arm and a soft robot arm, where the algorithm demonstrates\nstrong sample efficiency compared to model-free reinforcement learning methods,\nreaching comparable performance within hours. Robust adaptation to shifting\ndynamics was also observed when the payload condition was randomized. Our\napproach paves the way toward efficient and reliable on-robot learning for a\nbroad class of challenging control tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u7ebf\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u76f4\u63a5\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u8bad\u7ec3\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\uff0c\u663e\u8457\u51cf\u5c11\u6837\u672c\u9700\u6c42\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edfsim-to-real\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u79bb\u7ebf\u4eff\u771f\u548c\u6a21\u578b\u65e0\u5173\u7b56\u7565\u4f18\u5316\uff0c\u5b58\u5728\u6a21\u62df\u6570\u636e\u504f\u5dee\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u76f4\u63a5\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002", "method": "\u4ece\u5b9e\u65f6\u4ea4\u4e92\u6570\u636e\u6784\u5efa\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u57fa\u4e8e\u5b66\u4e60\u5230\u7684\u52a8\u529b\u5b66\u6a21\u578b\u6307\u5bfc\u7b56\u7565\u66f4\u65b0\uff0c\u91c7\u7528\u5728\u7ebf\u5b66\u4e60\u5206\u6790\u63a8\u5bfc\u6027\u80fd\u4fdd\u8bc1\u3002", "result": "\u5728\u6db2\u538b\u6316\u6398\u81c2\u548c\u8f6f\u4f53\u673a\u5668\u4eba\u81c2\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u6a21\u578b\u65e0\u5173\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5177\u6709\u66f4\u5f3a\u7684\u6837\u672c\u6548\u7387\uff0c\u6570\u5c0f\u65f6\u5185\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\uff0c\u5e76\u80fd\u9002\u5e94\u52a8\u6001\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6311\u6218\u6027\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u673a\u5668\u4eba\u4e0a\u5b66\u4e60\u65b9\u6cd5\uff0c\u51cf\u5c11\u6a21\u62df\u6570\u636e\u504f\u5dee\u5f71\u54cd\uff0c\u4fc3\u8fdb\u9ad8\u6027\u80fd\u63a7\u5236\u7b56\u7565\u7684\u641c\u7d22\u3002"}}
{"id": "2510.18546", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.18546", "abs": "https://arxiv.org/abs/2510.18546", "authors": ["Zebin Yang", "Sunjian Zheng", "Tong Xie", "Tianshi Xu", "Bo Yu", "Fan Wang", "Jie Tang", "Shaoshan Liu", "Meng Li"], "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval", "comment": "NeurIPS 2025", "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.", "AI": {"tldr": "EfficientNav\u662f\u4e00\u79cd\u57fa\u4e8e\u5c0f\u578bLLM\u7684\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u5185\u5b58\u68c0\u7d22\u4f18\u5316\u5bfc\u822a\u5730\u56fe\u7406\u89e3\uff0c\u4f7f\u7528\u79bb\u6563\u5185\u5b58\u7f13\u5b58\u548c\u6ce8\u610f\u529b\u805a\u7c7b\u51cf\u5c11\u89c4\u5212\u5ef6\u8fdf\uff0c\u5728HM3D\u57fa\u51c6\u4e0a\u6bd4GPT-4\u57fa\u7ebf\u63d0\u534711.1%\u6210\u529f\u7387\uff0c\u5ef6\u8fdf\u964d\u4f4e6.7\u500d\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u4e91LLM\u7684\u7269\u4f53\u5bfc\u822a\u65b9\u6cd5\u65e0\u6cd5\u5728\u672c\u5730\u8bbe\u5907\u90e8\u7f72\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5c0f\u578bLLM\u7406\u89e3\u590d\u6742\u5bfc\u822a\u5730\u56fe\u80fd\u529b\u6709\u9650\uff0c\u4e14\u957f\u63d0\u793a\u5bfc\u81f4\u9ad8\u89c4\u5212\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u611f\u77e5\u5185\u5b58\u68c0\u7d22\u6765\u4fee\u526a\u5bfc\u822a\u5730\u56fe\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\uff0c\u4f7f\u7528\u79bb\u6563\u5185\u5b58\u7f13\u5b58\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5185\u5b58\u805a\u7c7b\u6765\u9ad8\u6548\u4fdd\u5b58\u548c\u91cd\u7528KV\u7f13\u5b58\u3002", "result": "\u5728HM3D\u57fa\u51c6\u4e0a\u6bd4GPT-4\u57fa\u7ebf\u63d0\u534711.1%\u6210\u529f\u7387\uff0c\u5b9e\u73b06.7\u500d\u5b9e\u65f6\u5ef6\u8fdf\u964d\u4f4e\u548c4.7\u500d\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u3002", "conclusion": "EfficientNav\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5c0f\u578bLLM\u7684\u9ad8\u6548\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u672c\u5730\u8bbe\u5907\u90e8\u7f72\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2510.18558", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18558", "abs": "https://arxiv.org/abs/2510.18558", "authors": ["Yue Wang", "Lixian Zhang", "Yimin Zhu", "Yangguang Liu", "Xuwei Yang"], "title": "Flexbee: A Grasping and Perching UAV Based on Soft Vector-Propulsion Nozzle", "comment": "11 pages, 17 figures", "summary": "The aim of this paper is to design a new type of grasping and perching\nunmanned aerial vehicle (UAV), called Flexbee, which features a soft\nvector-propulsion nozzle (SVPN). Compared to previous UAVs, Flexbee integrates\nflight, grasping, and perching functionalities into the four SVPNs. This\nintegration offers advantages including decoupled position and attitude\ncontrol, high structural reuse, and strong adaptability strong adaptability for\ngrasping and perching. A dynamics model of Flexbee has been developed, and the\nnonlinear coupling issue of the moment has been resolved through linearization\nof the equivalent moment model. A hierarchical control strategy was used to\ndesign controllers for the two operational modes of Flexbee. Finally, flight,\ngrasping, and perching experiments were conducted to validate Flexbee's\nkinematic capabilities and the effectiveness of the control strategy.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u6293\u53d6\u548c\u6816\u606f\u65e0\u4eba\u673aFlexbee\uff0c\u91c7\u7528\u8f6f\u77e2\u91cf\u63a8\u8fdb\u55b7\u5634(SVPN)\u6280\u672f\uff0c\u5c06\u98de\u884c\u3001\u6293\u53d6\u548c\u6816\u606f\u529f\u80fd\u96c6\u6210\u5230\u56db\u4e2aSVPN\u4e2d\uff0c\u5b9e\u73b0\u4e86\u4f4d\u7f6e\u548c\u59ff\u6001\u63a7\u5236\u7684\u89e3\u8026\u3001\u9ad8\u7ed3\u6784\u91cd\u7528\u6027\u548c\u5f3a\u9002\u5e94\u6027\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u96c6\u6210\u98de\u884c\u3001\u6293\u53d6\u548c\u6816\u606f\u529f\u80fd\u7684\u591a\u529f\u80fd\u65e0\u4eba\u673a\uff0c\u89e3\u51b3\u4f20\u7edf\u65e0\u4eba\u673a\u5728\u8fd9\u4e9b\u529f\u80fd\u4e0a\u7684\u5206\u79bb\u95ee\u9898\uff0c\u63d0\u9ad8\u7ed3\u6784\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u5f00\u53d1\u4e86Flexbee\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u7b49\u6548\u529b\u77e9\u6a21\u578b\u7684\u7ebf\u6027\u5316\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u8026\u5408\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u63a7\u5236\u7b56\u7565\u8bbe\u8ba1\u4e86\u4e24\u79cd\u64cd\u4f5c\u6a21\u5f0f\u7684\u63a7\u5236\u5668\u3002", "result": "\u901a\u8fc7\u98de\u884c\u3001\u6293\u53d6\u548c\u6816\u606f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Flexbee\u7684\u8fd0\u52a8\u5b66\u80fd\u529b\u548c\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "Flexbee\u6210\u529f\u5b9e\u73b0\u4e86\u98de\u884c\u3001\u6293\u53d6\u548c\u6816\u606f\u529f\u80fd\u7684\u96c6\u6210\uff0c\u8bc1\u660e\u4e86\u8f6f\u77e2\u91cf\u63a8\u8fdb\u55b7\u5634\u6280\u672f\u5728\u591a\u529f\u80fd\u65e0\u4eba\u673a\u8bbe\u8ba1\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002"}}
{"id": "2510.18600", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18600", "abs": "https://arxiv.org/abs/2510.18600", "authors": ["Shubham Vyas", "Franek Stark", "Rohit Kumar", "Hannah Isermann", "Jonas Haack", "Mihaela Popescu", "Jakob Middelberg", "Dennis Mronga", "Frank Kirchner"], "title": "Quadrupeds for Planetary Exploration: Field Testing Control Algorithms on an Active Volcano", "comment": "Presented at 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA)", "summary": "Missions such as the Ingenuity helicopter have shown the advantages of using\nnovel locomotion modes to increase the scientific return of planetary\nexploration missions. Legged robots can further expand the reach and capability\nof future planetary missions by traversing more difficult terrain than wheeled\nrovers, such as jumping over cracks on the ground or traversing rugged terrain\nwith boulders. To develop and test algorithms for using quadruped robots, the\nAAPLE project was carried out at DFKI. As part of the project, we conducted a\nseries of field experiments on the Volcano on the Aeolian island of Vulcano, an\nactive stratovolcano near Sicily, Italy. The experiments focused on validating\nnewly developed state-of-the-art adaptive optimal control algorithms for\nquadrupedal locomotion in a high-fidelity analog environment for Lunar and\nMartian surfaces. This paper presents the technical approach, test plan,\nsoftware architecture, field deployment strategy, and evaluation results from\nthe Vulcano campaign.", "AI": {"tldr": "\u5728\u610f\u5927\u5229\u6b66\u5c14\u5361\u8bfa\u706b\u5c71\u8fdb\u884c\u7684\u56db\u8db3\u673a\u5668\u4eba\u5b9e\u5730\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u7528\u4e8e\u6708\u7403\u548c\u706b\u661f\u8868\u9762\u6a21\u62df\u73af\u5883\u7684\u65b0\u578b\u81ea\u9002\u5e94\u6700\u4f18\u63a7\u5236\u7b97\u6cd5\u3002", "motivation": "\u5229\u7528\u817f\u5f0f\u673a\u5668\u4eba\u6269\u5c55\u672a\u6765\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u7a7f\u8d8a\u6bd4\u8f6e\u5f0f\u6f2b\u6e38\u8f66\u66f4\u56f0\u96be\u7684\u5730\u5f62\uff08\u5982\u8df3\u8fc7\u5730\u9762\u88c2\u7f1d\u6216\u5728\u5d0e\u5c96\u5730\u5f62\u4e2d\u7a7f\u884c\uff09\u6765\u63d0\u9ad8\u79d1\u5b66\u56de\u62a5\u3002", "method": "\u5728\u610f\u5927\u5229\u6b66\u5c14\u5361\u8bfa\u706b\u5c71\u7684\u706b\u5c71\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u5730\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u65b0\u5f00\u53d1\u7684\u81ea\u9002\u5e94\u6700\u4f18\u63a7\u5236\u7b97\u6cd5\uff0c\u5305\u62ec\u6280\u672f\u65b9\u6cd5\u3001\u6d4b\u8bd5\u8ba1\u5212\u3001\u8f6f\u4ef6\u67b6\u6784\u548c\u73b0\u573a\u90e8\u7f72\u7b56\u7565\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u6708\u7403\u548c\u706b\u661f\u8868\u9762\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u7b97\u6cd5\u3002", "conclusion": "\u817f\u5f0f\u673a\u5668\u4eba\u80fd\u591f\u663e\u8457\u589e\u5f3a\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u5730\u5f62\u73af\u5883\u4e2d\uff0c\u4e3a\u672a\u6765\u884c\u661f\u63a2\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u9a8c\u8bc1\u3002"}}
{"id": "2510.18608", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.18608", "abs": "https://arxiv.org/abs/2510.18608", "authors": ["Luigi Quarantiello", "Elia Piccoli", "Jack Bell", "Malio Li", "Giacomo Carf\u00ec", "Eric Nuertey Coleman", "Gerlando Gramaglia", "Lanpei Li", "Mauro Madeddu", "Irene Testa", "Vincenzo Lomonaco"], "title": "A Compositional Paradigm for Foundation Models: Towards Smarter Robotic Agents", "comment": null, "summary": "The birth of Foundation Models brought unprecedented results in a wide range\nof tasks, from language to vision, to robotic control. These models are able to\nprocess huge quantities of data, and can extract and develop rich\nrepresentations, which can be employed across different domains and modalities.\nHowever, they still have issues in adapting to dynamic, real-world scenarios\nwithout retraining the entire model from scratch. In this work, we propose the\napplication of Continual Learning and Compositionality principles to foster the\ndevelopment of more flexible, efficient and smart AI solutions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u6301\u7eed\u5b66\u4e60\u548c\u7ec4\u5408\u6027\u539f\u5219\u5e94\u7528\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u5176\u5728\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u4e2d\u9002\u5e94\u6027\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9002\u5e94\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\uff0c\u8fd9\u65e2\u4f4e\u6548\u53c8\u6602\u8d35\u3002", "method": "\u5e94\u7528\u6301\u7eed\u5b66\u4e60\u548c\u7ec4\u5408\u6027\u539f\u5219\uff0c\u4f7f\u57fa\u7840\u6a21\u578b\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u65b0\u4efb\u52a1\u548c\u73af\u5883\uff0c\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u66f4\u7075\u6d3b\u3001\u9ad8\u6548\u548c\u667a\u80fd\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "\u6301\u7eed\u5b66\u4e60\u548c\u7ec4\u5408\u6027\u539f\u5219\u7684\u7ed3\u5408\u4e3a\u6784\u5efa\u66f4\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u52a8\u6001\u53d8\u5316\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2510.18643", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18643", "abs": "https://arxiv.org/abs/2510.18643", "authors": ["Mattias Trende", "Petter \u00d6gren"], "title": "Least Restrictive Hyperplane Control Barrier Functions", "comment": null, "summary": "Control Barrier Functions (CBFs) can provide provable safety guarantees for\ndynamic systems. However, finding a valid CBF for a system of interest is often\nnon-trivial, especially if the shape of the unsafe region is complex and the\nCBFs are of higher order. A common solution to this problem is to make a\nconservative approximation of the unsafe region in the form of a\nline/hyperplane, and use the corresponding conservative Hyperplane-CBF when\ndeciding on safe control actions. In this letter, we note that conservative\nconstraints are only a problem if they prevent us from doing what we want.\nThus, instead of first choosing a CBF and then choosing a safe control with\nrespect to the CBF, we optimize over a combination of CBFs and safe controls to\nget as close as possible to our desired control, while still having the safety\nguarantee provided by the CBF. We call the corresponding CBF the least\nrestrictive Hyperplane-CBF. Finally, we also provide a way of creating a smooth\nparameterization of the CBF-family for the optimization, and illustrate the\napproach on a double integrator dynamical system with acceleration constraints,\nmoving through a group of arbitrarily shaped static and moving obstacles.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u63a7\u5236\u5c4f\u969c\u51fd\u6570(CBFs)\u548c\u5b89\u5168\u7684\u63a7\u5236\u52a8\u4f5c\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6700\u5c0f\u9650\u5236\u8d85\u5e73\u9762CBF\uff0c\u65e8\u5728\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u5c3d\u53ef\u80fd\u63a5\u8fd1\u671f\u671b\u7684\u63a7\u5236\u52a8\u4f5c\u3002", "motivation": "\u4f20\u7edf\u7684CBF\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u5f62\u72b6\u7684\u4e0d\u5b89\u5168\u533a\u57df\u65f6\uff0c\u5f80\u5f80\u9700\u8981\u91c7\u7528\u4fdd\u5b88\u7684\u8d85\u5e73\u9762\u8fd1\u4f3c\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u8fc7\u5ea6\u9650\u5236\u63a7\u5236\u52a8\u4f5c\u3002\u672c\u6587\u6ce8\u610f\u5230\u4fdd\u5b88\u7ea6\u675f\u53ea\u6709\u5728\u963b\u6b62\u6211\u4eec\u5b9e\u73b0\u671f\u671b\u76ee\u6807\u65f6\u624d\u6210\u4e3a\u95ee\u9898\u3002", "method": "\u4e0d\u662f\u5148\u9009\u62e9CBF\u518d\u9009\u62e9\u5b89\u5168\u63a7\u5236\uff0c\u800c\u662f\u540c\u65f6\u4f18\u5316CBFs\u548c\u5b89\u5168\u63a7\u5236\u7684\u7ec4\u5408\uff0c\u4ee5\u83b7\u5f97\u5c3d\u53ef\u80fd\u63a5\u8fd1\u671f\u671b\u63a7\u5236\u7684\u5b89\u5168\u63a7\u5236\u3002\u63d0\u4f9b\u4e86CBF\u65cf\u7684\u5149\u6ed1\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u5177\u6709\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u53cc\u79ef\u5206\u5668\u52a8\u6001\u7cfb\u7edf\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u63a7\u5236\u52a8\u4f5c\u7684\u9650\u5236\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u66f4\u7075\u6d3b\u5730\u901a\u8fc7\u4efb\u610f\u5f62\u72b6\u7684\u9759\u6001\u548c\u79fb\u52a8\u969c\u788d\u7269\u3002", "conclusion": "\u63d0\u51fa\u7684\u6700\u5c0f\u9650\u5236\u8d85\u5e73\u9762CBF\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfCBF\u7684\u4fdd\u5b88\u6027\u95ee\u9898\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u63a7\u5236\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b89\u5168\u4fdd\u8bc1\u3002"}}
{"id": "2510.18678", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18678", "abs": "https://arxiv.org/abs/2510.18678", "authors": ["Alberto Sanchez-Delgado", "Jo\u00e3o Carlos Virgolino Soares", "David Omar Al Tawil", "Alessia Li Noce", "Matteo Villa", "Victor Barasuol", "Paolo Arena", "Claudio Semini"], "title": "Towards An Adaptive Locomotion Strategy For Quadruped Rovers: Quantifying When To Slide Or Walk On Planetary Slopes", "comment": "Published at the 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA 2025)", "summary": "Legged rovers provide enhanced mobility compared to wheeled platforms,\nenabling navigation on steep and irregular planetary terrains. However,\ntraditional legged locomotion might be energetically inefficient and\npotentially dangerous to the rover on loose and inclined surfaces, such as\ncrater walls and cave slopes. This paper introduces a preliminary study that\ncompares the Cost of Transport (CoT) of walking and torso-based sliding\nlocomotion for quadruped robots across different slopes, friction conditions\nand speed levels. By identifying intersections between walking and sliding CoT\ncurves, we aim to define threshold conditions that may trigger transitions\nbetween the two strategies. The methodology combines physics-based simulations\nin Isaac Sim with particle interaction validation in ANSYS-Rocky. Our results\nrepresent an initial step towards adaptive locomotion strategies for planetary\nlegged rovers.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5761\u5ea6\u3001\u6469\u64e6\u6761\u4ef6\u548c\u901f\u5ea6\u4e0b\u7684\u884c\u8d70\u4e0e\u57fa\u4e8e\u8eaf\u5e72\u6ed1\u52a8\u8fd0\u52a8\u7684\u8fd0\u8f93\u6210\u672c\uff0c\u65e8\u5728\u786e\u5b9a\u4e24\u79cd\u7b56\u7565\u4e4b\u95f4\u7684\u8f6c\u6362\u9608\u503c\u6761\u4ef6\u3002", "motivation": "\u4f20\u7edf\u817f\u5f0f\u8fd0\u52a8\u5728\u677e\u6563\u548c\u503e\u659c\u8868\u9762\uff08\u5982\u9668\u77f3\u5751\u58c1\u548c\u6d1e\u7a74\u659c\u5761\uff09\u4e0a\u53ef\u80fd\u80fd\u91cf\u6548\u7387\u4f4e\u4e0b\u4e14\u5bf9\u6f2b\u6e38\u8f66\u6709\u6f5c\u5728\u5371\u9669\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u66f4\u6709\u6548\u7684\u8fd0\u52a8\u7b56\u7565\u3002", "method": "\u7ed3\u5408Isaac Sim\u4e2d\u7684\u7269\u7406\u6a21\u62df\u548cANSYS-Rocky\u4e2d\u7684\u7c92\u5b50\u4ea4\u4e92\u9a8c\u8bc1\uff0c\u6bd4\u8f83\u884c\u8d70\u548c\u6ed1\u52a8\u8fd0\u52a8\u7684\u8fd0\u8f93\u6210\u672c\u3002", "result": "\u901a\u8fc7\u8bc6\u522b\u884c\u8d70\u548c\u6ed1\u52a8CoT\u66f2\u7ebf\u7684\u4ea4\u70b9\uff0c\u5b9a\u4e49\u4e86\u89e6\u53d1\u4e24\u79cd\u7b56\u7565\u4e4b\u95f4\u8f6c\u6362\u7684\u9608\u503c\u6761\u4ef6\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u662f\u671d\u7740\u884c\u661f\u817f\u5f0f\u6f2b\u6e38\u8f66\u81ea\u9002\u5e94\u8fd0\u52a8\u7b56\u7565\u8fc8\u51fa\u7684\u521d\u6b65\u6b65\u9aa4\u3002"}}
{"id": "2510.18697", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18697", "abs": "https://arxiv.org/abs/2510.18697", "authors": ["Phuoc Nguyen", "Francesco Verdoja", "Ville Kyrki"], "title": "Event-Grounding Graph: Unified Spatio-Temporal Scene Graph from Robotic Observations", "comment": "Submitted to RA-L", "summary": "A fundamental aspect for building intelligent autonomous robots that can\nassist humans in their daily lives is the construction of rich environmental\nrepresentations. While advances in semantic scene representations have enriched\nrobotic scene understanding, current approaches lack a connection between\nspatial features and dynamic events; e.g., connecting the blue mug to the event\nwashing a mug. In this work, we introduce the event-grounding graph (EGG), a\nframework grounding event interactions to spatial features of a scene. This\nrepresentation allows robots to perceive, reason, and respond to complex\nspatio-temporal queries. Experiments using real robotic data demonstrate EGG's\ncapability to retrieve relevant information and respond accurately to human\ninquiries concerning the environment and events within. Furthermore, the EGG\nframework's source code and evaluation dataset are released as open-source at:\nhttps://github.com/aalto-intelligent-robotics/EGG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e8b\u4ef6\u63a5\u5730\u56fe\uff08EGG\uff09\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u4ea4\u4e92\u4e0e\u573a\u666f\u7a7a\u95f4\u7279\u5f81\u8fde\u63a5\u8d77\u6765\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u548c\u54cd\u5e94\u590d\u6742\u7684\u65f6\u7a7a\u67e5\u8be2\u3002", "motivation": "\u6784\u5efa\u667a\u80fd\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u8981\u4e30\u5bcc\u7684\u73af\u5883\u8868\u793a\uff0c\u4f46\u73b0\u6709\u8bed\u4e49\u573a\u666f\u8868\u793a\u65b9\u6cd5\u7f3a\u4e4f\u7a7a\u95f4\u7279\u5f81\u4e0e\u52a8\u6001\u4e8b\u4ef6\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u5982\u5c06\u84dd\u8272\u676f\u5b50\u4e0e\u6d17\u676f\u5b50\u4e8b\u4ef6\u8054\u7cfb\u8d77\u6765\u3002", "method": "\u5f15\u5165\u4e8b\u4ef6\u63a5\u5730\u56fe\uff08EGG\uff09\u6846\u67b6\uff0c\u5c06\u4e8b\u4ef6\u4ea4\u4e92\u63a5\u5730\u5230\u573a\u666f\u7684\u7a7a\u95f4\u7279\u5f81\u4e0a\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEGG\u80fd\u591f\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u5e76\u51c6\u786e\u54cd\u5e94\u6709\u5173\u73af\u5883\u548c\u4e8b\u4ef6\u7684\u4eba\u7c7b\u67e5\u8be2\u3002", "conclusion": "EGG\u6846\u67b6\u6210\u529f\u8fde\u63a5\u4e86\u7a7a\u95f4\u7279\u5f81\u4e0e\u52a8\u6001\u4e8b\u4ef6\uff0c\u4e3a\u673a\u5668\u4eba\u73af\u5883\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u76f8\u5173\u6e90\u4ee3\u7801\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.18766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18766", "abs": "https://arxiv.org/abs/2510.18766", "authors": ["Alexander Krawciw", "Sven Lilge", "Luka Antonyshyn", "Timothy D. Barfoot"], "title": "Sharing the Load: Distributed Model-Predictive Control for Precise Multi-Rover Cargo Transport", "comment": "8 pages, 4 figures", "summary": "For autonomous cargo transportation, teams of mobile robots can provide more\noperational flexibility than a single large robot. In these scenarios,\nprecision in both inter-vehicle distance and path tracking is key. With this\nmotivation, we develop a distributed model-predictive controller (MPC) for\nmulti-vehicle cargo operations that builds on the precise path-tracking of\nlidar teach and repeat. To carry cargo, a following vehicle must maintain a\nEuclidean distance offset from a lead vehicle regardless of the path curvature.\nOur approach uses a shared map to localize the robots relative to each other\nwithout GNSS or direct observations. We compare our approach to a centralized\nMPC and a baseline approach that directly measures the inter-vehicle distance.\nThe distributed MPC shows equivalent nominal performance to the more complex\ncentralized MPC. Using a direct measurement of the relative distance between\nthe leader and follower shows improved tracking performance in close-range\nscenarios but struggles with long-range offsets. The operational flexibility\nprovided by distributing the computation makes it well suited for real\ndeployments. We evaluate four types of convoyed path trackers with over 10 km\nof driving in a coupled convoy. With convoys of two and three rovers, the\nproposed distributed MPC method works in real-time to allow map-based convoying\nto maintain maximum spacing within 20 cm of the target in various conditions.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u591a\u8f66\u8f86\u8d27\u7269\u8fd0\u8f93\u7684\u5206\u5e03\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u901a\u8fc7\u5171\u4eab\u5730\u56fe\u5b9e\u73b0\u8f66\u8f86\u95f4\u76f8\u5bf9\u5b9a\u4f4d\uff0c\u65e0\u9700GNSS\u6216\u76f4\u63a5\u89c2\u6d4b\uff0c\u572810\u516c\u91cc\u4ee5\u4e0a\u7684\u8f66\u961f\u884c\u9a76\u6d4b\u8bd5\u4e2d\u80fd\u4fdd\u630120\u5398\u7c73\u5185\u7684\u95f4\u8ddd\u7cbe\u5ea6\u3002", "motivation": "\u81ea\u4e3b\u8d27\u7269\u8fd0\u8f93\u4e2d\uff0c\u591a\u673a\u5668\u4eba\u8f66\u961f\u6bd4\u5355\u4e2a\u5927\u578b\u673a\u5668\u4eba\u66f4\u5177\u64cd\u4f5c\u7075\u6d3b\u6027\uff0c\u4f46\u9700\u8981\u7cbe\u786e\u7684\u8f66\u8f86\u95f4\u8ddd\u79bb\u548c\u8def\u5f84\u8ddf\u8e2a\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u6559\u5b66\u91cd\u590d\u7684\u7cbe\u786e\u8def\u5f84\u8ddf\u8e2a\uff0c\u5f00\u53d1\u5206\u5e03\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff0c\u4f7f\u7528\u5171\u4eab\u5730\u56fe\u8fdb\u884c\u673a\u5668\u4eba\u95f4\u76f8\u5bf9\u5b9a\u4f4d\uff0c\u65e0\u9700GNSS\u6216\u76f4\u63a5\u89c2\u6d4b\u3002", "result": "\u5206\u5e03\u5f0fMPC\u5728\u540d\u4e49\u6027\u80fd\u4e0a\u4e0e\u66f4\u590d\u6742\u7684\u96c6\u4e2d\u5f0fMPC\u76f8\u5f53\uff1b\u76f4\u63a5\u6d4b\u91cf\u76f8\u5bf9\u8ddd\u79bb\u7684\u65b9\u6cd5\u5728\u8fd1\u8ddd\u79bb\u573a\u666f\u4e2d\u8ddf\u8e2a\u6027\u80fd\u66f4\u597d\uff0c\u4f46\u5728\u957f\u8ddd\u79bb\u504f\u79fb\u65f6\u8868\u73b0\u4e0d\u4f73\uff1b\u5728\u8d85\u8fc710\u516c\u91cc\u7684\u8f66\u961f\u884c\u9a76\u6d4b\u8bd5\u4e2d\uff0c\u5206\u5e03\u5f0fMPC\u80fd\u5b9e\u65f6\u5de5\u4f5c\uff0c\u5728\u591a\u79cd\u6761\u4ef6\u4e0b\u4fdd\u6301\u6700\u5927\u95f4\u8ddd\u5728\u76ee\u680720\u5398\u7c73\u8303\u56f4\u5185\u3002", "conclusion": "\u5206\u5e03\u5f0f\u8ba1\u7b97\u65b9\u6cd5\u63d0\u4f9b\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5b9e\u9645\u90e8\u7f72\uff0c\u63d0\u51fa\u7684\u5206\u5e03\u5f0fMPC\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u57fa\u4e8e\u5730\u56fe\u7684\u8f66\u961f\u8fd0\u8f93\uff0c\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u4fdd\u6301\u7cbe\u786e\u7684\u95f4\u8ddd\u63a7\u5236\u3002"}}
{"id": "2510.18776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18776", "abs": "https://arxiv.org/abs/2510.18776", "authors": ["Emad Razavi", "Angelo Bratta", "Jo\u00e3o Carlos Virgolino Soares", "Carmine Recchiuto", "Claudio Semini"], "title": "Online Object-Level Semantic Mapping for Quadrupeds in Real-World Environments", "comment": "Published at the Italian Conference on Robotics and Intelligent\n  Machines (I-RIM) 3D, 2025", "summary": "We present an online semantic object mapping system for a quadruped robot\noperating in real indoor environments, turning sensor detections into named\nobjects in a global map. During a run, the mapper integrates range geometry\nwith camera detections, merges co-located detections within a frame, and\nassociates repeated detections into persistent object instances across frames.\nObjects remain in the map when they are out of view, and repeated sightings\nupdate the same instance rather than creating duplicates. The output is a\ncompact object layer that can be queried (class, pose, and confidence), is\nintegrated with the occupancy map and readable by a planner. In on-robot tests,\nthe layer remained stable across viewpoint changes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u5728\u7ebf\u8bed\u4e49\u7269\u4f53\u5efa\u56fe\u7cfb\u7edf\uff0c\u5c06\u4f20\u611f\u5668\u68c0\u6d4b\u8f6c\u6362\u4e3a\u5168\u5c40\u5730\u56fe\u4e2d\u7684\u547d\u540d\u7269\u4f53", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\uff0c\u5982\u4f55\u5c06\u4f20\u611f\u5668\u68c0\u6d4b\u7ed3\u679c\u6574\u5408\u4e3a\u6301\u4e45\u3001\u53ef\u67e5\u8be2\u7684\u8bed\u4e49\u7269\u4f53\u5730\u56fe\u7684\u95ee\u9898", "method": "\u96c6\u6210\u8ddd\u79bb\u51e0\u4f55\u4e0e\u76f8\u673a\u68c0\u6d4b\uff0c\u5728\u5e27\u5185\u5408\u5e76\u5171\u7f6e\u68c0\u6d4b\uff0c\u8de8\u5e27\u5173\u8054\u91cd\u590d\u68c0\u6d4b\u5f62\u6210\u6301\u4e45\u7269\u4f53\u5b9e\u4f8b\uff0c\u7269\u4f53\u79bb\u5f00\u89c6\u91ce\u65f6\u4ecd\u4fdd\u7559\u5728\u56fe\u4e2d", "result": "\u751f\u6210\u7d27\u51d1\u7684\u7269\u4f53\u5c42\uff0c\u53ef\u67e5\u8be2\u7c7b\u522b\u3001\u4f4d\u59ff\u548c\u7f6e\u4fe1\u5ea6\uff0c\u4e0e\u5360\u636e\u5730\u56fe\u96c6\u6210\uff0c\u89c4\u5212\u5668\u53ef\u8bfb\u53d6\uff0c\u5728\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\u89c6\u89d2\u53d8\u5316\u65f6\u4fdd\u6301\u7a33\u5b9a", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u7a33\u5b9a\u5730\u6784\u5efa\u8bed\u4e49\u7269\u4f53\u5730\u56fe\uff0c\u652f\u6301\u673a\u5668\u4eba\u5bfc\u822a\u548c\u89c4\u5212\u4efb\u52a1"}}
{"id": "2510.18845", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.18845", "abs": "https://arxiv.org/abs/2510.18845", "authors": ["Ryan Teoh", "Sander Tonkens", "William Sharpless", "Aijia Yang", "Zeyuan Feng", "Somil Bansal", "Sylvia Herbert"], "title": "MADR: MPC-guided Adversarial DeepReach", "comment": "8 pages, under review", "summary": "Hamilton-Jacobi (HJ) Reachability offers a framework for generating safe\nvalue functions and policies in the face of adversarial disturbance, but is\nlimited by the curse of dimensionality. Physics-informed deep learning is able\nto overcome this infeasibility, but itself suffers from slow and inaccurate\nconvergence, primarily due to weak PDE gradients and the complexity of\nself-supervised learning. A few works, recently, have demonstrated that\nenriching the self-supervision process with regular supervision (based on the\nnature of the optimal control problem), greatly accelerates convergence and\nsolution quality, however, these have been limited to single player problems\nand simple games. In this work, we introduce MADR: MPC-guided Adversarial\nDeepReach, a general framework to robustly approximate the two-player, zero-sum\ndifferential game value function. In doing so, MADR yields the corresponding\noptimal strategies for both players in zero-sum games as well as safe policies\nfor worst-case robustness. We test MADR on a multitude of high-dimensional\nsimulated and real robotic agents with varying dynamics and games, finding that\nour approach significantly out-performs state-of-the-art baselines in\nsimulation and produces impressive results in hardware.", "AI": {"tldr": "MADR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408MPC\u6307\u5bfc\u7684\u5bf9\u6297\u6027\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4Hamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u7684\u8ba1\u7b97\u96be\u9898\uff0c\u80fd\u591f\u9c81\u68d2\u5730\u903c\u8fd1\u4e24\u4eba\u96f6\u548c\u5fae\u5206\u535a\u5f08\u7684\u503c\u51fd\u6570\uff0c\u5e76\u5728\u591a\u79cd\u9ad8\u7ef4\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfHamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u9762\u4e34\u7ef4\u5ea6\u707e\u96be\u95ee\u9898\uff0c\u800c\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u867d\u7136\u80fd\u514b\u670d\u8ba1\u7b97\u4e0d\u53ef\u884c\u6027\uff0c\u4f46\u5b58\u5728\u6536\u655b\u6162\u548c\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e24\u4eba\u96f6\u548c\u535a\u5f08\u573a\u666f\u4e2d\u3002", "method": "\u63d0\u51faMADR\u6846\u67b6\uff0c\u5c06MPC\u6307\u5bfc\u4e0e\u5bf9\u6297\u6027\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\uff0c\u901a\u8fc7\u589e\u5f3a\u81ea\u76d1\u7763\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u6b63\u5219\u76d1\u7763\u6765\u52a0\u901f\u6536\u655b\u548c\u63d0\u9ad8\u89e3\u7684\u8d28\u91cf\u3002", "result": "\u5728\u591a\u79cd\u9ad8\u7ef4\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6d4b\u8bd5\uff0cMADR\u5728\u4eff\u771f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MADR\u4e3a\u4e24\u4eba\u96f6\u548c\u5fae\u5206\u535a\u5f08\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u503c\u51fd\u6570\u903c\u8fd1\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u6700\u4f18\u7b56\u7565\u548c\u9c81\u68d2\u5b89\u5168\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u53ef\u8fbe\u6027\u5206\u6790\u7684\u8ba1\u7b97\u6311\u6218\u3002"}}
