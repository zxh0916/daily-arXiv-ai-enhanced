<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction](https://arxiv.org/abs/2511.03931)
*Iman Adibnazari,Harsh Sharma,Myungsun Park,Jacobo Cervera-Torralba,Boris Kramer,Michael T. Tolley*

Main category: cs.RO

TL;DR: 本文比较了三种数据驱动模型降阶技术（ERA、DMDc、LOpInf）在鳗鱼仿生软体机器人动态形状控制中的效果，发现LOpInf方法在所有实验中都能产生最低的跟踪误差。


<details>
  <summary>Details</summary>
Motivation: 软体机器人在需要利用全身动态控制的场景中显示出巨大潜力，但有效的动态形状控制需要能够处理高维动力学的控制器，而缺乏适用于控制的通用建模工具加剧了这一挑战。

Method: 使用三种数据驱动模型降阶技术（特征系统实现算法、带控制的动态模态分解、拉格朗日算子推断方法）生成适用于动态形状控制的线性模型，并在模拟的鳗鱼仿生软体机器人上进行模型预测控制策略测试。

Result: 在所有三个实验中（跟踪保证可行的模拟参考轨迹、跟踪基于生物鳗鱼运动学模型生成的参考轨迹、跟踪由缩小物理模拟器生成的参考轨迹），基于LOpInf的策略产生的跟踪误差均低于基于其他模型的策略。

Conclusion: 拉格朗日算子推断方法在软体机器人动态形状控制中表现出优于其他数据驱动模型降阶技术的性能，为软体机器人的动态控制提供了有效的建模工具。

Abstract: Soft robots have shown immense promise in settings where they can leverage
dynamic control of their entire bodies. However, effective dynamic shape
control requires a controller that accounts for the robot's high-dimensional
dynamics--a challenge exacerbated by a lack of general-purpose tools for
modeling soft robots amenably for control. In this work, we conduct a
comparative study of data-driven model reduction techniques for generating
linear models amendable to dynamic shape control. We focus on three
methods--the eigensystem realization algorithm, dynamic mode decomposition with
control, and the Lagrangian operator inference (LOpInf) method. Using each
class of model, we explored their efficacy in model predictive control policies
for the dynamic shape control of a simulated eel-inspired soft robot in three
experiments: 1) tracking simulated reference trajectories guaranteed to be
feasible, 2) tracking reference trajectories generated from a biological model
of eel kinematics, and 3) tracking reference trajectories generated by a
reduced-scale physical analog. In all experiments, the LOpInf-based policies
generated lower tracking errors than policies based on other models.

</details>


### [2] [Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots](https://arxiv.org/abs/2511.03996)
*Yushi Wang,Changsheng Luo,Penghui Chen,Jianran Liu,Weijian Sun,Tong Guo,Kechang Yang,Biao Hu,Yangang Zhang,Mingguo Zhao*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的统一控制器，使人形机器人能够通过视觉感知和运动控制的直接集成来获得反应式足球技能。该方法将对抗运动先验扩展到真实动态环境中的感知设置，结合编码器-解码器架构和虚拟感知系统，在RoboCup比赛中表现出强大的反应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有系统依赖解耦模块导致的延迟响应和不连贯行为问题，以及真实世界感知限制对动态环境的进一步影响。

Method: 扩展对抗运动先验到感知设置，引入编码器-解码器架构和虚拟感知系统，从非完美观察中恢复特权状态，建立感知与动作的主动协调。

Result: 控制器表现出强大的反应性，在各种场景中持续执行连贯且鲁棒的足球行为，包括真实的RoboCup比赛。

Conclusion: 通过统一强化学习控制器直接集成视觉感知和运动控制，成功实现了人形机器人在动态环境中的反应式足球技能获取。

Abstract: Humanoid soccer poses a representative challenge for embodied intelligence,
requiring robots to operate within a tightly coupled perception-action loop.
However, existing systems typically rely on decoupled modules, resulting in
delayed responses and incoherent behaviors in dynamic environments, while
real-world perceptual limitations further exacerbate these issues. In this
work, we present a unified reinforcement learning-based controller that enables
humanoid robots to acquire reactive soccer skills through the direct
integration of visual perception and motion control. Our approach extends
Adversarial Motion Priors to perceptual settings in real-world dynamic
environments, bridging motion imitation and visually grounded dynamic control.
We introduce an encoder-decoder architecture combined with a virtual perception
system that models real-world visual characteristics, allowing the policy to
recover privileged states from imperfect observations and establish active
coordination between perception and action. The resulting controller
demonstrates strong reactivity, consistently executing coherent and robust
soccer behaviors across various scenarios, including real RoboCup matches.

</details>


### [3] [Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration](https://arxiv.org/abs/2511.04009)
*Chenzui Li,Yiming Chen,Xi Wu,Giacinto Barresi,Fei Chen*

Main category: cs.RO

TL;DR: 提出了一种用于双人协作搬运任务的上肢姿态优化方法，同时考虑人体工程学和力操纵性，通过优化简化人体骨骼模型的关节角度来提升安全性和操作能力，并使用模型预测阻抗控制器引导机器人调整末端执行器姿态。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常只关注人类安全或操作效率，而该方法独特地将这两个方面结合起来，以在不同条件下（如不同的抓握姿势和物体形状）加强协作。

Method: 通过最小化成本函数优化简化人体骨骼模型的关节角度，优先考虑安全性和操纵能力；使用变换模块生成机器人末端执行器的参考姿态；提出双手机器人的模型预测阻抗控制器来重新校准末端执行器姿态。

Result: 该方法在人与人协作和人机协作中通过多个受试者和物体进行了验证，实验结果显示通过比较优化前后目标肌肉的激活情况，肌肉状况有显著改善。

Conclusion: 所提出的方法能够有效改善双人协作搬运任务中的肌肉状况，提升物理人体工程学和力操纵性，为人机协作提供了有效的姿态优化解决方案。

Abstract: This paper introduces an upper limb postural optimization method for
enhancing physical ergonomics and force manipulability during bimanual
human-robot co-carrying tasks. Existing research typically emphasizes human
safety or manipulative efficiency, whereas our proposed method uniquely
integrates both aspects to strengthen collaboration across diverse conditions
(e.g., different grasping postures of humans, and different shapes of objects).
Specifically, the joint angles of a simplified human skeleton model are
optimized by minimizing the cost function to prioritize safety and manipulative
capability. To guide humans towards the optimized posture, the reference
end-effector poses of the robot are generated through a transformation module.
A bimanual model predictive impedance controller (MPIC) is proposed for our
human-like robot, CURI, to recalibrate the end effector poses through planned
trajectories. The proposed method has been validated through various subjects
and objects during human-human collaboration (HHC) and human-robot
collaboration (HRC). The experimental results demonstrate significant
improvement in muscle conditions by comparing the activation of target muscles
before and after optimization.

</details>


### [4] [An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue](https://arxiv.org/abs/2511.04042)
*Kailun Ji,Xiaoyu Hu,Xinyu Zhang,Jun Chen*

Main category: cs.RO

TL;DR: 该研究提出了一种LLM-CRF系统，利用大型语言模型来建模和增强人-群协作认知，通过自然交互方式捕获操作员意图，并自动分解任务和规划无人机群任务，显著提高了搜救任务的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 大规模灾难搜救行动面临复杂地形和通信中断的挑战，无人机群虽然提供了有前景的解决方案，但有效协调给人类操作员带来了巨大的认知负担。核心的人机协作瓶颈在于"意图到行动差距"，即在高压环境下将高级救援目标转化为低级群命令的易出错过程。

Method: 提出LLM-CRF系统，通过语音或图形标注等自然多模态交互捕获操作员意图，利用LLM作为认知引擎进行意图理解、分层任务分解和无人机群任务规划，形成闭环框架使群系统成为主动合作伙伴。

Result: 在模拟搜救场景中的实验结果显示，与传统命令式接口相比，LLM驱动方法将任务完成时间减少了约64.2%，任务成功率提高了7%，NASA-TLX主观认知负荷评分下降了42.9%。

Conclusion: 这项工作确立了LLM在高风险场景中创建更直观有效的人-群协作的潜力，使无人机群能够作为主动合作伙伴提供实时反馈，同时减少手动监控和控制的需求。

Abstract: Large-scale disaster Search And Rescue (SAR) operations are persistently
challenged by complex terrain and disrupted communications. While Unmanned
Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area
search and supply delivery, yet their effective coordination places a
significant cognitive burden on human operators. The core human-machine
collaboration bottleneck lies in the ``intention-to-action gap'', which is an
error-prone process of translating a high-level rescue objective into a
low-level swarm command under high intensity and pressure. To bridge this gap,
this study proposes a novel LLM-CRF system that leverages Large Language Models
(LLMs) to model and augment human-swarm teaming cognition. The proposed
framework initially captures the operator's intention through natural and
multi-modal interactions with the device via voice or graphical annotations. It
then employs the LLM as a cognitive engine to perform intention comprehension,
hierarchical task decomposition, and mission planning for the UAV swarm. This
closed-loop framework enables the swarm to act as a proactive partner,
providing active feedback in real-time while reducing the need for manual
monitoring and control, which considerably advances the efficacy of the SAR
task. We evaluate the proposed framework in a simulated SAR scenario.
Experimental results demonstrate that, compared to traditional order and
command-based interfaces, the proposed LLM-driven approach reduced task
completion time by approximately $64.2\%$ and improved task success rate by
$7\%$. It also leads to a considerable reduction in subjective cognitive
workload, with NASA-TLX scores dropping by $42.9\%$. This work establishes the
potential of LLMs to create more intuitive and effective human-swarm
collaborations in high-stakes scenarios.

</details>


### [5] [Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors](https://arxiv.org/abs/2511.04052)
*Kyongsik Yun,David Bayard,Gerik Kubiak,Austin Owens,Andrew Johnson,Ryan Johnson,Dan Scharf,Thomas Lu*

Main category: cs.RO

TL;DR: 该论文评估了在下一代多核处理器上部署GNC和LVS算法的性能，展示了相比传统航天硬件高达15倍的LVS图像处理加速和250倍的GFOLD轨迹优化加速，并提出了ARBITER多核投票机制进行实时故障检测和纠正。


<details>
  <summary>Details</summary>
Motivation: 未来行星探测任务需要高性能、容错的计算能力，以支持在进入、下降和着陆过程中的自主制导、导航与控制以及着陆视觉系统操作。

Method: 在HPSC、Snapdragon VOXL2和AMD Xilinx Versal等多核处理器上部署GNC和LVS算法，并开发ARBITER多核投票机制进行实时故障检测和纠正。

Result: LVS图像处理实现15倍加速，GFOLD轨迹优化实现250倍加速；故障注入研究识别出GFOLD中的梯度计算阶段对位级错误最敏感。

Conclusion: 这项工作为未来任务建立了可扩展且节能的架构，包括火星样本返回、土卫二轨道着陆器和谷神星样本返回等任务，其中机载自主性、低延迟和容错能力至关重要。

Abstract: Future planetary exploration missions demand high-performance, fault-tolerant
computing to enable autonomous Guidance, Navigation, and Control (GNC) and
Lander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).
This paper evaluates the deployment of GNC and LVS algorithms on
next-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx
Versal--demonstrating up to 15x speedup for LVS image processing and over 250x
speedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory
optimization compared to legacy spaceflight hardware. To ensure computational
reliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for
Trusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that
performs real-time fault detection and correction across redundant cores.
ARBITER is validated in both static optimization tasks (GFOLD) and dynamic
closed-loop control (Attitude Control System). A fault injection study further
identifies the gradient computation stage in GFOLD as the most sensitive to
bit-level errors, motivating selective protection strategies and vector-based
output arbitration. This work establishes a scalable and energy-efficient
architecture for future missions, including Mars Sample Return, Enceladus
Orbilander, and Ceres Sample Return, where onboard autonomy, low latency, and
fault resilience are critical.

</details>


### [6] [BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning](https://arxiv.org/abs/2511.04131)
*Yitang Li,Zhengyi Luo,Tonghe Zhang,Cunxi Dai,Anssi Kanervisto,Andrea Tirinzoni,Haoyang Weng,Kris Kitani,Mateusz Guzek,Ahmed Touati,Alessandro Lazaric,Matteo Pirotta,Guanya Shi*

Main category: cs.RO

TL;DR: BFM-Zero是一个用于人形机器人的行为基础模型框架，通过共享潜在表示统一多种控制任务，支持零样本运动跟踪、目标到达和奖励优化等功能，并在真实世界的Unitree G1人形机器人上实现了稳健的全身技能。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么仅限于模拟人形角色，要么专门针对特定任务如跟踪。BFM-Zero旨在开发一个单一、可提示的通用策略，统一不同的控制任务。

Method: 基于无监督强化学习和前向-后向模型，学习嵌入运动、目标和奖励的共享潜在表示，结合关键奖励塑造、领域随机化和历史依赖非对称学习来弥合模拟到现实的差距。

Result: BFM-Zero在真实世界人形机器人上实现了多样化的推理方法，包括零样本运动跟踪、目标到达和奖励优化，以及少量样本的基于优化的适应。

Conclusion: BFM-Zero是首个此类模型，为可扩展、可提示的全身人形控制行为基础模型迈出了重要一步。

Abstract: Building Behavioral Foundation Models (BFMs) for humanoid robots has the
potential to unify diverse control tasks under a single, promptable generalist
policy. However, existing approaches are either exclusively deployed on
simulated humanoid characters, or specialized to specific tasks such as
tracking. We propose BFM-Zero, a framework that learns an effective shared
latent representation that embeds motions, goals, and rewards into a common
space, enabling a single policy to be prompted for multiple downstream tasks
without retraining. This well-structured latent space in BFM-Zero enables
versatile and robust whole-body skills on a Unitree G1 humanoid in the real
world, via diverse inference methods, including zero-shot motion tracking, goal
reaching, and reward optimization, and few-shot optimization-based adaptation.
Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds
upon recent advancements in unsupervised RL and Forward-Backward (FB) models,
which offer an objective-centric, explainable, and smooth latent representation
of whole-body motions. We further extend BFM-Zero with critical reward shaping,
domain randomization, and history-dependent asymmetric learning to bridge the
sim-to-real gap. Those key design choices are quantitatively ablated in
simulation. A first-of-its-kind model, BFM-Zero establishes a step toward
scalable, promptable behavioral foundation models for whole-body humanoid
control.

</details>


### [7] [PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration](https://arxiv.org/abs/2511.04180)
*Yizhen Yin,Dapeng Feng,Hongbo Chen,Yuhua Qi*

Main category: cs.RO

TL;DR: 提出了一种结合路径-不确定性协同优化深度强化学习和轻量级停滞检测机制的混合框架，显著提高了主动SLAM的探索效率和路径质量。


<details>
  <summary>Details</summary>
Motivation: 现有主动SLAM方法存在探索速度慢和路径次优的问题，需要解决探索与利用的平衡以及冗余探索的问题。

Method: 采用路径-不确定性协同优化深度强化学习框架，通过双目标奖励函数同时优化旅行距离和地图不确定性；结合轻量级停滞检测机制，包括激光雷达静态异常检测和地图更新停滞检测，在低扩展率时终止探索。

Result: 相比前沿方法和RRT方法，探索时间缩短达65%，路径距离减少达42%，在复杂环境中显著提高探索效率，同时保持可靠的地图完整性。消融研究证实协作机制加速训练收敛。

Conclusion: 该算法在物理机器人平台上得到实证验证，展示了从仿真到现实环境的成功可迁移性，具有实际应用价值。

Abstract: Existing Active SLAM methodologies face issues such as slow exploration speed
and suboptimal paths. To address these limitations, we propose a hybrid
framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement
Learning framework and a Lightweight Stagnation Detection mechanism. The
Path-Uncertainty Co-Optimization framework jointly optimizes travel distance
and map uncertainty through a dual-objective reward function, balancing
exploration and exploitation. The Lightweight Stagnation Detection reduces
redundant exploration through Lidar Static Anomaly Detection and Map Update
Stagnation Detection, terminating episodes on low expansion rates. Experimental
results show that compared with the frontier-based method and RRT method, our
approach shortens exploration time by up to 65% and reduces path distance by up
to 42%, significantly improving exploration efficiency in complex environments
while maintaining reliable map completeness. Ablation studies confirm that the
collaborative mechanism accelerates training convergence. Empirical validation
on a physical robotic platform demonstrates the algorithm's practical
applicability and its successful transferability from simulation to real-world
environments.

</details>


### [8] [GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments](https://arxiv.org/abs/2511.04199)
*Shenglin Wang,Mingtong Dai,Jingxuan Su,Lingbo Liu,Chunjie Chen,Xinyu Wu,Liang Lin*

Main category: cs.RO

TL;DR: GraspView是一个仅使用RGB相机的机器人抓取系统，在杂乱环境中实现精确操作，无需深度传感器，特别擅长处理遮挡、近距离感知和透明物体。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB-D相机的抓取系统在透明或反光物体上会失败，且在近距离时性能下降。作者希望开发一个仅使用RGB相机的可靠抓取系统来解决这些问题。

Method: 集成三个关键组件：全局感知场景重建（从单RGB视图提供局部一致几何）、渲染评分主动感知策略（动态选择最佳视角揭示遮挡区域）、在线度量对齐模块（校准抓取预测与机器人运动学）。

Result: 在多样化桌面物体上的实验表明，GraspView显著优于RGB-D和单视角RGB基线方法，特别是在严重遮挡、近距离感知和透明物体情况下。

Conclusion: GraspView是RGB-D管道的实用且多功能替代方案，能够在非结构化真实环境中实现可靠抓取。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation, yet
remains highly challenging in cluttered environments where occlusion, poor
perception quality, and inconsistent 3D reconstructions often lead to unstable
or failed grasps. Conventional pipelines have widely relied on RGB-D cameras to
provide geometric information, which fail on transparent or glossy objects and
degrade at close range. We present GraspView, an RGB-only robotic grasping
pipeline that achieves accurate manipulation in cluttered environments without
depth sensors. Our framework integrates three key components: (i) global
perception scene reconstruction, which provides locally consistent, up-to-scale
geometry from a single RGB view and fuses multi-view projections into a
coherent global 3D scene; (ii) a render-and-score active perception strategy,
which dynamically selects next-best-views to reveal occluded regions; and (iii)
an online metric alignment module that calibrates VGGT predictions against
robot kinematics to ensure physical scale consistency. Building on these
tailor-designed modules, GraspView performs best-view global grasping, fusing
multi-view reconstructions and leveraging GraspNet for robust execution.
Experiments on diverse tabletop objects demonstrate that GraspView
significantly outperforms both RGB-D and single-view RGB baselines, especially
under heavy occlusion, near-field sensing, and with transparent objects. These
results highlight GraspView as a practical and versatile alternative to RGB-D
pipelines, enabling reliable grasping in unstructured real-world environments.

</details>


### [9] [Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies](https://arxiv.org/abs/2511.04249)
*Marco Iannotta,Yuxuan Yang,Johannes A. Stork,Erik Schaffernicht,Todor Stoyanov*

Main category: cs.RO

TL;DR: 本文研究了在机器人强化学习中通过条件化策略于动态参数估计（上下文）来改进仿真到现实的迁移性能，在标准控制基准和真实世界推动任务中验证了上下文感知策略优于上下文无关基线。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中仿真到现实迁移的主要挑战，即由于环境动态差异导致仿真训练的策略在现实世界中泛化失败的问题。

Method: 将上下文估计模块集成到基于域随机化的强化学习框架中，并系统比较最先进的监督策略，使策略能够根据动态参数估计进行条件化。

Result: 上下文感知策略在所有设置中都优于上下文无关基线，但最佳监督策略取决于具体任务。

Conclusion: 通过条件化策略于动态参数估计可以显著改进仿真到现实的迁移性能，但需要根据任务特性选择合适的监督策略。

Abstract: Sim-to-real transfer remains a major challenge in reinforcement learning (RL)
for robotics, as policies trained in simulation often fail to generalize to the
real world due to discrepancies in environment dynamics. Domain Randomization
(DR) mitigates this issue by exposing the policy to a wide range of randomized
dynamics during training, yet leading to a reduction in performance. While
standard approaches typically train policies agnostic to these variations, we
investigate whether sim-to-real transfer can be improved by conditioning the
policy on an estimate of the dynamics parameters -- referred to as context. To
this end, we integrate a context estimation module into a DR-based RL framework
and systematically compare SOTA supervision strategies. We evaluate the
resulting context-aware policies in both a canonical control benchmark and a
real-world pushing task using a Franka Emika Panda robot. Results show that
context-aware policies outperform the context-agnostic baseline across all
settings, although the best supervision strategy depends on the task.

</details>


### [10] [Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism](https://arxiv.org/abs/2511.04251)
*Jinfeng Liang,Haocheng Guo,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种可重构机翼的尾坐式垂直起降无人机，通过机翼伸缩设计、同轴异构双旋翼配置和无斜盘机制，解决了传统尾坐式无人机在多旋翼模式下易受风扰、功耗高和结构复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 传统尾坐式无人机在多旋翼模式下由于暴露较大的机身面积而容易受到风扰，同时存在功耗高和结构复杂的问题。

Method: 采用可伸缩机翼设计（多旋翼模式收缩、固定翼模式展开）、同轴异构双旋翼配置以提升能效、改进的无斜盘机制控制俯仰和滚转，并通过添加挥舞铰链优化结构减少振动。

Result: 显著降低了总功耗，减少了结构重量和复杂性，在过渡飞行测试中验证了无人机在整个飞行包线内的稳定性能。

Conclusion: 所提出的可重构机翼尾坐式无人机设计有效解决了风扰、功耗和结构复杂度问题，实现了稳定高效的飞行性能。

Abstract: The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to
its lower dead weight, which eliminates the actuators and mechanisms for
tilting. However, the tailsitter UAV is susceptible to wind disturbances in
multi-rotor mode, as it exposes a large frontal fuselage area. To address this
issue, our tailsitter UAV features a reconfigurable wing design, allowing wings
to retract in multi-rotor mode and extend in fixed- wing mode. Considering
power efficiency, we design a coaxial heterogeneous dual-rotor configuration,
which significantly re- duces the total power consumption. To reduce structural
weight and simplify structural complexity, we employ a swashplateless mechanism
with an improved design to control pitch and roll in multi-rotor mode. We
optimize the structure of the swashplateless mechanism by adding flapping
hinges, which reduces vibration during cyclic acceleration and deceleration.
Finally, we perform comprehensive transition flight tests to validate stable
flight performance across the entire flight envelope of the tailsitter UAV.

</details>


### [11] [MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments](https://arxiv.org/abs/2511.04320)
*Kuankuan Sima,Longbin Tang,Haozhe Ma,Lin Zhao*

Main category: cs.RO

TL;DR: MacroNav是一个基于学习的导航框架，通过轻量级上下文编码器和强化学习策略，在未知环境中实现高效导航，显著提升了成功率和路径效率。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中实现自主导航需要紧凑而富有表现力的空间理解能力，现有方法难以平衡丰富的上下文表示与导航效率之间的关系。

Method: 提出MacroNav框架，包含：(1)通过多任务自监督学习训练的轻量级上下文编码器，捕获多尺度导航中心空间表示；(2)结合图推理的强化学习策略，将这些表示无缝集成以进行高效动作选择。

Result: 大量实验证明上下文编码器具有高效和鲁棒的环境理解能力。真实世界部署进一步验证了MacroNav的有效性，在成功率和路径长度加权成功率方面显著优于最先进的导航方法，同时保持低计算成本。

Conclusion: MacroNav框架成功解决了未知环境中导航的效率和表示平衡问题，为自主导航提供了有效的解决方案。

Abstract: Autonomous navigation in unknown environments requires compact yet expressive
spatial understanding under partial observability to support high-level
decision making. Existing approaches struggle to balance rich contextual
representation with navigation efficiency. We present MacroNav, a
learning-based navigation framework featuring two key components: (1) a
lightweight context encoder trained via multi-task self-supervised learning to
capture multi-scale, navigation-centric spatial representations; and (2) a
reinforcement learning policy that seamlessly integrates these representations
with graph-based reasoning for efficient action selection. Extensive
experiments demonstrate the context encoder's efficient and robust
environmental understanding. Real-world deployments further validate MacroNav's
effectiveness, yielding significant gains over state-of-the-art navigation
methods in both Success Rate (SR) and Success weighted by Path Length (SPL),
while maintaining low computational cost. Code will be released upon
acceptance.

</details>


### [12] [GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies](https://arxiv.org/abs/2511.04357)
*Maëlic Neau,Zoe Falomir,Paulo E. Santos,Anne-Gwenn Bosser,Cédric Buche*

Main category: cs.RO

TL;DR: GraSP-VLA是一个神经符号框架，使用连续场景图表示来生成人类演示的符号表示，用于推理时生成新的规划域，并协调低层VLA策略以扩展连续执行动作的数量。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，端到端模仿学习的视觉语言动作模型缺乏高层符号规划能力，而符号方法中的动作模型学习缺乏泛化和可扩展性。需要结合两者优势来解决长时程任务中的挑战。

Method: 提出GraSP-VLA框架，使用连续场景图表示生成符号表示，在推理时生成新的规划域，并作为低层VLA策略的协调器。

Result: 实验表明GraSP-VLA在从观察自动生成规划域的任务中能有效建模符号表示，真实世界实验显示连续场景图表示在长时程任务中协调低层VLA策略具有潜力。

Conclusion: GraSP-VLA通过神经符号方法成功结合了符号规划和视觉语言动作模型的优势，为长时程任务提供了有效的解决方案。

Abstract: Deploying autonomous robots that can learn new skills from demonstrations is
an important challenge of modern robotics. Existing solutions often apply
end-to-end imitation learning with Vision-Language Action (VLA) models or
symbolic approaches with Action Model Learning (AML). On the one hand, current
VLA models are limited by the lack of high-level symbolic planning, which
hinders their abilities in long-horizon tasks. On the other hand, symbolic
approaches in AML lack generalization and scalability perspectives. In this
paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that
uses a Continuous Scene Graph representation to generate a symbolic
representation of human demonstrations. This representation is used to generate
new planning domains during inference and serves as an orchestrator for
low-level VLA policies, scaling up the number of actions that can be reproduced
in a row. Our results show that GraSP-VLA is effective for modeling symbolic
representations on the task of automatic planning domain generation from
observations. In addition, results on real-world experiments show the potential
of our Continuous Scene Graph representation to orchestrate low-level VLA
policies in long-horizon tasks.

</details>


### [13] [Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/abs/2511.04555)
*Tao Lin,Yilei Zhong,Yuxin Du,Jingjing Zhang,Jiting Liu,Yinxinyu Chen,Encheng Gu,Ziyan Liu,Hongyi Cai,Yanwen Zou,Lixing Zou,Zhaoye Zhou,Gen Li,Bo Zhao*

Main category: cs.RO

TL;DR: Evo-1是一个轻量级的视觉-语言-动作模型，通过新颖的跨调制扩散变换器和优化的集成模块，在仅0.77亿参数下实现SOTA性能，无需机器人数据预训练。


<details>
  <summary>Details</summary>
Motivation: 解决当前VLA模型参数庞大、依赖大规模机器人数据预训练、计算成本高、部署效率低以及感知表示退化的问题。

Method: 基于原生多模态VLM，引入跨调制扩散变换器和优化集成模块，采用两阶段训练范式逐步对齐动作与感知。

Result: 在Meta-World和RoboTwin套件上分别超越之前最佳模型12.4%和6.9%，LIBERO上达到94.8%，真实世界评估成功率78%，推理频率高且内存开销低。

Conclusion: Evo-1证明了轻量级VLA模型在保持高性能的同时显著提升部署效率的可行性，为未来高效VLA研究提供了新方向。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that
unifies perception, language, and control, enabling robots to perform diverse
tasks through multimodal understanding. However, current VLA models typically
contain massive parameters and rely heavily on large-scale robot data
pretraining, leading to high computational costs during training, as well as
limited deployability for real-time inference. Moreover, most training
paradigms often degrade the perceptual representations of the vision-language
backbone, resulting in overfitting and poor generalization to downstream tasks.
In this work, we present Evo-1, a lightweight VLA model that reduces
computation and improves deployment efficiency, while maintaining strong
performance without pretraining on robot data. Evo-1 builds on a native
multimodal Vision-Language model (VLM), incorporating a novel cross-modulated
diffusion transformer along with an optimized integration module, together
forming an effective architecture. We further introduce a two-stage training
paradigm that progressively aligns action with perception, preserving the
representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1
achieves state-of-the-art results on the Meta-World and RoboTwin suite,
surpassing the previous best models by 12.4% and 6.9%, respectively, and also
attains a competitive result of 94.8% on LIBERO. In real-world evaluations,
Evo-1 attains a 78% success rate with high inference frequency and low memory
overhead, outperforming all baseline methods. We release code, data, and model
weights to facilitate future research on lightweight and efficient VLA models.

</details>


### [14] [SAFe-Copilot: Unified Shared Autonomy Framework](https://arxiv.org/abs/2511.04664)
*Phat Nguyen,Erfan Aasi,Shiva Sreeram,Guy Rosman,Andrew Silva,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: 提出一个统一的共享自治框架，通过视觉语言模型从多模态线索推断驾驶员意图，在语义层面协调人类输入和自主规划，显著提升自动驾驶在罕见和模糊场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在罕见、模糊和分布外场景中表现脆弱，而人类驾驶员通过上下文推理能够成功应对。现有方法局限于低层轨迹协调，无法保持驾驶意图的连续性。

Method: 利用视觉语言模型从驾驶员动作和环境上下文等多模态线索推断驾驶意图，在更高抽象层面合成协调人类和自主控制的策略。

Result: 在模拟人类设置中实现完美召回率和高精度；人类主体调查显示92%的参与者同意仲裁结果；在Bench2Drive基准测试中显著降低碰撞率并提升整体性能。

Conclusion: 基于语义、语言表示的仲裁成为共享自治的设计原则，使系统能够运用常识推理并保持与人类意图的连续性。

Abstract: Autonomous driving systems remain brittle in rare, ambiguous, and
out-of-distribution scenarios, where human driver succeed through contextual
reasoning. Shared autonomy has emerged as a promising approach to mitigate such
failures by incorporating human input when autonomy is uncertain. However, most
existing methods restrict arbitration to low-level trajectories, which
represent only geometric paths and therefore fail to preserve the underlying
driving intent. We propose a unified shared autonomy framework that integrates
human input and autonomous planners at a higher level of abstraction. Our
method leverages Vision Language Models (VLMs) to infer driver intent from
multi-modal cues -- such as driver actions and environmental context -- and to
synthesize coherent strategies that mediate between human and autonomous
control. We first study the framework in a mock-human setting, where it
achieves perfect recall alongside high accuracy and precision. A human-subject
survey further shows strong alignment, with participants agreeing with
arbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive
benchmark demonstrates a substantial reduction in collision rate and
improvement in overall performance compared to pure autonomy. Arbitration at
the level of semantic, language-based representations emerges as a design
principle for shared autonomy, enabling systems to exercise common-sense
reasoning and maintain continuity with human intent.

</details>


### [15] [Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions](https://arxiv.org/abs/2511.04665)
*Kaifeng Zhang,Shuo Sha,Hanxiao Jiang,Matthew Loper,Hyunjong Song,Guangyan Cai,Zhuo Xu,Xiaochen Hu,Changxi Zheng,Yunzhu Li*

Main category: cs.RO

TL;DR: 本文提出了一个从真实世界视频构建软体数字孪生的真实到仿真策略评估框架，使用3D高斯泼溅技术实现逼真渲染，用于评估机器人对可变形物体的操作策略。


<details>
  <summary>Details</summary>
Motivation: 真实世界中直接评估机器人操作策略成本高、耗时长且难以复现，特别是涉及可变形物体的任务。现有模拟器难以捕捉软体交互的视觉和物理复杂性。

Method: 构建软体数字孪生，使用3D高斯泼溅技术实现机器人、物体和环境的照片级真实感渲染，结合物理信息重建技术。

Result: 在代表性可变形操作任务（毛绒玩具打包、绳索布线、T形块推动）上验证，模拟运行与真实世界执行性能高度相关，并能揭示学习策略的关键行为模式。

Conclusion: 将物理信息重建与高质量渲染相结合，能够实现机器人操作策略的可重复、可扩展和准确评估。

Abstract: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

</details>


### [16] [X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations](https://arxiv.org/abs/2511.04671)
*Maximus A. Pace,Prithwish Dan,Chuanruo Ning,Atiksh Bhardwaj,Audrey Du,Edward W. Duan,Wei-Chiu Ma,Kushal Kedia*

Main category: cs.RO

TL;DR: X-Diffusion是一个利用人类视频数据训练机器人扩散策略的框架，通过添加噪声来消除人类与机器人之间的执行差异，同时保留高层次任务指导。


<details>
  <summary>Details</summary>
Motivation: 人类视频数据丰富易得，但人类与机器人在执行动作上存在根本差异，直接使用会导致物理不可行的动作。需要一种方法既能利用人类数据，又避免学习不可行的动作。

Method: 首先训练一个分类器来区分人类和机器人执行的动作，然后在策略训练中，只有当添加足够噪声使得分类器无法区分执行主体时，才将人类动作纳入训练。与机器人执行一致的动作在低噪声水平下进行精细去噪监督，不匹配的人类动作仅在高噪声水平下提供粗略指导。

Result: 实验表明，在存在执行不匹配的情况下，简单的共同训练会降低策略性能，而X-Diffusion能持续提升性能。在五个操作任务中，X-Diffusion比最佳基线实现了16%的平均成功率提升。

Conclusion: X-Diffusion提供了一个原则性框架，能够最大限度地利用人类数据训练扩散策略，同时避免学习动态不可行的动作，在机器人操作任务中表现出优越性能。

Abstract: Human videos can be recorded quickly and at scale, making them an appealing
source of training data for robot learning. However, humans and robots differ
fundamentally in embodiment, resulting in mismatched action execution. Direct
kinematic retargeting of human hand motion can therefore produce actions that
are physically infeasible for robots. Despite these low-level differences,
human demonstrations provide valuable motion cues about how to manipulate and
interact with objects. Our key idea is to exploit the forward diffusion
process: as noise is added to actions, low-level execution differences fade
while high-level task guidance is preserved. We present X-Diffusion, a
principled framework for training diffusion policies that maximally leverages
human data without learning dynamically infeasible motions. X-Diffusion first
trains a classifier to predict whether a noisy action is executed by a human or
robot. Then, a human action is incorporated into policy training only after
adding sufficient noise such that the classifier cannot discern its embodiment.
Actions consistent with robot execution supervise fine-grained denoising at low
noise levels, while mismatched human actions provide only coarse guidance at
higher noise levels. Our experiments show that naive co-training under
execution mismatches degrades policy performance, while X-Diffusion
consistently improves it. Across five manipulation tasks, X-Diffusion achieves
a 16% higher average success rate than the best baseline. The project website
is available at https://portal-cornell.github.io/X-Diffusion/.

</details>


### [17] [GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction](https://arxiv.org/abs/2511.04679)
*Qingzhou Lu,Yao Feng,Baiyu Shi,Michael Piseno,Zhenan Bao,C. Karen Liu*

Main category: cs.RO

TL;DR: GentleHumanoid框架将阻抗控制集成到全身运动跟踪策略中，实现上半身柔顺性，通过统一的弹簧模型处理阻力和引导接触，在保持任务成功率的同时显著降低接触力峰值。


<details>
  <summary>Details</summary>
Motivation: 人形机器人需要在以人为中心的环境中安全自然地物理交互，但现有强化学习策略过于强调刚性跟踪而抑制外力，现有阻抗控制方法通常局限于基座或末端执行器控制，且主要关注抵抗极端力而非实现柔顺性。

Method: 提出GentleHumanoid框架，核心是统一的弹簧模型，同时建模阻力接触（按压表面时的恢复力）和引导接触（从人类运动数据采样的推拉动作），确保肩部、肘部和腕部的运动学一致性力，并通过任务可调力阈值增强安全性。

Result: 在仿真和Unitree G1人形机器人上评估，在需要不同柔顺性水平的任务中（温柔拥抱、坐站辅助、安全物体操作），相比基线方法，该策略在保持任务成功率的同时持续降低峰值接触力，实现更平滑自然的交互。

Conclusion: 该研究向能够在真实环境中安全有效与人类协作和处理物体的人形机器人迈出了一步，展示了通过阻抗控制实现柔顺交互的可行性。

Abstract: Humanoid robots are expected to operate in human-centered environments where
safe and natural physical interaction is essential. However, most recent
reinforcement learning (RL) policies emphasize rigid tracking and suppress
external forces. Existing impedance-augmented approaches are typically
restricted to base or end-effector control and focus on resisting extreme
forces rather than enabling compliance. We introduce GentleHumanoid, a
framework that integrates impedance control into a whole-body motion tracking
policy to achieve upper-body compliance. At its core is a unified spring-based
formulation that models both resistive contacts (restoring forces when pressing
against surfaces) and guiding contacts (pushes or pulls sampled from human
motion data). This formulation ensures kinematically consistent forces across
the shoulder, elbow, and wrist, while exposing the policy to diverse
interaction scenarios. Safety is further supported through task-adjustable
force thresholds. We evaluate our approach in both simulation and on the
Unitree G1 humanoid across tasks requiring different levels of compliance,
including gentle hugging, sit-to-stand assistance, and safe object
manipulation. Compared to baselines, our policy consistently reduces peak
contact forces while maintaining task success, resulting in smoother and more
natural interactions. These results highlight a step toward humanoid robots
that can safely and effectively collaborate with humans and handle objects in
real-world environments.

</details>
