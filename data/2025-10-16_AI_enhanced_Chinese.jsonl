{"id": "2510.12866", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12866", "abs": "https://arxiv.org/abs/2510.12866", "authors": ["Dantong Niu", "Yuvan Sharma", "Baifeng Shi", "Rachel Ding", "Matteo Gioia", "Haoru Xue", "Henry Tsai", "Konstantinos Kallidromitis", "Anirudh Pai", "Shankar Shastry", "Trevor Darrell", "Jitendra Malik", "Roei Herzig"], "title": "Learning to Grasp Anything by Playing with Random Toys", "comment": null, "summary": "Robotic manipulation policies often struggle to generalize to novel objects,\nlimiting their real-world utility. In contrast, cognitive science suggests that\nchildren develop generalizable dexterous manipulation skills by mastering a\nsmall set of simple toys and then applying that knowledge to more complex\nitems. Inspired by this, we study if similar generalization capabilities can\nalso be achieved by robots. Our results indicate robots can learn generalizable\ngrasping using randomly assembled objects that are composed from just four\nshape primitives: spheres, cuboids, cylinders, and rings. We show that training\non these \"toys\" enables robust generalization to real-world objects, yielding\nstrong zero-shot performance. Crucially, we find the key to this generalization\nis an object-centric visual representation induced by our proposed detection\npooling mechanism. Evaluated in both simulation and on physical robots, our\nmodel achieves a 67% real-world grasping success rate on the YCB dataset,\noutperforming state-of-the-art approaches that rely on substantially more\nin-domain data. We further study how zero-shot generalization performance\nscales by varying the number and diversity of training toys and the\ndemonstrations per toy. We believe this work offers a promising path to\nscalable and generalizable learning in robotic manipulation. Demonstration\nvideos, code, checkpoints and our dataset are available on our project page:\nhttps://lego-grasp.github.io/ .", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.12919", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12919", "abs": "https://arxiv.org/abs/2510.12919", "authors": ["Mouhyemen Khan", "Tatsuya Ibuki", "Abhijit Chatterjee"], "title": "Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation", "comment": "8 pages, 7 figures, under review", "summary": "Level set methods underpin modern safety techniques such as control barrier\nfunctions (CBFs), while also serving as implicit surface representations for\ngeometric shapes via distance fields. Inspired by these two paradigms, we\npropose a unified framework where the implicit surface itself acts as a CBF. We\nleverage Gaussian process (GP) implicit surface (GPIS) to represent the safety\nboundaries, using safety samples which are derived from sensor measurements to\ncondition the GP. The GP posterior mean defines the implicit safety surface\n(safety belief), while the posterior variance provides a robust safety margin.\nAlthough GPs have favorable properties such as uncertainty estimation and\nanalytical tractability, they scale cubically with data. To alleviate this\nissue, we develop a sparse solution called sparse Gaussian CBFs. To the best of\nour knowledge, GPIS have not been explicitly used to synthesize CBFs. We\nvalidate the approach on collision avoidance tasks in two settings: a simulated\n7-DOF manipulator operating around the Stanford bunny, and a quadrotor\nnavigating in 3D around a physical chair. In both cases, Gaussian CBFs (with\nand without sparsity) enable safe interaction and collision-free execution of\ntrajectories that would otherwise intersect the objects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u9ad8\u65af\u8fc7\u7a0b\u9690\u5f0f\u8868\u9762\uff08GPIS\uff09\u4f5c\u4e3a\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBF\uff09\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5b89\u5168\u63a7\u5236\u3002\u901a\u8fc7\u7a00\u758f\u5316\u89e3\u51b3\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5e76\u5728\u673a\u68b0\u81c2\u548c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u9a8c\u8bc1\u4e86\u78b0\u649e\u907f\u514d\u6548\u679c\u3002", "motivation": "\u7ed3\u5408\u6c34\u5e73\u96c6\u65b9\u6cd5\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u63a7\u5236\u6280\u672f\uff0c\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u9690\u5f0f\u8868\u9762\u6765\u8868\u793a\u5b89\u5168\u8fb9\u754c\uff0c\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u9c81\u68d2\u5b89\u5168\u8fb9\u754c\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u9690\u5f0f\u8868\u9762\uff08GPIS\uff09\u8868\u793a\u5b89\u5168\u8fb9\u754c\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u6d4b\u91cf\u6570\u636e\u8bad\u7ec3GP\u6a21\u578b\u3002GP\u540e\u9a8c\u5747\u503c\u5b9a\u4e49\u5b89\u5168\u8868\u9762\uff0c\u540e\u9a8c\u65b9\u5dee\u63d0\u4f9b\u5b89\u5168\u8fb9\u754c\u3002\u5f00\u53d1\u7a00\u758f\u9ad8\u65afCBF\u89e3\u51b3\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002", "result": "\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u64cd\u4f5c\u65af\u5766\u798f\u5154\u5b50\u548c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7ed5\u7269\u7406\u6905\u5b50\u5bfc\u822a\u7684\u5b9e\u9a8c\u4e2d\uff0c\u9ad8\u65afCBF\uff08\u5305\u62ec\u7a00\u758f\u7248\u672c\uff09\u5b9e\u73b0\u4e86\u5b89\u5168\u4ea4\u4e92\u548c\u78b0\u649e\u907f\u514d\uff0c\u80fd\u591f\u6267\u884c\u539f\u672c\u4f1a\u4e0e\u7269\u4f53\u76f8\u4ea4\u7684\u8f68\u8ff9\u3002", "conclusion": "\u9ad8\u65af\u8fc7\u7a0b\u9690\u5f0f\u8868\u9762\u4f5c\u4e3a\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b89\u5168\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5b89\u5168\u8fb9\u754c\uff0c\u7a00\u758f\u5316\u65b9\u6848\u89e3\u51b3\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u5b89\u5168\u5bfc\u822a\u4e2d\u8868\u73b0\u826f\u597d\u3002"}}
{"id": "2510.12924", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12924", "abs": "https://arxiv.org/abs/2510.12924", "authors": ["Pavel Pochobradsk\u00fd", "Ond\u0159ej Proch\u00e1zka", "Robert P\u011bni\u010dka", "Vojt\u011bch Von\u00e1sek", "Martin Saska"], "title": "Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this letter, we introduce Geometric Model Predictive Path Integral\n(GMPPI), a sampling-based controller capable of tracking agile trajectories\nwhile avoiding obstacles. In each iteration, GMPPI generates a large number of\ncandidate rollout trajectories and then averages them to create a nominal\ncontrol to be followed by the Unmanned Aerial Vehicle (UAV). We propose using\ngeometric SE(3) control to generate part of the rollout trajectories,\nsignificantly increasing precision in agile flight. Furthermore, we introduce\nvarying rollout simulation time step length and dynamic cost and noise\nparameters, vastly improving tracking performance of smooth and low-speed\ntrajectories over an existing Model Predictive Path Integral (MPPI)\nimplementation. Finally, we propose an integration of GMPPI with a stereo depth\ncamera, enabling online obstacle avoidance at high speeds, a crucial step\ntowards autonomous UAV flights in complex environments. The proposed controller\ncan track simulated agile reference trajectories with position error similar to\nthe geometric SE(3) controller. However, the same configuration of the proposed\ncontroller can avoid obstacles in a simulated forest environment at speeds of\nup to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware\nplanner. In real-world experiments, GMPPI retains the capability to track agile\ntrajectories and avoids obstacles at speeds of up to 10m/s.", "AI": {"tldr": "GMPPI\u662f\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u51e0\u4f55SE(3)\u63a7\u5236\u548c\u52a8\u6001\u53c2\u6570\u8c03\u6574\uff0c\u80fd\u591f\u5728\u8ddf\u8e2a\u654f\u6377\u8f68\u8ff9\u7684\u540c\u65f6\u8fdb\u884c\u5728\u7ebf\u907f\u969c\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u901f\u98de\u884c\u65f6\u65e2\u8981\u7cbe\u786e\u8ddf\u8e2a\u654f\u6377\u8f68\u8ff9\u53c8\u8981\u5b9e\u65f6\u907f\u969c\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u901f\u5e73\u6ed1\u8f68\u8ff9\u8ddf\u8e2a\u548c\u9ad8\u901f\u907f\u969c\u6027\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206(GMPPI)\u63a7\u5236\u5668\uff0c\u4f7f\u7528SE(3)\u51e0\u4f55\u63a7\u5236\u751f\u6210\u90e8\u5206\u8f68\u8ff9\uff0c\u5f15\u5165\u53d8\u5316\u7684\u4eff\u771f\u65f6\u95f4\u6b65\u957f\u548c\u52a8\u6001\u6210\u672c\u566a\u58f0\u53c2\u6570\uff0c\u5e76\u4e0e\u7acb\u4f53\u6df1\u5ea6\u76f8\u673a\u96c6\u6210\u5b9e\u73b0\u5728\u7ebf\u907f\u969c\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0cGMPPI\u80fd\u591f\u4ee5\u4e0e\u51e0\u4f55SE(3)\u63a7\u5236\u5668\u76f8\u4f3c\u7684\u7cbe\u5ea6\u8ddf\u8e2a\u654f\u6377\u8f68\u8ff9\uff0c\u540c\u65f6\u5728\u6a21\u62df\u68ee\u6797\u73af\u5883\u4e2d\u4ee513m/s\u7684\u901f\u5ea6\u907f\u969c\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u907f\u969c\u89c4\u5212\u5668\u3002\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u80fd\u591f\u572810m/s\u901f\u5ea6\u4e0b\u8ddf\u8e2a\u654f\u6377\u8f68\u8ff9\u5e76\u907f\u969c\u3002", "conclusion": "GMPPI\u63a7\u5236\u5668\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u9ad8\u901f\u98de\u884c\u4e2d\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6709\u6548\u907f\u969c\u7684\u80fd\u529b\uff0c\u662f\u5411\u590d\u6742\u73af\u5883\u4e2d\u81ea\u4e3b\u65e0\u4eba\u673a\u98de\u884c\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2510.12962", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12962", "abs": "https://arxiv.org/abs/2510.12962", "authors": ["Michal Mina\u0159\u00edk", "Vojt\u011bch Von\u00e1sek", "Robert P\u011bni\u010dka"], "title": "Enhancing Sampling-based Planning with a Library of Paths", "comment": null, "summary": "Path planning for 3D solid objects is a challenging problem, requiring a\nsearch in a six-dimensional configuration space, which is, nevertheless,\nessential in many robotic applications such as bin-picking and assembly. The\ncommonly used sampling-based planners, such as Rapidly-exploring Random Trees,\nstruggle with narrow passages where the sampling probability is low, increasing\nthe time needed to find a solution. In scenarios like robotic bin-picking,\nvarious objects must be transported through the same environment. However,\ntraditional planners start from scratch each time, losing valuable information\ngained during the planning process. We address this by using a library of past\nsolutions, allowing the reuse of previous experiences even when planning for a\nnew, previously unseen object. Paths for a set of objects are stored, and when\nplanning for a new object, we find the most similar one in the library and use\nits paths as approximate solutions, adjusting for possible mutual\ntransformations. The configuration space is then sampled along the approximate\npaths. Our method is tested in various narrow passage scenarios and compared\nwith state-of-the-art methods from the OMPL library. Results show significant\nspeed improvements (up to 85% decrease in the required time) of our method,\noften finding a solution in cases where the other planners fail. Our\nimplementation of the proposed method is released as an open-source package.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5386\u53f2\u8def\u5f84\u5e93\u76843D\u7269\u4f53\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528\u8fc7\u53bb\u89c4\u5212\u7ecf\u9a8c\u6765\u52a0\u901f\u65b0\u7269\u4f53\u7684\u8def\u5f84\u89c4\u5212\uff0c\u7279\u522b\u9488\u5bf9\u7a84\u901a\u9053\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u7684\u91c7\u6837\u89c4\u5212\u5668\u5728\u7a84\u901a\u9053\u573a\u666f\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u6bcf\u6b21\u89c4\u5212\u90fd\u4ece\u5934\u5f00\u59cb\uff0c\u65e0\u6cd5\u5229\u7528\u5386\u53f2\u7ecf\u9a8c\u3002\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\uff0c\u591a\u4e2a\u4e0d\u540c\u7269\u4f53\u9700\u8981\u7ecf\u8fc7\u76f8\u540c\u73af\u5883\uff0c\u91cd\u7528\u5386\u53f2\u8def\u5f84\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u3002", "method": "\u6784\u5efa\u5386\u53f2\u8def\u5f84\u5e93\u5b58\u50a8\u5df2\u89c4\u5212\u7269\u4f53\u7684\u8def\u5f84\uff0c\u5f53\u89c4\u5212\u65b0\u7269\u4f53\u65f6\uff0c\u5728\u5e93\u4e2d\u5bfb\u627e\u6700\u76f8\u4f3c\u7269\u4f53\u5e76\u4f7f\u7528\u5176\u8def\u5f84\u4f5c\u4e3a\u8fd1\u4f3c\u89e3\uff0c\u901a\u8fc7\u76f8\u4e92\u53d8\u6362\u8c03\u6574\u540e\u6cbf\u8fd1\u4f3c\u8def\u5f84\u91c7\u6837\u914d\u7f6e\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u79cd\u7a84\u901a\u9053\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0c\u76f8\u6bd4OMPL\u5e93\u4e2d\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u89c4\u5212\u65f6\u95f4\u6700\u591a\u51cf\u5c1185%\uff0c\u4e14\u5728\u4f20\u7edf\u89c4\u5212\u5668\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u627e\u5230\u89e3\u3002", "conclusion": "\u57fa\u4e8e\u5386\u53f2\u8def\u5f84\u91cd\u7528\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad83D\u7269\u4f53\u8def\u5f84\u89c4\u5212\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u7a84\u901a\u9053\u573a\u666f\u4e2d\uff0c\u4e14\u5df2\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2510.12970", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12970", "abs": "https://arxiv.org/abs/2510.12970", "authors": ["Baxi Chong", "Tianyu Wang", "Kelimar Diaz", "Christopher J. Pierce", "Eva Erickson", "Julian Whitman", "Yuelin Deng", "Esteban Flores", "Ruijie Fu", "Juntao He", "Jianfeng Lin", "Hang Lu", "Guillaume Sartoretti", "Howie Choset", "Daniel I. Goldman"], "title": "The Omega Turn: A General Turning Template for Elongate Robots", "comment": null, "summary": "Elongate limbless robots have the potential to locomote through tightly\npacked spaces for applications such as search-and-rescue and industrial\ninspections. The capability to effectively and robustly maneuver elongate\nlimbless robots is crucial to realize such potential. However, there has been\nlimited research on turning strategies for such systems. To achieve effective\nand robust turning performance in cluttered spaces, we take inspiration from a\nmicroscopic nematode, C. elegans, which exhibits remarkable maneuverability in\nrheologically complex environments partially because of its ability to perform\nomega turns. Despite recent efforts to analyze omega turn kinematics, it\nremains unknown if there exists a wave equation sufficient to prescribe an\nomega turn, let alone its reconstruction on robot platforms. Here, using a\ncomparative theory-biology approach, we prescribe the omega turn as a\nsuperposition of two traveling waves. With wave equations as a guideline, we\ndesign a controller for limbless robots enabling robust and effective turning\nbehaviors in lab and cluttered field environments. Finally, we show that such\nomega turn controllers can also generalize to elongate multi-legged robots,\ndemonstrating an alternative effective body-driven turning strategy for\nelongate robots, with and without limbs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4ece\u7ebf\u866bC. elegans\u7684omega\u8f6c\u5f2f\u884c\u4e3a\u4e2d\u83b7\u5f97\u7075\u611f\uff0c\u63d0\u51fa\u4e86\u5c06omega\u8f6c\u5f2f\u63cf\u8ff0\u4e3a\u4e24\u4e2a\u884c\u6ce2\u53e0\u52a0\u7684\u6ce2\u65b9\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86\u9002\u7528\u4e8e\u65e0\u80a2\u548c\u591a\u80a2\u7ec6\u957f\u673a\u5668\u4eba\u7684\u63a7\u5236\u5668\uff0c\u5728\u5b9e\u9a8c\u5ba4\u548c\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u7a33\u5065\u6709\u6548\u7684\u8f6c\u5f2f\u6027\u80fd\u3002", "motivation": "\u7ec6\u957f\u65e0\u80a2\u673a\u5668\u4eba\u5728\u7d27\u5bc6\u7a7a\u95f4\uff08\u5982\u641c\u6551\u548c\u5de5\u4e1a\u68c0\u67e5\uff09\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u8fd9\u7c7b\u7cfb\u7edf\u7684\u8f6c\u5f2f\u7b56\u7565\u5173\u6ce8\u6709\u9650\u3002\u4e3a\u5b9e\u73b0\u5728\u6742\u4e71\u7a7a\u95f4\u4e2d\u7684\u6709\u6548\u7a33\u5065\u8f6c\u5f2f\u6027\u80fd\uff0c\u7814\u7a76\u8005\u4ece\u7ebf\u866bC. elegans\u7684\u5353\u8d8a\u673a\u52a8\u6027\u4e2d\u83b7\u5f97\u542f\u53d1\u3002", "method": "\u91c7\u7528\u7406\u8bba-\u751f\u7269\u5b66\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5c06omega\u8f6c\u5f2f\u63cf\u8ff0\u4e3a\u4e24\u4e2a\u884c\u6ce2\u7684\u53e0\u52a0\u3002\u57fa\u4e8e\u6ce2\u65b9\u7a0b\u8bbe\u8ba1\u63a7\u5236\u5668\uff0c\u5e76\u5728\u5b9e\u9a8c\u5ba4\u548c\u590d\u6742\u73b0\u573a\u73af\u5883\u4e2d\u6d4b\u8bd5\u7ec6\u957f\u673a\u5668\u4eba\u7684\u8f6c\u5f2f\u884c\u4e3a\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u6709\u6548\u8f6c\u5f2f\u884c\u4e3a\u7684\u63a7\u5236\u5668\uff0c\u8be5\u63a7\u5236\u5668\u4e0d\u4ec5\u9002\u7528\u4e8e\u65e0\u80a2\u7ec6\u957f\u673a\u5668\u4eba\uff0c\u8fd8\u80fd\u63a8\u5e7f\u5230\u591a\u80a2\u7ec6\u957f\u673a\u5668\u4eba\uff0c\u5c55\u793a\u4e86\u4e00\u79cd\u66ff\u4ee3\u6027\u7684\u8eab\u4f53\u9a71\u52a8\u8f6c\u5f2f\u7b56\u7565\u3002", "conclusion": "\u57fa\u4e8e\u6ce2\u65b9\u7a0b\u7684omega\u8f6c\u5f2f\u63a7\u5236\u5668\u4e3a\u7ec6\u957f\u673a\u5668\u4eba\uff08\u65e0\u8bba\u6709\u65e0\u80a2\u4f53\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8eab\u4f53\u9a71\u52a8\u8f6c\u5f2f\u7b56\u7565\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.12971", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12971", "abs": "https://arxiv.org/abs/2510.12971", "authors": ["Anran Zhang", "Hanzhi Chen", "Yannick Burkhardt", "Yao Zhong", "Johannes Betz", "Helen Oleynikova", "Stefan Leutenegger"], "title": "Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation", "comment": "8 pages, 5 figures", "summary": "We present Actron3D, a framework that enables robots to acquire transferable\n6-DoF manipulation skills from just a few monocular, uncalibrated, RGB-only\nhuman videos. At its core lies the Neural Affordance Function, a compact\nobject-centric representation that distills actionable cues from diverse\nuncalibrated videos-geometry, visual appearance, and affordance-into a\nlightweight neural network, forming a memory bank of manipulation skills.\nDuring deployment, we adopt a pipeline that retrieves relevant affordance\nfunctions and transfers precise 6-DoF manipulation policies via coarse-to-fine\noptimization, enabled by continuous queries to the multimodal features encoded\nin the neural functions. Experiments in both simulation and the real world\ndemonstrate that Actron3D significantly outperforms prior methods, achieving a\n14.9 percentage point improvement in average success rate across 13 tasks while\nrequiring only 2-3 demonstration videos per task.", "AI": {"tldr": "Actron3D\u662f\u4e00\u4e2a\u4ece\u5c11\u91cf\u5355\u76ee\u3001\u672a\u6807\u5b9a\u3001\u4ec5RGB\u7684\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u53ef\u8fc1\u79fb6\u81ea\u7531\u5ea6\u64cd\u4f5c\u6280\u80fd\u7684\u673a\u5668\u4eba\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u795e\u7ecf\u529f\u80fd\u51fd\u6570\u8868\u793a\uff0c\u901a\u8fc7\u7c97\u5230\u7ec6\u4f18\u5316\u5b9e\u73b0\u7cbe\u786e\u64cd\u4f5c\u7b56\u7565\u8fc1\u79fb\u3002", "motivation": "\u4ece\u5c11\u91cf\u672a\u6807\u5b9a\u7684\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u53ef\u8fc1\u79fb\u76846\u81ea\u7531\u5ea6\u64cd\u4f5c\u6280\u80fd\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u590d\u6742\u4f20\u611f\u5668\u8bbe\u7f6e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u529f\u80fd\u51fd\u6570\u4f5c\u4e3a\u7d27\u51d1\u7684\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\uff0c\u4ece\u591a\u6837\u5316\u672a\u6807\u5b9a\u89c6\u9891\u4e2d\u63d0\u53d6\u51e0\u4f55\u3001\u89c6\u89c9\u5916\u89c2\u548c\u529f\u80fd\u4fe1\u606f\uff1b\u90e8\u7f72\u65f6\u901a\u8fc7\u68c0\u7d22\u76f8\u5173\u529f\u80fd\u51fd\u6570\u548c\u7c97\u5230\u7ec6\u4f18\u5316\u5b9e\u73b0\u7b56\u7565\u8fc1\u79fb\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cActron3D\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u572813\u4e2a\u4efb\u52a1\u4e0a\u5e73\u5747\u6210\u529f\u7387\u63d0\u534714.9\u4e2a\u767e\u5206\u70b9\uff0c\u6bcf\u4e2a\u4efb\u52a1\u4ec5\u97002-3\u4e2a\u6f14\u793a\u89c6\u9891\u3002", "conclusion": "Actron3D\u8bc1\u660e\u4e86\u4ece\u5c11\u91cf\u672a\u6807\u5b9a\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u53ef\u8fc1\u79fb6\u81ea\u7531\u5ea6\u64cd\u4f5c\u6280\u80fd\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.12992", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.12992", "abs": "https://arxiv.org/abs/2510.12992", "authors": ["Neel P. Bhatt", "Po-han Li", "Kushagra Gupta", "Rohan Siva", "Daniel Milan", "Alexander T. Hogue", "Sandeep P. Chinchali", "David Fridovich-Keil", "Zhangyang Wang", "Ufuk Topcu"], "title": "UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles", "comment": null, "summary": "Safe large-scale coordination of multiple cooperative connected autonomous\nvehicles (CAVs) hinges on communication that is both efficient and\ninterpretable. Existing approaches either rely on transmitting high-bandwidth\nraw sensor data streams or neglect perception and planning uncertainties\ninherent in shared data, resulting in systems that are neither scalable nor\nsafe. To address these limitations, we propose Uncertainty-Guided Natural\nLanguage Cooperative Autonomous Planning (UNCAP), a vision-language model-based\nplanning approach that enables CAVs to communicate via lightweight natural\nlanguage messages while explicitly accounting for perception uncertainty in\ndecision-making. UNCAP features a two-stage communication protocol: (i) an ego\nCAV first identifies the subset of vehicles most relevant for information\nexchange, and (ii) the selected CAVs then transmit messages that quantitatively\nexpress their perception uncertainty. By selectively fusing messages that\nmaximize mutual information, this strategy allows the ego vehicle to integrate\nonly the most relevant signals into its decision-making, improving both the\nscalability and reliability of cooperative planning. Experiments across diverse\ndriving scenarios show a 63% reduction in communication bandwidth with a 31%\nincrease in driving safety score, a 61% reduction in decision uncertainty, and\na four-fold increase in collision distance margin during near-miss events.\nProject website: https://uncap-project.github.io/", "AI": {"tldr": "UNCAP\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u7136\u8bed\u8a00\u6d88\u606f\u8fdb\u884c\u8f66\u8f86\u95f4\u901a\u4fe1\uff0c\u5e76\u660e\u786e\u8003\u8651\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5e26\u5bbd\u540c\u65f6\u63d0\u9ad8\u9a7e\u9a76\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4f20\u8f93\u9ad8\u5e26\u5bbd\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u6d41\uff0c\u8981\u4e48\u5ffd\u89c6\u5171\u4eab\u6570\u636e\u4e2d\u7684\u611f\u77e5\u548c\u89c4\u5212\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u65e2\u4e0d\u53ef\u6269\u5c55\u4e5f\u4e0d\u5b89\u5168\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u901a\u4fe1\u534f\u8bae\uff1a\u9996\u5148\u8bc6\u522b\u6700\u76f8\u5173\u7684\u4fe1\u606f\u4ea4\u6362\u8f66\u8f86\u5b50\u96c6\uff0c\u7136\u540e\u9009\u5b9a\u7684CAV\u4f20\u8f93\u91cf\u5316\u8868\u8fbe\u5176\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u6d88\u606f\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u878d\u5408\u6700\u5927\u5316\u4e92\u4fe1\u606f\u7684\u6d88\u606f\u6765\u6574\u5408\u6700\u76f8\u5173\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u901a\u4fe1\u5e26\u5bbd\u51cf\u5c1163%\uff0c\u9a7e\u9a76\u5b89\u5168\u8bc4\u5206\u63d0\u9ad831%\uff0c\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e61%\uff0c\u8fd1\u78b0\u649e\u4e8b\u4ef6\u4e2d\u7684\u78b0\u649e\u8ddd\u79bb\u88d5\u5ea6\u589e\u52a0\u56db\u500d\u3002", "conclusion": "UNCAP\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u548c\u663e\u5f0f\u5904\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u5b89\u5168\u7684\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u3002"}}
{"id": "2510.13005", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13005", "abs": "https://arxiv.org/abs/2510.13005", "authors": ["Robert Muldrow", "Channing Ludden", "Christopher Petersen"], "title": "Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations", "comment": "12 pages, 4 figures, AAS/AIAA Space Flight Mechanics", "summary": "In-Space Servicing, Assembly, and Manufacturing (ISAM) is a set of emerging\noperations that provides several benefits to improve the longevity, capacity,\nmo- bility, and expandability of existing and future space assets. Serial\nrobotic ma- nipulators are particularly vital in accomplishing ISAM operations,\nhowever, the complex perturbation forces and motions associated with movement\nof a robotic arm on a free-flying satellite presents a complex controls problem\nrequiring addi- tional study. While many dynamical models are developed,\nexperimentally test- ing and validating these models is challenging given that\nthe models operate in space, where satellites have six-degrees-of-freedom\n(6-DOF). This paper attempts to resolve those challenges by presenting the\ndesign and development of a new hardware-in-the-loop (HIL) experimental testbed\nutilized to emulate ISAM. This emulation will be accomplished by means of a\n6-DOF UR3e robotic arm attached to a satellite bus. This satellite bus is\nmounted to a 1-DOF guide-rail system, en- abling the satellite bus and robotic\narm to move freely in one linear direction. This experimental ISAM emulation\nsystem will explore and validate models for space motion, serial robot\nmanipulation, and contact mechanics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6a21\u62df\u5728\u8f68\u670d\u52a1\u3001\u7ec4\u88c5\u548c\u5236\u9020(ISAM)\u64cd\u4f5c\u7684\u65b0\u578b\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u5c066\u81ea\u7531\u5ea6UR3e\u673a\u68b0\u81c2\u5b89\u88c5\u5728\u536b\u661f\u603b\u7ebf\u4e0a\uff0c\u7ed3\u54081\u81ea\u7531\u5ea6\u5bfc\u8f68\u7cfb\u7edf\u6765\u6a21\u62df\u7a7a\u95f4\u8fd0\u52a8\u3001\u4e32\u8054\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u63a5\u89e6\u529b\u5b66\u3002", "motivation": "\u5728\u8f68\u670d\u52a1\u3001\u7ec4\u88c5\u548c\u5236\u9020(ISAM)\u64cd\u4f5c\u5bf9\u63d0\u9ad8\u7a7a\u95f4\u8d44\u4ea7\u7684\u5bff\u547d\u3001\u5bb9\u91cf\u3001\u673a\u52a8\u6027\u548c\u53ef\u6269\u5c55\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u673a\u68b0\u81c2\u5728\u81ea\u7531\u98de\u884c\u536b\u661f\u4e0a\u7684\u8fd0\u52a8\u4f1a\u4ea7\u751f\u590d\u6742\u7684\u6270\u52a8\u529b\u548c\u8fd0\u52a8\uff0c\u8fd9\u6784\u6210\u4e86\u4e00\u4e2a\u590d\u6742\u7684\u63a7\u5236\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "method": "\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u786c\u4ef6\u5728\u73af(HIL)\u5b9e\u9a8c\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4f7f\u75286\u81ea\u7531\u5ea6UR3e\u673a\u68b0\u81c2\u5b89\u88c5\u5728\u536b\u661f\u603b\u7ebf\u4e0a\uff0c\u8be5\u536b\u661f\u603b\u7ebf\u5b89\u88c5\u57281\u81ea\u7531\u5ea6\u5bfc\u8f68\u7cfb\u7edf\u4e0a\uff0c\u4f7f\u536b\u661f\u603b\u7ebf\u548c\u673a\u68b0\u81c2\u80fd\u591f\u5728\u4e00\u4e2a\u7ebf\u6027\u65b9\u5411\u4e0a\u81ea\u7531\u79fb\u52a8\u3002", "result": "\u8be5\u5b9e\u9a8cISAM\u4eff\u771f\u7cfb\u7edf\u5c06\u7528\u4e8e\u63a2\u7d22\u548c\u9a8c\u8bc1\u7a7a\u95f4\u8fd0\u52a8\u3001\u4e32\u8054\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u63a5\u89e6\u529b\u5b66\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u6d4b\u8bd5\u5e73\u53f0\u89e3\u51b3\u4e86\u5728\u7a7a\u95f4\u73af\u5883\u4e2d\u5b9e\u9a8c\u6d4b\u8bd5\u548c\u9a8c\u8bc1\u52a8\u529b\u5b66\u6a21\u578b\u7684\u6311\u6218\uff0c\u4e3aISAM\u64cd\u4f5c\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u9a8c\u624b\u6bb5\u3002"}}
{"id": "2510.13048", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.13048", "abs": "https://arxiv.org/abs/2510.13048", "authors": ["Minghao Guo", "Victor Zordan", "Sheldon Andrews", "Wojciech Matusik", "Maneesh Agrawala", "Hsueh-Ti Derek Liu"], "title": "Kinematic Kitbashing for Modeling Functional Articulated Objects", "comment": null, "summary": "We introduce Kinematic Kitbashing, an automatic framework that synthesizes\nfunctionality-aware articulated objects by reusing parts from existing models.\nGiven a kinematic graph with a small collection of articulated parts, our\noptimizer jointly solves for the spatial placement of every part so that (i)\nattachments remain geometrically sound over the entire range of motion and (ii)\nthe assembled object satisfies user-specified functional goals such as\ncollision-free actuation, reachability, or trajectory following. At its core is\na kinematics-aware attachment energy that aligns vector distance function\nfeatures sampled across multiple articulation snapshots. We embed this\nattachment term within an annealed Riemannian Langevin dynamics sampler that\ntreats functionality objectives as additional energies, enabling robust global\nexploration while accommodating non-differentiable functionality objectives and\nconstraints. Our framework produces a wide spectrum of assembled articulated\nshapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,\ngear-driven paddlers, and reconfigurable furniture, and delivers strong\nquantitative improvements over state-of-the-art baselines across geometric,\nkinematic, and functional metrics. By tightly coupling articulation-aware\ngeometry matching with functionality-driven optimization, Kinematic Kitbashing\nbridges part-based shape modeling and functional assembly design, empowering\nrapid creation of interactive articulated assets.", "AI": {"tldr": "Kinematic Kitbashing\u662f\u4e00\u4e2a\u81ea\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u73b0\u6709\u6a21\u578b\u7684\u96f6\u4ef6\u6765\u5408\u6210\u529f\u80fd\u611f\u77e5\u7684\u94f0\u63a5\u5bf9\u8c61\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u96f6\u4ef6\u7a7a\u95f4\u5e03\u5c40\uff0c\u786e\u4fdd\u51e0\u4f55\u8fde\u63a5\u7684\u5408\u7406\u6027\u5e76\u6ee1\u8db3\u7528\u6237\u6307\u5b9a\u7684\u529f\u80fd\u76ee\u6807\u3002", "motivation": "\u65e8\u5728\u5c06\u57fa\u4e8e\u96f6\u4ef6\u7684\u5f62\u72b6\u5efa\u6a21\u4e0e\u529f\u80fd\u88c5\u914d\u8bbe\u8ba1\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u4ea4\u4e92\u5f0f\u94f0\u63a5\u8d44\u4ea7\u7684\u5feb\u901f\u521b\u5efa\u3002", "method": "\u4f7f\u7528\u8fd0\u52a8\u5b66\u611f\u77e5\u7684\u9644\u7740\u80fd\u91cf\u5bf9\u9f50\u591a\u4e2a\u94f0\u63a5\u5feb\u7167\u4e2d\u7684\u5411\u91cf\u8ddd\u79bb\u51fd\u6570\u7279\u5f81\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u9000\u706b\u7684\u9ece\u66fc\u6717\u4e4b\u4e07\u52a8\u529b\u5b66\u91c7\u6837\u5668\u4e2d\uff0c\u5904\u7406\u4e0d\u53ef\u5fae\u5206\u7684\u529f\u80fd\u76ee\u6807\u548c\u7ea6\u675f\u3002", "result": "\u751f\u6210\u4e86\u5e7f\u6cdb\u7684\u94f0\u63a5\u5f62\u72b6\uff0c\u4ece\u5783\u573e\u6876\u8f6e\u5b50\u5230\u591a\u6bb5\u706f\u5177\u3001\u9f7f\u8f6e\u9a71\u52a8\u7684\u5212\u6868\u5668\u548c\u53ef\u91cd\u6784\u5bb6\u5177\uff0c\u5728\u51e0\u4f55\u3001\u8fd0\u52a8\u5b66\u548c\u529f\u80fd\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u94f0\u63a5\u611f\u77e5\u7684\u51e0\u4f55\u5339\u914d\u4e0e\u529f\u80fd\u9a71\u52a8\u7684\u4f18\u5316\uff0cKinematic Kitbashing\u6210\u529f\u8fde\u63a5\u4e86\u57fa\u4e8e\u96f6\u4ef6\u7684\u5f62\u72b6\u5efa\u6a21\u548c\u529f\u80fd\u88c5\u914d\u8bbe\u8ba1\u3002"}}
{"id": "2510.13054", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13054", "abs": "https://arxiv.org/abs/2510.13054", "authors": ["Ankit Goyal", "Hugo Hadfield", "Xuning Yang", "Valts Blukis", "Fabio Ramos"], "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification", "comment": null, "summary": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding $\\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like $\\pi_0.5$-KI, $\\pi_0$, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.", "AI": {"tldr": "VLA-0\u5c55\u793a\u4e86\u5c06\u52a8\u4f5c\u76f4\u63a5\u8868\u793a\u4e3a\u6587\u672c\u7684\u7b80\u5355\u65b9\u6cd5\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\u5177\u6709\u5f3a\u5927\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u66f4\u590d\u6742\u7684\u6a21\u578b\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u6784\u5efa\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u65b9\u6cd5\u5f80\u5f80\u8fc7\u4e8e\u590d\u6742\uff0c\u800c\u5c06\u52a8\u4f5c\u76f4\u63a5\u8868\u793a\u4e3a\u6587\u672c\u8fd9\u4e00\u6700\u7b80\u5355\u7b56\u7565\u5374\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faVLA-0\u6a21\u578b\uff0c\u91c7\u7528\u5c06\u52a8\u4f5c\u76f4\u63a5\u8868\u793a\u4e3a\u6587\u672c\u7684\u7b80\u5355\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7279\u5b9a\u6280\u672f\u89e3\u9501\u5176\u9ad8\u6027\u80fd\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVLA-0\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u03c00.5-KI\u3001OpenVLA-OFT\u548cSmolVLA\u7b49\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u4e5f\u4f18\u4e8e\u7ecf\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684SmolVLA\u3002", "conclusion": "\u5c06\u52a8\u4f5c\u8868\u793a\u4e3a\u6587\u672c\u7684\u7b80\u5355VLA\u8bbe\u8ba1\u4e0d\u4ec5\u6709\u6548\uff0c\u800c\u4e14\u51fa\u4eba\u610f\u6599\u5730\u5f3a\u5927\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7b80\u6d01\u7684\u8def\u5f84\u3002"}}
{"id": "2510.13149", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13149", "abs": "https://arxiv.org/abs/2510.13149", "authors": ["Yangtao Chen", "Zixuan Chen", "Nga Teng Chan", "Junting Chen", "Junhui Yin", "Jieqi Shi", "Yang Gao", "Yong-Lu Li", "Jing Huo"], "title": "RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation", "comment": "Under review. These first two authors contributed equally to this\n  work", "summary": "Enabling robots to flexibly schedule and compose learned skills for novel\nlong-horizon manipulation under diverse perturbations remains a core challenge.\nEarly explorations with end-to-end VLA models show limited success, as these\nmodels struggle to generalize beyond the training distribution. Hierarchical\napproaches, where high-level planners generate subgoals for low-level policies,\nbring certain improvements but still suffer under complex perturbations,\nrevealing limited capability in skill composition. However, existing benchmarks\nprimarily emphasize task completion in long-horizon settings, offering little\ninsight into compositional generalization, robustness, and the interplay\nbetween planning and execution. To systematically investigate these gaps, we\npropose RoboHiMan, a hierarchical evaluation paradigm for compositional\ngeneralization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,\na benchmark of atomic and compositional tasks under diverse perturbations,\nsupported by a multi-level training dataset for analyzing progressive data\nscaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)\nthat probe the necessity of skill composition and reveal bottlenecks in\nhierarchical architectures. Experiments highlight clear capability gaps across\nrepresentative models and architectures, pointing to directions for advancing\nmodels better suited to real-world long-horizon manipulation tasks. Videos and\nopen-source code can be found on our project website:\nhttps://chenyt31.github.io/robo-himan.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86RoboHiMan\u5c42\u6b21\u5316\u8bc4\u4f30\u8303\u5f0f\uff0c\u7528\u4e8e\u7cfb\u7edf\u7814\u7a76\u957f\u65f6\u7a0b\u64cd\u4f5c\u4e2d\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ecHiMan-Bench\u57fa\u51c6\u6d4b\u8bd5\u3001\u591a\u7ea7\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u4e09\u79cd\u8bc4\u4f30\u8303\u5f0f\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6280\u80fd\u7ec4\u5408\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u80fd\u529b\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6280\u80fd\u8c03\u5ea6\u548c\u7ec4\u5408\u65b9\u6cd5\u5728\u9762\u4e34\u590d\u6742\u6270\u52a8\u65f6\u8868\u73b0\u6709\u9650\uff0c\u7aef\u5230\u7aefVLA\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5c42\u6b21\u5316\u65b9\u6cd5\u5728\u6280\u80fd\u7ec4\u5408\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u5b8c\u6210\u800c\u975e\u7ec4\u5408\u6cdb\u5316\u3001\u9c81\u68d2\u6027\u53ca\u89c4\u5212\u4e0e\u6267\u884c\u7684\u4ea4\u4e92\u3002", "method": "\u63d0\u51faRoboHiMan\u5c42\u6b21\u5316\u8bc4\u4f30\u8303\u5f0f\uff0c\u5305\u542bHiMan-Bench\u57fa\u51c6\uff08\u539f\u5b50\u548c\u7ec4\u5408\u4efb\u52a1\u3001\u591a\u6837\u5316\u6270\u52a8\uff09\u3001\u591a\u7ea7\u8bad\u7ec3\u6570\u636e\u96c6\u7528\u4e8e\u6e10\u8fdb\u6570\u636e\u7f29\u653e\u5206\u6790\uff0c\u4ee5\u53ca\u4e09\u79cd\u8bc4\u4f30\u8303\u5f0f\uff08vanilla\u3001decoupled\u3001coupled\uff09\u6765\u63a2\u7a76\u6280\u80fd\u7ec4\u5408\u7684\u5fc5\u8981\u6027\u548c\u5c42\u6b21\u67b6\u6784\u74f6\u9888\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4ee3\u8868\u6027\u6a21\u578b\u548c\u67b6\u6784\u5728\u80fd\u529b\u4e0a\u7684\u660e\u663e\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u6280\u80fd\u7ec4\u5408\u548c\u5e94\u5bf9\u6270\u52a8\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6307\u51fa\u4e86\u63a8\u8fdb\u66f4\u9002\u5408\u73b0\u5b9e\u4e16\u754c\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u7684\u6a21\u578b\u53d1\u5c55\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5728\u7ec4\u5408\u6cdb\u5316\u548c\u9c81\u68d2\u6027\u65b9\u9762\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2510.13284", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13284", "abs": "https://arxiv.org/abs/2510.13284", "authors": ["Haoyang Wu", "Siheng Wu", "William X. Liu", "Fangui Zeng"], "title": "ALOHA2 Robot Kitchen Application Scenario Reproduction Report", "comment": null, "summary": "ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,\nfeaturing higher performance and robustness compared to the original design,\nwhile also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers\nand two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control\nthe follower mechanical arms by operating the leader mechanical arms through\nback-driving. The device also includes cameras that generate images from\nmultiple viewpoints, allowing for RGB data collection during teleoperation. The\nrobot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame\nthat provides additional mounting points for cameras and gravity compensation\nsystems.", "AI": {"tldr": "ALOHA2\u662fALOHA\u53cc\u81c2\u9065\u64cd\u4f5c\u673a\u5668\u4eba\u7684\u589e\u5f3a\u7248\u672c\uff0c\u5177\u6709\u66f4\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u66f4\u7b26\u5408\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bbe\u8ba1\u3002\u5b83\u5305\u542b\u4e24\u4e2a\u5939\u722a\u3001\u4e24\u4e2aViperX 6\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c\u4e24\u4e2a\u8f83\u5c0f\u7684WidowX\u673a\u68b0\u81c2\uff0c\u901a\u8fc7\u53cd\u5411\u9a71\u52a8\u5b9e\u73b0\u9065\u64cd\u4f5c\u63a7\u5236\uff0c\u5e76\u914d\u5907\u591a\u89c6\u89d2\u6444\u50cf\u5934\u7528\u4e8eRGB\u6570\u636e\u91c7\u96c6\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u6027\u80fd\u66f4\u9ad8\u3001\u9c81\u68d2\u6027\u66f4\u5f3a\u4e14\u66f4\u7b26\u5408\u4eba\u4f53\u5de5\u7a0b\u5b66\u7684\u53cc\u81c2\u9065\u64cd\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4ee5\u6539\u8fdb\u539f\u59cbALOHA\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u7528\u6237\u4f53\u9a8c\u548c\u6570\u636e\u91c7\u96c6\u80fd\u529b\u3002", "method": "\u91c7\u7528\u53cc\u5939\u722a\u3001\u53ccViperX 6\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c\u53ccWidowX\u673a\u68b0\u81c2\u7684\u7ec4\u5408\u8bbe\u8ba1\uff0c\u901a\u8fc7\u53cd\u5411\u9a71\u52a8\u673a\u5236\u5b9e\u73b0\u9065\u64cd\u4f5c\u63a7\u5236\uff0c\u96c6\u6210\u591a\u89c6\u89d2\u6444\u50cf\u5934\u7cfb\u7edf\uff0c\u5e76\u5b89\u88c5\u5728\u5e26\u6709\u94dd\u5236\u6846\u67b6\u7684\u684c\u9762\u4e0a\uff0c\u63d0\u4f9b\u989d\u5916\u7684\u6444\u50cf\u5934\u5b89\u88c5\u70b9\u548c\u91cd\u529b\u8865\u507f\u7cfb\u7edf\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51faALOHA2\u7cfb\u7edf\uff0c\u76f8\u6bd4\u539f\u59cbALOHA\u5177\u6709\u66f4\u9ad8\u7684\u6027\u80fd\u3001\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u6539\u8fdb\u7684\u4eba\u4f53\u5de5\u7a0b\u5b66\u8bbe\u8ba1\uff0c\u80fd\u591f\u6709\u6548\u6536\u96c6\u591a\u89c6\u89d2RGB\u6570\u636e\u3002", "conclusion": "ALOHA2\u4f5c\u4e3aALOHA\u7684\u589e\u5f3a\u7248\u672c\uff0c\u5728\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u4eba\u4f53\u5de5\u7a0b\u5b66\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u9065\u64cd\u4f5c\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5148\u8fdb\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.13287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13287", "abs": "https://arxiv.org/abs/2510.13287", "authors": ["Nishant Chandna", "Akshat Kaushal"], "title": "DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping", "comment": "Accepted at IROS Active Perception Workshop", "summary": "LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for\nenabling precise navigation and environmental reconstruction across various\napplications. Although current point-to-plane ICP algorithms perform effec-\ntively in structured, feature-rich environments, they struggle in scenarios\nwith sparse features, repetitive geometric structures, and high-frequency\nmotion. This leads to degeneracy in 6- DOF pose estimation. Most\nstate-of-the-art algorithms address these challenges by incorporating\nadditional sensing modalities, but LiDAR-only solutions continue to face\nlimitations under such conditions. To address these issues, we propose a novel\nDegeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.\nOur system improves mapping accuracy through point cloud classification based\non surface normals and neighborhood analysis. Points are classified into\nground, walls, roof, edges, and non-planar points, enabling accurate\ncorrespondences. A Degeneracy-based weighted least squares-based ICP algorithm\nis then applied for accurate odom- etry estimation. Additionally, a Scan\nContext based back-end is implemented to support robust loop closures.\nDAMM-LOAM demonstrates significant improvements in odometry accuracy,\nespecially in indoor environments such as long corridors", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9000\u5316\u611f\u77e5\u591a\u5ea6\u91cfLiDAR\u91cc\u7a0b\u8ba1\u548c\u5efa\u56fe\u6a21\u5757DAMM-LOAM\uff0c\u901a\u8fc7\u70b9\u4e91\u5206\u7c7b\u548c\u9000\u5316\u611f\u77e5ICP\u7b97\u6cd5\uff0c\u5728\u7a00\u758f\u7279\u5f81\u548c\u91cd\u590d\u51e0\u4f55\u7ed3\u6784\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u70b9\u5bf9\u9762ICP\u7b97\u6cd5\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7a00\u758f\u7279\u5f81\u3001\u91cd\u590d\u51e0\u4f55\u7ed3\u6784\u548c\u9ad8\u9891\u8fd0\u52a8\u573a\u666f\u4e0b\u5b58\u57286\u81ea\u7531\u5ea6\u59ff\u6001\u4f30\u8ba1\u9000\u5316\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdbLiDAR-only\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u57fa\u4e8e\u8868\u9762\u6cd5\u7ebf\u548c\u90bb\u57df\u5206\u6790\u7684\u70b9\u4e91\u5206\u7c7b\uff08\u5730\u9762\u3001\u5899\u58c1\u3001\u5c4b\u9876\u3001\u8fb9\u7f18\u548c\u975e\u5e73\u9762\u70b9\uff09\uff1b2. \u9000\u5316\u611f\u77e5\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58ICP\u7b97\u6cd5\uff1b3. Scan Context\u540e\u7aef\u652f\u6301\u9c81\u68d2\u95ed\u73af\u68c0\u6d4b\u3002", "result": "DAMM-LOAM\u5728\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u957f\u8d70\u5eca\u7b49\u5ba4\u5185\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u9000\u5316\u611f\u77e5\u591a\u5ea6\u91cf\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LiDAR SLAM\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u7eafLiDAR\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6027\u80fd\u3002"}}
{"id": "2510.13324", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13324", "abs": "https://arxiv.org/abs/2510.13324", "authors": ["Erik Helmut", "Niklas Funk", "Tim Schneider", "Cristiana de Farias", "Jan Peters"], "title": "Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation", "comment": null, "summary": "Contact-rich manipulation depends on applying the correct grasp forces\nthroughout the manipulation task, especially when handling fragile or\ndeformable objects. Most existing imitation learning approaches often treat\nvisuotactile feedback only as an additional observation, leaving applied forces\nas an uncontrolled consequence of gripper commands. In this work, we present\nForce-Aware Robotic Manipulation (FARM), an imitation learning framework that\nintegrates high-dimensional tactile data to infer tactile-conditioned force\nsignals, which in turn define a matching force-based action space. We collect\nhuman demonstrations using a modified version of the handheld Universal\nManipulation Interface (UMI) gripper that integrates a GelSight Mini visual\ntactile sensor. For deploying the learned policies, we developed an actuated\nvariant of the UMI gripper with geometry matching our handheld version. During\npolicy rollouts, the proposed FARM diffusion policy jointly predicts robot\npose, grip width, and grip force. FARM outperforms several baselines across\nthree tasks with distinct force requirements -- high-force, low-force, and\ndynamic force adaptation -- demonstrating the advantages of its two key\ncomponents: leveraging force-grounded, high-dimensional tactile observations\nand a force-based control space. The codebase and design files are open-sourced\nand available at https://tactile-farm.github.io .", "AI": {"tldr": "FARM\u662f\u4e00\u4e2a\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9ad8\u7ef4\u89e6\u89c9\u6570\u636e\u6765\u63a8\u65ad\u89e6\u89c9\u6761\u4ef6\u5316\u7684\u529b\u4fe1\u53f7\uff0c\u5e76\u5b9a\u4e49\u57fa\u4e8e\u529b\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4ec5\u5c06\u89c6\u89c9\u89e6\u89c9\u53cd\u9988\u89c6\u4e3a\u989d\u5916\u89c2\u5bdf\uff0c\u800c\u5c06\u65bd\u52a0\u7684\u529b\u89c6\u4e3a\u5939\u722a\u547d\u4ee4\u7684\u4e0d\u53ef\u63a7\u540e\u679c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u63a7\u5236\u64cd\u4f5c\u529b\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6613\u788e\u6216\u53ef\u53d8\u5f62\u7269\u4f53\u65f6\u3002", "method": "\u4f7f\u7528\u96c6\u6210\u4e86GelSight Mini\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6539\u8fdb\u7248\u624b\u6301UMI\u5939\u722a\u6536\u96c6\u4eba\u7c7b\u6f14\u793a\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e0e\u624b\u6301\u7248\u672c\u51e0\u4f55\u5339\u914d\u7684\u9a71\u52a8\u53d8\u4f53\u7528\u4e8e\u7b56\u7565\u90e8\u7f72\u3002FARM\u6269\u6563\u7b56\u7565\u8054\u5408\u9884\u6d4b\u673a\u5668\u4eba\u4f4d\u59ff\u3001\u5939\u6301\u5bbd\u5ea6\u548c\u5939\u6301\u529b\u3002", "result": "FARM\u5728\u4e09\u4e2a\u5177\u6709\u4e0d\u540c\u529b\u8981\u6c42\uff08\u9ad8\u529b\u3001\u4f4e\u529b\u548c\u52a8\u6001\u529b\u9002\u5e94\uff09\u7684\u4efb\u52a1\u4e2d\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5229\u7528\u57fa\u4e8e\u529b\u7684\u9ad8\u7ef4\u89e6\u89c9\u89c2\u6d4b\u548c\u57fa\u4e8e\u529b\u7684\u63a7\u5236\u7a7a\u95f4\u7684\u4f18\u52bf\u3002", "conclusion": "FARM\u6846\u67b6\u901a\u8fc7\u6574\u5408\u89e6\u89c9\u6570\u636e\u548c\u529b\u63a7\u5236\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u548c\u8bbe\u8ba1\u6587\u4ef6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.13356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13356", "abs": "https://arxiv.org/abs/2510.13356", "authors": ["Jie Gu", "Tin Lun Lam", "Chunxu Tian", "Zhihao Xia", "Yongheng Xing", "Dan Zhang"], "title": "MODUR: A Modular Dual-reconfigurable Robot", "comment": null, "summary": "Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots\ncapable of forming higher-level robotic systems by altering the topological\nrelationships between modules, offering enhanced adaptability and robustness in\nvarious environments. This paper presents a novel MSRR called MODUR, featuring\ndual-level reconfiguration capabilities designed to integrate reconfigurable\nmechanisms into MSRR. Specifically, MODUR can perform high-level\nself-reconfiguration among modules to create different configurations, while\neach module is also able to change its shape to execute basic motions. The\ndesign of MODUR primarily includes a compact connector and scissor linkage\ngroups that provide actuation, forming a parallel mechanism capable of\nachieving both connector motion decoupling and adjacent position migration\ncapabilities. Furthermore, the workspace, considering the interdependent\nconnectors, is comprehensively analyzed, laying a theoretical foundation for\nthe design of the module's basic motion. Finally, the motion of MODUR is\nvalidated through a series of experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMODUR\u7684\u65b0\u578b\u6a21\u5757\u5316\u81ea\u91cd\u6784\u673a\u5668\u4eba\uff0c\u5177\u6709\u53cc\u91cd\u91cd\u6784\u80fd\u529b\uff0c\u65e2\u80fd\u8fdb\u884c\u6a21\u5757\u95f4\u7684\u9ad8\u5c42\u91cd\u6784\u5f62\u6210\u4e0d\u540c\u6784\u578b\uff0c\u53c8\u80fd\u6539\u53d8\u5355\u4e2a\u6a21\u5757\u5f62\u72b6\u6267\u884c\u57fa\u672c\u8fd0\u52a8\u3002", "motivation": "\u6a21\u5757\u5316\u81ea\u91cd\u6784\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u6539\u53d8\u6a21\u5757\u95f4\u7684\u62d3\u6251\u5173\u7cfb\u5f62\u6210\u66f4\u9ad8\u7ea7\u522b\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5728\u5404\u79cd\u73af\u5883\u4e2d\u63d0\u4f9b\u589e\u5f3a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "MODUR\u7684\u8bbe\u8ba1\u4e3b\u8981\u5305\u62ec\u7d27\u51d1\u8fde\u63a5\u5668\u548c\u526a\u5200\u8fde\u6746\u7ec4\uff0c\u5f62\u6210\u5e76\u8054\u673a\u6784\uff0c\u5b9e\u73b0\u8fde\u63a5\u5668\u8fd0\u52a8\u89e3\u8026\u548c\u76f8\u90bb\u4f4d\u7f6e\u8fc1\u79fb\u80fd\u529b\uff0c\u5e76\u5bf9\u5de5\u4f5c\u7a7a\u95f4\u8fdb\u884c\u7efc\u5408\u5206\u6790\u3002", "result": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u9a8c\u8bc1\u4e86MODUR\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "MODUR\u6210\u529f\u5b9e\u73b0\u4e86\u53cc\u91cd\u91cd\u6784\u80fd\u529b\uff0c\u4e3a\u6a21\u5757\u5316\u81ea\u91cd\u6784\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u548c\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2510.13358", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.13358", "abs": "https://arxiv.org/abs/2510.13358", "authors": ["Shingo Ayabe", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control", "comment": "16 pages, 8 figures", "summary": "Offline reinforcement learning enables sample-efficient policy acquisition\nwithout risky online interaction, yet policies trained on static datasets\nremain brittle under action-space perturbations such as actuator faults. This\nstudy introduces an offline-to-online framework that trains policies on clean\ndata and then performs adversarial fine-tuning, where perturbations are\ninjected into executed actions to induce compensatory behavior and improve\nresilience. A performance-aware curriculum further adjusts the perturbation\nprobability during training via an exponential-moving-average signal, balancing\nrobustness and stability throughout the learning process. Experiments on\ncontinuous-control locomotion tasks demonstrate that the proposed method\nconsistently improves robustness over offline-only baselines and converges\nfaster than training from scratch. Matching the fine-tuning and evaluation\nconditions yields the strongest robustness to action-space perturbations, while\nthe adaptive curriculum strategy mitigates the degradation of nominal\nperformance observed with the linear curriculum strategy. Overall, the results\nshow that adversarial fine-tuning enables adaptive and robust control under\nuncertain environments, bridging the gap between offline efficiency and online\nadaptability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u79bb\u7ebf\u8bad\u7ec3\u7684\u7b56\u7565\u4e0a\u6ce8\u5165\u52a8\u4f5c\u7a7a\u95f4\u6270\u52a8\u8fdb\u884c\u5bf9\u6297\u6027\u5fae\u8c03\uff0c\u7ed3\u5408\u6027\u80fd\u611f\u77e5\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u5728\u52a8\u4f5c\u7a7a\u95f4\u6270\u52a8\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u591f\u9ad8\u6548\u5730\u4ece\u9759\u6001\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u7b56\u7565\uff0c\u4f46\u8fd9\u4e9b\u7b56\u7565\u5728\u9762\u5bf9\u6267\u884c\u5668\u6545\u969c\u7b49\u52a8\u4f5c\u7a7a\u95f4\u6270\u52a8\u65f6\u8868\u73b0\u8106\u5f31\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u79bb\u7ebf\u7b56\u7565\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u5230\u5728\u7ebf\u6846\u67b6\uff1a\u5148\u5728\u5e72\u51c0\u6570\u636e\u4e0a\u8bad\u7ec3\u7b56\u7565\uff0c\u7136\u540e\u901a\u8fc7\u5411\u6267\u884c\u52a8\u4f5c\u6ce8\u5165\u6270\u52a8\u8fdb\u884c\u5bf9\u6297\u6027\u5fae\u8c03\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u6307\u6570\u79fb\u52a8\u5e73\u5747\u4fe1\u53f7\u7684\u6027\u80fd\u611f\u77e5\u8bfe\u7a0b\u6765\u52a8\u6001\u8c03\u6574\u6270\u52a8\u6982\u7387\u3002", "result": "\u5728\u8fde\u7eed\u63a7\u5236\u8fd0\u52a8\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u7eaf\u79bb\u7ebf\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u4e86\u9c81\u68d2\u6027\uff0c\u6bd4\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6536\u655b\u66f4\u5feb\u3002\u5339\u914d\u5fae\u8c03\u4e0e\u8bc4\u4f30\u6761\u4ef6\u53ef\u83b7\u5f97\u6700\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u81ea\u9002\u5e94\u8bfe\u7a0b\u7b56\u7565\u7f13\u89e3\u4e86\u7ebf\u6027\u8bfe\u7a0b\u7b56\u7565\u4e2d\u89c2\u5bdf\u5230\u7684\u540d\u4e49\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u5bf9\u6297\u6027\u5fae\u8c03\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u7684\u63a7\u5236\uff0c\u5f25\u5408\u4e86\u79bb\u7ebf\u6548\u7387\u4e0e\u5728\u7ebf\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.13488", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13488", "abs": "https://arxiv.org/abs/2510.13488", "authors": ["Maximilian Stasica", "Arne Bick", "Nico Bohlinger", "Omid Mohseni", "Max Johannes Alois Fritzsche", "Clemens H\u00fcbler", "Jan Peters", "Andr\u00e9 Seyfarth"], "title": "Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations", "comment": null, "summary": "Legged robots, particularly quadrupeds, excel at navigating rough terrains,\nyet their performance under vertical ground perturbations, such as those from\noscillating surfaces, remains underexplored. This study introduces a novel\napproach to enhance quadruped locomotion robustness by training the Unitree Go2\nrobot on an oscillating bridge - a 13.24-meter steel-and-concrete structure\nwith a 2.0 Hz eigenfrequency designed to perturb locomotion. Using\nReinforcement Learning (RL) with the Proximal Policy Optimization (PPO)\nalgorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,\ncombining five gaits (trot, pace, bound, free, default) with three training\nconditions: rigid bridge and two oscillating bridge setups with differing\nheight regulation strategies (relative to bridge surface or ground). Domain\nrandomization ensured zero-shot transfer to the real-world bridge. Our results\ndemonstrate that policies trained on the oscillating bridge exhibit superior\nstability and adaptability compared to those trained on rigid surfaces. Our\nframework enables robust gait patterns even without prior bridge exposure.\nThese findings highlight the potential of simulation-based RL to improve\nquadruped locomotion during dynamic ground perturbations, offering insights for\ndesigning robots capable of traversing vibrating environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5728\u632f\u8361\u6865\u6881\u4e0a\u8bad\u7ec3\u56db\u8db3\u673a\u5668\u4eba\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u4e8615\u79cd\u4e0d\u540c\u7684\u8fd0\u52a8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u5730\u9762\u6270\u52a8\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u5d0e\u5c96\u5730\u5f62\u5bfc\u822a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5782\u76f4\u5730\u9762\u6270\u52a8\uff08\u5982\u632f\u8361\u8868\u9762\uff09\u4e0b\u7684\u6027\u80fd\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u589e\u5f3a\u5176\u5728\u8fd9\u79cd\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u4f7f\u7528Unitree Go2\u673a\u5668\u4eba\u572813.24\u7c73\u94a2\u6df7\u7ed3\u6784\u7684\u632f\u8361\u6865\u6881\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528PPO\u7b97\u6cd5\u5728MuJoCo\u6a21\u62df\u4e2d\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u54085\u79cd\u6b65\u6001\u548c3\u79cd\u8bad\u7ec3\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u968f\u673a\u5316\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "result": "\u5728\u632f\u8361\u6865\u6881\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\u6bd4\u5728\u521a\u6027\u8868\u9762\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\uff0c\u5373\u4f7f\u6ca1\u6709\u4e8b\u5148\u63a5\u89e6\u8fc7\u6865\u6881\u4e5f\u80fd\u4ea7\u751f\u9c81\u68d2\u7684\u6b65\u6001\u6a21\u5f0f\u3002", "conclusion": "\u57fa\u4e8e\u6a21\u62df\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u5730\u9762\u6270\u52a8\u4e0b\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u4e3a\u8bbe\u8ba1\u80fd\u591f\u7a7f\u8d8a\u632f\u52a8\u73af\u5883\u7684\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2510.13535", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13535", "abs": "https://arxiv.org/abs/2510.13535", "authors": ["Wentao Guo", "Yizhou Wang", "Wenzeng Zhang"], "title": "A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints", "comment": "Accepted by IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025, Hangzhou. This version includes updated contact\n  information", "summary": "This paper presents a novel underactuated adaptive robotic hand, Hockens-A\nHand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,\nand a specialized four-bar linkage to achieve three adaptive grasping modes:\nparallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand\nrequires only a single linear actuator, leveraging passive mechanical\nintelligence to ensure adaptability and compliance in unstructured\nenvironments. Specifically, the vertical motion of the Hoeckens mechanism\nintroduces compliance, the double-parallelogram linkage ensures line contact at\nthe fingertip, and the four-bar amplification system enables natural\ntransitions between different grasping modes. Additionally, the inclusion of a\nmesh-textured silicone phalanx further enhances the ability to envelop objects\nof various shapes and sizes. This study employs detailed kinematic analysis to\noptimize the push angle and design the linkage lengths for optimal performance.\nSimulations validated the design by analyzing the fingertip motion and ensuring\nsmooth transitions between grasping modes. Furthermore, the grasping force was\nanalyzed using power equations to enhance the understanding of the system's\nperformance.Experimental validation using a 3D-printed prototype demonstrates\nthe three grasping modes of the hand in various scenarios under environmental\nconstraints, verifying its grasping stability and broad applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6b20\u9a71\u52a8\u81ea\u9002\u5e94\u673a\u5668\u4eba\u624bHockens-A Hand\uff0c\u96c6\u6210\u4e86Hoeckens\u673a\u6784\u3001\u53cc\u5e73\u884c\u56db\u8fb9\u5f62\u8fde\u6746\u548c\u4e13\u7528\u56db\u6746\u673a\u6784\uff0c\u5b9e\u73b0\u4e09\u79cd\u81ea\u9002\u5e94\u6293\u53d6\u6a21\u5f0f\uff1a\u5e73\u884c\u634f\u53d6\u3001\u975e\u5bf9\u79f0\u8200\u53d6\u548c\u5305\u7edc\u6293\u53d6\u3002\u4ec5\u9700\u5355\u4e2a\u7ebf\u6027\u6267\u884c\u5668\uff0c\u5229\u7528\u88ab\u52a8\u673a\u68b0\u667a\u80fd\u786e\u4fdd\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u987a\u5e94\u6027\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u81ea\u9002\u5e94\u6293\u53d6\u5404\u79cd\u5f62\u72b6\u548c\u5927\u5c0f\u7269\u4f53\u7684\u673a\u5668\u4eba\u624b\uff0c\u901a\u8fc7\u673a\u68b0\u667a\u80fd\u5b9e\u73b0\u591a\u6a21\u5f0f\u6293\u53d6\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u7b80\u5355\u548c\u4f4e\u6210\u672c\u3002", "method": "\u7ed3\u5408Hoeckens\u673a\u6784\u63d0\u4f9b\u5782\u76f4\u987a\u5e94\u6027\uff0c\u53cc\u5e73\u884c\u56db\u8fb9\u5f62\u8fde\u6746\u786e\u4fdd\u6307\u5c16\u7ebf\u63a5\u89e6\uff0c\u56db\u6746\u653e\u5927\u7cfb\u7edf\u5b9e\u73b0\u4e0d\u540c\u6293\u53d6\u6a21\u5f0f\u95f4\u7684\u81ea\u7136\u8fc7\u6e21\uff0c\u5e76\u901a\u8fc7\u7f51\u683c\u7eb9\u7406\u7845\u80f6\u6307\u8282\u589e\u5f3a\u5305\u7edc\u80fd\u529b\u3002\u91c7\u7528\u8be6\u7ec6\u8fd0\u52a8\u5b66\u5206\u6790\u4f18\u5316\u63a8\u89d2\u548c\u8fde\u6746\u957f\u5ea6\u8bbe\u8ba1\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u6307\u5c16\u8fd0\u52a8\u548c\u6293\u53d6\u6a21\u5f0f\u95f4\u5e73\u6ed1\u8fc7\u6e21\uff0c\u901a\u8fc7\u529f\u7387\u65b9\u7a0b\u5206\u6790\u6293\u53d6\u529b\u30023D\u6253\u5370\u539f\u578b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e09\u79cd\u6293\u53d6\u6a21\u5f0f\u5728\u5404\u79cd\u73af\u5883\u7ea6\u675f\u4e0b\u7684\u6293\u53d6\u7a33\u5b9a\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "Hockens-A Hand\u6210\u529f\u5b9e\u73b0\u4e86\u4ec5\u7528\u5355\u4e2a\u6267\u884c\u5668\u7684\u591a\u6a21\u5f0f\u81ea\u9002\u5e94\u6293\u53d6\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u6293\u53d6\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2510.13553", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13553", "abs": "https://arxiv.org/abs/2510.13553", "authors": ["Wentao Guo", "Wenzeng Zhang"], "title": "Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping", "comment": "Accepted by IEEE International Conference on Robotics and Biomimetics\n  (IROS) 2025, Hangzhou, China. This version includes updated contact\n  information", "summary": "This paper presents the Hoecken-D Hand, an underactuated robotic gripper that\ncombines a modified Hoecken linkage with a differential spring mechanism to\nachieve both linear parallel pinching and a mid-stroke transition to adaptive\nenvelope. The original Hoecken linkage is reconfigured by replacing one member\nwith differential links, preserving straight-line guidance while enabling\ncontact-triggered reconfiguration without additional actuators. A\ndouble-parallelogram arrangement maintains fingertip parallelism during\nconventional pinching, whereas the differential mechanism allows one finger to\nwrap inward upon encountering an obstacle, improving stability on irregular or\nthin objects. The mechanism can be driven by a single linear actuator,\nminimizing complexity and cost; in our prototype, each finger is driven by its\nown linear actuator for simplicity. We perform kinematic modeling and force\nanalysis to characterize grasp performance, including simulated grasping forces\nand spring-opening behavior under varying geometric parameters. The design was\nprototyped using PLA-based 3D printing, achieving a linear pinching span of\napproximately 200 mm. Preliminary tests demonstrate reliable grasping in both\nmodes across a wide range of object geometries, highlighting the Hoecken-D Hand\nas a compact, adaptable, and cost-effective solution for manipulation in\nunstructured environments.", "AI": {"tldr": "Hoecken-D Hand\u662f\u4e00\u79cd\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u5939\u722a\uff0c\u901a\u8fc7\u6539\u8fdb\u7684Hoecken\u8fde\u6746\u673a\u6784\u548c\u5dee\u52a8\u5f39\u7c27\u673a\u6784\uff0c\u5b9e\u73b0\u7ebf\u6027\u5e73\u884c\u5939\u6301\u548c\u81ea\u9002\u5e94\u5305\u7edc\u6293\u53d6\uff0c\u4ec5\u9700\u5355\u4e2a\u7ebf\u6027\u9a71\u52a8\u5668\u5373\u53ef\u64cd\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u5939\u6301\uff0c\u540c\u65f6\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u81ea\u9002\u5e94\u6293\u53d6\u4e0d\u89c4\u5219\u7269\u4f53\u7684\u7d27\u51d1\u3001\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5939\u722a\u3002", "method": "\u91cd\u65b0\u914d\u7f6eHoecken\u8fde\u6746\u673a\u6784\uff0c\u7528\u5dee\u52a8\u8fde\u6746\u66ff\u6362\u4e00\u4e2a\u6784\u4ef6\uff0c\u7ed3\u5408\u53cc\u5e73\u884c\u56db\u8fb9\u5f62\u4fdd\u6301\u6307\u5c16\u5e73\u884c\u6027\uff0c\u4f7f\u7528\u5dee\u52a8\u673a\u6784\u5b9e\u73b0\u63a5\u89e6\u89e6\u53d1\u91cd\u6784\uff0c\u65e0\u9700\u989d\u5916\u6267\u884c\u5668\u3002", "result": "\u539f\u578b\u673a\u5b9e\u73b0\u4e86\u7ea6200mm\u7684\u7ebf\u6027\u5939\u6301\u8303\u56f4\uff0c\u5728\u591a\u79cd\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u4e0b\u53ef\u9760\u6293\u53d6\uff0c\u5c55\u793a\u4e86\u7d27\u51d1\u3001\u81ea\u9002\u5e94\u548c\u6210\u672c\u6548\u76ca\u9ad8\u7684\u7279\u70b9\u3002", "conclusion": "Hoecken-D Hand\u4e3a\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7d27\u51d1\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u7ecf\u6d4e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u7cbe\u786e\u5939\u6301\u548c\u81ea\u9002\u5e94\u5305\u7edc\u6293\u53d6\u7684\u4f18\u70b9\u3002"}}
{"id": "2510.13594", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13594", "abs": "https://arxiv.org/abs/2510.13594", "authors": ["Austin Barret", "Meng Cheng Lau"], "title": "Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots", "comment": "9 Figure. Presented at FIRA Summit 2025, Daegu, S. Korea", "summary": "The operation of humanoid robotics is an essential field of research with\nmany practical and competitive applications. Many of these systems, however, do\nnot invest heavily in developing a non-expert-centered graphical user interface\n(GUI) for operation. The focus of this research is to develop a scalable GUI\nthat is tailored to be simple and intuitive so non-expert operators can control\nthe robot through a FIRA-regulated obstacle course. Using common practices from\nuser interface development (UI) and understanding concepts described in\nhuman-robot interaction (HRI) and other related concepts, we will develop a new\ninterface with the goal of a non-expert teleoperation system.", "AI": {"tldr": "\u5f00\u53d1\u4e00\u4e2a\u9762\u5411\u975e\u4e13\u5bb6\u7528\u6237\u7684\u7b80\u5355\u76f4\u89c2\u7684\u4eba\u5f62\u673a\u5668\u4eba\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u7528\u4e8e\u63a7\u5236\u673a\u5668\u4eba\u901a\u8fc7FIRA\u89c4\u5b9a\u7684\u969c\u788d\u8d5b\u9053", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u7cfb\u7edf\u7f3a\u4e4f\u4e13\u95e8\u4e3a\u975e\u4e13\u5bb6\u64cd\u4f5c\u8005\u8bbe\u8ba1\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u5b9e\u7528\u6027", "method": "\u7ed3\u5408\u7528\u6237\u754c\u9762\u5f00\u53d1\u5b9e\u8df5\u3001\u4eba\u673a\u4ea4\u4e92\u6982\u5ff5\u53ca\u76f8\u5173\u7406\u8bba\uff0c\u5f00\u53d1\u65b0\u7684\u975e\u4e13\u5bb6\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\u754c\u9762", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684GUI\uff0c\u4e13\u95e8\u4e3a\u975e\u4e13\u5bb6\u64cd\u4f5c\u8005\u8bbe\u8ba1\uff0c\u4f7f\u5176\u80fd\u591f\u7b80\u5355\u76f4\u89c2\u5730\u63a7\u5236\u673a\u5668\u4eba", "conclusion": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u9002\u5408\u975e\u4e13\u5bb6\u7528\u6237\u7684\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\u754c\u9762\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u548c\u53ef\u8bbf\u95ee\u6027"}}
{"id": "2510.13595", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13595", "abs": "https://arxiv.org/abs/2510.13595", "authors": ["Ethan K. Gordon", "Bruke Baraki", "Hien Bui", "Michael Posa"], "title": "Active Tactile Exploration for Rigid Body Pose and Shape Estimation", "comment": "8 pages, 6 figures", "summary": "General robot manipulation requires the handling of previously unseen\nobjects. Learning a physically accurate model at test time can provide\nsignificant benefits in data efficiency, predictability, and reuse between\ntasks. Tactile sensing can compliment vision with its robustness to occlusion,\nbut its temporal sparsity necessitates careful online exploration to maintain\ndata efficiency. Direct contact can also cause an unrestrained object to move,\nrequiring both shape and location estimation. In this work, we propose a\nlearning and exploration framework that uses only tactile data to\nsimultaneously determine the shape and location of rigid objects with minimal\nrobot motion. We build on recent advances in contact-rich system identification\nto formulate a loss function that penalizes physical constraint violation\nwithout introducing the numerical stiffness inherent in rigid-body contact.\nOptimizing this loss, we can learn cuboid and convex polyhedral geometries with\nless than 10s of randomly collected data after first contact. Our exploration\nscheme seeks to maximize Expected Information Gain and results in significantly\nfaster learning in both simulated and real-robot experiments. More information\ncan be found at https://dairlab.github.io/activetactile", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f7f\u7528\u89e6\u89c9\u6570\u636e\u540c\u65f6\u786e\u5b9a\u521a\u6027\u7269\u4f53\u5f62\u72b6\u548c\u4f4d\u7f6e\u7684\u5b66\u4e60\u4e0e\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u4f18\u5316\u7684\u65b9\u5f0f\u9ad8\u6548\u5b66\u4e60\u7269\u4f53\u51e0\u4f55\u5f62\u72b6", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u5904\u7406\u672a\u89c1\u8fc7\u7684\u7269\u4f53\uff0c\u5b66\u4e60\u7269\u7406\u7cbe\u786e\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u80fd\u63d0\u4f9b\u6570\u636e\u6548\u7387\u3001\u53ef\u9884\u6d4b\u6027\u548c\u4efb\u52a1\u95f4\u91cd\u7528\u7684\u4f18\u52bf\u3002\u89e6\u89c9\u611f\u77e5\u53ef\u4ee5\u8865\u5145\u89c6\u89c9\u611f\u77e5\u7684\u906e\u6321\u9c81\u68d2\u6027\uff0c\u4f46\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5728\u7ebf\u63a2\u7d22\u6765\u4fdd\u6301\u6570\u636e\u6548\u7387", "method": "\u57fa\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u7cfb\u7edf\u8bc6\u522b\u6700\u65b0\u8fdb\u5c55\uff0c\u6784\u5efa\u60e9\u7f5a\u7269\u7406\u7ea6\u675f\u8fdd\u53cd\u7684\u635f\u5931\u51fd\u6570\uff0c\u907f\u514d\u521a\u4f53\u63a5\u89e6\u5f15\u5165\u7684\u6570\u503c\u521a\u5ea6\u3002\u901a\u8fc7\u4f18\u5316\u8be5\u635f\u5931\u51fd\u6570\u5b66\u4e60\u7acb\u65b9\u4f53\u548c\u51f8\u591a\u9762\u4f53\u51e0\u4f55\u5f62\u72b6\uff0c\u63a2\u7d22\u65b9\u6848\u65e8\u5728\u6700\u5927\u5316\u671f\u671b\u4fe1\u606f\u589e\u76ca", "result": "\u5728\u9996\u6b21\u63a5\u89e6\u540e\u4ec5\u7528\u4e0d\u523010\u79d2\u7684\u968f\u673a\u6536\u96c6\u6570\u636e\u5c31\u80fd\u5b66\u4e60\u51e0\u4f55\u5f62\u72b6\uff0c\u63a2\u7d22\u65b9\u6848\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u90fd\u80fd\u663e\u8457\u52a0\u5feb\u5b66\u4e60\u901f\u5ea6", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4ec5\u4f7f\u7528\u89e6\u89c9\u6570\u636e\u9ad8\u6548\u5730\u540c\u65f6\u786e\u5b9a\u7269\u4f53\u7684\u5f62\u72b6\u548c\u4f4d\u7f6e\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6570\u636e\u6548\u7387\u9ad8\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.13599", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13599", "abs": "https://arxiv.org/abs/2510.13599", "authors": ["Jiahao Wang", "Nived Chebrolu", "Yifu Tao", "Lintong Zhang", "Ayoung Kim", "Maurice Fallon"], "title": "PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction", "comment": null, "summary": "Building an online 3D LiDAR mapping system that produces a detailed surface\nreconstruction while remaining computationally efficient is a challenging task.\nIn this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR\nreconstruction system that adaptively adjusts mesh resolution to achieve\ncompact, detailed reconstructions in real-time. It introduces a new\nrepresentation, planar-mesh, which combines plane modeling and meshing to\ncapture both large surfaces and detailed geometry. The planar-mesh can be\nincrementally updated considering both local surface curvature and free-space\ninformation from sensor measurements. We employ a multi-threaded architecture\nwith a Bounding Volume Hierarchy (BVH) for efficient data storage and fast\nsearch operations, enabling real-time performance. Experimental results show\nthat our method achieves reconstruction accuracy on par with, or exceeding,\nstate-of-the-art techniques-including truncated signed distance functions,\noccupancy mapping, and voxel-based meshing-while producing smaller output file\nsizes (10 times smaller than raw input and more than 5 times smaller than\nmesh-based methods) and maintaining real-time performance (around 2 Hz for a\n64-beam sensor).", "AI": {"tldr": "PlanarMesh\u662f\u4e00\u79cd\u65b0\u9896\u7684\u589e\u91cf\u5f0f\u3001\u57fa\u4e8e\u7f51\u683c\u7684LiDAR\u91cd\u5efa\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u7f51\u683c\u5206\u8fa8\u7387\u5b9e\u73b0\u5b9e\u65f6\u7d27\u51d1\u8be6\u7ec6\u91cd\u5efa\uff0c\u7ed3\u5408\u5e73\u9762\u5efa\u6a21\u548c\u7f51\u683c\u5316\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c0f\u8f93\u51fa\u6587\u4ef6\u5927\u5c0f\u3002", "motivation": "\u6784\u5efa\u5728\u7ebf3D LiDAR\u5efa\u56fe\u7cfb\u7edf\u9700\u8981\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u751f\u6210\u8be6\u7ec6\u8868\u9762\u91cd\u5efa\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u65f6\u6027\u80fd\u548c\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u63d0\u51faplanar-mesh\u8868\u793a\u6cd5\uff0c\u7ed3\u5408\u5e73\u9762\u5efa\u6a21\u548c\u7f51\u683c\u5316\uff1b\u91c7\u7528\u591a\u7ebf\u7a0b\u67b6\u6784\u548cBVH\u8fdb\u884c\u9ad8\u6548\u6570\u636e\u5b58\u50a8\u548c\u5feb\u901f\u641c\u7d22\uff1b\u8003\u8651\u5c40\u90e8\u8868\u9762\u66f2\u7387\u548c\u4f20\u611f\u5668\u6d4b\u91cf\u7684\u81ea\u7531\u7a7a\u95f4\u4fe1\u606f\u8fdb\u884c\u589e\u91cf\u66f4\u65b0\u3002", "result": "\u91cd\u5efa\u7cbe\u5ea6\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u6280\u672f\uff08\u5305\u62ec\u622a\u65ad\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u3001\u5360\u7528\u6620\u5c04\u548c\u57fa\u4e8e\u4f53\u7d20\u7684\u7f51\u683c\u5316\uff09\uff1b\u8f93\u51fa\u6587\u4ef6\u5927\u5c0f\u6bd4\u539f\u59cb\u8f93\u5165\u5c0f10\u500d\uff0c\u6bd4\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\u5c0f5\u500d\u4ee5\u4e0a\uff1b\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\uff0864\u6ce2\u675f\u4f20\u611f\u5668\u7ea62Hz\uff09\u3002", "conclusion": "PlanarMesh\u7cfb\u7edf\u5728\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5b58\u50a8\u9700\u6c42\u5e76\u4fdd\u6301\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u5728\u7ebf3D LiDAR\u5efa\u56fe\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.13616", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.13616", "abs": "https://arxiv.org/abs/2510.13616", "authors": ["Preston Fairchild", "Claudia Chen", "Xiaobo Tan"], "title": "Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor", "comment": "For supplementary videos, see\n  https://drive.google.com/drive/folders/1jol-_z6gaUfjpL1Qi7EG420usTbVSodv?usp=sharing", "summary": "Properly handling delicate produce with robotic manipulators is a major part\nof the future role of automation in agricultural harvesting and processing.\nGrasping with the correct amount of force is crucial in not only ensuring\nproper grip on the object, but also to avoid damaging or bruising the product.\nIn this work, a flexible pressure sensor that is both low cost and easy to\nfabricate is integrated with robotic grippers for working with produce of\nvarying shapes, sizes, and stiffnesses. The sensor is successfully integrated\nwith both a rigid robotic gripper, as well as a pneumatically actuated soft\nfinger. Furthermore, an algorithm is proposed for accelerated estimation of the\nsteady-state value of the sensor output based on the transient response data,\nto enable real-time applications. The sensor is shown to be effective in\nincorporating feedback to correctly grasp objects of unknown sizes and\nstiffnesses. At the same time, the sensor provides estimates for these values\nwhich can be utilized for identification of qualities such as ripeness levels\nand bruising. It is also shown to be able to provide force feedback for objects\nof variable stiffnesses. This enables future use not only for produce\nidentification, but also for tasks such as quality control and selective\ndistribution based on ripeness levels.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u5236\u9020\u7684\u67d4\u6027\u538b\u529b\u4f20\u611f\u5668\uff0c\u96c6\u6210\u5230\u673a\u5668\u4eba\u6293\u624b\u4e2d\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u540c\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u786c\u5ea6\u7684\u519c\u4ea7\u54c1\u3002\u8be5\u4f20\u611f\u5668\u80fd\u591f\u63d0\u4f9b\u529b\u53cd\u9988\uff0c\u4f30\u8ba1\u7269\u4f53\u7279\u6027\uff0c\u5e76\u652f\u6301\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u5728\u519c\u4e1a\u6536\u83b7\u548c\u52a0\u5de5\u4e2d\uff0c\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u9700\u8981\u6b63\u786e\u5904\u7406\u6613\u635f\u519c\u4ea7\u54c1\u3002\u6b63\u786e\u7684\u6293\u53d6\u529b\u4e0d\u4ec5\u786e\u4fdd\u7262\u56fa\u6293\u63e1\uff0c\u8fd8\u8981\u907f\u514d\u635f\u574f\u6216\u538b\u4f24\u4ea7\u54c1\u3002", "method": "\u5c06\u67d4\u6027\u538b\u529b\u4f20\u611f\u5668\u96c6\u6210\u5230\u521a\u6027\u673a\u5668\u4eba\u6293\u624b\u548c\u6c14\u52a8\u8f6f\u6307\u4e2d\uff0c\u63d0\u51fa\u57fa\u4e8e\u77ac\u6001\u54cd\u5e94\u6570\u636e\u52a0\u901f\u4f30\u8ba1\u4f20\u611f\u5668\u7a33\u6001\u503c\u7684\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002", "result": "\u4f20\u611f\u5668\u6210\u529f\u96c6\u6210\u5230\u4e24\u79cd\u6293\u624b\u4e2d\uff0c\u80fd\u591f\u4e3a\u672a\u77e5\u5927\u5c0f\u548c\u786c\u5ea6\u7684\u7269\u4f53\u63d0\u4f9b\u6b63\u786e\u6293\u53d6\u53cd\u9988\uff0c\u540c\u65f6\u4f30\u8ba1\u8fd9\u4e9b\u503c\u7528\u4e8e\u8bc6\u522b\u6210\u719f\u5ea6\u548c\u538b\u4f24\u7b49\u8d28\u91cf\u7279\u6027\u3002", "conclusion": "\u8be5\u4f20\u611f\u5668\u6280\u672f\u4e0d\u4ec5\u53ef\u7528\u4e8e\u519c\u4ea7\u54c1\u8bc6\u522b\uff0c\u8fd8\u53ef\u7528\u4e8e\u8d28\u91cf\u63a7\u5236\u548c\u57fa\u4e8e\u6210\u719f\u5ea6\u7684\u9009\u62e9\u6027\u5206\u62e3\u4efb\u52a1\uff0c\u5177\u6709\u5e7f\u9614\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2510.13619", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13619", "abs": "https://arxiv.org/abs/2510.13619", "authors": ["Daniel Choate", "Jason Rife"], "title": "Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization", "comment": "This is the preprint version of the paper published in: Proceedings\n  of the 37th International Technical Meeting of the Satellite Division of The\n  Institute of Navigation (ION GNSS+ 2024), September 2024 The final version is\n  available at https://doi.org/10.33012/2024.19864", "summary": "In this paper we introduce a visualization methodology to aid a human analyst\nin classifying adversity modes that impact lidar scan matching. Our methodology\nis intended for offline rather than real-time analysis. The method generates a\nvector-field plot that characterizes local discrepancies between a pair of\nregistered point clouds. The vector field plot reveals patterns that would be\ndifficult for the analyst to extract from raw point-cloud data. After\nintroducing our methodology, we apply the process to two proof-of-concept\nexamples: one a simulation study and the other a field experiment. For both\ndata sets, a human analyst was able to reason about a series of adversity\nmechanisms and iteratively remove those mechanisms from the raw data, to help\nfocus attention on progressively smaller discrepancies.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u5e2e\u52a9\u5206\u6790\u5e08\u8bc6\u522b\u5f71\u54cd\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u5339\u914d\u7684\u9006\u5883\u6a21\u5f0f\uff0c\u901a\u8fc7\u751f\u6210\u5411\u91cf\u573a\u56fe\u6765\u663e\u793a\u914d\u51c6\u70b9\u4e91\u4e4b\u95f4\u7684\u5c40\u90e8\u5dee\u5f02\u3002", "motivation": "\u5e2e\u52a9\u4eba\u7c7b\u5206\u6790\u5e08\u66f4\u597d\u5730\u7406\u89e3\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u5339\u914d\u4e2d\u7684\u9006\u5883\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u5728\u539f\u59cb\u70b9\u4e91\u6570\u636e\u4e2d\u96be\u4ee5\u76f4\u63a5\u89c2\u5bdf\u3002", "method": "\u5f00\u53d1\u53ef\u89c6\u5316\u65b9\u6cd5\u751f\u6210\u5411\u91cf\u573a\u56fe\uff0c\u8868\u5f81\u914d\u51c6\u70b9\u4e91\u5bf9\u4e4b\u95f4\u7684\u5c40\u90e8\u5dee\u5f02\uff0c\u7528\u4e8e\u79bb\u7ebf\u5206\u6790\u800c\u975e\u5b9e\u65f6\u5904\u7406\u3002", "result": "\u5728\u4e24\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u793a\u4f8b\uff08\u4eff\u771f\u7814\u7a76\u548c\u73b0\u573a\u5b9e\u9a8c\uff09\u4e2d\uff0c\u5206\u6790\u5e08\u80fd\u591f\u8bc6\u522b\u4e00\u7cfb\u5217\u9006\u5883\u673a\u5236\uff0c\u5e76\u8fed\u4ee3\u5730\u4ece\u539f\u59cb\u6570\u636e\u4e2d\u79fb\u9664\u8fd9\u4e9b\u673a\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e2e\u52a9\u5206\u6790\u5e08\u9010\u6b65\u5173\u6ce8\u66f4\u5c0f\u7684\u5dee\u5f02\uff0c\u901a\u8fc7\u53ef\u89c6\u5316\u624b\u6bb5\u589e\u5f3a\u5bf9\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u5339\u914d\u95ee\u9898\u7684\u7406\u89e3\u3002"}}
{"id": "2510.13625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13625", "abs": "https://arxiv.org/abs/2510.13625", "authors": ["Nicolas Pottier", "Meng Cheng Lau"], "title": "A Modular Object Detection System for Humanoid Robots Using YOLO", "comment": "7 Figures, 5 tables. This article was presented at FIRA Summit 2025.\n  It will be updated for journal submission", "summary": "Within the field of robotics, computer vision remains a significant barrier\nto progress, with many tasks hindered by inefficient vision systems. This\nresearch proposes a generalized vision module leveraging YOLOv9, a\nstate-of-the-art framework optimized for computationally constrained\nenvironments like robots. The model is trained on a dataset tailored to the\nFIRA robotics Hurocup. A new vision module is implemented in ROS1 using a\nvirtual environment to enable YOLO compatibility. Performance is evaluated\nusing metrics such as frames per second (FPS) and Mean Average Precision (mAP).\nPerformance is then compared to the existing geometric framework in static and\ndynamic contexts. The YOLO model achieved comparable precision at a higher\ncomputational cost then the geometric model, while providing improved\nrobustness.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLOv9\u7684\u901a\u7528\u89c6\u89c9\u6a21\u5757\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\uff0c\u5728FIRA\u673a\u5668\u4ebaHurocup\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5e76\u5728ROS1\u4e2d\u5b9e\u73b0\u3002\u76f8\u6bd4\u73b0\u6709\u51e0\u4f55\u6a21\u578b\uff0cYOLO\u6a21\u578b\u5b9e\u73b0\u4e86\u76f8\u5f53\u7684\u7cbe\u5ea6\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u4e2d\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u4ecd\u7136\u662f\u8fdb\u5c55\u7684\u91cd\u8981\u969c\u788d\uff0c\u8bb8\u591a\u4efb\u52a1\u53d7\u5230\u4f4e\u6548\u89c6\u89c9\u7cfb\u7edf\u7684\u963b\u788d\uff0c\u9700\u8981\u4e3a\u8ba1\u7b97\u53d7\u9650\u73af\u5883\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528YOLOv9\u6846\u67b6\u6784\u5efa\u901a\u7528\u89c6\u89c9\u6a21\u5757\uff0c\u5728FIRA\u673a\u5668\u4ebaHurocup\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u901a\u8fc7ROS1\u4e2d\u7684\u865a\u62df\u73af\u5883\u5b9e\u73b0YOLO\u517c\u5bb9\u6027\uff0c\u4f7f\u7528FPS\u548cmAP\u7b49\u6307\u6807\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u4e0e\u73b0\u6709\u51e0\u4f55\u6846\u67b6\u5728\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u4e0b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "YOLO\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0a\u4e0e\u51e0\u4f55\u6a21\u578b\u76f8\u5f53\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "YOLO\u6a21\u578b\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\uff0c\u5c3d\u7ba1\u9700\u8981\u66f4\u9ad8\u7684\u8ba1\u7b97\u8d44\u6e90\u3002"}}
{"id": "2510.13626", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13626", "abs": "https://arxiv.org/abs/2510.13626", "authors": ["Senyu Fei", "Siyin Wang", "Junhao Shi", "Zihao Dai", "Jikun Cai", "Pengfang Qian", "Li Ji", "Xinzhe He", "Shiduo Zhang", "Zhaoye Fei", "Jinlan Fu", "Jingjing Gong", "Xipeng Qiu"], "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models", "comment": null, "summary": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8106\u5f31\u6027\u5206\u6790\uff0c\u53d1\u73b0\u5728\u770b\u4f3c\u9ad8\u6210\u529f\u7387\u4e0b\u5b58\u5728\u4e25\u91cd\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u6a21\u578b\u5bf9\u591a\u79cd\u6270\u52a8\u56e0\u7d20\u6781\u4e3a\u654f\u611f\uff0c\u6027\u80fd\u53ef\u4ece95%\u964d\u81f330%\u4ee5\u4e0b\uff0c\u4e14\u6a21\u578b\u503e\u5411\u4e8e\u5b8c\u5168\u5ffd\u7565\u8bed\u8a00\u6307\u4ee4\u3002", "motivation": "\u5c3d\u7ba1VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u62a5\u544a\u4e86\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u6210\u529f\u7387\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u679c\u53ef\u80fd\u63a9\u76d6\u4e86\u9c81\u68d2\u6027\u7684\u6839\u672c\u5f31\u70b9\u3002\u7814\u7a76\u8005\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u6027\u8106\u5f31\u6027\u5206\u6790\u63ed\u793a\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u53d8\u5316\u4e0b\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e03\u4e2a\u7ef4\u5ea6\u7684\u53d7\u63a7\u6270\u52a8\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\uff1a\u7269\u4f53\u5e03\u5c40\u3001\u76f8\u673a\u89c6\u89d2\u3001\u673a\u5668\u4eba\u521d\u59cb\u72b6\u6001\u3001\u8bed\u8a00\u6307\u4ee4\u3001\u5149\u7167\u6761\u4ef6\u3001\u80cc\u666f\u7eb9\u7406\u548c\u4f20\u611f\u5668\u566a\u58f0\u3002\u5bf9\u591a\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u5206\u6790\u3002", "result": "\u6a21\u578b\u8868\u73b0\u51fa\u5bf9\u6270\u52a8\u56e0\u7d20\u7684\u6781\u7aef\u654f\u611f\u6027\uff0c\u7279\u522b\u662f\u76f8\u673a\u89c6\u89d2\u548c\u673a\u5668\u4eba\u521d\u59cb\u72b6\u6001\uff0c\u6027\u80fd\u4ece95%\u964d\u81f330%\u4ee5\u4e0b\u3002\u4ee4\u4eba\u60ca\u8bb6\u7684\u662f\uff0c\u6a21\u578b\u5bf9\u8bed\u8a00\u53d8\u5316\u4e0d\u654f\u611f\uff0c\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u503e\u5411\u4e8e\u5b8c\u5168\u5ffd\u7565\u8bed\u8a00\u6307\u4ee4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\u9ad8\u57fa\u51c6\u5206\u6570\u7b49\u540c\u4e8e\u771f\u6b63\u80fd\u529b\u7684\u5047\u8bbe\uff0c\u5f3a\u8c03\u9700\u8981\u8bc4\u4f30\u5728\u771f\u5b9e\u53d8\u5316\u4e0b\u7684\u53ef\u9760\u6027\u7684\u5b9e\u8df5\u65b9\u6cd5\u3002"}}
{"id": "2510.13644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13644", "abs": "https://arxiv.org/abs/2510.13644", "authors": ["Michael Bosello", "Flavio Pinzarrone", "Sara Kiade", "Davide Aguiari", "Yvo Keuter", "Aaesha AlShehhi", "Gyordan Caminati", "Kei Long Wong", "Ka Seng Chou", "Junaid Halepota", "Fares Alneyadi", "Jacopo Panerati", "Giovanni Pau"], "title": "On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas", "comment": null, "summary": "Drone technology is proliferating in many industries, including agriculture,\nlogistics, defense, infrastructure, and environmental monitoring. Vision-based\nautonomy is one of its key enablers, particularly for real-world applications.\nThis is essential for operating in novel, unstructured environments where\ntraditional navigation methods may be unavailable. Autonomous drone racing has\nbecome the de facto benchmark for such systems. State-of-the-art research has\nshown that autonomous systems can surpass human-level performance in racing\narenas. However, direct applicability to commercial and field operations is\nstill limited as current systems are often trained and evaluated in highly\ncontrolled environments. In our contribution, the system's capabilities are\nanalyzed within a controlled environment -- where external tracking is\navailable for ground-truth comparison -- but also demonstrated in a\nchallenging, uninstrumented environment -- where ground-truth measurements were\nnever available. We show that our approach can match the performance of\nprofessional human pilots in both scenarios. We also publicly release the data\nfrom the flights carried out by our approach and a world-class human pilot.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u65e0\u4eba\u673a\u89c6\u89c9\u81ea\u4e3b\u7cfb\u7edf\u5728\u53d7\u63a7\u73af\u5883\u548c\u65e0\u4eea\u5668\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u8fbe\u5230\u4e13\u4e1a\u4eba\u7c7b\u98de\u884c\u5458\u6c34\u5e73\uff0c\u5e76\u516c\u5f00\u4e86\u76f8\u5173\u98de\u884c\u6570\u636e\u3002", "motivation": "\u65e0\u4eba\u673a\u6280\u672f\u5728\u591a\u4e2a\u884c\u4e1a\u5feb\u901f\u53d1\u5c55\uff0c\u89c6\u89c9\u81ea\u4e3b\u7cfb\u7edf\u662f\u5173\u952e\u63a8\u52a8\u529b\u3002\u867d\u7136\u73b0\u6709\u7cfb\u7edf\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5546\u4e1a\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u76f4\u63a5\u9002\u7528\u6027\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30\u7cfb\u7edf\u5728\u53d7\u63a7\u548c\u65e0\u4eea\u5668\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u4f7f\u7528\u5916\u90e8\u8ddf\u8e2a\u8fdb\u884c\u5730\u9762\u5b9e\u51b5\u6bd4\u8f83\uff0c\u540c\u65f6\u5728\u65e0\u5730\u9762\u5b9e\u51b5\u6d4b\u91cf\u7684\u65e0\u4eea\u5668\u73af\u5883\u4e2d\u8fdb\u884c\u6f14\u793a\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u79cd\u573a\u666f\u4e0b\u90fd\u80fd\u8fbe\u5230\u4e13\u4e1a\u4eba\u7c7b\u98de\u884c\u5458\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u89c6\u89c9\u81ea\u4e3b\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u98de\u884c\u6570\u636e\u8d44\u6e90\u3002"}}
{"id": "2510.13686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.13686", "abs": "https://arxiv.org/abs/2510.13686", "authors": ["Miana Smith", "Paul Arthur Richard", "Alexander Htet Kyaw", "Neil Gershenfeld"], "title": "Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures", "comment": "In ACM Symposium on Computational Fabrication (SCF '25), November\n  20-21, 2025, Cambridge, MA, USA. ACM, New York, NY, USA, 15 pages", "summary": "Although digital fabrication processes at the desktop scale have become\nproficient and prolific, systems aimed at producing larger-scale structures are\nstill typically complex, expensive, and unreliable. In this work, we present an\napproach for the fabrication of scalable macroscale structures using simple\nrobots and interlocking lattice building blocks. A target structure is first\nvoxelized so that it can be populated with an architected lattice. These voxels\nare then grouped into larger interconnected blocks, which are produced using\nstandard digital fabrication processes, leveraging their capability to produce\nhighly complex geometries at a small scale. These blocks, on the size scale of\ntens of centimeters, are then fed to mobile relative robots that are able to\ntraverse over the structure and place new blocks to form structures on the\nmeter scale. To facilitate the assembly of large structures, we introduce a\nlive digital twin simulation tool for controlling and coordinating assembly\nrobots that enables both global planning for a target structure and live user\ndesign, interaction, or intervention. To improve assembly throughput, we\nintroduce a new modular assembly robot, designed for hierarchical voxel\nhandling. We validate this system by demonstrating the voxelization,\nhierarchical blocking, path planning, and robotic fabrication of a set of\nmeter-scale objects.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u7b80\u5355\u673a\u5668\u4eba\u548c\u4e92\u9501\u6676\u683c\u6784\u5efa\u5757\u6765\u5236\u9020\u53ef\u6269\u5c55\u5b8f\u89c2\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f53\u7d20\u5316\u3001\u5206\u5c42\u5206\u7ec4\u548c\u79fb\u52a8\u673a\u5668\u4eba\u7ec4\u88c5\uff0c\u5b9e\u73b0\u4e86\u7c73\u7ea7\u7ed3\u6784\u7684\u81ea\u52a8\u5316\u5236\u9020\u3002", "motivation": "\u5f53\u524d\u9762\u5411\u5927\u578b\u7ed3\u6784\u7684\u6570\u5b57\u5236\u9020\u7cfb\u7edf\u901a\u5e38\u590d\u6742\u3001\u6602\u8d35\u4e14\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7b80\u5355\u3001\u53ef\u9760\u7684\u5927\u89c4\u6a21\u5236\u9020\u65b9\u6cd5\u3002", "method": "\u5c06\u76ee\u6807\u7ed3\u6784\u4f53\u7d20\u5316\u5e76\u586b\u5145\u67b6\u6784\u6676\u683c\uff0c\u5c06\u4f53\u7d20\u5206\u7ec4\u4e3a\u66f4\u5927\u4e92\u8fde\u5757\uff0c\u4f7f\u7528\u6807\u51c6\u6570\u5b57\u5236\u9020\u5de5\u827a\u751f\u4ea7\uff0c\u7136\u540e\u901a\u8fc7\u79fb\u52a8\u673a\u5668\u4eba\u904d\u5386\u7ed3\u6784\u5e76\u653e\u7f6e\u65b0\u5757\u8fdb\u884c\u7ec4\u88c5\u3002", "result": "\u901a\u8fc7\u6f14\u793a\u4f53\u7d20\u5316\u3001\u5206\u5c42\u5206\u7ec4\u3001\u8def\u5f84\u89c4\u5212\u548c\u673a\u5668\u4eba\u5236\u9020\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u80fd\u591f\u5236\u9020\u7c73\u7ea7\u7269\u4f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5229\u7528\u7b80\u5355\u673a\u5668\u4eba\u548c\u4e92\u9501\u6676\u683c\u6784\u5efa\u5757\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u5b8f\u89c2\u7ed3\u6784\u7684\u53ef\u9760\u5236\u9020\uff0c\u4e3a\u5927\u578b\u6570\u5b57\u5236\u9020\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.13778", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.13778", "abs": "https://arxiv.org/abs/2510.13778", "authors": ["Xinyi Chen", "Yilun Chen", "Yanwei Fu", "Ning Gao", "Jiaya Jia", "Weiyang Jin", "Hao Li", "Yao Mu", "Jiangmiao Pang", "Yu Qiao", "Yang Tian", "Bin Wang", "Bolun Wang", "Fangjing Wang", "Hanqing Wang", "Tai Wang", "Ziqin Wang", "Xueyuan Wei", "Chao Wu", "Shuai Yang", "Jinhui Ye", "Junqiu Yu", "Jia Zeng", "Jingjing Zhang", "Jinyu Zhang", "Shi Zhang", "Feng Zheng", "Bowen Zhou", "Yangkun Zhu"], "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy", "comment": "Technical report", "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.", "AI": {"tldr": "InternVLA-M1\u662f\u4e00\u4e2a\u7528\u4e8e\u7a7a\u95f4\u5b9a\u4f4d\u548c\u673a\u5668\u4eba\u63a7\u5236\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u5f15\u5bfc\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u8bad\u7ec3\uff0c\u5c06\u7a7a\u95f4\u5b9a\u4f4d\u4f5c\u4e3a\u6307\u4ee4\u548c\u673a\u5668\u4eba\u52a8\u4f5c\u4e4b\u95f4\u7684\u5173\u952e\u8fde\u63a5\u3002\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u63a8\u52a8\u6307\u4ee4\u8ddf\u968f\u673a\u5668\u4eba\u5411\u53ef\u6269\u5c55\u3001\u901a\u7528\u667a\u80fd\u65b9\u5411\u53d1\u5c55\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\u4e4b\u95f4\u7684\u8131\u8282\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a(1) \u5728230\u4e07\u7a7a\u95f4\u63a8\u7406\u6570\u636e\u4e0a\u8fdb\u884c\u7a7a\u95f4\u5b9a\u4f4d\u9884\u8bad\u7ec3\uff0c\u786e\u5b9a\"\u5728\u54ea\u91cc\u884c\u52a8\"\uff1b(2) \u7a7a\u95f4\u5f15\u5bfc\u7684\u52a8\u4f5c\u540e\u8bad\u7ec3\uff0c\u901a\u8fc7\u5373\u63d2\u5373\u7528\u7684\u7a7a\u95f4\u63d0\u793a\u51b3\u5b9a\"\u5982\u4f55\u884c\u52a8\"\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1a\u5728SimplerEnv Google Robot\u4e0a\u63d0\u534714.6%\uff0cWidowX\u4e0a\u63d0\u534717%\uff0cLIBERO Franka\u4e0a\u63d0\u53474.3%\u3002\u5728\u771f\u5b9e\u4e16\u754c\u96c6\u7fa4\u62fe\u53d6\u4efb\u52a1\u4e2d\u63d0\u53477.3%\uff0c\u901a\u8fc7\u5408\u6210\u534f\u540c\u8bad\u7ec3\u5728\u672a\u89c1\u5bf9\u8c61\u4e0a\u63d0\u534720.6%\u3002", "conclusion": "\u7a7a\u95f4\u5f15\u5bfc\u8bad\u7ec3\u662f\u6784\u5efa\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u901a\u7528\u673a\u5668\u4eba\u7684\u7edf\u4e00\u539f\u5219\uff0c\u5728\u957f\u671f\u63a8\u7406\u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u5de5\u4f5c10%\u4ee5\u4e0a\u3002"}}
