{"id": "2510.16205", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16205", "abs": "https://arxiv.org/abs/2510.16205", "authors": ["João Carlos Virgolino Soares", "Gabriel Fischer Abati", "Claudio Semini"], "title": "VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments", "comment": "Code available at https://github.com/iit-DLSLab/VAR-SLAM", "summary": "Visual SLAM in dynamic environments remains challenging, as several existing\nmethods rely on semantic filtering that only handles known object classes, or\nuse fixed robust kernels that cannot adapt to unknown moving objects, leading\nto degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual\nAdaptive and Robust SLAM), an ORB-SLAM3-based system that combines a\nlightweight semantic keypoint filter to deal with known moving objects, with\nBarron's adaptive robust loss to handle unknown ones. The shape parameter of\nthe robust kernel is estimated online from residuals, allowing the system to\nautomatically adjust between Gaussian and heavy-tailed behavior. We evaluate\nVAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which\ninclude both known and unknown moving objects. Results show improved trajectory\naccuracy and robustness over state-of-the-art baselines, achieving up to 25%\nlower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining\nperformance at 27 FPS on average."}
{"id": "2510.16231", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16231", "abs": "https://arxiv.org/abs/2510.16231", "authors": ["Bihao Zhang", "Davood Soleymanzadeh", "Xiao Liang", "Minghui Zheng"], "title": "DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly", "comment": null, "summary": "Intelligent robotic disassembly of end-of-life (EOL) products has been a\nlong-standing challenge in robotics. While machine learning techniques have\nshown promise, the lack of specialized hardware limits their application in\nreal-world scenarios. We introduce DeGrip, a customized gripper designed for\nthe disassembly of EOL computer desktops. DeGrip provides three degrees of\nfreedom (DOF), enabling arbitrary configurations within the disassembly\nenvironment when mounted on a robotic manipulator. It employs a cable-driven\ntransmission mechanism that reduces its overall size and enables operation in\nconfined spaces. The wrist is designed to decouple the actuation of wrist and\njaw joints. We also developed an EOL desktop disassembly environment in Isaac\nSim to evaluate the effectiveness of DeGrip. The tasks were designed to\ndemonstrate its ability to operate in confined spaces and disassemble\ncomponents in arbitrary configurations. The evaluation results confirm the\ncapability of DeGrip for EOL desktop disassembly."}
{"id": "2510.16240", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16240", "abs": "https://arxiv.org/abs/2510.16240", "authors": ["Lukas Zbinden", "Nigel Nelson", "Juo-Tung Chen", "Xinhao Chen", "Ji Woong", "Kim", "Mahdi Azizian", "Axel Krieger", "Sean Huver"], "title": "Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning", "comment": null, "summary": "The rise of surgical robots and vision-language-action models has accelerated\nthe development of autonomous surgical policies and efficient assessment\nstrategies. However, evaluating these policies directly on physical robotic\nplatforms such as the da Vinci Research Kit (dVRK) remains hindered by high\ncosts, time demands, reproducibility challenges, and variability in execution.\nWorld foundation models (WFM) for physical AI offer a transformative approach\nto simulate complex real-world surgical tasks, such as soft tissue deformation,\nwith high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune\nof the Cosmos WFM, which, together with a trained video classifier, enables\nfully automated online evaluation and benchmarking of surgical policies. We\nevaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop\nsuture pad tasks, the automated pipeline achieves strong correlation between\nonline rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si\nplatform, as well as good agreement between human labelers and the V-JEPA\n2-derived video classifier. Additionally, preliminary experiments with ex-vivo\nporcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising\nalignment with real-world evaluations, highlighting the platform's potential\nfor more complex surgical procedures."}
{"id": "2510.16263", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.16263", "abs": "https://arxiv.org/abs/2510.16263", "authors": ["Jierui Peng", "Yanyan Zhang", "Yicheng Duan", "Tuo Liang", "Vipin Chaudhary", "Yu Yin"], "title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?", "comment": "Homepage: https://vulab-ai.github.io/NEBULA-Alpha/", "summary": "The evaluation of Vision-Language-Action (VLA) agents is hindered by the\ncoarse, end-task success metric that fails to provide precise skill diagnosis\nor measure robustness to real-world perturbations. This challenge is\nexacerbated by a fragmented data landscape that impedes reproducible research\nand the development of generalist models. To address these limitations, we\nintroduce \\textbf{NEBULA}, a unified ecosystem for single-arm manipulation that\nenables diagnostic and reproducible evaluation. NEBULA features a novel\ndual-axis evaluation protocol that combines fine-grained \\textit{capability\ntests} for precise skill diagnosis with systematic \\textit{stress tests} that\nmeasure robustness. A standardized API and a large-scale, aggregated dataset\nare provided to reduce fragmentation and support cross-dataset training and\nfair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle\nwith key capabilities such as spatial reasoning and dynamic adaptation, which\nare consistently obscured by conventional end-task success metrics. By\nmeasuring both what an agent can do and when it does so reliably, NEBULA\nprovides a practical foundation for robust, general-purpose embodied agents."}
{"id": "2510.16281", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.16281", "abs": "https://arxiv.org/abs/2510.16281", "authors": ["Yilin Wu", "Anqi Li", "Tucker Hermans", "Fabio Ramos", "Andrea Bajcsy", "Claudia P'erez-D'Arpino"], "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification", "comment": null, "summary": "Reasoning Vision Language Action (VLA) models improve robotic\ninstruction-following by generating step-by-step textual plans before low-level\nactions, an approach inspired by Chain-of-Thought (CoT) reasoning in language\nmodels. Yet even with a correct textual plan, the generated actions can still\nmiss the intended outcomes in the plan, especially in out-of-distribution (OOD)\nscenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,\nand introduce a training-free, runtime policy steering method for\nreasoning-action alignment. Given a reasoning VLA's intermediate textual plan,\nour framework samples multiple candidate action sequences from the same model,\npredicts their outcomes via simulation, and uses a pre-trained Vision-Language\nModel (VLM) to select the sequence whose outcome best aligns with the VLA's own\ntextual plan. Only executing action sequences that align with the textual\nreasoning turns our base VLA's natural action diversity from a source of error\ninto a strength, boosting robustness to semantic and visual OOD perturbations\nand enabling novel behavior composition without costly re-training. We also\ncontribute a reasoning-annotated extension of LIBERO-100, environment\nvariations tailored for OOD evaluation, and demonstrate up to 15% performance\ngain over prior work on behavior composition tasks and scales with compute and\ndata diversity. Project Website at:\nhttps://yilin-wu98.github.io/steering-reasoning-vla/"}
{"id": "2510.16308", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16308", "abs": "https://arxiv.org/abs/2510.16308", "authors": ["Chi Zhang", "Xian Huang", "Wei Dong"], "title": "SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling", "comment": null, "summary": "UAVs equipped with a single depth camera encounter significant challenges in\ndynamic obstacle avoidance due to limited field of view and inevitable blind\nspots. While active vision strategies that steer onboard cameras have been\nproposed to expand sensing coverage, most existing methods separate motion\nplanning from sensing considerations, resulting in less effective and delayed\nobstacle response. To address this limitation, we introduce SPOT\n(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning\nframework for observation-aware trajectory planning that explicitly\nincorporates sensing objectives into motion optimization. At the core of our\nmethod is a Gaussian Process-based obstacle belief map, which establishes a\nunified probabilistic representation of both recognized (previously observed)\nand potential obstacles. This belief is further processed through a\ncollision-aware inference mechanism that transforms spatial uncertainty and\ntrajectory proximity into a time-varying observation urgency map. By\nintegrating urgency values within the current field of view, we define\ndifferentiable objectives that enable real-time, observation-aware trajectory\nplanning with computation times under 10 ms. Simulation and real-world\nexperiments in dynamic, cluttered, and occluded environments show that our\nmethod detects potential dynamic obstacles 2.8 seconds earlier than baseline\napproaches, increasing dynamic obstacle visibility by over 500\\%, and enabling\nsafe navigation through cluttered, occluded environments."}
{"id": "2510.16344", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16344", "abs": "https://arxiv.org/abs/2510.16344", "authors": ["Chenrui Tie", "Shengxiang Sun", "Yudi Lin", "Yanbo Wang", "Zhongrui Li", "Zhouhan Zhong", "Jinxuan Zhu", "Yiman Pang", "Haonan Chen", "Junting Chen", "Ruihai Wu", "Lin Shao"], "title": "Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models", "comment": null, "summary": "Assembly hinges on reliably forming connections between parts; yet most\nrobotic approaches plan assembly sequences and part poses while treating\nconnectors as an afterthought. Connections represent the critical \"last mile\"\nof assembly execution, while task planning may sequence operations and motion\nplan may position parts, the precise establishment of physical connections\nultimately determines assembly success or failure. In this paper, we consider\nconnections as first-class primitives in assembly representation, including\nconnector types, specifications, quantities, and placement locations. Drawing\ninspiration from how humans learn assembly tasks through step-by-step\ninstruction manuals, we present Manual2Skill++, a vision-language framework\nthat automatically extracts structured connection information from assembly\nmanuals. We encode assembly tasks as hierarchical graphs where nodes represent\nparts and sub-assemblies, and edges explicitly model connection relationships\nbetween components. A large-scale vision-language model parses symbolic\ndiagrams and annotations in manuals to instantiate these graphs, leveraging the\nrich connection knowledge embedded in human-designed instructions. We curate a\ndataset containing over 20 assembly tasks with diverse connector types to\nvalidate our representation extraction approach, and evaluate the complete task\nunderstanding-to-execution pipeline across four complex assembly scenarios in\nsimulation, spanning furniture, toys, and manufacturing components with\nreal-world correspondence."}
{"id": "2510.16424", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16424", "abs": "https://arxiv.org/abs/2510.16424", "authors": ["Dan Guo", "Xibin Jin", "Shuai Wang", "Zhigang Wen", "Miaowen Wen", "Chengzhong Xu"], "title": "Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach", "comment": null, "summary": "Edge robotics involves frequent exchanges of large-volume multi-modal data.\nExisting methods ignore the interdependency between robotic functionalities and\ncommunication conditions, leading to excessive communication overhead. This\npaper revolutionizes edge robotics systems through integrated perception,\nmotion, and communication (IPMC). As such, robots can dynamically adapt their\ncommunication strategies (i.e., compression ratio, transmission frequency,\ntransmit power) by leveraging the knowledge of robotic perception and motion\ndynamics, thus reducing the need for excessive sensor data uploads.\nFurthermore, by leveraging the learning to optimize (LTO) paradigm, an\nimitation learning neural network is designed and implemented, which reduces\nthe computational complexity by over 10x compared to state-of-the art\noptimization solvers. Experiments demonstrate the superiority of the proposed\nIPMC and the real-time execution capability of LTO."}
{"id": "2510.16435", "categories": ["cs.RO", "cs.CL", "cs.HC", "I.2.9; H.5.2; H.5.0; I.2.8; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2510.16435", "abs": "https://arxiv.org/abs/2510.16435", "authors": ["Lennart Wachowiak", "Andrew Coles", "Gerard Canal", "Oya Celiktutan"], "title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics", "comment": null, "summary": "With the growing use of large language models and conversational interfaces\nin human-robot interaction, robots' ability to answer user questions is more\nimportant than ever. We therefore introduce a dataset of 1,893 user questions\nfor household robots, collected from 100 participants and organized into 12\ncategories and 70 subcategories. Most work in explainable robotics focuses on\nwhy-questions. In contrast, our dataset provides a wide variety of questions,\nfrom questions about simple execution details to questions about how the robot\nwould act in hypothetical scenarios -- thus giving roboticists valuable\ninsights into what questions their robot needs to be able to answer. To collect\nthe dataset, we created 15 video stimuli and 7 text stimuli, depicting robots\nperforming varied household tasks. We then asked participants on Prolific what\nquestions they would want to ask the robot in each portrayed situation. In the\nfinal dataset, the most frequent categories are questions about task execution\ndetails (22.5%), the robot's capabilities (12.7%), and performance assessments\n(11.3%). Although questions about how robots would handle potentially difficult\nscenarios and ensure correct behavior are less frequent, users rank them as the\nmost important for robots to be able to answer. Moreover, we find that users\nwho identify as novices in robotics ask different questions than more\nexperienced users. Novices are more likely to inquire about simple facts, such\nas what the robot did or the current state of the environment. As robots enter\nenvironments shared with humans and language becomes central to giving\ninstructions and interaction, this dataset provides a valuable foundation for\n(i) identifying the information robots need to log and expose to conversational\ninterfaces, (ii) benchmarking question-answering modules, and (iii) designing\nexplanation strategies that align with user expectations."}
{"id": "2510.16500", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16500", "abs": "https://arxiv.org/abs/2510.16500", "authors": ["Chen Min", "Jilin Mei", "Heng Zhai", "Shuai Wang", "Tong Sun", "Fanjie Kong", "Haoyang Li", "Fangyuan Mao", "Fuyang Liu", "Shuo Wang", "Yiming Nie", "Qi Zhu", "Liang Xiao", "Dawei Zhao", "Yu Hu"], "title": "Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks", "comment": "Off-road robotics", "summary": "A major bottleneck in off-road autonomous driving research lies in the\nscarcity of large-scale, high-quality datasets and benchmarks. To bridge this\ngap, we present ORAD-3D, which, to the best of our knowledge, is the largest\ndataset specifically curated for off-road autonomous driving. ORAD-3D covers a\nwide spectrum of terrains, including woodlands, farmlands, grasslands,\nriversides, gravel roads, cement roads, and rural areas, while capturing\ndiverse environmental variations across weather conditions (sunny, rainy,\nfoggy, and snowy) and illumination levels (bright daylight, daytime, twilight,\nand nighttime). Building upon this dataset, we establish a comprehensive suite\nof benchmark evaluations spanning five fundamental tasks: 2D free-space\ndetection, 3D occupancy prediction, rough GPS-guided path planning,\nvision-language model-driven autonomous driving, and world model for off-road\nenvironments. Together, the dataset and benchmarks provide a unified and robust\nresource for advancing perception and planning in challenging off-road\nscenarios. The dataset and code will be made publicly available at\nhttps://github.com/chaytonmin/ORAD-3D."}
{"id": "2510.16517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16517", "abs": "https://arxiv.org/abs/2510.16517", "authors": ["Haokai Ding", "Wenzeng Zhang"], "title": "A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping", "comment": "Accepted author manuscript (AAM) for IEEE/RSJ IROS 2025. 6 pages, 10\n  figures", "summary": "This paper introduces a novel robotic gripper, named as the SPD gripper. It\nfeatures a palm and two mechanically identical and symmetrically arranged\nfingers, which can be driven independently or by a single motor. The fingertips\nof the fingers follow a linear motion trajectory, facilitating the grasping of\nobjects of various sizes on a tabletop without the need to adjust the overall\nheight of the gripper. Traditional industrial grippers with parallel gripping\ncapabilities often exhibit an arcuate motion at the fingertips, requiring the\nentire robotic arm to adjust its height to avoid collisions with the tabletop.\nThe SPD gripper, with its linear parallel gripping mechanism, effectively\naddresses this issue. Furthermore, the SPD gripper possesses adaptive\ncapabilities, accommodating objects of different shapes and sizes. This paper\npresents the design philosophy, fundamental composition principles, and\noptimization analysis theory of the SPD gripper. Based on the design theory, a\nrobotic gripper prototype was developed and tested. The experimental results\ndemonstrate that the robotic gripper successfully achieves linear parallel\ngripping functionality and exhibits good adaptability. In the context of the\nongoing development of embodied intelligence technologies, this robotic gripper\ncan assist various robots in achieving effective grasping, laying a solid\nfoundation for collecting data to enhance deep learning training."}
{"id": "2510.16518", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.16518", "abs": "https://arxiv.org/abs/2510.16518", "authors": ["Jesús Ortega-Peimbert", "Finn Lukas Busch", "Timon Homberger", "Quantao Yang", "Olov Andersson"], "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation", "comment": null, "summary": "Advances in open-vocabulary semantic mapping and object navigation have\nenabled robots to perform an informed search of their environment for an\narbitrary object. However, such zero-shot object navigation is typically\ndesigned for simple queries with an object name like \"television\" or \"blue\nrug\". Here, we consider more complex free-text queries with spatial\nrelationships, such as \"find the remote on the table\" while still leveraging\nrobustness of a semantic map. We present DIV-Nav, a real-time navigation system\nthat efficiently addresses this problem through a series of relaxations: i)\nDecomposing natural language instructions with complex spatial constraints into\nsimpler object-level queries on a semantic map, ii) computing the Intersection\nof individual semantic belief maps to identify regions where all objects\nco-exist, and iii) Validating the discovered objects against the original,\ncomplex spatial constrains via a LVLM. We further investigate how to adapt the\nfrontier exploration objectives of online semantic mapping to such spatial\nsearch queries to more effectively guide the search process. We validate our\nsystem through extensive experiments on the MultiON benchmark and real-world\ndeployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More\ndetails and videos are available at https://anonsub42.github.io/reponame/"}
{"id": "2510.16524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16524", "abs": "https://arxiv.org/abs/2510.16524", "authors": ["Haokai Ding", "Zhaohan Chen", "Tao Yang", "Wenzeng Zhang"], "title": "Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping", "comment": "6 pages, 9 figures, Accepted author manuscript for IEEE CASE 2025", "summary": "This paper presents the SP-Diff parallel gripper system, addressing the\nlimited adaptability of conventional end-effectors in intelligent industrial\nautomation. The proposed design employs an innovative differential linkage\nmechanism with a modular symmetric dual-finger configuration to achieve\nlinear-parallel grasping. By integrating a planetary gear transmission, the\nsystem enables synchronized linear motion and independent finger pose\nadjustment while maintaining structural rigidity, reducing Z-axis recalibration\nrequirements by 30% compared to arc-trajectory grippers. The compact palm\narchitecture incorporates a kinematically optimized parallelogram linkage and\nDifferential mechanism, demonstrating adaptive grasping capabilities for\ndiverse industrial workpieces and deformable objects such as citrus fruits.\nFuture-ready interfaces are embedded for potential force/vision sensor\nintegration to facilitate multimodal data acquisition (e.g., trajectory\nplanning and object deformation) in digital twin frameworks. Designed as a\nflexible manufacturing solution, SP-Diff advances robotic end-effector\nintelligence through its adaptive architecture, showing promising applications\nin collaborative robotics, logistics automation, and specialized operational\nscenarios."}
{"id": "2510.16617", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16617", "abs": "https://arxiv.org/abs/2510.16617", "authors": ["Ruihan Zhao", "Tyler Ingebrand", "Sandeep Chinchali", "Ufuk Topcu"], "title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation", "comment": null, "summary": "Vision-Language-Action (VLA) models trained on large robot datasets promise\ngeneral-purpose, robust control across diverse domains and embodiments.\nHowever, existing approaches often fail out-of-the-box when deployed in novel\nenvironments, embodiments, or tasks. We introduce Mixture of Skills VLA\n(MoS-VLA), a framework that represents robot manipulation policies as linear\ncombinations of a finite set of learned basis functions. During pretraining,\nMoS-VLA jointly learns these basis functions across datasets from the Open\nX-Embodiment project, producing a structured skill space. At test time,\nadapting to a new task requires only a single expert demonstration. The\ncorresponding skill representation is then inferred via a lightweight convex\noptimization problem that minimizes the L1 action error, without requiring\ngradient updates. This gradient-free adaptation incurs minimal overhead while\nenabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower\naction-prediction error on five out of five unseen datasets and succeeds in\nboth simulation and real-robot tasks where a pretrained VLA model fails\noutright. Project page: mos-vla.github.io/"}
{"id": "2510.16692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16692", "abs": "https://arxiv.org/abs/2510.16692", "authors": ["Tianshu Ruan", "Zoe Betta", "Georgios Tzoumas", "Rustam Stolkin", "Manolis Chiou"], "title": "First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response", "comment": null, "summary": "This study investigates First Responders' (FRs) attitudes toward the use of\nsemantic information and Situational Awareness (SA) in robotic systems during\nemergency operations. A structured questionnaire was administered to 22 FRs\nacross eight countries, capturing their demographic profiles, general attitudes\ntoward robots, and experiences with semantics-enhanced SA. Results show that\nmost FRs expressed positive attitudes toward robots, and rated the usefulness\nof semantic information for building SA at an average of 3.6 out of 5. Semantic\ninformation was also valued for its role in predicting unforeseen emergencies\n(mean 3.9). Participants reported requiring an average of 74.6\\% accuracy to\ntrust semantic outputs and 67.8\\% for them to be considered useful, revealing a\nwillingness to use imperfect but informative AI support tools.\n  To the best of our knowledge, this study offers novel insights by being one\nof the first to directly survey FRs on semantic-based SA in a cross-national\ncontext. It reveals the types of semantic information most valued in the field,\nsuch as object identity, spatial relationships, and risk context-and connects\nthese preferences to the respondents' roles, experience, and education levels.\nThe findings also expose a critical gap between lab-based robotics capabilities\nand the realities of field deployment, highlighting the need for more\nmeaningful collaboration between FRs and robotics researchers. These insights\ncontribute to the development of more user-aligned and situationally aware\nrobotic systems for emergency response."}
{"id": "2510.16738", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16738", "abs": "https://arxiv.org/abs/2510.16738", "authors": ["Matteo El-Hariry", "Vittorio Franzese", "Miguel Olivares-Mendez"], "title": "Towards Active Excitation-Based Dynamic Inertia Identification in Satellites", "comment": null, "summary": "This paper presents a comprehensive analysis of how excitation design\ninfluences the identification of the inertia properties of rigid nano- and\nmicro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel\ncoupling, actuator limits, and external disturbances, and excite the system\nusing eight torque profiles of varying spectral richness. Two estimators are\ncompared, a batch Least Squares method and an Extended Kalman Filter, across\nthree satellite configurations and time-varying inertia scenarios. Results show\nthat excitation frequency content and estimator assumptions jointly determine\nestimation accuracy and robustness, offering practical guidance for in-orbit\nadaptive inertia identification by outlining the conditions under which each\nmethod performs best. The code is provided as open-source ."}
{"id": "2510.16755", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.16755", "abs": "https://arxiv.org/abs/2510.16755", "authors": ["Kyung-Hwan Kim", "DongHyun Ahn", "Dong-hyun Lee", "JuYoung Yoon", "Dong Jin Hyun"], "title": "Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation", "comment": "6 pages, accepted to IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2025", "summary": "State estimation is crucial for legged robots as it directly affects control\nperformance and locomotion stability. In this paper, we propose an Adaptive\nInvariant Extended Kalman Filter to improve proprioceptive state estimation for\nlegged robots. The proposed method adaptively adjusts the noise level of the\ncontact foot model based on online covariance estimation, leading to improved\nstate estimation under varying contact conditions. It effectively handles small\nslips that traditional slip rejection fails to address, as overly sensitive\nslip rejection settings risk causing filter divergence. Our approach employs a\ncontact detection algorithm instead of contact sensors, reducing the reliance\non additional hardware. The proposed method is validated through real-world\nexperiments on the quadruped robot LeoQuad, demonstrating enhanced state\nestimation performance in dynamic locomotion scenarios."}
{"id": "2510.16767", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16767", "abs": "https://arxiv.org/abs/2510.16767", "authors": ["Jia Li", "Guoxiang Zhao"], "title": "T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic", "comment": null, "summary": "Translating natural language instructions into executable motion plans is a\nfundamental challenge in robotics. Traditional approaches are typically\nconstrained by their reliance on domain-specific expertise to customize\nplanners, and often struggle with spatio-temporal couplings that usually lead\nto infeasible motions or discrepancies between task planning and motion\nexecution. Despite the proficiency of Large Language Models (LLMs) in\nhigh-level semantic reasoning, hallucination could result in infeasible motion\nplans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic\nmotion planning framework that self-corrects it output with formal methods. The\nframework decomposes spatio-temporal task constraints via three cascaded\nmodules, each of which stimulates an LLM to generate candidate trajectory\nsequences and examines their feasibility via a Signal Temporal Logic (STL)\nverifier until one that satisfies complex spatial, temporal, and logical\nconstraints is found.Experiments across different scenarios show that T3\nPlanner significantly outperforms the baselines. The required reasoning can be\ndistilled into a lightweight Qwen3-4B model that enables efficient deployment.\nAll supplementary materials are accessible at\nhttps://github.com/leeejia/T3_Planner."}
{"id": "2510.16771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16771", "abs": "https://arxiv.org/abs/2510.16771", "authors": ["Xu He", "Xiaolin Meng", "Wenxuan Yin", "Youdong Zhang", "Lingfei Mo", "Xiangdong An", "Fangwen Yu", "Shuguo Pan", "Yufeng Liu", "Jingnan Liu", "Yujia Zhang", "Wang Gao"], "title": "A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT", "comment": null, "summary": "Developing universal Positioning, Navigation, and Timing (PNT) is our\nenduring goal. Today's complex environments demand PNT that is more resilient,\nenergy-efficient and cognitively capable. This paper asks how we can endow\nunmanned systems with brain-inspired spatial cognition navigation while\nexploiting the high precision of machine PNT to advance universal PNT. We\nprovide a new perspective and roadmap for shifting PNT from \"tool-oriented\" to\n\"cognition-driven\". Contributions: (1) multi-level dissection of differences\namong traditional PNT, biological brain PNT and brain-inspired PNT; (2) a\nfour-layer (observation-capability-decision-hardware) fusion framework that\nunites numerical precision and brain-inspired intelligence; (3) forward-looking\nrecommendations for future development of brain-inspired PNT."}
{"id": "2510.16905", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16905", "abs": "https://arxiv.org/abs/2510.16905", "authors": ["Yukang Cao", "Rahul Moorthy", "O. Goktug Poyrazoglu", "Volkan Isler"], "title": "C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control", "comment": "Submitted to the 2026 IEEE International Conference on Robotics and\n  Automation (ICRA). 8 pages, 4 figures", "summary": "Trajectory sampling is a key component of sampling-based control mechanisms.\nTrajectory samplers rely on control input samplers, which generate control\ninputs u from a distribution p(u | x) where x is the current state. We\nintroduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for\nshort) which has two key features: (i) it generates a control input\ndistribution so as to uniformly sample the free configuration space, and (ii)\nin contrast to previously introduced trajectory sampling mechanisms where the\ndistribution p(u | x) is independent of the environment, C-Free-Uniform is\nexplicitly conditioned on the current local map. Next, we integrate this\nsampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.\nExperiments show that CFU-MPPI outperforms existing methods in terms of success\nrate in challenging navigation tasks in cluttered polygonal environments while\nrequiring a much smaller sampling budget."}
{"id": "2510.16931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.16931", "abs": "https://arxiv.org/abs/2510.16931", "authors": ["Zhaoliang Wan", "Zida Zhou", "Zetong Bi", "Zehui Yang", "Hao Ding", "Hui Cheng"], "title": "Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems", "comment": "Accepted by IROS2025", "summary": "This paper addresses the scarcity of affordable, fully-actuated five-fingered\nhands for dexterous teleoperation, which is crucial for collecting large-scale\nreal-robot data within the \"Learning from Demonstrations\" paradigm. We\nintroduce the prototype version of the RAPID Hand, the first low-cost,\n20-degree-of-actuation (DoA) dexterous hand that integrates a novel\nanthropomorphic actuation and transmission scheme with an optimized motor\nlayout and structural design to enhance dexterity. Specifically, the RAPID Hand\nfeatures a universal phalangeal transmission scheme for the non-thumb fingers\nand an omnidirectional thumb actuation mechanism. Prioritizing affordability,\nthe hand employs 3D-printed parts combined with custom gears for easier\nreplacement and repair. We assess the RAPID Hand's performance through\nquantitative metrics and qualitative testing in a dexterous teleoperation\nsystem, which is evaluated on three challenging tasks: multi-finger retrieval,\nladle handling, and human-like piano playing. The results indicate that the\nRAPID Hand's fully actuated 20-DoF design holds significant promise for\ndexterous teleoperation."}
{"id": "2510.17038", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17038", "abs": "https://arxiv.org/abs/2510.17038", "authors": ["Pedram Fekri", "Majid Roshanfar", "Samuel Barbeau", "Seyedfarzad Famouri", "Thomas Looi", "Dale Podolsky", "Mehrdad Zadeh", "Javad Dargahi"], "title": "DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation", "comment": null, "summary": "Cardiac catheterization remains a cornerstone of minimally invasive\ninterventions, yet it continues to rely heavily on manual operation. Despite\nadvances in robotic platforms, existing systems are predominantly follow-leader\nin nature, requiring continuous physician input and lacking intelligent\nautonomy. This dependency contributes to operator fatigue, more radiation\nexposure, and variability in procedural outcomes. This work moves towards\nautonomous catheter navigation by introducing DINO-CVA, a multimodal\ngoal-conditioned behavior cloning framework. The proposed model fuses visual\nobservations and joystick kinematics into a joint embedding space, enabling\npolicies that are both vision-aware and kinematic-aware. Actions are predicted\nautoregressively from expert demonstrations, with goal conditioning guiding\nnavigation toward specified destinations. A robotic experimental setup with a\nsynthetic vascular phantom was designed to collect multimodal datasets and\nevaluate performance. Results show that DINO-CVA achieves high accuracy in\npredicting actions, matching the performance of a kinematics-only baseline\nwhile additionally grounding predictions in the anatomical environment. These\nfindings establish the feasibility of multimodal, goal-conditioned\narchitectures for catheter navigation, representing an important step toward\nreducing operator dependency and improving the reliability of catheterbased\ntherapies."}
{"id": "2510.17086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17086", "abs": "https://arxiv.org/abs/2510.17086", "authors": ["Xueqian Bai", "Nicklas Hansen", "Adabhav Singh", "Michael T. Tolley", "Yan Duan", "Pieter Abbeel", "Xiaolong Wang", "Sha Yi"], "title": "Learning to Design Soft Hands using Reward Models", "comment": null, "summary": "Soft robotic hands promise to provide compliant and safe interaction with\nobjects and environments. However, designing soft hands to be both compliant\nand functional across diverse use cases remains challenging. Although co-design\nof hardware and control better couples morphology to behavior, the resulting\nsearch space is high-dimensional, and even simulation-based evaluation is\ncomputationally expensive. In this paper, we propose a Cross-Entropy Method\nwith Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven\nsoft robotic hands based on teleoperation control policy, reducing design\nevaluations by more than half compared to pure optimization while learning a\ndistribution of optimized hand designs from pre-collected teleoperation data.\nWe derive a design space for a soft robotic hand composed of flexural soft\nfingers and implement parallelized training in simulation. The optimized hands\nare then 3D-printed and deployed in the real world using both teleoperation\ndata and real-time teleoperation. Experiments in both simulation and hardware\ndemonstrate that our optimized design significantly outperforms baseline hands\nin grasping success rates across a diverse set of challenging objects."}
{"id": "2510.17111", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17111", "abs": "https://arxiv.org/abs/2510.17111", "authors": ["Weifan Guan", "Qinghao Hu", "Aosheng Li", "Jian Cheng"], "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "comment": null, "summary": "Vision-Language-Action (VLA) models extend vision-language models to embodied\ncontrol by mapping natural-language instructions and visual observations to\nrobot actions. Despite their capabilities, VLA systems face significant\nchallenges due to their massive computational and memory demands, which\nconflict with the constraints of edge platforms such as on-board mobile\nmanipulators that require real-time performance. Addressing this tension has\nbecome a central focus of recent research. In light of the growing efforts\ntoward more efficient and scalable VLA systems, this survey provides a\nsystematic review of approaches for improving VLA efficiency, with an emphasis\non reducing latency, memory footprint, and training and inference costs. We\ncategorize existing solutions into four dimensions: model architecture,\nperception feature, action generation, and training/inference strategies,\nsummarizing representative techniques within each category. Finally, we discuss\nfuture trends and open challenges, highlighting directions for advancing\nefficient embodied intelligence."}
{"id": "2510.17143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17143", "abs": "https://arxiv.org/abs/2510.17143", "authors": ["Shantnav Agarwal", "Javier Alonso-Mora", "Sihao Sun"], "title": "Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning", "comment": "Accepted by IEEE MRS 2025", "summary": "Existing approaches for transporting and manipulating cable-suspended loads\nusing multiple UAVs along reference trajectories typically rely on either\ncentralized control architectures or reliable inter-agent communication. In\nthis work, we propose a novel machine learning based method for decentralized\nkinodynamic planning that operates effectively under partial observability and\nwithout inter-agent communication. Our method leverages imitation learning to\ntrain a decentralized student policy for each UAV by imitating a centralized\nkinodynamic motion planner with access to privileged global observations. The\nstudent policy generates smooth trajectories using physics-informed neural\nnetworks that respect the derivative relationships in motion. During training,\nthe student policies utilize the full trajectory generated by the teacher\npolicy, leading to improved sample efficiency. Moreover, each student policy\ncan be trained in under two hours on a standard laptop. We validate our method\nin both simulation and real-world environments to follow an agile reference\ntrajectory, demonstrating performance comparable to that of centralized\napproaches."}
{"id": "2510.17148", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17148", "abs": "https://arxiv.org/abs/2510.17148", "authors": ["Yu Gao", "Yiru Wang", "Anqing Jiang", "Heng Yuwen", "Wang Shuo", "Sun Hao", "Wang Jijun"], "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment", "comment": null, "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12."}
{"id": "2510.17150", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17150", "abs": "https://arxiv.org/abs/2510.17150", "authors": ["Heng Zhang", "Wei-Hsing Huang", "Gokhan Solak", "Arash Ajoudani"], "title": "OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation", "comment": "Code, video and RAG dataset are available at\n  \\url{https://sites.google.com/view/omni-vic}", "summary": "We present OmniVIC, a universal variable impedance controller (VIC) enhanced\nby a vision language model (VLM), which improves safety and adaptation in any\ncontact-rich robotic manipulation task to enhance safe physical interaction.\nTraditional VIC have shown advantages when the robot physically interacts with\nthe environment, but lack generalization in unseen, complex, and unstructured\nsafe interactions in universal task scenarios involving contact or uncertainty.\nTo this end, the proposed OmniVIC interprets task context derived reasoning\nfrom images and natural language and generates adaptive impedance parameters\nfor a VIC controller. Specifically, the core of OmniVIC is a self-improving\nRetrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG\nretrieves relevant prior experiences from a structured memory bank to inform\nthe controller about similar past tasks, and ICL leverages these retrieved\nexamples and the prompt of current task to query the VLM for generating\ncontext-aware and adaptive impedance parameters for the current manipulation\nscenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in\nuniversal task scenarios. The impedance parameter regulation is further\ninformed by real-time force/torque feedback to ensure interaction forces remain\nwithin safe thresholds. We demonstrate that our method outperforms baselines on\na suite of complex contact-rich tasks, both in simulation and on real-world\nrobotic tasks, with improved success rates and reduced force violations.\nOmniVIC takes a step towards bridging high-level semantic reasoning and\nlow-level compliant control, enabling safer and more generalizable\nmanipulation. Overall, the average success rate increases from 27% (baseline)\nto 61.4% (OmniVIC)."}
{"id": "2510.17191", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.17191", "abs": "https://arxiv.org/abs/2510.17191", "authors": ["Peiru Zheng", "Yun Zhao", "Zhan Gong", "Hong Zhu", "Shaohua Wu"], "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving", "comment": "6 pages, 2 figures, 2 tables", "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency."}
{"id": "2510.17203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17203", "abs": "https://arxiv.org/abs/2510.17203", "authors": ["Ryota Soga", "Masataka Kobayashi", "Tsukasa Shimizu", "Shintaro Shiba", "Quan Kong", "Shan Lu", "Takaya Yamazato"], "title": "Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera", "comment": "7pages, APCC2025", "summary": "Event cameras, featuring high temporal resolution and high dynamic range,\noffer visual sensing capabilities comparable to conventional image sensors\nwhile capturing fast-moving objects and handling scenes with extreme lighting\ncontrasts such as tunnel exits. Leveraging these properties, this study\nproposes a novel self-localization system that integrates visible light\ncommunication (VLC) and visible light positioning (VLP) within a single event\ncamera. The system enables a vehicle to estimate its position even in\nGPS-denied environments, such as tunnels, by using VLC to obtain coordinate\ninformation from LED transmitters and VLP to estimate the distance to each\ntransmitter.\n  Multiple LEDs are installed on the transmitter side, each assigned a unique\npilot sequence based on Walsh-Hadamard codes. The event camera identifies\nindividual LEDs within its field of view by correlating the received signal\nwith these codes, allowing clear separation and recognition of each light\nsource. This mechanism enables simultaneous high-capacity MISO (multi-input\nsingle-output) communication through VLC and precise distance estimation via\nphase-only correlation (POC) between multiple LED pairs.\n  To the best of our knowledge, this is the first vehicle-mounted system to\nachieve simultaneous VLC and VLP functionalities using a single event camera.\nField experiments were conducted by mounting the system on a vehicle traveling\nat 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,\nwith a root mean square error (RMSE) of distance estimation within 0.75 m for\nranges up to 100 m and a bit error rate (BER) below 0.01 across the same range."}
{"id": "2510.17237", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17237", "abs": "https://arxiv.org/abs/2510.17237", "authors": ["Wuhao Xie", "Kanji Tanaka"], "title": "Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance", "comment": "4 pages, technical report", "summary": "Long-term autonomy for mobile robots requires both robust self-localization\nand reliable map maintenance. Conventional landmark-based methods face a\nfundamental trade-off between landmarks with high detectability but low\ndistinctiveness (e.g., poles) and those with high distinctiveness but difficult\nstable detection (e.g., local point cloud structures). This work addresses the\nchallenge of descriptively identifying a unique \"signature\" (local point cloud)\nby leveraging a detectable, high-precision \"anchor\" (like a pole). To solve\nthis, we propose a novel canonical representation, \"Pole-Image,\" as a hybrid\nmethod that uses poles as anchors to generate signatures from the surrounding\n3D structure. Pole-Image represents a pole-like landmark and its surrounding\nenvironment, detected from a LiDAR point cloud, as a 2D polar coordinate image\nwith the pole itself as the origin. This representation leverages the pole's\nnature as a high-precision reference point, explicitly encoding the \"relative\ngeometry\" between the stable pole and the variable surrounding point cloud. The\nkey advantage of pole landmarks is that \"detection\" is extremely easy. This\nease of detection allows the robot to easily track the same pole, enabling the\nautomatic and large-scale collection of diverse observational data (positive\npairs). This data acquisition feasibility makes \"Contrastive Learning (CL)\"\napplicable. By applying CL, the model learns a viewpoint-invariant and highly\ndiscriminative descriptor. The contributions are twofold: 1) The descriptor\novercomes perceptual aliasing, enabling robust self-localization. 2) The\nhigh-precision encoding enables high-sensitivity change detection, contributing\nto map maintenance."}
{"id": "2510.17249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17249", "abs": "https://arxiv.org/abs/2510.17249", "authors": ["Franek Stark", "Rohit Kumar", "Shubham Vyas", "Hannah Isermann", "Jonas Haack", "Mihaela Popescu", "Jakob Middelberg", "Dennis Mronga", "Frank Kirchner"], "title": "An adaptive hierarchical control framework for quadrupedal robots in planetary exploration", "comment": "Presented at 18th Symposium on Advanced Space Technologies in\n  Robotics and Automation (ASTRA)", "summary": "Planetary exploration missions require robots capable of navigating extreme\nand unknown environments. While wheeled rovers have dominated past missions,\ntheir mobility is limited to traversable surfaces. Legged robots, especially\nquadrupeds, can overcome these limitations by handling uneven, obstacle-rich,\nand deformable terrains. However, deploying such robots in unknown conditions\nis challenging due to the need for environment-specific control, which is\ninfeasible when terrain and robot parameters are uncertain. This work presents\na modular control framework that combines model-based dynamic control with\nonline model adaptation and adaptive footstep planning to address uncertainties\nin both robot and terrain properties. The framework includes state estimation\nfor quadrupeds with and without contact sensing, supports runtime\nreconfiguration, and is integrated into ROS 2 with open-source availability.\nIts performance was validated on two quadruped platforms, multiple hardware\narchitectures, and in a volcano field test, where the robot walked over 700 m."}
{"id": "2510.17261", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17261", "abs": "https://arxiv.org/abs/2510.17261", "authors": ["Fernando Salanova", "Jesús Roche", "Cristian Mahuela", "Eduardo Montijano"], "title": "High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection", "comment": "6 pages,3 figures, Iberian Robotics Conference 2025", "summary": "The reliable execution of high-level missions in multi-robot systems with\nheterogeneous agents, requires robust methods for detecting spurious behaviors.\nIn this paper, we address the challenge of identifying spurious executions of\nplans specified as a Linear Temporal Logic (LTL) formula, as incorrect task\nsequences, violations of spatial constraints, timing inconsis- tencies, or\ndeviations from intended mission semantics. To tackle this, we introduce a\nstructured data generation framework based on the Nets-within-Nets (NWN)\nparadigm, which coordinates robot actions with LTL-derived global mission\nspecifications. We further propose a Transformer-based anomaly detection\npipeline that classifies robot trajectories as normal or anomalous. Experi-\nmental evaluations show that our method achieves high accuracy (91.3%) in\nidentifying execution inefficiencies, and demonstrates robust detection\ncapabilities for core mission violations (88.3%) and constraint-based adaptive\nanomalies (66.8%). An ablation experiment of the embedding and architecture was\ncarried out, obtaining successful results where our novel proposition performs\nbetter than simpler representations."}
{"id": "2510.17270", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17270", "abs": "https://arxiv.org/abs/2510.17270", "authors": ["Lucas Schulze", "Juliano Decico Negri", "Victor Barasuol", "Vivian Suzano Medeiros", "Marcelo Becker", "Jan Peters", "Oleg Arenz"], "title": "Floating-Base Deep Lagrangian Networks", "comment": null, "summary": "Grey-box methods for system identification combine deep learning with\nphysics-informed constraints, capturing complex dependencies while improving\nout-of-distribution generalization. Yet, despite the growing importance of\nfloating-base systems such as humanoids and quadrupeds, current grey-box models\nignore their specific physical constraints. For instance, the inertia matrix is\nnot only positive definite but also exhibits branch-induced sparsity and input\nindependence. Moreover, the 6x6 composite spatial inertia of the floating base\ninherits properties of single-rigid-body inertia matrices. As we show, this\nincludes the triangle inequality on the eigenvalues of the composite rotational\ninertia. To address the lack of physical consistency in deep learning models of\nfloating-base systems, we introduce a parameterization of inertia matrices that\nsatisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),\nwe train neural networks to predict physically plausible inertia matrices that\nminimize inverse dynamics error under Lagrangian mechanics. For evaluation, we\ncollected and released a dataset on multiple quadrupeds and humanoids. In these\nexperiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly\ncompetitive performance on both simulated and real robots, while providing\ngreater physical interpretability."}
{"id": "2510.17315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17315", "abs": "https://arxiv.org/abs/2510.17315", "authors": ["Po-Chen Ko", "Jiayuan Mao", "Yu-Hsiang Fu", "Hsien-Jeng Yeh", "Chu-Rong Chen", "Wei-Chiu Ma", "Yilun Du", "Shao-Hua Sun"], "title": "Implicit State Estimation via Video Replanning", "comment": null, "summary": "Video-based representations have gained prominence in planning and\ndecision-making due to their ability to encode rich spatiotemporal dynamics and\ngeometric relationships. These representations enable flexible and\ngeneralizable solutions for complex tasks such as object manipulation and\nnavigation. However, existing video planning frameworks often struggle to adapt\nto failures at interaction time due to their inability to reason about\nuncertainties in partially observed environments. To overcome these\nlimitations, we introduce a novel framework that integrates interaction-time\ndata into the planning process. Our approach updates model parameters online\nand filters out previously failed plans during generation. This enables\nimplicit state estimation, allowing the system to adapt dynamically without\nexplicitly modeling unknown state variables. We evaluate our framework through\nextensive experiments on a new simulated manipulation benchmark, demonstrating\nits ability to improve replanning performance and advance the field of\nvideo-based decision-making."}
{"id": "2510.17335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17335", "abs": "https://arxiv.org/abs/2510.17335", "authors": ["Xintong Yang", "Minglun Wei", "Ze Ji", "Yu-Kun Lai"], "title": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials", "comment": "Accepted as a regular paper by the IEEE Transactions on Robotics", "summary": "Automating the manipulation of granular materials poses significant\nchallenges due to complex contact dynamics, unpredictable material properties,\nand intricate system states. Existing approaches often fail to achieve\nefficiency and accuracy in such tasks. To fill the research gap, this paper\nstudies the small-scale and high-precision granular material digging task with\nunknown physical properties. A new framework, named differentiable digging\nrobot (DDBot), is proposed to manipulate granular materials, including sand and\nsoil.\n  Specifically, we equip DDBot with a differentiable physics-based simulator,\ntailored for granular material manipulation, powered by GPU-accelerated\nparallel computing and automatic differentiation. DDBot can perform efficient\ndifferentiable system identification and high-precision digging skill\noptimisation for unknown granular materials, which is enabled by a\ndifferentiable skill-to-action mapping, a task-oriented demonstration method,\ngradient clipping and line search-based gradient descent.\n  Experimental results show that DDBot can efficiently (converge within 5 to 20\nminutes) identify unknown granular material dynamics and optimise digging\nskills, with high-precision results in zero-shot real-world deployments,\nhighlighting its practicality. Benchmark results against state-of-the-art\nbaselines also confirm the robustness and efficiency of DDBot in such digging\ntasks."}
{"id": "2510.17341", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17341", "abs": "https://arxiv.org/abs/2510.17341", "authors": ["Fan Shao", "Satoshi Endo", "Sandra Hirche", "Fanny Ficuciello"], "title": "Interactive Force-Impedance Control", "comment": null, "summary": "Human collaboration with robots requires flexible role adaptation, enabling\nrobot to switch between active leader and passive follower. Effective role\nswitching depends on accurately estimating human intention, which is typically\nachieved through external force analysis, nominal robot dynamics, or\ndata-driven approaches. However, these methods are primarily effective in\ncontact-sparse environments. When robots under hybrid or unified\nforce-impedance control physically interact with active humans or non-passive\nenvironments, the robotic system may lose passivity and thus compromise safety.\nTo address this challenge, this paper proposes the unified Interactive\nForce-Impedance Control (IFIC) framework that adapts to the interaction power\nflow, ensuring effortless and safe interaction in contact-rich environments.\nThe proposed control architecture is formulated within a port-Hamiltonian\nframework, incorporating both interaction and task control ports, through which\nsystem passivity is guaranteed."}
{"id": "2510.17369", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17369", "abs": "https://arxiv.org/abs/2510.17369", "authors": ["Haochen Su", "Cristian Meo", "Francesco Stella", "Andrea Peirone", "Kai Junge", "Josie Hughes"], "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots", "comment": "Accepted by NeurIPS 2025 SpaVLE workshop. 4 pages, 2 figures(in main\n  paper, excluding references and supplements)", "summary": "Robotic systems are increasingly expected to operate in human-centered,\nunstructured environments where safety, adaptability, and generalization are\nessential. Vision-Language-Action (VLA) models have been proposed as a language\nguided generalized control framework for real robots. However, their deployment\nhas been limited to conventional serial link manipulators. Coupled by their\nrigidity and unpredictability of learning based control, the ability to safely\ninteract with the environment is missing yet critical. In this work, we present\nthe deployment of a VLA model on a soft continuum manipulator to demonstrate\nautonomous safe human-robot interaction. We present a structured finetuning and\ndeployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and\n$\\pi_0$) across representative manipulation tasks, and show while\nout-of-the-box policies fail due to embodiment mismatch, through targeted\nfinetuning the soft robot performs equally to the rigid counterpart. Our\nfindings highlight the necessity of finetuning for bridging embodiment gaps,\nand demonstrate that coupling VLA models with soft robots enables safe and\nflexible embodied AI in human-shared environments."}
{"id": "2510.17408", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.17408", "abs": "https://arxiv.org/abs/2510.17408", "authors": ["Halima I. Kure", "Jishna Retnakumari", "Augustine O. Nwajana", "Umar M. Ismail", "Bilyaminu A. Romo", "Ehigiator Egho-Promise"], "title": "Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting", "comment": "5 pages, 2 figures", "summary": "This paper presents a novel methodology that integrates trustworthy\nartificial intelligence (AI) with an energy-efficient robotic arm for\nintelligent waste classification and sorting. By utilizing a convolutional\nneural network (CNN) enhanced through transfer learning with MobileNetV2, the\nsystem accurately classifies waste into six categories: plastic, glass, metal,\npaper, cardboard, and trash. The model achieved a high training accuracy of\n99.8% and a validation accuracy of 80.5%, demonstrating strong learning and\ngeneralization. A robotic arm simulator is implemented to perform virtual\nsorting, calculating the energy cost for each action using Euclidean distance\nto ensure optimal and efficient movement. The framework incorporates key\nelements of trustworthy AI, such as transparency, robustness, fairness, and\nsafety, making it a reliable and scalable solution for smart waste management\nsystems in urban settings."}
{"id": "2510.17439", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17439", "abs": "https://arxiv.org/abs/2510.17439", "authors": ["Zhengshen Zhang", "Hao Li", "Yalun Dai", "Zhengbang Zhu", "Lei Zhou", "Chenchen Liu", "Dong Wang", "Francis E. H. Tay", "Sijin Chen", "Ziwei Liu", "Yuxiao Liu", "Xinghang Li", "Pan Zhou"], "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors", "comment": "Project page: https://falcon-vla.github.io/", "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height."}
{"id": "2510.17448", "categories": ["cs.RO", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.17448", "abs": "https://arxiv.org/abs/2510.17448", "authors": ["Mirko Mizzoni", "Pieter van Goor", "Barbara Bazzana", "Antonio Franchi"], "title": "A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions", "comment": null, "summary": "This letter presents a systematic framework for switching between different\nsets of outputs for the control of nonlinear systems via feedback\nlinearization. We introduce the concept of a meld to formally define a valid,\nfeedback-linearizable subset of outputs that can be selected from a larger deck\nof possible outputs. The main contribution is a formal proof establishing that\nunder suitable dwell-time and compatibility conditions, it is possible to\nswitch between different melds while guaranteeing the uniform boundedness of\nthe system state. We further show that the error dynamics of the active outputs\nremain exponentially stable within each switching interval and that outputs\ncommon to consecutive melds are tracked seamlessly through transitions. The\nproposed theory is valid for any feedback linearizable nonlinear system, such\nas, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a\nsimple numerical simulation of a robotic manipulator."}
{"id": "2510.17525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17525", "abs": "https://arxiv.org/abs/2510.17525", "authors": ["Simon Schaefer", "Helen Oleynikova", "Sandra Hirche", "Stefan Leutenegger"], "title": "HumanMPC - Safe and Efficient MAV Navigation among Humans", "comment": null, "summary": "Safe and efficient robotic navigation among humans is essential for\nintegrating robots into everyday environments. Most existing approaches focus\non simplified 2D crowd navigation and fail to account for the full complexity\nof human body dynamics beyond root motion. We present HumanMPC, a Model\nPredictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation\namong humans that combines theoretical safety guarantees with data-driven\nmodels for realistic human motion forecasting. Our approach introduces a novel\ntwist to reachability-based safety formulation that constrains only the initial\ncontrol input for safety while modeling its effects over the entire planning\nhorizon, enabling safe yet efficient navigation. We validate HumanMPC in both\nsimulated experiments using real human trajectories and in the real-world,\ndemonstrating its effectiveness across tasks ranging from goal-directed\nnavigation to visual servoing for human tracking. While we apply our method to\nMAVs in this work, it is generic and can be adapted by other platforms. Our\nresults show that the method ensures safety without excessive conservatism and\noutperforms baseline approaches in both efficiency and reliability."}
{"id": "2510.17541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17541", "abs": "https://arxiv.org/abs/2510.17541", "authors": ["Xiaobo Zheng", "Pan Tang", "Defu Lin", "Shaoming He"], "title": "Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm", "comment": null, "summary": "Swarm trajectory optimization problems are a well-recognized class of\nmulti-agent optimal control problems with strong nonlinearity. However, the\nheuristic nature of needing to set the final time for agents beforehand and the\ntime-consuming limitation of the significant number of iterations prohibit the\napplication of existing methods to large-scale swarm of Unmanned Aerial\nVehicles (UAVs) in practice. In this paper, we propose a spatial-temporal\ntrajectory optimization framework that accomplishes multi-UAV consensus based\non the Alternating Direction Multiplier Method (ADMM) and uses Differential\nDynamic Programming (DDP) for fast local planning of individual UAVs. The\nintroduced framework is a two-level architecture that employs Parameterized DDP\n(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local\nconstraints and accomplish the spatial-temporal parameter consensus among all\nUAVs. This results in a fully distributed algorithm called Distributed\nParameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on\nthe spectral gradient method for the penalty parameter is proposed to reduce\nthe number of algorithmic iterations. Several simulation examples are presented\nto verify the effectiveness of the proposed algorithm."}
{"id": "2510.17576", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.17576", "abs": "https://arxiv.org/abs/2510.17576", "authors": ["Cansu Erdogan", "Cesar Alan Contreras", "Alireza Rastegarpanah", "Manolis Chiou", "Rustam Stolkin"], "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries", "comment": "This work is funded by the project called \"Research and Development\n  of a Highly Automated and Safe Streamlined Process for Increasing Lithium-ion\n  Battery Repurposing and Recycling\" (REBELION) under Grant 101104241, and\n  partially supported by the Ministry of National Education, Republic of\n  Turkey. Submitted to Frontiers for Review", "summary": "This paper addresses the problem of planning complex manipulation tasks, in\nwhich multiple robots with different end-effectors and capabilities, informed\nby computer vision, must plan and execute concatenated sequences of actions on\na variety of objects that can appear in arbitrary positions and configurations\nin unstructured scenes. We propose an intent-driven planning pipeline which can\nrobustly construct such action sequences with varying degrees of supervisory\ninput from a human using simple language instructions. The pipeline integrates:\n(i) perception-to-text scene encoding, (ii) an ensemble of large language\nmodels (LLMs) that generate candidate removal sequences based on the operator's\nintent, (iii) an LLM-based verifier that enforces formatting and precedence\nconstraints, and (iv) a deterministic consistency filter that rejects\nhallucinated objects. The pipeline is evaluated on an example task in which two\nrobot arms work collaboratively to dismantle an Electric Vehicle battery for\nrecycling applications. A variety of components must be grasped and removed in\nspecific sequences, determined by human instructions and/or by task-order\nfeasibility decisions made by the autonomous system. On 200 real scenes with\n600 operator prompts across five component classes, we used metrics of\nfull-sequence correctness and next-task correctness to evaluate and compare\nfive LLM-based planners (including ablation analyses of pipeline components).\nWe also evaluated the LLM-based human interface in terms of time to execution\nand NASA TLX with human participant experiments. Results indicate that our\nensemble-with-verification approach reliably maps operator intent to safe,\nexecutable multi-robot plans while maintaining low user effort."}
{"id": "2510.17604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17604", "abs": "https://arxiv.org/abs/2510.17604", "authors": ["Hao Qiao", "Yan Wang", "Shuo Yang", "Xiaoyao Yu", "Jian kuang", "Xiaoji Niu"], "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm", "comment": null, "summary": "With the rapid growth of bike sharing and the increasing diversity of cycling\napplications, accurate bicycle localization has become essential. traditional\nGNSS-based methods suffer from multipath effects, while existing inertial\nnavigation approaches rely on precise modeling and show limited robustness.\nTight Learned Inertial Odometry (TLIO) achieves low position drift by combining\nraw IMU data with predicted displacements by neural networks, but its high\ncomputational cost restricts deployment on mobile devices. To overcome this, we\nextend TLIO to bicycle localization and introduce an improved Mixture-of\nExperts (MoE) model that reduces both training and inference costs. Experiments\nshow that, compared to the state-of-the-art LLIO framework, our method achieves\ncomparable accuracy while reducing parameters by 64.7% and computational cost\nby 81.8%."}
{"id": "2510.17640", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17640", "abs": "https://arxiv.org/abs/2510.17640", "authors": ["Yuquan Xue", "Guanxing Lu", "Zhenyu Wu", "Chuanrui Zhang", "Bofang Jia", "Zhengyi Gu", "Yansong Tang", "Ziwei Wang"], "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation", "comment": "9 pages,7 figures, submitted to ICRA2026", "summary": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance\non complex robotic manipulation tasks through imitation learning. However,\nexisting imitation learning datasets contain only successful trajectories and\nlack failure or recovery data, especially for out-of-distribution (OOD) states\nwhere the robot deviates from the main policy due to minor perturbations or\nerrors, leading VLA models to struggle with states deviating from the training\ndistribution. To this end, we propose an automated OOD data augmentation\nframework named RESample through exploratory sampling. Specifically, we first\nleverage offline reinforcement learning to obtain an action-value network that\naccurately identifies sub-optimal actions under the current manipulation\npolicy. We further sample potential OOD states from trajectories via rollout,\nand design an exploratory sampling mechanism that adaptively incorporates these\naction proxies into the training dataset to ensure efficiency. Subsequently,\nour framework explicitly encourages the VLAs to recover from OOD states and\nenhances their robustness against distributional shifts. We conduct extensive\nexperiments on the LIBERO benchmark as well as real-world robotic manipulation\ntasks, demonstrating that RESample consistently improves the stability and\ngeneralization ability of VLA models."}
{"id": "2510.17783", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.17783", "abs": "https://arxiv.org/abs/2510.17783", "authors": ["Simeon Adebola", "Chung Min Kim", "Justin Kerr", "Shuangyu Xie", "Prithvi Akella", "Jose Luis Susa Rincon", "Eugen Solowjow", "Ken Goldberg"], "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats", "comment": "2025 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2025)", "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/."}
{"id": "2510.17792", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.17792", "abs": "https://arxiv.org/abs/2510.17792", "authors": ["Gabriel B. Margolis", "Michelle Wang", "Nolan Fey", "Pulkit Agrawal"], "title": "SoftMimic: Learning Compliant Whole-body Control from Examples", "comment": "Website: https://gmargo11.github.io/softmimic/", "summary": "We introduce SoftMimic, a framework for learning compliant whole-body control\npolicies for humanoid robots from example motions. Imitating human motions with\nreinforcement learning allows humanoids to quickly learn new skills, but\nexisting methods incentivize stiff control that aggressively corrects\ndeviations from a reference motion, leading to brittle and unsafe behavior when\nthe robot encounters unexpected contacts. In contrast, SoftMimic enables robots\nto respond compliantly to external forces while maintaining balance and\nposture. Our approach leverages an inverse kinematics solver to generate an\naugmented dataset of feasible compliant motions, which we use to train a\nreinforcement learning policy. By rewarding the policy for matching compliant\nresponses rather than rigidly tracking the reference motion, SoftMimic learns\nto absorb disturbances and generalize to varied tasks from a single motion\nclip. We validate our method through simulations and real-world experiments,\ndemonstrating safe and effective interaction with the environment."}
{"id": "2510.17801", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.17801", "abs": "https://arxiv.org/abs/2510.17801", "authors": ["Yulin Luo", "Chun-Kai Fan", "Menghang Dong", "Jiayu Shi", "Mengdi Zhao", "Bo-Wen Zhang", "Cheng Chi", "Jiaming Liu", "Gaole Dai", "Rongyu Zhang", "Ruichuan An", "Kun Wu", "Zhengping Che", "Shaoxuan Xie", "Guocai Yao", "Zhongxia Zhao", "Pengwei Wang", "Guang Liu", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain", "comment": null, "summary": "Building robots that can perceive, reason, and act in dynamic, unstructured\nenvironments remains a core challenge. Recent embodied systems often adopt a\ndual-system paradigm, where System 2 handles high-level reasoning while System\n1 executes low-level control. In this work, we refer to System 2 as the\nembodied brain, emphasizing its role as the cognitive core for reasoning and\ndecision-making in manipulation tasks. Given this role, systematic evaluation\nof the embodied brain is essential. Yet existing benchmarks emphasize execution\nsuccess, or when targeting high-level reasoning, suffer from incomplete\ndimensions and limited task realism, offering only a partial picture of\ncognitive capability. To bridge this gap, we introduce RoboBench, a benchmark\nthat systematically evaluates multimodal large language models (MLLMs) as\nembodied brains. Motivated by the critical roles across the full manipulation\npipeline, RoboBench defines five dimensions-instruction comprehension,\nperception reasoning, generalized planning, affordance prediction, and failure\nanalysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure\nrealism, we curate datasets across diverse embodiments, attribute-rich objects,\nand multi-view scenes, drawing from large-scale real robotic data. For\nplanning, RoboBench introduces an evaluation framework,\nMLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether\npredicted plans can achieve critical object-state changes. Experiments on 14\nMLLMs reveal fundamental limitations: difficulties with implicit instruction\ncomprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained\naffordance understanding, and execution failure diagnosis. RoboBench provides a\ncomprehensive scaffold to quantify high-level cognition, and guide the\ndevelopment of next-generation embodied MLLMs. The project page is in\nhttps://robo-bench.github.io."}
