<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 23]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding](https://arxiv.org/abs/2511.13961)
*Jiarui Li,Alessandro Zanardi,Runyu Zhang,Gioele Zardini*

Main category: cs.RO

TL;DR: 本文提出了一个系统级的多智能体路径规划框架，将规划与执行集成，统一处理各种变体问题，并显式建模不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有MAPF方法将规划与执行分离，且以临时方式处理问题变体，缺乏统一框架来应对执行时的不确定性。

Method: 提出MAPF系统作为核心形式模型，将MAPF视为控制设计问题，并引入FICO算法——基于因子分解的有限时域闭环算法，利用组合结构实现高效闭环操作。

Result: FICO算法实现毫秒级响应，可扩展到数千个智能体，在随机延迟和智能体到达情况下，相比开环基线减少两个数量级的计算时间，并显著提高吞吐量。

Conclusion: 通过系统级建模、因子分解和闭环设计，为MAPF的分析和发展建立了理论基础。

Abstract: Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design.

</details>


### [2] [LIO-MARS: Non-uniform Continuous-time Trajectories for Real-time LiDAR-Inertial-Odometry](https://arxiv.org/abs/2511.13985)
*Jan Quenzel,Sven Behnke*

Main category: cs.RO

TL;DR: 本文提出了一种激光雷达-惯性里程计（LIO）系统LIO-MARS，通过结合IMU和LiDAR数据，使用连续时间B样条轨迹和多分辨率面元地图对齐，实现了高精度的实时定位与建图。


<details>
  <summary>Details</summary>
Motivation: 自主机器人系统需要鲁棒的实时感知能力进行安全导航。IMU提供加速度和旋转约束，LiDAR提供精确距离测量，两者互补可提高定位精度和鲁棒性。

Method: 使用连续时间B样条轨迹，通过高斯混合模型（GMM）对齐多分辨率面元地图；采用非均匀时间节点放置确保轨迹连续性；使用Kronecker和与积加速协方差和GMM计算；通过无迹变换去偏斜面元；引入相对位姿和预积分IMU伪测量的软约束。

Result: 将关键协方差和GMM计算加速3.3倍；在多种手持、地面和空中车辆数据集上展示了与当前最先进LIO系统相比的优越性能。

Conclusion: LIO-MARS系统在各种平台上都表现出最先进的定位精度和鲁棒性，验证了所提方法的有效性。

Abstract: Autonomous robotic systems heavily rely on environment knowledge to safely navigate. For search & rescue, a flying robot requires robust real-time perception, enabled by complementary sensors. IMU data constrains acceleration and rotation, whereas LiDAR measures accurate distances around the robot. Building upon the LiDAR odometry MARS, our LiDAR-inertial odometry (LIO) jointly aligns multi-resolution surfel maps with a Gaussian mixture model (GMM) using a continuous-time B-spline trajectory. Our new scan window uses non-uniform temporal knot placement to ensure continuity over the whole trajectory without additional scan delay. Moreover, we accelerate essential covariance and GMM computations with Kronecker sums and products by a factor of 3.3. An unscented transform de-skews surfels, while a splitting into intra-scan segments facilitates motion compensation during spline optimization. Complementary soft constraints on relative poses and preintegrated IMU pseudo-measurements further improve robustness and accuracy. Extensive evaluation showcases the state-of-the-art quality of our LIO-MARS w.r.t. recent LIO systems on various handheld, ground and aerial vehicle-based datasets.

</details>


### [3] [Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval](https://arxiv.org/abs/2511.14004)
*Taijing Chen,Sateesh Kumar,Junhong Xu,George Pavlakos,J oydeep Biswas,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: STAR框架统一处理时空对象检索问题，结合长期记忆和工作记忆，在动态开放世界中通过视觉语言模型选择时空动作，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能部分解决服务机器人在动态开放世界中的对象检索问题：场景图忽略时间维度，时序推理方法不支持具身交互，动态场景图仍是封闭世界。需要统一处理时空查询和具身动作的框架。

Method: 提出STAR框架，整合非参数化长期记忆和工作记忆支持高效回忆，使用视觉语言模型在每一步选择时空动作，将时间搜索和空间搜索作为统一问题处理。

Result: 在STARBench基准测试和Tiago机器人实验中，STAR始终优于场景图和仅记忆的基线方法，证明了统一处理时空搜索问题的优势。

Conclusion: 将时间搜索和空间搜索作为统一问题处理能够显著提升服务机器人在动态开放世界中的对象检索性能，STAR框架为此提供了有效解决方案。

Abstract: Service robots must retrieve objects in dynamic, open-world settings where requests may reference attributes ("the red mug"), spatial context ("the mug on the table"), or past states ("the mug that was here yesterday"). Existing approaches capture only parts of this problem: scene graphs capture spatial relations but ignore temporal grounding, temporal reasoning methods model dynamics but do not support embodied interaction, and dynamic scene graphs handle both but remain closed-world with fixed vocabularies. We present STAR (SpatioTemporal Active Retrieval), a framework that unifies memory queries and embodied actions within a single decision loop. STAR leverages non-parametric long-term memory and a working memory to support efficient recall, and uses a vision-language model to select either temporal or spatial actions at each step. We introduce STARBench, a benchmark of spatiotemporal object search tasks across simulated and real environments. Experiments in STARBench and on a Tiago robot show that STAR consistently outperforms scene-graph and memory-only baselines, demonstrating the benefits of treating search in time and search in space as a unified problem.

</details>


### [4] [BIM-Discrepancy-Driven Active Sensing for Risk-Aware UAV-UGV Navigation](https://arxiv.org/abs/2511.14037)
*Hesam Mojtahedi,Reza Akhavian*

Main category: cs.RO

TL;DR: 本文提出了一个BIM差异驱动的主动感知框架，用于无人机和无人地面车辆在动态建筑环境中的协同导航，通过融合实时LiDAR数据和BIM先验来维护动态2D占用地图，并在风险超过阈值时触发无人机重新扫描以降低不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统导航方法依赖静态BIM先验或有限的机载感知能力，无法有效应对动态建筑环境中的不确定性，需要开发能够融合实时感知与BIM先验的主动感知框架。

Method: 开发了BIM差异驱动的主动感知框架，持续融合空中和地面机器人的实时LiDAR数据与BIM先验，维护动态2D占用地图，通过统一的走廊风险度量（整合占用不确定性、BIM地图差异和间隙）评估导航安全性，并在风险超过阈值时触发无人机自主重新扫描。

Result: 在PX4-Gazebo仿真中验证，风险触发重新扫描相比静态BIM导航将平均走廊风险降低58%，地图熵降低43%，同时保持间隙余量大于0.4米；相比前沿探索方法，在任务时间减半的情况下实现了相似的不确定性降低。

Conclusion: 将BIM先验与风险自适应空中感知相结合，能够为建筑机器人提供可扩展的、不确定性感知的自主能力。

Abstract: This paper presents a BIM-discrepancy-driven active sensing framework for cooperative navigation between unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) in dynamic construction environments. Traditional navigation approaches rely on static Building Information Modeling (BIM) priors or limited onboard perception. In contrast, our framework continuously fuses real-time LiDAR data from aerial and ground robots with BIM priors to maintain an evolving 2D occupancy map. We quantify navigation safety through a unified corridor-risk metric integrating occupancy uncertainty, BIM-map discrepancy, and clearance. When risk exceeds safety thresholds, the UAV autonomously re-scans affected regions to reduce uncertainty and enable safe replanning. Validation in PX4-Gazebo simulation with Robotec GPU LiDAR demonstrates that risk-triggered re-scanning reduces mean corridor risk by 58% and map entropy by 43% compared to static BIM navigation, while maintaining clearance margins above 0.4 m. Compared to frontier-based exploration, our approach achieves similar uncertainty reduction in half the mission time. These results demonstrate that integrating BIM priors with risk-adaptive aerial sensing enables scalable, uncertainty-aware autonomy for construction robotics.

</details>


### [5] [FlexiCup: Wireless Multimodal Suction Cup with Dual-Zone Vision-Tactile Sensing](https://arxiv.org/abs/2511.14139)
*Junhao Gong,Shoujie Li,Kit-Wa Sou,Changqing Guo,Hourong Huang,Tong Wu,Yifan Xie,Chenxin Liang,Chuqiao Lyu,Xiaojun Liang,Wenbo Ding*

Main category: cs.RO

TL;DR: FlexiCup是一种完全无线多模态吸盘，集成双区域视觉触觉传感，支持真空和伯努利两种吸附模式，通过模块化机械配置实现无线自主操作。


<details>
  <summary>Details</summary>
Motivation: 传统吸盘在非结构化环境中缺乏接触感知能力，无法实现接触感知操作。

Method: 中央区域通过照明控制在视觉和触觉模态间动态切换进行接触检测，外围区域提供连续空间感知用于接近规划；支持真空和伯努利两种吸附模式；采用基于扩散的端到端学习方法。

Result: 真空模式平均成功率90.0%，伯努利模式86.7%；在倾斜运输任务中成功率73.3%，橙子提取任务66.7%；多头部注意力协调双区域观测使接触感知操作提升13%。

Conclusion: FlexiCup通过双区域多模态传感和模块化设计，为接触感知操作提供了有效的硬件解决方案。

Abstract: Conventional suction cups lack sensing capabilities for contact-aware manipulation in unstructured environments. This paper presents FlexiCup, a fully wireless multimodal suction cup that integrates dual-zone vision-tactile sensing. The central zone dynamically switches between vision and tactile modalities via illumination control for contact detection, while the peripheral zone provides continuous spatial awareness for approach planning. FlexiCup supports both vacuum and Bernoulli suction modes through modular mechanical configurations, achieving complete wireless autonomy with onboard computation and power. We validate hardware versatility through dual control paradigms. Modular perception-driven grasping across structured surfaces with varying obstacle densities demonstrates comparable performance between vacuum (90.0% mean success) and Bernoulli (86.7% mean success) modes. Diffusion-based end-to-end learning achieves 73.3% success on inclined transport and 66.7% on orange extraction tasks. Ablation studies confirm that multi-head attention coordinating dual-zone observations provides 13% improvements for contact-aware manipulation. Hardware designs and firmware are available at https://anonymous.4open.science/api/repo/FlexiCup-DA7D/file/index.html?v=8f531b44.

</details>


### [6] [AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models](https://arxiv.org/abs/2511.14148)
*Yuhua Jiang,Shuang Cheng,Yan Ding,Feifei Gao,Biqing Qi*

Main category: cs.RO

TL;DR: 本文提出了AsyncVLA，一种异步流匹配的视觉-语言-动作模型，通过非均匀时间调度和自校正机制解决传统同步流匹配在长时程任务中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型使用同步流匹配(SFM)生成动作，依赖刚性的均匀时间调度，缺乏动作上下文感知和异步自校正能力，在长时程任务中容易因单个动作错误导致级联失败。

Method: 提出异步流匹配VLA(AsyncVLA)，引入异步流匹配(AFM)的时间灵活性，通过置信度评估器提取初始生成动作的置信度，选择性精炼不准确的动作令牌，并提出了SFM和AFM的统一训练过程。

Result: 在机器人操作基准测试中，AsyncVLA表现出数据效率和自校正能力，由于AFM中的异步生成，在通用具身评估中取得了最先进的结果。

Conclusion: AsyncVLA通过引入异步流匹配和自校正机制，有效解决了传统VLA模型在长时程任务中的稳定性问题，为构建更鲁棒的通用机器人提供了新思路。

Abstract: Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.

</details>


### [7] [RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action](https://arxiv.org/abs/2511.14161)
*Xiaoquan Sun,Ruijian Zhang,Kang Pang,Bingchen Miao,Yuxiang Tan,Zhen Yang,Ming Li,Jiayu Chen*

Main category: cs.RO

TL;DR: RoboTidy是一个统一的语言引导家庭整理基准，支持视觉-语言-动作和视觉-语言导航的训练与评估，提供500个真实3D场景、6.4k操作轨迹和1.5k导航轨迹。


<details>
  <summary>Details</summary>
Motivation: 当前的家庭整理基准缺乏用户偏好建模、移动性支持，且泛化能力差，难以全面评估语言到动作的集成能力。

Method: 使用500个3D高斯散射家庭场景，将整理建模为"动作(对象,容器)"列表，提供高质量的操作和导航演示轨迹。

Result: 构建了包含500个对象和容器的基准平台，支持少样本和大规模训练，并在真实世界中部署用于端到端评估。

Conclusion: RoboTidy填补了具身AI中的关键空白，为语言引导机器人提供了全面且现实的评估平台。

Abstract: Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an "Action (Object, Container)" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.

</details>


### [8] [Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion](https://arxiv.org/abs/2511.14178)
*Zhuo Li,Junjia Liu,Zhipeng Dong,Tao Teng,Quentin Rouxel,Darwin Caldwell,Fei Chen*

Main category: cs.RO

TL;DR: VLA-Pilot是一种即插即用的推理时策略引导方法，无需微调或数据收集即可实现预训练VLA模型的零样本部署，显著提升下游机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练的VLA模型在下游部署时存在显著的性能下降问题，而传统的微调方法需要昂贵的演示数据收集和密集计算，在实际应用中不实用。

Method: 提出VLA-Pilot方法，这是一种即插即用的推理时策略引导技术，无需额外微调或数据收集，直接引导预训练VLA模型进行零样本部署。

Result: 在六个真实世界下游操作任务和两种不同机器人平台上进行评估，涵盖分布内和分布外场景，实验结果显示VLA-Pilot显著提高了现成预训练VLA策略的成功率。

Conclusion: VLA-Pilot能够实现预训练VLA模型的鲁棒零样本泛化，适用于多样化的任务和机器人平台，为实际部署提供了实用解决方案。

Abstract: Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.

</details>


### [9] [Dual-Variable Force Characterisation method for Human-Robot Interaction in Wearable Robotics](https://arxiv.org/abs/2511.14327)
*Felipe Ballen-Moreno,Pasquale Ferrentino,Milan Amighi,Bram Vanderborght,Tom Verstraten*

Main category: cs.RO

TL;DR: 本文提出了一种双变量表征方法，用于分析可穿戴机器人与人体软组织之间的物理相互作用，解决了现有单变量方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的表征方法通常只依赖单一拟合变量和一个自由度，限制了其适用性，因为与可穿戴机器人的交互通常涉及多个自由度。

Method: 引入了一种涉及法向力和切向力的双变量表征方法，旨在识别可靠的材料参数并评估单变量拟合对力和扭矩响应的影响。

Result: 通过分析不同场景和材料模型下的归一化均方误差(NMSE)，证明了在表征过程中纳入两个变量的重要性。

Conclusion: 该方法为模拟可穿戴机器人与用户之间物理交互提供了更接近实际水平的基础，重点关注了袖带和人体肢体之间的相互作用。

Abstract: Understanding the physical interaction with wearable robots is essential to ensure safety and comfort. However, this interaction is complex in two key aspects: (1) the motion involved, and (2) the non-linear behaviour of soft tissues. Multiple approaches have been undertaken to better understand this interaction and to improve the quantitative metrics of physical interfaces or cuffs. As these two topics are closely interrelated, finite modelling and soft tissue characterisation offer valuable insights into pressure distribution and shear stress induced by the cuff. Nevertheless, current characterisation methods typically rely on a single fitting variable along one degree of freedom, which limits their applicability, given that interactions with wearable robots often involve multiple degrees of freedom. To address this limitation, this work introduces a dual-variable characterisation method, involving normal and tangential forces, aimed at identifying reliable material parameters and evaluating the impact of single-variable fitting on force and torque responses. This method demonstrates the importance of incorporating two variables into the characterisation process by analysing the normalized mean square error (NMSE) across different scenarios and material models, providing a foundation for simulation at the closest possible level, with a focus on the cuff and the human limb involved in the physical interaction between the user and the wearable robot.

</details>


### [10] [MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning](https://arxiv.org/abs/2511.14330)
*Yizhen Yin,Yuhua Qi,Dapeng Feng,Hongbo Chen,Hongjun Ma,Jin Wu,Yi Jiang*

Main category: cs.RO

TL;DR: MA-SLAM是一个基于深度强化学习的主动SLAM系统，通过新颖的结构化地图表示和全局规划器，在大规模环境中实现高效探索，显著减少探索时间和距离。


<details>
  <summary>Details</summary>
Motivation: 当前主动SLAM方法在小规模受控环境中有效，但在大规模多样化环境中面临探索时间长和路径优化不足的挑战，需要开发更高效的探索策略。

Method: 提出结构化地图表示方法，通过空间数据离散化、边界点和历史轨迹整合来有效表示已访问区域；采用深度强化学习决策模块，结合全局规划器优化探索路径。

Result: 在三个仿真环境和真实无人地面车辆上的实验表明，该方法相比最先进方法显著减少了探索时间和距离。

Conclusion: MA-SLAM系统通过结构化地图表示和全局路径规划，有效解决了大规模环境中的主动SLAM探索效率问题，具有实际应用价值。

Abstract: Active Simultaneous Localization and Mapping (Active SLAM) involves the strategic planning and precise control of a robotic system's movement in order to construct a highly accurate and comprehensive representation of its surrounding environment, which has garnered significant attention within the research community. While the current methods demonstrate efficacy in small and controlled settings, they face challenges when applied to large-scale and diverse environments, marked by extended periods of exploration and suboptimal paths of discovery. In this paper, we propose MA-SLAM, a Map-Aware Active SLAM system based on Deep Reinforcement Learning (DRL), designed to address the challenge of efficient exploration in large-scale environments. In pursuit of this objective, we put forward a novel structured map representation. By discretizing the spatial data and integrating the boundary points and the historical trajectory, the structured map succinctly and effectively encapsulates the visited regions, thereby serving as input for the deep reinforcement learning based decision module. Instead of sequentially predicting the next action step within the decision module, we have implemented an advanced global planner to optimize the exploration path by leveraging long-range target points. We conducted experiments in three simulation environments and deployed in a real unmanned ground vehicle (UGV), the results demonstrate that our approach significantly reduces both the duration and distance of exploration compared with state-of-the-art methods.

</details>


### [11] [Simultaneous Localization and 3D-Semi Dense Mapping for Micro Drones Using Monocular Camera and Inertial Sensors](https://arxiv.org/abs/2511.14335)
*Jeryes Danial,Yosi Ben Asher,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种结合稀疏关键点姿态估计和密集边缘重建的边缘感知轻量级单目SLAM系统，通过深度学习深度预测和边缘检测，结合惯性数据解决尺度模糊问题，在低功耗平台上实现实时运行。


<details>
  <summary>Details</summary>
Motivation: 解决当前单目SLAM算法中稀疏方法缺乏详细几何信息、学习驱动方法计算量大以及尺度模糊影响精度的问题。

Method: 结合稀疏关键点姿态估计与密集边缘重建，使用深度学习进行深度预测和边缘检测，通过扩展卡尔曼滤波器融合惯性数据解决尺度模糊，在低功耗平台上实现实时运行。

Result: 系统在DJI Tello无人机上实时运行，在室内走廊和TUM RGBD数据集上展示了鲁棒的自主导航和避障能力。

Conclusion: 该方法为资源受限环境中的实时建图和导航提供了有效实用的解决方案。

Abstract: Monocular simultaneous localization and mapping (SLAM) algorithms estimate drone poses and build a 3D map using a single camera. Current algorithms include sparse methods that lack detailed geometry, while learning-driven approaches produce dense maps but are computationally intensive. Monocular SLAM also faces scale ambiguities, which affect its accuracy. To address these challenges, we propose an edge-aware lightweight monocular SLAM system combining sparse keypoint-based pose estimation with dense edge reconstruction. Our method employs deep learning-based depth prediction and edge detection, followed by optimization to refine keypoints and edges for geometric consistency, without relying on global loop closure or heavy neural computations. We fuse inertial data with vision by using an extended Kalman filter to resolve scale ambiguity and improve accuracy. The system operates in real time on low-power platforms, as demonstrated on a DJI Tello drone with a monocular camera and inertial sensors. In addition, we demonstrate robust autonomous navigation and obstacle avoidance in indoor corridors and on the TUM RGBD dataset. Our approach offers an effective, practical solution to real-time mapping and navigation in resource-constrained environments.

</details>


### [12] [Going Places: Place Recognition in Artificial and Natural Systems](https://arxiv.org/abs/2511.14341)
*Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 这篇综述综合了机器人系统、动物研究和人类研究，探讨不同系统如何编码和回忆地点，提出了统一的概念框架来考虑和开发地点识别机制。


<details>
  <summary>Details</summary>
Motivation: 地点识别对于生物导航和自主系统都至关重要，需要从多个领域综合理解地点编码和回忆的策略，以促进人工定位系统的创新。

Method: 通过综合机器人系统、动物研究和人类研究的发现，分析计算和表征策略，包括拓扑映射、线索整合和记忆管理等收敛解决方案。

Result: 揭示了动物系统的多模态导航和环境适应机制，人类研究的语义地点概念、文化影响和内省能力，以及人工系统的可扩展架构和数据驱动模型。

Conclusion: 提出了统一的概念框架来开发地点识别机制，识别了泛化性、鲁棒性和环境变异性等关键挑战，旨在通过连接动物导航研究和人类空间认知研究的见解来促进人工定位系统的创新。

Abstract: Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.

</details>


### [13] [Perception-aware Exploration for Consumer-grade UAVs](https://arxiv.org/abs/2511.14393)
*Svetlana Seliunina,Daniel Schleich,Sven Behnke*

Main category: cs.RO

TL;DR: 将多无人机自主探索技术扩展到消费级无人机（如DJI Mini 3 Pro），提出包含深度估计视点选择、满足运动约束的轨迹规划以及半分布式通信方案的完整流程。


<details>
  <summary>Details</summary>
Motivation: 将先进的多无人机自主探索技术应用于消费级无人机，克服硬件限制，实现安全环境探索和地图重建。

Method: 提出完整流程：选择可进行深度估计的视点对，规划满足里程计估计运动约束的轨迹，采用半分布式通信方案平衡多无人机工作负载。

Result: 通过模拟实验验证了模型在不同无人机数量下的性能，证明即使在消费级无人机硬件限制下也能安全探索环境并重建地图。

Conclusion: 成功将多无人机自主探索技术扩展到消费级平台，为低成本无人机群协同作业提供了可行方案。

Abstract: In our work, we extend the current state-of-the-art approach for autonomous multi-UAV exploration to consumer-level UAVs, such as the DJI Mini 3 Pro. We propose a pipeline that selects viewpoint pairs from which the depth can be estimated and plans the trajectory that satisfies motion constraints necessary for odometry estimation. For the multi-UAV exploration, we propose a semi-distributed communication scheme that distributes the workload in a balanced manner. We evaluate our model performance in simulation for different numbers of UAVs and prove its ability to safely explore the environment and reconstruct the map even with the hardware limitations of consumer-grade UAVs.

</details>


### [14] [Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning](https://arxiv.org/abs/2511.14396)
*Xiuxiu Qi,Yu Yang,Jiannong Cao,Luyao Bai,Chongshan Fan,Chengtai Cao,Hongpeng Wang*

Main category: cs.RO

TL;DR: CCoL是一个新颖的行为克隆框架，通过视觉-语言-动作的连续协同学习实现语义-物理对齐，解决了现有方法中的物理不连续和语义-物理错位问题，在模拟和真实机器人测试中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 克服行为克隆中序列动作决策的复合误差是核心挑战。现有方法通过数据增强、表达性表示或时间抽象来缓解复合误差，但存在物理不连续和语义-物理错位问题，导致动作克隆不准确和执行间歇性。

Method: 提出CCoL框架，通过视觉、语言和本体感觉输入的连续协同学习生成鲁棒平滑的动作执行轨迹；使用双向交叉注意力将语言语义锚定到视觉运动表示中，学习动作生成的上下文信息。

Result: 在三个模拟套件中平均相对提升8.0%，在人类演示的双臂插入任务中相对增益达19.2%；在7自由度机器人上的真实世界测试证实了CCoL在未见和噪声物体状态下的泛化能力。

Conclusion: CCoL通过连续协同学习和语义-物理对齐，成功解决了行为克隆中的复合误差问题，实现了时间一致执行和细粒度语义基础，在模拟和真实环境中都表现出优异的性能。

Abstract: Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.

</details>


### [15] [Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning](https://arxiv.org/abs/2511.14427)
*Rickmer Krohn,Vignesh Prasad,Gabriele Tiboni,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: MSDP是一个用于多感官机器人操作的新框架，通过掩码自编码训练Transformer编码器，实现跨模态预测和传感器融合，在下游策略学习中采用非对称架构，显著提升学习效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在多感官环境中学习困难的问题，特别是在存在感官噪声和动态变化的情况下，需要开发能够有效融合视觉、力和本体感觉的表达性多感官表示。

Method: 提出多感官动态预训练（MSDP）框架，基于掩码自编码训练Transformer编码器，仅使用传感器嵌入的子集重建多感官观测；在下游策略学习中采用非对称架构，评论家通过交叉注意力从冻结嵌入中提取动态特征，演员接收稳定的池化表示。

Result: 在多个具有挑战性的接触丰富机器人操作任务中，该方法展示了加速学习和鲁棒性能，对传感器噪声和物体动态变化具有强鲁棒性，在真实机器人上仅需6000次在线交互即可实现高成功率。

Conclusion: MSDP为复杂多感官机器人控制提供了一个简单而强大的解决方案，能够有效处理感官噪声和动态变化，显著提升学习效率和任务性能。

Abstract: Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.

</details>


### [16] [Mutation Testing for Industrial Robotic Systems](https://arxiv.org/abs/2511.14432)
*Marcela Gonçalves dos Santos,Sylvain Hallé,Fábio Petrillo*

Main category: cs.RO

TL;DR: 本文探讨了将变异测试应用于工业机器人系统的适应性，通过定义特定领域的变异算子来捕获机器人动作和传感器读数的语义，提出了一种在高层次读写操作级别生成有意义变异体的方法，并在拾取放置场景中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 工业机器人系统在多样化环境中部署时，故障可能导致严重事故和昂贵停机时间。确保控制这些系统的软件可靠性至关重要，但传统的变异测试算子不适用于涉及基于消息的命令和物理世界交互的机器人程序。

Method: 提出了一种方法，在高层读写操作级别生成有意义的变异体，包括运动、夹爪动作和传感器噪声注入。定义了特定领域的变异算子来捕获机器人动作和传感器读数的语义。

Result: 在拾取放置场景的实证研究表明，该方法产生更具信息量的变异体，与传统算子相比减少了无效或等价情况的数量。

Conclusion: 变异测试有潜力提高测试套件质量，并为更安全、更可靠的工业机器人系统做出贡献。

Abstract: Industrial robotic systems (IRS) are increasingly deployed in diverse environments, where failures can result in severe accidents and costly downtime. Ensuring the reliability of the software controlling these systems is therefore critical. Mutation testing, a technique widely used in software engineering, evaluates the effectiveness of test suites by introducing small faults, or mutants, into the code. However, traditional mutation operators are poorly suited to robotic programs, which involve message-based commands and interactions with the physical world. This paper explores the adaptation of mutation testing to IRS by defining domain-specific mutation operators that capture the semantics of robot actions and sensor readings. We propose a methodology for generating meaningful mutants at the level of high-level read and write operations, including movement, gripper actions, and sensor noise injection. An empirical study on a pick-and-place scenario demonstrates that our approach produces more informative mutants and reduces the number of invalid or equivalent cases compared to conventional operators. Results highlight the potential of mutation testing to enhance test suite quality and contribute to safer, more reliable industrial robotic systems.

</details>


### [17] [Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies](https://arxiv.org/abs/2511.14434)
*Marlow Fawn,Matthias Scheutz*

Main category: cs.RO

TL;DR: 提出一种将基于信号时序逻辑的谐波控制李雅普诺夫-屏障函数与任意机器人策略结合的方法，将不安全策略转化为具有形式化保证的安全策略。


<details>
  <summary>Details</summary>
Motivation: 将不安全机器人策略转化为具有形式化安全保证的策略，同时保持任务驱动行为。

Method: 通过HCLBF导出的安全证书将机器人策略与安全约束相结合，产生既保证安全又保持任务行为的控制命令。

Result: 在固定机械臂移动任务的简单概念验证中，经过安全约束增强的基于强化学习的力控策略能够成功避开桌面障碍物。

Conclusion: 该方法可推广到更复杂规范和动态任务场景，为机器人安全控制提供形式化保证。

Abstract: We propose a method for combining Harmonic Control Lyapunov-Barrier Functions (HCLBFs) derived from Signal Temporal Logic (STL) specifications with any given robot policy to turn an unsafe policy into a safe one with formal guarantees.  The two components are combined via HCLBF-derived safety certificates, thus producing commands that preserve both safety and task-driven behavior.  We demonstrate with a simple proof-of-concept implementation for an object-centric force-based policy trained through reinforcement learning for a movement task of a stationary robot arm that is able to avoid colliding with obstacles on a table top after combining the policy with the safety constraints.  The proposed method can be generalized to more complex specifications and dynamic task settings.

</details>


### [18] [Advancing Minimally Invasive Precision Surgery in Open Cavities with Robotic Flexible Endoscopy](https://arxiv.org/abs/2511.14458)
*Michelle Mattille,Alexandre Mesot,Miriam Weisskopf,Nicole Ochsenbein-Kölble,Ueli Moehrlen,Bradley J. Nelson,Quentin Boehler*

Main category: cs.RO

TL;DR: 提出了一种用于开放式腔体内窥镜手术的机器人平台，结合磁驱动柔性内窥镜、遥操作和半自主导航能力，通过实时场景拼接增强手术感知，并在羊模型中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 柔性机器人可提升微创手术效果，但在开放式腔体内进行内窥镜干预仍面临挑战，包括缺乏解剖约束、设备灵活性控制困难以及内窥镜视野受限等问题。

Method: 开发了结合磁驱动柔性内窥镜的机器人平台，具备遥操作和半自主导航能力，能够执行靶向激光消融，并通过实时内窥镜场景拼接提供扩展的连续视觉环境。

Result: 在羊模型中进行的体内验证表明，该系统能够有效解决开放式空间微创手术的关键限制。

Conclusion: 该机器人平台成功克服了开放式腔体内窥镜手术的挑战，展示了在复杂微创手术（如胎儿镜激光凝固术）中的应用潜力。

Abstract: Flexible robots hold great promise for enhancing minimally invasive surgery (MIS) by providing superior dexterity, precise control, and safe tissue interaction. Yet, translating these advantages into endoscopic interventions within open cavities remains challenging. The lack of anatomical constraints and the inherent flexibility of such devices complicate their control, while the limited field of view of endoscopes restricts situational awareness. We present a robotic platform designed to overcome these challenges and demonstrate its potential in fetoscopic laser coagulation, a complex MIS procedure typically performed only by highly experienced surgeons. Our system combines a magnetically actuated flexible endoscope with teleoperated and semi-autonomous navigation capabilities for performing targeted laser ablations. To enhance surgical awareness, the platform reconstructs real-time mosaics of the endoscopic scene, providing an extended and continuous visual context. The ability of this system to address the key limitations of MIS in open spaces is validated in vivo in an ovine model.

</details>


### [19] [Aerial Assistance System for Automated Firefighting during Turntable Ladder Operations](https://arxiv.org/abs/2511.14504)
*Jan Quenzel,Valerij Sekin,Daniel Schleich,Alexander Miller,Merlin Stampa,Norbert Pahlke,Christof Röhrig,Sven Behnke*

Main category: cs.RO

TL;DR: 提出一种基于无人机和消防水炮的自动化消防辅助系统，用于工业设施火灾的精准定位和灭火。


<details>
  <summary>Details</summary>
Motivation: 工业设施火灾由于建筑规模大、视觉遮挡严重，导致消防员难以准确定位火源，影响灭火效率并增加损失。

Method: 使用无人机在障碍物自由飞行通道内自主飞行，检测和定位热源；操作员选择目标后，无人机自动规划三角测量路径持续定位，同时系统控制消防水炮将水流导向检测到的热源。

Result: 初步测试表明，该系统成功定位了多个热源并将水流准确导向火源。

Conclusion: 该自动化辅助系统能够有效提高工业火灾的定位精度和灭火效率。

Abstract: Fires in industrial facilities pose special challenges to firefighters, e.g., due to the sheer size and scale of the buildings. The resulting visual obstructions impair firefighting accuracy, further compounded by inaccurate assessments of the fire's location. Such imprecision simultaneously increases the overall damage and prolongs the fire-brigades operation unnecessarily.
  We propose an automated assistance system for firefighting using a motorized fire monitor on a turntable ladder with aerial support from an unmanned aerial vehicle (UAV). The UAV flies autonomously within an obstacle-free flight funnel derived from geodata, detecting and localizing heat sources. An operator supervises the operation on a handheld controller and selects a fire target in reach. After the selection, the UAV automatically plans and traverses between two triangulation poses for continued fire localization. Simultaneously, our system steers the fire monitor to ensure the water jet reaches the detected heat source. In preliminary tests, our assistance system successfully localized multiple heat sources and directed a water jet towards the fires.

</details>


### [20] [Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language](https://arxiv.org/abs/2511.14565)
*Minyoung Hwang,Alexandra Forsey-Smerek,Nathaniel Dennler,Andreea Bobu*

Main category: cs.RO

TL;DR: 本文提出了Masked IRL框架，利用大语言模型结合演示和语言指令的优势，通过推断状态相关性掩码来提升奖励学习的样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于演示的奖励学习容易过拟合到虚假相关性，而现有语言条件方法未能充分利用语言来消除歧义。演示展示如何行动，而语言指定什么是重要的，两者具有互补信息。

Method: Masked IRL使用LLM从语言指令推断状态相关性掩码，强制对不相关状态组件保持不变性。当指令模糊时，利用LLM在演示上下文中澄清指令。

Result: 在仿真和真实机器人实验中，Masked IRL比现有语言条件IRL方法性能提升达15%，同时数据使用量减少4.7倍，显示出更好的样本效率、泛化能力和对模糊语言的鲁棒性。

Conclusion: 通过结合演示和语言指令的互补优势，Masked IRL框架显著提升了奖励学习的效率和性能，为解决奖励函数泛化问题提供了有效方案。

Abstract: Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL

</details>


### [21] [Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks](https://arxiv.org/abs/2511.14592)
*Xianhui Meng,Yuchen Zhang,Zhijian Huang,Zheng Lu,Ziling Ji,Yaoyao Yin,Hongyuan Zhang,Guangfeng Jiang,Yandan Lin,Long Chen,Hangjun Ye,Li Zhang,Jun Liu,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: DSBench是首个综合性的驾驶安全基准测试，用于统一评估视觉语言模型在外部环境风险和车内驾驶行为安全方面的表现，包含10个主要类别和28个子类别，通过大规模数据集微调可显著提升模型的安全性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在自动驾驶中应用前景广阔，但其在安全关键场景下的适用性尚未充分探索，缺乏同时评估外部环境风险和车内驾驶行为安全的综合基准测试，存在安全隐患。

Method: 提出DSBench基准测试框架，涵盖外部环境风险和车内驾驶行为安全两大类别，细分为10个主要类别和28个子类别，构建包含98K实例的大规模安全场景数据集，并通过微调现有模型来提升安全性能。

Result: 对各种主流开源和闭源视觉语言模型的广泛评估显示，在复杂安全关键情况下性能显著下降，凸显了紧迫的安全问题。使用构建的数据集进行微调后，现有模型的安全性能得到显著提升。

Conclusion: DSBench填补了驾驶安全评估的关键空白，揭示了当前视觉语言模型在安全关键场景中的局限性，通过数据集微调可有效提升模型安全性，为推进自动驾驶技术发展铺平道路。

Abstract: Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.

</details>


### [22] [Gallant: Voxel Grid-based Humanoid Locomotion and Local-navigation across 3D Constrained Terrains](https://arxiv.org/abs/2511.14625)
*Qingwei Ben,Botian Xu,Kailin Li,Feiyu Jia,Wentao Zhang,Jingping Wang,Jingbo Wang,Dahua Lin,Jiangmiao Pang*

Main category: cs.RO

TL;DR: Gallant是一个基于体素网格的人形机器人3D约束地形运动框架，利用体素化LiDAR数据作为轻量级感知表示，通过z分组2D CNN实现端到端控制策略优化，在复杂3D环境中实现高成功率运动。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度图像或高程图的感知模块只能提供局部平坦的环境视图，无法捕捉完整的3D结构，限制了人形机器人在复杂3D约束地形中的运动能力。

Method: 使用体素化LiDAR数据作为结构化感知表示，采用z分组2D CNN将感知映射到控制策略，开发高保真LiDAR模拟器支持可扩展训练和sim-to-real一致性。

Result: Gallant的广泛感知覆盖使单一策略能够处理地面障碍物、侧向杂物、顶部约束、多层结构和狭窄通道等复杂场景，在楼梯攀爬和平台登高等挑战性场景中首次实现接近100%的成功率。

Conclusion: 基于体素网格的感知框架Gallant通过端到端优化显著提升了人形机器人在3D约束地形中的运动能力，超越了现有方法的局限性。

Abstract: Robust humanoid locomotion requires accurate and globally consistent perception of the surrounding 3D environment. However, existing perception modules, mainly based on depth images or elevation maps, offer only partial and locally flattened views of the environment, failing to capture the full 3D structure. This paper presents Gallant, a voxel-grid-based framework for humanoid locomotion and local navigation in 3D constrained terrains. It leverages voxelized LiDAR data as a lightweight and structured perceptual representation, and employs a z-grouped 2D CNN to map this representation to the control policy, enabling fully end-to-end optimization. A high-fidelity LiDAR simulation that dynamically generates realistic observations is developed to support scalable, LiDAR-based training and ensure sim-to-real consistency. Experimental results show that Gallant's broader perceptual coverage facilitates the use of a single policy that goes beyond the limitations of previous methods confined to ground-level obstacles, extending to lateral clutter, overhead constraints, multi-level structures, and narrow passages. Gallant also firstly achieves near 100% success rates in challenging scenarios such as stair climbing and stepping onto elevated platforms through improved end-to-end optimization.

</details>


### [23] [NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards](https://arxiv.org/abs/2511.14659)
*Chia-Yu Hung,Navonil Majumder,Haoyuan Deng,Liu Renhang,Yankang Ang,Amir Zadeh,Chuan Li,Dorien Herremans,Ziwei Wang,Soujanya Poria*

Main category: cs.RO

TL;DR: NORA-1.5是一个基于预训练NORA骨干的视觉-语言-动作模型，通过添加基于流匹配的动作专家和奖励驱动的后训练，显著提升了在具身任务中的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在可靠性和泛化能力方面存在不足，特别是在不同具身系统和真实环境中的部署表现不佳，需要更可靠的方法来提升性能。

Method: 在预训练NORA骨干基础上添加流匹配动作专家，开发包含动作条件世界模型和偏离地面真值启发式的奖励模型，通过直接偏好优化进行后训练适配。

Result: NORA-1.5在模拟和真实世界基准测试中超越了NORA和多个最先进的VLA模型，奖励驱动的后训练在仿真和真实机器人设置中持续提升性能。

Conclusion: NORA-1.5和奖励引导的后训练为开发适合真实世界部署的更可靠具身智能体提供了可行路径。

Abstract: Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.

</details>
