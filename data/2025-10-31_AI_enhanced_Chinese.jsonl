{"id": "2510.25913", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.25913", "abs": "https://arxiv.org/abs/2510.25913", "authors": ["Gilbert Bahati", "Ryan M. Bena", "Meg Wilkinson", "Pol Mestres", "Ryan K. Cosner", "Aaron D. Ames"], "title": "Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields", "comment": null, "summary": "Robotic systems navigating in real-world settings require a semantic\nunderstanding of their environment to properly determine safe actions. This\nwork aims to develop the mathematical underpinnings of such a\nrepresentation--specifically, the goal is to develop safety filters that are\nrisk-aware. To this end, we take a two step approach: encoding an understanding\nof the environment via Poisson's equation, and associated risk via Laplace\nguidance fields. That is, we first solve a Dirichlet problem for Poisson's\nequation to generate a safety function that encodes system safety as its\n0-superlevel set. We then separately solve a Dirichlet problem for Laplace's\nequation to synthesize a safe \\textit{guidance field} that encodes variable\nlevels of caution around obstacles -- by enforcing a tunable flux boundary\ncondition. The safety function and guidance fields are then combined to define\na safety constraint and used to synthesize a risk-aware safety filter which,\ngiven a semantic understanding of an environment with associated risk levels of\nenvironmental features, guarantees safety while prioritizing avoidance of\nhigher risk obstacles. We demonstrate this method in simulation and discuss how\n\\textit{a priori} understandings of obstacle risk can be directly incorporated\ninto the safety filter to generate safe behaviors that are risk-aware.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u98ce\u9669\u611f\u77e5\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u65b9\u6cd5\uff0c\u901a\u8fc7\u6cca\u677e\u65b9\u7a0b\u548c\u62c9\u666e\u62c9\u65af\u5f15\u5bfc\u573a\u6765\u7f16\u7801\u73af\u5883\u7406\u89e3\u548c\u98ce\u9669\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5bfc\u822a\u65f6\u4f18\u5148\u907f\u5f00\u9ad8\u98ce\u9669\u969c\u788d\u7269\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u9700\u8981\u5bf9\u5176\u73af\u5883\u8fdb\u884c\u8bed\u4e49\u7406\u89e3\uff0c\u4ee5\u6b63\u786e\u786e\u5b9a\u5b89\u5168\u52a8\u4f5c\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u8fd9\u79cd\u8868\u793a\u7684\u6570\u5b66\u57fa\u7840\uff0c\u7279\u522b\u662f\u5f00\u53d1\u98ce\u9669\u611f\u77e5\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u3002", "method": "\u91c7\u7528\u4e24\u6b65\u6cd5\uff1a\u9996\u5148\u901a\u8fc7\u6cca\u677e\u65b9\u7a0b\u7684\u72c4\u5229\u514b\u96f7\u95ee\u9898\u751f\u6210\u5b89\u5168\u51fd\u6570\uff0c\u5c06\u7cfb\u7edf\u5b89\u5168\u7f16\u7801\u4e3a\u51760-\u8d85\u6c34\u5e73\u96c6\uff1b\u7136\u540e\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u65b9\u7a0b\u7684\u72c4\u5229\u514b\u96f7\u95ee\u9898\u5408\u6210\u5b89\u5168\u5f15\u5bfc\u573a\uff0c\u901a\u8fc7\u53ef\u8c03\u901a\u91cf\u8fb9\u754c\u6761\u4ef6\u7f16\u7801\u969c\u788d\u7269\u5468\u56f4\u7684\u53ef\u53d8\u8c28\u614e\u7ea7\u522b\u3002", "result": "\u5b89\u5168\u51fd\u6570\u548c\u5f15\u5bfc\u573a\u7ed3\u5408\u5b9a\u4e49\u5b89\u5168\u7ea6\u675f\uff0c\u5e76\u7528\u4e8e\u5408\u6210\u98ce\u9669\u611f\u77e5\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u5728\u7ed9\u5b9a\u73af\u5883\u8bed\u4e49\u7406\u89e3\u548c\u76f8\u5173\u98ce\u9669\u7ea7\u522b\u7684\u60c5\u51b5\u4e0b\u4fdd\u8bc1\u5b89\u5168\uff0c\u540c\u65f6\u4f18\u5148\u907f\u5f00\u9ad8\u98ce\u9669\u969c\u788d\u7269\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u5148\u9a8c\u7684\u969c\u788d\u7269\u98ce\u9669\u7406\u89e3\u76f4\u63a5\u7eb3\u5165\u5b89\u5168\u8fc7\u6ee4\u5668\uff0c\u4ee5\u751f\u6210\u98ce\u9669\u611f\u77e5\u7684\u5b89\u5168\u884c\u4e3a\u3002"}}
{"id": "2510.25985", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.25985", "abs": "https://arxiv.org/abs/2510.25985", "authors": ["Francisco M. F. R. Gon\u00e7alves", "Ryan M. Bena", "N\u00e9stor O. P\u00e9rez-Arancibia"], "title": "A New Type of Axis-Angle Attitude Control Law for Rotational Systems: Synthesis, Analysis, and Experiments", "comment": "2025 International Conference on Advanced Robotics (ICAR)", "summary": "Over the past few decades, continuous quaternion-based attitude control has\nbeen proven highly effective for driving rotational systems that can be modeled\nas rigid bodies, such as satellites and drones. However, methods rooted in this\napproach do not enforce the existence of a unique closed-loop (CL) equilibrium\nattitude-error quaternion (AEQ); and, for rotational errors about the\nattitude-error Euler axis larger than {\\pi}rad, their proportional-control\neffect diminishes as the system state moves away from the stable equilibrium of\nthe CL rotational dynamics. In this paper, we introduce a new type of attitude\ncontrol law that more effectively leverages the attitude-error Euler axis-angle\ninformation to guarantee a unique CL equilibrium AEQ and to provide greater\nflexibility in the use of proportional-control efforts. Furthermore, using two\ndifferent control laws as examples-through the construction of a strict\nLyapunov function for the CL dynamics-we demonstrate that the resulting unique\nequilibrium of the CL rotational system can be enforced to be uniformly\nasymptotically stable. To assess and demonstrate the functionality and\nperformance of the proposed approach, we performed numerical simulations and\nexecuted dozens of real-time tumble-recovery maneuvers using a small quadrotor.\nThese simulations and flight tests compellingly demonstrate that the proposed\naxis-angle-based method achieves superior flight performance-compared with that\nobtained using a high-performance quaternion-based controller-in terms of\nstabilization time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u59ff\u6001\u8bef\u5dee\u6b27\u62c9\u8f74\u89d2\u4fe1\u606f\u7684\u65b0\u578b\u59ff\u6001\u63a7\u5236\u5f8b\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u56db\u5143\u6570\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u4fdd\u8bc1\u95ed\u73af\u7cfb\u7edf\u5177\u6709\u552f\u4e00\u7684\u5e73\u8861\u70b9\uff0c\u5e76\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u6bd4\u4f8b\u63a7\u5236\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u56db\u5143\u6570\u59ff\u6001\u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4e0d\u4fdd\u8bc1\u95ed\u73af\u7cfb\u7edf\u5177\u6709\u552f\u4e00\u7684\u5e73\u8861\u70b9\uff1b2) \u5f53\u59ff\u6001\u8bef\u5dee\u6b27\u62c9\u8f74\u65cb\u8f6c\u89d2\u5ea6\u8d85\u8fc7\u03c0\u5f27\u5ea6\u65f6\uff0c\u6bd4\u4f8b\u63a7\u5236\u6548\u679c\u4f1a\u51cf\u5f31\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u59ff\u6001\u8bef\u5dee\u6b27\u62c9\u8f74\u89d2\u4fe1\u606f\u7684\u65b0\u578b\u63a7\u5236\u5f8b\uff0c\u901a\u8fc7\u6784\u5efa\u4e25\u683c\u7684Lyapunov\u51fd\u6570\u6765\u8bc1\u660e\u95ed\u73af\u65cb\u8f6c\u7cfb\u7edf\u7684\u552f\u4e00\u5e73\u8861\u70b9\u662f\u5747\u5300\u6e10\u8fd1\u7a33\u5b9a\u7684\u3002", "result": "\u6570\u503c\u4eff\u771f\u548c\u5b9e\u65f6\u7ffb\u6eda\u6062\u590d\u673a\u52a8\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8f74\u89d2\u65b9\u6cd5\u5728\u7a33\u5b9a\u65f6\u95f4\u65b9\u9762\u4f18\u4e8e\u9ad8\u6027\u80fd\u56db\u5143\u6570\u63a7\u5236\u5668\u3002", "conclusion": "\u57fa\u4e8e\u8f74\u89d2\u7684\u65b9\u6cd5\u5728\u59ff\u6001\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7a33\u5b9a\u65f6\u95f4\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u56db\u5143\u6570\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2510.26004", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26004", "abs": "https://arxiv.org/abs/2510.26004", "authors": ["Bai Li", "Achilleas Kourtellis", "Rong Cao", "Joseph Post", "Brian Porter", "Yu Zhang"], "title": "DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System", "comment": "Preprint version. This manuscript is currently under review at\n  Transportation Research Part C: Emerging Technologies. The PDF corresponds to\n  the version submitted in June 2025. The main findings of this work were\n  recognized with the Best Intelligent Transportation Systems Paper Award at\n  the 2025 TRB Annual Meeting", "summary": "Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.", "AI": {"tldr": "DARTS\u662f\u4e00\u4e2a\u57fa\u4e8e\u65e0\u4eba\u673a\u548cAI\u7684\u5b9e\u65f6\u4ea4\u901a\u4e8b\u4ef6\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u7684\u9ad8\u673a\u52a8\u6027\u548c\u70ed\u6210\u50cf\u6280\u672f\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4e8699%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u524d12\u5206\u949f\u68c0\u6d4b\u5230\u8ffd\u5c3e\u4e8b\u6545\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u68c0\u6d4b\u4e0e\u9a8c\u8bc1\u5206\u79bb\u3001\u7075\u6d3b\u6027\u6709\u9650\u3001\u9700\u8981\u5bc6\u96c6\u57fa\u7840\u8bbe\u65bd\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "DARTS\u6574\u5408\u4e86\u65e0\u4eba\u673a\u7684\u9ad8\u673a\u52a8\u6027\u548c\u7a7a\u4e2d\u89c6\u89d2\u8fdb\u884c\u81ea\u9002\u5e94\u76d1\u63a7\uff0c\u4f7f\u7528\u70ed\u6210\u50cf\u6280\u672f\u63d0\u5347\u4f4e\u80fd\u89c1\u5ea6\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u5b9e\u65f6\u8f66\u8f86\u8f68\u8ff9\u63d0\u53d6\u548c\u4e8b\u4ef6\u68c0\u6d4b\u3002", "result": "\u7cfb\u7edf\u5728\u81ea\u6536\u96c6\u6570\u636e\u96c6\u4e0a\u8fbe\u523099%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u652f\u6301\u540c\u65f6\u5728\u7ebf\u89c6\u89c9\u9a8c\u8bc1\u3001\u4e25\u91cd\u6027\u8bc4\u4f30\u548c\u4e8b\u4ef6\u5f15\u53d1\u62e5\u5835\u4f20\u64ad\u76d1\u63a7\u3002\u5728\u4f5b\u7f57\u91cc\u8fbe\u5dde75\u53f7\u5dde\u9645\u516c\u8def\u7684\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u5f53\u5730\u4ea4\u901a\u7ba1\u7406\u4e2d\u5fc3\u63d0\u524d12\u5206\u949f\u68c0\u6d4b\u5230\u8ffd\u5c3e\u4e8b\u6545\u3002", "conclusion": "DARTS\u5c55\u793a\u4e86\u66f4\u7075\u6d3b\u548c\u96c6\u6210\u7684\u5b9e\u65f6\u4ea4\u901a\u4e8b\u4ef6\u68c0\u6d4b\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u5177\u6709\u663e\u8457\u63d0\u9ad8\u73b0\u4ee3\u4ea4\u901a\u7ba1\u7406\u8fd0\u8425\u6548\u7387\u548c\u54cd\u5e94\u80fd\u529b\u7684\u610f\u4e49\uff0c\u7279\u522b\u662f\u5728\u504f\u8fdc\u5730\u533a\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2510.26018", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26018", "abs": "https://arxiv.org/abs/2510.26018", "authors": ["Petr Stibinger", "Tomas Baca", "Daniela Doubravova", "Jan Rusnak", "Jaroslav Solc", "Jan Jakubek", "Petr Stepan", "Martin Saska"], "title": "RADRON: Cooperative Localization of Ionizing Radiation Sources by MAVs with Compton Cameras", "comment": "8 pages, 9 figures, submitted for review to IEEE RA-L", "summary": "We present a novel approach to localizing radioactive material by cooperating\nMicro Aerial Vehicles (MAVs). Our approach utilizes a state-of-the-art\nsingle-detector Compton camera as a highly sensitive, yet miniature detector of\nionizing radiation. The detector's exceptionally low weight (40 g) opens up new\npossibilities of radiation detection by a team of cooperating agile MAVs. We\npropose a new fundamental concept of fusing the Compton camera measurements to\nestimate the position of the radiation source in real time even from extremely\nsparse measurements. The data readout and processing are performed directly\nonboard and the results are used in a dynamic feedback to drive the motion of\nthe vehicles. The MAVs are stabilized in a tightly cooperating swarm to\nmaximize the information gained by the Compton cameras, rapidly locate the\nradiation source, and even track a moving radiation source.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5fae\u578b\u65e0\u4eba\u673a\u7fa4\u5408\u4f5c\u5b9a\u4f4d\u653e\u5c04\u6027\u6750\u6599\u7684\u65b0\u65b9\u6cd5\uff0c\u91c7\u7528\u5355\u63a2\u6d4b\u5668\u5eb7\u666e\u987f\u76f8\u673a\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u8f90\u5c04\u63a2\u6d4b\u5668\uff0c\u901a\u8fc7\u878d\u5408\u6d4b\u91cf\u6570\u636e\u5b9e\u65f6\u4f30\u8ba1\u8f90\u5c04\u6e90\u4f4d\u7f6e\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u5408\u4f5c\u5fae\u578b\u65e0\u4eba\u673a\u7fa4\u5feb\u901f\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u653e\u5c04\u6027\u6750\u6599\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u5eb7\u666e\u987f\u76f8\u673a\u7684\u9ad8\u7075\u654f\u5ea6\u7279\u6027\u3002", "method": "\u4f7f\u752840\u514b\u91cd\u7684\u5355\u63a2\u6d4b\u5668\u5eb7\u666e\u987f\u76f8\u673a\u4f5c\u4e3a\u8f90\u5c04\u63a2\u6d4b\u5668\uff0c\u5728\u65e0\u4eba\u673a\u4e0a\u76f4\u63a5\u8fdb\u884c\u6570\u636e\u8bfb\u53d6\u548c\u5904\u7406\uff0c\u901a\u8fc7\u52a8\u6001\u53cd\u9988\u63a7\u5236\u65e0\u4eba\u673a\u8fd0\u52a8\uff0c\u91c7\u7528\u7d27\u5bc6\u5408\u4f5c\u7684\u8702\u7fa4\u7b56\u7565\u6700\u5927\u5316\u4fe1\u606f\u83b7\u53d6\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u6781\u5176\u7a00\u758f\u7684\u6d4b\u91cf\u6570\u636e\u4e2d\u5b9e\u65f6\u4f30\u8ba1\u8f90\u5c04\u6e90\u4f4d\u7f6e\uff0c\u80fd\u591f\u5feb\u901f\u5b9a\u4f4d\u9759\u6001\u8f90\u5c04\u6e90\u5e76\u8ddf\u8e2a\u79fb\u52a8\u8f90\u5c04\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u5eb7\u666e\u987f\u76f8\u673a\u4e0e\u5fae\u578b\u65e0\u4eba\u673a\u7fa4\u5408\u4f5c\u5728\u653e\u5c04\u6027\u6750\u6599\u5b9a\u4f4d\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8f90\u5c04\u68c0\u6d4b\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\u3002"}}
{"id": "2510.26040", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26040", "abs": "https://arxiv.org/abs/2510.26040", "authors": ["Emily Steiner", "Daniel van der Spuy", "Futian Zhou", "Afereti Pama", "Minas Liarokapis", "Henry Williams"], "title": "Accelerating Real-World Overtaking in F1TENTH Racing Employing Reinforcement Learning Methods", "comment": null, "summary": "While autonomous racing performance in Time-Trial scenarios has seen\nsignificant progress and development, autonomous wheel-to-wheel racing and\novertaking are still severely limited. These limitations are particularly\napparent in real-life driving scenarios where state-of-the-art algorithms\nstruggle to safely or reliably complete overtaking manoeuvres. This is\nimportant, as reliable navigation around other vehicles is vital for safe\nautonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful\nopportunity for developing wheel-to-wheel racing algorithms on a standardised\nphysical platform. The competition format makes it possible to evaluate\novertaking and wheel-to-wheel racing algorithms against the state-of-the-art.\nThis research presents a novel racing and overtaking agent capable of learning\nto reliably navigate a track and overtake opponents in both simulation and\nreality. The agent was deployed on an F1Tenth vehicle and competed against\nopponents running varying competitive algorithms in the real world. The results\ndemonstrate that the agent's training against opponents enables deliberate\novertaking behaviours with an overtaking rate of 87% compared 56% for an agent\ntrained just to race.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u53ef\u9760\u5bfc\u822a\u8d5b\u9053\u5e76\u8d85\u8d8a\u5bf9\u624b\u7684\u65b0\u578b\u8d5b\u8f66\u548c\u8d85\u8f66\u667a\u80fd\u4f53\uff0c\u5728F1Tenth\u8f66\u8f86\u4e0a\u90e8\u7f72\u5e76\u4e0e\u5176\u4ed6\u7ade\u4e89\u7b97\u6cd5\u5bf9\u6297\uff0c\u5b9e\u73b0\u4e8687%\u7684\u8d85\u8f66\u6210\u529f\u7387\u3002", "motivation": "\u867d\u7136\u81ea\u4e3b\u8d5b\u8f66\u5728\u8ba1\u65f6\u8d5b\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u8f6e\u5bf9\u8f6e\u81ea\u4e3b\u8d5b\u8f66\u548c\u8d85\u8f66\u4ecd\u7136\u4e25\u91cd\u53d7\u9650\uff0c\u7279\u522b\u662f\u5728\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u73b0\u6709\u7b97\u6cd5\u96be\u4ee5\u5b89\u5168\u53ef\u9760\u5730\u5b8c\u6210\u8d85\u8f66\u64cd\u4f5c\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u8d5b\u8f66\u548c\u8d85\u8f66\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u5728F1Tenth\u5e73\u53f0\u4e0a\u5b66\u4e60\u53ef\u9760\u5bfc\u822a\u548c\u8d85\u8f66\uff0c\u5e76\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e2d\u90e8\u7f72\u6d4b\u8bd5\uff0c\u4e0e\u8fd0\u884c\u4e0d\u540c\u7ade\u4e89\u7b97\u6cd5\u7684\u5bf9\u624b\u8fdb\u884c\u5bf9\u6297\u3002", "result": "\u8be5\u667a\u80fd\u4f53\u5728\u4e0e\u5bf9\u624b\u7684\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e8687%\u7684\u8d85\u8f66\u6210\u529f\u7387\uff0c\u800c\u4ec5\u63a5\u53d7\u8d5b\u8f66\u8bad\u7ec3\u7684\u667a\u80fd\u4f53\u8d85\u8f66\u6210\u529f\u7387\u4ec5\u4e3a56%\u3002", "conclusion": "\u901a\u8fc7\u4e0e\u5bf9\u624b\u8fdb\u884c\u8bad\u7ec3\uff0c\u667a\u80fd\u4f53\u80fd\u591f\u5b9e\u73b0\u6709\u610f\u8bc6\u7684\u8d85\u8f66\u884c\u4e3a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u8f6e\u5bf9\u8f6e\u81ea\u4e3b\u8d5b\u8f66\u4e2d\u7684\u8d85\u8f66\u80fd\u529b\u3002"}}
{"id": "2510.26082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26082", "abs": "https://arxiv.org/abs/2510.26082", "authors": ["Fan Yang", "Lingyao Li", "Yaxin Hu", "Michael Rodgers", "Renkai Ma"], "title": "Beyond the Uncanny Valley: A Mixed-Method Investigation of Anthropomorphism in Protective Responses to Robot Abuse", "comment": null, "summary": "Robots with anthropomorphic features are increasingly shaping how humans\nperceive and morally engage with them. Our research investigates how different\nlevels of anthropomorphism influence protective responses to robot abuse,\nextending the Computers as Social Actors (CASA) and uncanny valley theories\ninto a moral domain. In an experiment, we invite 201 participants to view\nvideos depicting abuse toward a robot with low (Spider), moderate (Two-Foot),\nor high (Humanoid) anthropomorphism. To provide a comprehensive analysis, we\ntriangulate three modalities: self-report surveys measuring emotions and\nuncanniness, physiological data from automated facial expression analysis, and\nqualitative reflections. Findings indicate that protective responses are not\nlinear. The moderately anthropomorphic Two-Foot robot, rated highest in\neeriness and \"spine-tingling\" sensations consistent with the uncanny valley,\nelicited the strongest physiological anger expressions. Self-reported anger and\nguilt are significantly higher for both the Two-Foot and Humanoid robots\ncompared to the Spider. Qualitative findings further reveal that as\nanthropomorphism increases, moral reasoning shifts from technical assessments\nof property damage to condemnation of the abuser's character, while governance\nproposals expand from property law to calls for quasi-animal rights and broader\nsocietal responsibility. These results suggest that the uncanny valley does not\ndampen moral concern but paradoxically heightens protective impulses, offering\ncritical implications for robot design, policy, and future legal frameworks.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2510.26139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26139", "abs": "https://arxiv.org/abs/2510.26139", "authors": ["Minseo Kwon", "Young J. Kim"], "title": "Kinodynamic Task and Motion Planning using VLM-guided and Interleaved Sampling", "comment": null, "summary": "Task and Motion Planning (TAMP) integrates high-level task planning with\nlow-level motion feasibility, but existing methods are costly in long-horizon\nproblems due to excessive motion sampling. While LLMs provide commonsense\npriors, they lack 3D spatial reasoning and cannot ensure geometric or dynamic\nfeasibility. We propose a kinodynamic TAMP framework based on a hybrid state\ntree that uniformly represents symbolic and numeric states during planning,\nenabling task and motion decisions to be jointly decided. Kinodynamic\nconstraints embedded in the TAMP problem are verified by an off-the-shelf\nmotion planner and physics simulator, and a VLM guides exploring a TAMP\nsolution and backtracks the search based on visual rendering of the states.\nExperiments on the simulated domains and in the real world show 32.14% -\n1166.67% increased average success rates compared to traditional and LLM-based\nTAMP planners and reduced planning time on complex problems, with ablations\nfurther highlighting the benefits of VLM guidance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u72b6\u6001\u6811\u7684\u8fd0\u52a8\u52a8\u529b\u5b66TAMP\u6846\u67b6\uff0c\u5c06\u7b26\u53f7\u548c\u6570\u503c\u72b6\u6001\u7edf\u4e00\u8868\u793a\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u641c\u7d22\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u957f\u65f6\u57df\u89c4\u5212\u95ee\u9898\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709TAMP\u65b9\u6cd5\u5728\u957f\u65f6\u57df\u95ee\u9898\u4e2d\u7531\u4e8e\u8fc7\u5ea6\u8fd0\u52a8\u91c7\u6837\u800c\u6210\u672c\u9ad8\u6602\uff0c\u800cLLM\u867d\u7136\u63d0\u4f9b\u5e38\u8bc6\u5148\u9a8c\u4f46\u7f3a\u4e4f3D\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u786e\u4fdd\u51e0\u4f55\u6216\u52a8\u529b\u5b66\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u72b6\u6001\u6811\u7edf\u4e00\u8868\u793a\u7b26\u53f7\u548c\u6570\u503c\u72b6\u6001\uff0c\u901a\u8fc7\u73b0\u6210\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u548c\u7269\u7406\u6a21\u62df\u5668\u9a8c\u8bc1\u8fd0\u52a8\u52a8\u529b\u5b66\u7ea6\u675f\uff0c\u5229\u7528VLM\u57fa\u4e8e\u72b6\u6001\u89c6\u89c9\u6e32\u67d3\u5f15\u5bfcTAMP\u89e3\u63a2\u7d22\u548c\u56de\u6eaf\u641c\u7d22\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u548c\u57fa\u4e8eLLM\u7684TAMP\u89c4\u5212\u5668\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad8\u4e8632.14%-1166.67%\uff0c\u590d\u6742\u95ee\u9898\u89c4\u5212\u65f6\u95f4\u51cf\u5c11\uff0c\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86VLM\u5f15\u5bfc\u7684\u76ca\u5904\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u8868\u793a\u548cVLM\u5f15\u5bfc\u6709\u6548\u89e3\u51b3\u4e86TAMP\u4e2d\u7684\u8fd0\u52a8\u91c7\u6837\u6210\u672c\u548c\u7a7a\u95f4\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2510.26142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26142", "abs": "https://arxiv.org/abs/2510.26142", "authors": ["Hahjin Lee", "Young J. Kim"], "title": "Adaptive Trajectory Refinement for Optimization-based Local Planning in Narrow Passages", "comment": null, "summary": "Trajectory planning for mobile robots in cluttered environments remains a\nmajor challenge due to narrow passages, where conventional methods often fail\nor generate suboptimal paths. To address this issue, we propose the adaptive\ntrajectory refinement algorithm, which consists of two main stages. First, to\nensure safety at the path-segment level, a segment-wise conservative collision\ntest is applied, where risk-prone trajectory path segments are recursively\nsubdivided until collision risks are eliminated. Second, to guarantee\npose-level safety, pose correction based on penetration direction and line\nsearch is applied, ensuring that each pose in the trajectory is collision-free\nand maximally clear from obstacles. Simulation results demonstrate that the\nproposed method achieves up to 1.69x higher success rates and up to 3.79x\nfaster planning times than state-of-the-art approaches. Furthermore, real-world\nexperiments confirm that the robot can safely pass through narrow passages\nwhile maintaining rapid planning performance.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6bb5\u4fdd\u5b88\u78b0\u649e\u68c0\u6d4b\u548c\u4f4d\u59ff\u4fee\u6b63\uff0c\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u72ed\u7a84\u901a\u9053\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u9ad8\u6210\u529f\u7387\u548c\u89c4\u5212\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u72ed\u7a84\u901a\u9053\u73af\u5883\u4e2d\u7ecf\u5e38\u5931\u8d25\u6216\u751f\u6210\u6b21\u4f18\u8def\u5f84\uff0c\u9700\u8981\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u62e5\u6324\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u5206\u6bb5\u4fdd\u5b88\u78b0\u649e\u6d4b\u8bd5\uff0c\u9012\u5f52\u7ec6\u5206\u98ce\u9669\u8f68\u8ff9\u6bb5\u6d88\u9664\u78b0\u649e\u98ce\u9669\uff1b2) \u57fa\u4e8e\u7a7f\u900f\u65b9\u5411\u548c\u7ebf\u641c\u7d22\u7684\u4f4d\u59ff\u4fee\u6b63\uff0c\u786e\u4fdd\u6bcf\u4e2a\u4f4d\u59ff\u65e0\u78b0\u649e\u4e14\u8fdc\u79bb\u969c\u788d\u7269\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u9ad81.69\u500d\uff0c\u89c4\u5212\u65f6\u95f4\u52a0\u5feb3.79\u500d\u3002\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u673a\u5668\u4eba\u80fd\u5b89\u5168\u901a\u8fc7\u72ed\u7a84\u901a\u9053\u5e76\u4fdd\u6301\u5feb\u901f\u89c4\u5212\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u81ea\u9002\u5e94\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u72ed\u7a84\u901a\u9053\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u548c\u89c4\u5212\u6548\u7387\u3002"}}
{"id": "2510.26170", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26170", "abs": "https://arxiv.org/abs/2510.26170", "authors": ["Satoshi Kikuch", "Masaya Kato", "Tsuyoshi Tasaki"], "title": "Self-localization on a 3D map by fusing global and local features from a monocular camera", "comment": null, "summary": "Self-localization on a 3D map by using an inexpensive monocular camera is\nrequired to realize autonomous driving. Self-localization based on a camera\noften uses a convolutional neural network (CNN) that can extract local features\nthat are calculated by nearby pixels. However, when dynamic obstacles, such as\npeople, are present, CNN does not work well. This study proposes a new method\ncombining CNN with Vision Transformer, which excels at extracting global\nfeatures that show the relationship of patches on whole image. Experimental\nresults showed that, compared to the state-of-the-art method (SOTA), the\naccuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times\nhigher than that without dynamic obstacles. Moreover, the self-localization\nerror of our method is 20.1% smaller than that of SOTA on public datasets.\nAdditionally, our robot using our method can localize itself with 7.51cm error\non average, which is more accurate than SOTA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CNN\u548cVision Transformer\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u52a8\u6001\u969c\u788d\u7269\u76843D\u5730\u56fe\u4e0a\u8fdb\u884c\u5355\u76ee\u76f8\u673a\u81ea\u5b9a\u4f4d\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f7f\u7528\u5ec9\u4ef7\u5355\u76ee\u76f8\u673a\u57283D\u5730\u56fe\u4e0a\u5b9e\u73b0\u81ea\u5b9a\u4f4d\u662f\u81ea\u52a8\u9a7e\u9a76\u7684\u5173\u952e\u9700\u6c42\u3002\u73b0\u6709\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\u5728\u5904\u7406\u52a8\u6001\u969c\u788d\u7269\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3aCNN\u4e3b\u8981\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\u3002", "method": "\u5c06CNN\u4e0eVision Transformer\u76f8\u7ed3\u5408\uff0cCNN\u64c5\u957f\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u800cVision Transformer\u64c5\u957f\u63d0\u53d6\u5168\u5c40\u7279\u5f81\uff0c\u80fd\u591f\u5904\u7406\u56fe\u50cf\u4e2d\u5404\u4e2apatch\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a\u5728\u5305\u542b\u52a8\u6001\u969c\u788d\u7269\u7684CG\u6570\u636e\u96c6\u4e0a\uff0c\u7cbe\u5ea6\u63d0\u5347\u7387\u6bd4\u65e0\u52a8\u6001\u969c\u788d\u7269\u65f6\u9ad81.5\u500d\uff1b\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u81ea\u5b9a\u4f4d\u8bef\u5dee\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u51cf\u5c1120.1%\uff1b\u673a\u5668\u4eba\u4f7f\u7528\u8be5\u65b9\u6cd5\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\u4e3a7.51cm\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408CNN\u548cVision Transformer\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u969c\u788d\u7269\uff0c\u663e\u8457\u63d0\u5347\u81ea\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26236", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26236", "abs": "https://arxiv.org/abs/2510.26236", "authors": ["Kyungmin Lee", "Sibeen Kim", "Minho Park", "Hyunseung Kim", "Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo"], "title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset", "comment": null, "summary": "Motion imitation is a promising approach for humanoid locomotion, enabling\nagents to acquire humanlike behaviors. Existing methods typically rely on\nhigh-quality motion capture datasets such as AMASS, but these are scarce and\nexpensive, limiting scalability and diversity. Recent studies attempt to scale\ndata collection by converting large-scale internet videos, exemplified by\nHumanoid-X. However, they often introduce physical artifacts such as floating,\npenetration, and foot skating, which hinder stable imitation. In response, we\nintroduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that\nleverages human video at scale, while addressing physical artifacts through\ncareful data curation and physics-constrained retargeting. PHUMA enforces joint\nlimits, ensures ground contact, and eliminates foot skating, producing motions\nthat are both large-scale and physically reliable. We evaluated PHUMA in two\nsets of conditions: (i) imitation of unseen motion from self-recorded test\nvideos and (ii) path following with pelvis-only guidance. In both cases,\nPHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant\ngains in imitating diverse motions. The code is available at\nhttps://davian-robotics.github.io/PHUMA.", "AI": {"tldr": "PHUMA\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u7269\u7406\u4f2a\u5f71\u95ee\u9898\uff0c\u5728\u8fd0\u52a8\u6a21\u4eff\u548c\u8def\u5f84\u8ddf\u968f\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eHumanoid-X\u548cAMASS\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u6a21\u4eff\u65b9\u6cd5\u4f9d\u8d56\u7a00\u7f3a\u6602\u8d35\u7684\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\uff08\u5982AMASS\uff09\uff0c\u800c\u57fa\u4e8e\u4e92\u8054\u7f51\u89c6\u9891\u7684\u65b9\u6cd5\uff08\u5982Humanoid-X\uff09\u5b58\u5728\u7269\u7406\u4f2a\u5f71\u95ee\u9898\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u6570\u636e\u7b5b\u9009\u548c\u7269\u7406\u7ea6\u675f\u91cd\u5b9a\u5411\u6280\u672f\uff0c\u5f3a\u5236\u6267\u884c\u5173\u8282\u9650\u5236\u3001\u786e\u4fdd\u5730\u9762\u63a5\u89e6\u3001\u6d88\u9664\u811a\u90e8\u6ed1\u52a8\uff0c\u751f\u6210\u5927\u89c4\u6a21\u4e14\u7269\u7406\u53ef\u9760\u7684\u8fd0\u52a8\u6570\u636e\u3002", "result": "\u5728\u672a\u89c1\u8fd0\u52a8\u6a21\u4eff\u548c\u9aa8\u76c6\u5f15\u5bfc\u8def\u5f84\u8ddf\u968f\u4e24\u79cd\u6761\u4ef6\u4e0b\uff0cPHUMA\u8bad\u7ec3\u7684\u7b56\u7565\u5747\u4f18\u4e8eHumanoid-X\u548cAMASS\uff0c\u5728\u591a\u6837\u5316\u8fd0\u52a8\u6a21\u4eff\u65b9\u9762\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "PHUMA\u6210\u529f\u89e3\u51b3\u4e86\u57fa\u4e8e\u89c6\u9891\u7684\u8fd0\u52a8\u6570\u636e\u751f\u6210\u4e2d\u7684\u7269\u7406\u4f2a\u5f71\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6a21\u4eff\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2510.26280", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26280", "abs": "https://arxiv.org/abs/2510.26280", "authors": ["Gangyang Li", "Qing Shi", "Youhao Hu", "Jincheng Hu", "Zhongyuan Wang", "Xinlong Wang", "Shaqi Luo"], "title": "Thor: Towards Human-Level Whole-Body Reactions for Intense Contact-Rich Environments", "comment": null, "summary": "Humanoids hold great potential for service, industrial, and rescue\napplications, in which robots must sustain whole-body stability while\nperforming intense, contact-rich interactions with the environment. However,\nenabling humanoids to generate human-like, adaptive responses under such\nconditions remains a major challenge. To address this, we propose Thor, a\nhumanoid framework for human-level whole-body reactions in contact-rich\nenvironments. Based on the robot's force analysis, we design a force-adaptive\ntorso-tilt (FAT2) reward function to encourage humanoids to exhibit human-like\nresponses during force-interaction tasks. To mitigate the high-dimensional\nchallenges of humanoid control, Thor introduces a reinforcement learning\narchitecture that decouples the upper body, waist, and lower body. Each\ncomponent shares global observations of the whole body and jointly updates its\nparameters. Finally, we deploy Thor on the Unitree G1, and it substantially\noutperforms baselines in force-interaction tasks. Specifically, the robot\nachieves a peak pulling force of 167.7 N (approximately 48% of the G1's body\nweight) when moving backward and 145.5 N when moving forward, representing\nimprovements of 68.9% and 74.7%, respectively, compared with the\nbest-performing baseline. Moreover, Thor is capable of pulling a loaded rack\n(130 N) and opening a fire door with one hand (60 N). These results highlight\nThor's effectiveness in enhancing humanoid force-interaction capabilities.", "AI": {"tldr": "\u63d0\u51faThor\u4eba\u5f62\u673a\u5668\u4eba\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u7c7b\u4eba\u6c34\u5e73\u7684\u5168\u8eab\u53cd\u5e94\uff0c\u901a\u8fc7\u529b\u81ea\u9002\u5e94\u8eaf\u5e72\u503e\u659c\u5956\u52b1\u51fd\u6570\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u529b\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5728\u670d\u52a1\u3001\u5de5\u4e1a\u548c\u6551\u63f4\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u5728\u4fdd\u6301\u5168\u8eab\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u6267\u884c\u5f3a\u70c8\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u73af\u5883\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ea7\u751f\u7c7b\u4eba\u7684\u81ea\u9002\u5e94\u54cd\u5e94\u3002", "method": "\u57fa\u4e8e\u673a\u5668\u4eba\u53d7\u529b\u5206\u6790\u8bbe\u8ba1\u529b\u81ea\u9002\u5e94\u8eaf\u5e72\u503e\u659c\u5956\u52b1\u51fd\u6570\uff1b\u5f15\u5165\u5c06\u4e0a\u534a\u8eab\u3001\u8170\u90e8\u548c\u4e0b\u534a\u8eab\u89e3\u8026\u7684\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u5404\u7ec4\u4ef6\u5171\u4eab\u5168\u8eab\u5168\u5c40\u89c2\u6d4b\u5e76\u8054\u5408\u66f4\u65b0\u53c2\u6570\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u90e8\u7f72Thor\uff0c\u5411\u540e\u79fb\u52a8\u65f6\u5cf0\u503c\u62c9\u529b\u8fbe\u5230167.7N\uff08\u7ea6G1\u4f53\u91cd\u768448%\uff09\uff0c\u5411\u524d\u79fb\u52a8\u65f6145.5N\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u5206\u522b\u63d0\u534768.9%\u548c74.7%\uff1b\u80fd\u591f\u62c9\u52a8130N\u8d1f\u8f7d\u7684\u8d27\u67b6\u548c\u5355\u624b\u6253\u5f0060N\u7684\u9632\u706b\u95e8\u3002", "conclusion": "Thor\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u529b\u4ea4\u4e92\u80fd\u529b\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u7c7b\u4eba\u6c34\u5e73\u7684\u5168\u8eab\u53cd\u5e94\u3002"}}
{"id": "2510.26358", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.26358", "abs": "https://arxiv.org/abs/2510.26358", "authors": ["Mirko Usuelli", "David Rapado-Rincon", "Gert Kootstra", "Matteo Matteucci"], "title": "AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM", "comment": null, "summary": "Autonomous robots in orchards require real-time 3D scene understanding\ndespite repetitive row geometry, seasonal appearance changes, and wind-driven\nfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that\ncouples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian\nSplatting (3DGS) rendering. Batch rasterization across complementary viewpoints\nrecovers orchard structure under occlusions, while a unified gradient-driven\nmap lifecycle executed between keyframes preserves fine details and bounds\nmemory. Pose refinement is guided by a probabilistic LiDAR-based depth\nconsistency term, back-propagated through the camera projection to tighten\ngeometry-appearance coupling. We deploy the system on a field platform in apple\nand pear orchards across dormancy, flowering, and harvesting, using a\nstandardized trajectory protocol that evaluates both training-view and\nnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons\nand sites, AgriGS-SLAM delivers sharper, more stable reconstructions and\nsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while\nmaintaining real-time performance on-tractor. While demonstrated in orchard\nmonitoring, the approach can be applied to other outdoor domains requiring\nrobust multimodal perception.", "AI": {"tldr": "AgriGS-SLAM\u662f\u4e00\u4e2a\u89c6\u89c9-LiDAR SLAM\u6846\u67b6\uff0c\u7ed3\u5408\u76f4\u63a5LiDAR\u91cc\u7a0b\u8ba1\u548c\u95ed\u73af\u68c0\u6d4b\u4e0e\u591a\u76f8\u673a3D\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\uff0c\u7528\u4e8e\u679c\u56ed\u73af\u5883\u4e0b\u7684\u5b9e\u65f63D\u573a\u666f\u7406\u89e3\u3002", "motivation": "\u679c\u56ed\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u9700\u8981\u5b9e\u65f63D\u573a\u666f\u7406\u89e3\uff0c\u4f46\u9762\u4e34\u91cd\u590d\u884c\u51e0\u4f55\u3001\u5b63\u8282\u6027\u5916\u89c2\u53d8\u5316\u548c\u98ce\u9a71\u52a8\u53f6\u7247\u8fd0\u52a8\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u6279\u91cf\u5149\u6805\u5316\u4e92\u8865\u89c6\u89d2\u6062\u590d\u88ab\u906e\u6321\u7684\u679c\u56ed\u7ed3\u6784\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u68af\u5ea6\u9a71\u52a8\u5730\u56fe\u751f\u547d\u5468\u671f\u5728\u5173\u952e\u5e27\u4e4b\u95f4\u6267\u884c\uff0c\u901a\u8fc7\u6982\u7387LiDAR\u6df1\u5ea6\u4e00\u81f4\u6027\u9879\u6307\u5bfc\u59ff\u6001\u4f18\u5316\u3002", "result": "\u5728\u82f9\u679c\u548c\u68a8\u56ed\u7684\u591a\u5b63\u8282\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u73b0\u67093DGS-SLAM\u57fa\u7ebf\u65b9\u6cd5\uff0cAgriGS-SLAM\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u3001\u66f4\u7a33\u5b9a\u7684\u91cd\u5efa\u548c\u66f4\u5e73\u7a33\u7684\u8f68\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u679c\u56ed\u76d1\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e5f\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u9700\u8981\u9c81\u68d2\u591a\u6a21\u6001\u611f\u77e5\u7684\u6237\u5916\u9886\u57df\u3002"}}
{"id": "2510.26362", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.26362", "abs": "https://arxiv.org/abs/2510.26362", "authors": ["Tobias L\u00f6w", "Cem Bilaloglu", "Sylvain Calinon"], "title": "Cooperative Task Spaces for Multi-Arm Manipulation Control based on Similarity Transformations", "comment": null, "summary": "Many tasks in human environments require collaborative behavior between\nmultiple kinematic chains, either to provide additional support for carrying\nbig and bulky objects or to enable the dexterity that is required for in-hand\nmanipulation. Since these complex systems often have a very high number of\ndegrees of freedom coordinating their movements is notoriously difficult to\nmodel. In this article, we present the derivation of the theoretical\nfoundations for cooperative task spaces of multi-arm robotic systems based on\ngeometric primitives defined using conformal geometric algebra. Based on the\nsimilarity transformations of these cooperative geometric primitives, we derive\nan abstraction of complex robotic systems that enables representing these\nsystems in a way that directly corresponds to single-arm systems. By deriving\nthe associated analytic and geometric Jacobian matrices, we then show the\nstraightforward integration of our approach into classical control techniques\nrooted in operational space control. We demonstrate this using bimanual\nmanipulators, humanoids and multi-fingered hands in optimal control experiments\nfor reaching desired geometric primitives and in teleoperation experiments\nusing differential kinematics control. We then discuss how the geometric\nprimitives naturally embed nullspace structures into the controllers that can\nbe exploited for introducing secondary control objectives. This work,\nrepresents the theoretical foundations of this cooperative manipulation control\nframework, and thus the experiments are presented in an abstract way, while\ngiving pointers towards potential future applications.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u5171\u5f62\u51e0\u4f55\u4ee3\u6570\u63d0\u51fa\u591a\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u534f\u4f5c\u4efb\u52a1\u7a7a\u95f4\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u590d\u6742\u7cfb\u7edf\u62bd\u8c61\u4e3a\u7c7b\u4f3c\u5355\u81c2\u7cfb\u7edf\uff0c\u4fbf\u4e8e\u96c6\u6210\u5230\u7ecf\u5178\u63a7\u5236\u65b9\u6cd5\u4e2d\u3002", "motivation": "\u4eba\u7c7b\u73af\u5883\u4e2d\u8bb8\u591a\u4efb\u52a1\u9700\u8981\u591a\u4e2a\u8fd0\u52a8\u94fe\u534f\u4f5c\uff0c\u5982\u642c\u8fd0\u5927\u7269\u4f53\u6216\u7075\u5de7\u64cd\u4f5c\uff0c\u4f46\u8fd9\u4e9b\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u7684\u8fd0\u52a8\u534f\u8c03\u5efa\u6a21\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u5171\u5f62\u51e0\u4f55\u4ee3\u6570\u5b9a\u4e49\u51e0\u4f55\u57fa\u5143\uff0c\u57fa\u4e8e\u5176\u76f8\u4f3c\u53d8\u6362\u63a8\u5bfc\u534f\u4f5c\u51e0\u4f55\u57fa\u5143\uff0c\u83b7\u5f97\u89e3\u6790\u548c\u51e0\u4f55\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u96c6\u6210\u5230\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\u4e2d\u3002", "result": "\u5728\u53cc\u624b\u673a\u5668\u4eba\u3001\u4eba\u5f62\u673a\u5668\u4eba\u548c\u591a\u6307\u624b\u7684\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff0c\u5305\u62ec\u6700\u4f18\u63a7\u5236\u8fbe\u5230\u671f\u671b\u51e0\u4f55\u57fa\u5143\u548c\u5fae\u5206\u8fd0\u52a8\u5b66\u9065\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u534f\u4f5c\u64cd\u4f5c\u63a7\u5236\u6846\u67b6\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u51e0\u4f55\u57fa\u5143\u81ea\u7136\u5d4c\u5165\u96f6\u7a7a\u95f4\u7ed3\u6784\u53ef\u7528\u4e8e\u6b21\u7ea7\u63a7\u5236\u76ee\u6807\uff0c\u4e3a\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2510.26406", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26406", "abs": "https://arxiv.org/abs/2510.26406", "authors": ["Guanxing Lu", "Rui Zhao", "Haitao Lin", "He Zhang", "Yansong Tang"], "title": "Human-in-the-loop Online Rejection Sampling for Robotic Manipulation", "comment": "8 pages", "summary": "Reinforcement learning (RL) is widely used to produce robust robotic\nmanipulation policies, but fine-tuning vision-language-action (VLA) models with\nRL can be unstable due to inaccurate value estimates and sparse supervision at\nintermediate steps. In contrast, imitation learning (IL) is easy to train but\noften underperforms due to its offline nature. In this paper, we propose\nHi-ORS, a simple yet effective post-training method that utilizes rejection\nsampling to achieve both training stability and high robustness. Hi-ORS\nstabilizes value estimation by filtering out negatively rewarded samples during\nonline fine-tuning, and adopts a reward-weighted supervised training objective\nto provide dense intermediate-step supervision. For systematic study, we\ndevelop an asynchronous inference-training framework that supports flexible\nonline human-in-the-loop corrections, which serve as explicit guidance for\nlearning error-recovery behaviors. Across three real-world tasks and two\nembodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich\nmanipulation in just 1.5 hours of real-world training, outperforming RL and IL\nbaselines by a substantial margin in both effectiveness and efficiency.\nNotably, the fine-tuned policy exhibits strong test-time scalability by\nreliably executing complex error-recovery behaviors to achieve better\nperformance.", "AI": {"tldr": "Hi-ORS\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u5b9e\u73b0\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u9ad8\u9c81\u68d2\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5728\u7ebf\u5fae\u8c03\u4e2d\u8fc7\u6ee4\u8d1f\u5956\u52b1\u6837\u672c\u6765\u7a33\u5b9a\u4ef7\u503c\u4f30\u8ba1\uff0c\u5e76\u91c7\u7528\u5956\u52b1\u52a0\u6743\u7684\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\u63d0\u4f9b\u5bc6\u96c6\u7684\u4e2d\u95f4\u6b65\u9aa4\u76d1\u7763\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7528\u4e8e\u751f\u6210\u9c81\u68d2\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u4f46\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u65f6\u53ef\u80fd\u56e0\u4e0d\u51c6\u786e\u7684\u4ef7\u503c\u4f30\u8ba1\u548c\u7a00\u758f\u7684\u4e2d\u95f4\u6b65\u9aa4\u76d1\u7763\u800c\u4e0d\u7a33\u5b9a\u3002\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u6613\u4e8e\u8bad\u7ec3\u4f46\u6027\u80fd\u8f83\u5dee\u3002", "method": "\u63d0\u51faHi-ORS\u65b9\u6cd5\uff0c\u4f7f\u7528\u62d2\u7edd\u91c7\u6837\u8fc7\u6ee4\u8d1f\u5956\u52b1\u6837\u672c\u7a33\u5b9a\u4ef7\u503c\u4f30\u8ba1\uff0c\u91c7\u7528\u5956\u52b1\u52a0\u6743\u7684\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\u63d0\u4f9b\u5bc6\u96c6\u76d1\u7763\u3002\u5f00\u53d1\u5f02\u6b65\u63a8\u7406-\u8bad\u7ec3\u6846\u67b6\u652f\u6301\u7075\u6d3b\u7684\u5728\u7ebf\u4eba\u673a\u4ea4\u4e92\u6821\u6b63\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u548c\u4e24\u4e2a\u5b9e\u4f53\u4e0a\uff0cHi-ORS\u4ec5\u75281.5\u5c0f\u65f6\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u5c31\u80fd\u5fae\u8c03pi-base\u7b56\u7565\u638c\u63e1\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\uff0c\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u5927\u5e45\u4f18\u4e8eRL\u548cIL\u57fa\u7ebf\u3002", "conclusion": "Hi-ORS\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u9ad8\u9c81\u68d2\u6027\uff0c\u5fae\u8c03\u540e\u7684\u7b56\u7565\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\uff0c\u80fd\u591f\u53ef\u9760\u6267\u884c\u590d\u6742\u7684\u9519\u8bef\u6062\u590d\u884c\u4e3a\u4ee5\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u3002"}}
{"id": "2510.26536", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26536", "abs": "https://arxiv.org/abs/2510.26536", "authors": ["Huajie Tan", "Cheng Chi", "Xiansheng Chen", "Yuheng Ji", "Zhongxia Zhao", "Xiaoshuai Hao", "Yaoxu Lyu", "Mingyu Cao", "Junkai Zhao", "Huaihai Lyu", "Enshen Zhou", "Ning Chen", "Yankai Fu", "Cheng Peng", "Wei Guo", "Dong Liang", "Zhuo Chen", "Mengsi Lyu", "Chenrui He", "Yulong Ao", "Yonghua Lin", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration", "comment": null, "summary": "The proliferation of collaborative robots across diverse tasks and\nembodiments presents a central challenge: achieving lifelong adaptability,\nscalable coordination, and robust scheduling in multi-agent systems. Existing\napproaches, from vision-language-action (VLA) models to hierarchical\nframeworks, fall short due to their reliance on limited or dividual-agent\nmemory. This fundamentally constrains their ability to learn over long\nhorizons, scale to heterogeneous teams, or recover from failures, highlighting\nthe need for a unified memory representation. To address these limitations, we\nintroduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable,\nand robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel\nSpatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene\ngeometry, temporal event history, and embodiment profiles into a shared\nrepresentation. This memory-centric design is integrated into a\nbrain-cerebellum framework, where a high-level brain model performs global\nplanning by retrieving and updating STEM, while low-level controllers execute\nactions locally. This closed loop between cognition, memory, and execution\nenables dynamic task allocation, fault-tolerant collaboration, and consistent\nstate synchronization. We conduct extensive experiments spanning complex\ncoordination tasks in restaurants, supermarkets, and households. Our results\ndemonstrate that RoboOS-NeXT achieves superior performance across heterogeneous\nembodiments, validating its effectiveness in enabling lifelong, scalable, and\nrobust multi-robot collaboration. Project website:\nhttps://flagopen.github.io/RoboOS/", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRoboOS-NeXT\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u7a7a-\u5177\u8eab\u8bb0\u5fc6(STEM)\u5b9e\u73b0\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7ec8\u8eab\u9002\u5e94\u3001\u53ef\u6269\u5c55\u534f\u8c03\u548c\u9c81\u68d2\u8c03\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u7684\u5355\u667a\u80fd\u4f53\u8bb0\u5fc6\uff0c\u65e0\u6cd5\u5b9e\u73b0\u957f\u671f\u5b66\u4e60\u3001\u5f02\u6784\u56e2\u961f\u6269\u5c55\u6216\u6545\u969c\u6062\u590d\uff0c\u9700\u8981\u7edf\u4e00\u7684\u8bb0\u5fc6\u8868\u793a\u6765\u89e3\u51b3\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5f15\u5165RoboOS-NeXT\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u65f6\u7a7a-\u5177\u8eab\u8bb0\u5fc6(STEM)\uff0c\u96c6\u6210\u7a7a\u95f4\u573a\u666f\u51e0\u4f55\u3001\u65f6\u95f4\u4e8b\u4ef6\u5386\u53f2\u548c\u5177\u8eab\u914d\u7f6e\u6587\u4ef6\u3002\u91c7\u7528\u5927\u8111-\u5c0f\u8111\u67b6\u6784\uff0c\u9ad8\u5c42\u5927\u8111\u6a21\u578b\u8fdb\u884c\u5168\u5c40\u89c4\u5212\uff0c\u4f4e\u5c42\u63a7\u5236\u5668\u672c\u5730\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u5728\u9910\u5385\u3001\u8d85\u5e02\u548c\u5bb6\u5ead\u7b49\u590d\u6742\u534f\u8c03\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRoboOS-NeXT\u5728\u5f02\u6784\u5177\u8eab\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "RoboOS-NeXT\u901a\u8fc7\u8bb0\u5fc6\u4e2d\u5fc3\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u7ec8\u8eab\u3001\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2510.26551", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26551", "abs": "https://arxiv.org/abs/2510.26551", "authors": ["Prathamesh Kothavale", "Sravani Boddepalli"], "title": "Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool Manipulation in Robotics", "comment": "10 pages, 5 figures. Demonstrates a reinforcement learning framework\n  for adaptive tool manipulation with variable-length extensions", "summary": "Conventional robots possess a limited understanding of their kinematics and\nare confined to preprogrammed tasks, hindering their ability to leverage tools\nefficiently. Driven by the essential components of tool usage - grasping the\ndesired outcome, selecting the most suitable tool, determining optimal tool\norientation, and executing precise manipulations - we introduce a pioneering\nframework. Our novel approach expands the capabilities of the robot's inverse\nkinematics solver, empowering it to acquire a sequential repertoire of actions\nusing tools of varying lengths. By integrating a simulation-learned action\ntrajectory with the tool, we showcase the practicality of transferring acquired\nskills from simulation to real-world scenarios through comprehensive\nexperimentation. Remarkably, our extended inverse kinematics solver\ndemonstrates an impressive error rate of less than 1 cm. Furthermore, our\ntrained policy achieves a mean error of 8 cm in simulation. Noteworthy, our\nmodel achieves virtually indistinguishable performance when employing two\ndistinct tools of different lengths. This research provides an indication of\npotential advances in the exploration of all four fundamental aspects of tool\nusage, enabling robots to master the intricate art of tool manipulation across\ndiverse tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u673a\u5668\u4eba\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u7684\u65b0\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5b66\u4e60\u4f7f\u7528\u4e0d\u540c\u957f\u5ea6\u5de5\u5177\u7684\u987a\u5e8f\u52a8\u4f5c\u5e93\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u6280\u80fd\u8fc1\u79fb\u5b9e\u73b0\u7cbe\u786e\u7684\u5de5\u5177\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u5bf9\u81ea\u8eab\u8fd0\u52a8\u5b66\u7406\u89e3\u6709\u9650\uff0c\u53ea\u80fd\u6267\u884c\u9884\u7f16\u7a0b\u4efb\u52a1\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5de5\u5177\u3002\u9700\u8981\u89e3\u51b3\u5de5\u5177\u4f7f\u7528\u7684\u56db\u4e2a\u5173\u952e\u8981\u7d20\uff1a\u7406\u89e3\u671f\u671b\u7ed3\u679c\u3001\u9009\u62e9\u5408\u9002\u5de5\u5177\u3001\u786e\u5b9a\u6700\u4f73\u5de5\u5177\u65b9\u5411\u3001\u6267\u884c\u7cbe\u786e\u64cd\u4f5c\u3002", "method": "\u6269\u5c55\u673a\u5668\u4eba\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\uff0c\u96c6\u6210\u4eff\u771f\u5b66\u4e60\u7684\u52a8\u4f5c\u8f68\u8ff9\u4e0e\u5de5\u5177\uff0c\u5b9e\u73b0\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u573a\u666f\u7684\u6280\u80fd\u8fc1\u79fb\u3002", "result": "\u6269\u5c55\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u8bef\u5dee\u5c0f\u4e8e1cm\uff0c\u8bad\u7ec3\u7b56\u7565\u5728\u4eff\u771f\u4e2d\u5e73\u5747\u8bef\u5dee8cm\uff0c\u4f7f\u7528\u4e24\u79cd\u4e0d\u540c\u957f\u5ea6\u5de5\u5177\u65f6\u6027\u80fd\u51e0\u4e4e\u65e0\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63a2\u7d22\u5de5\u5177\u4f7f\u7528\u7684\u56db\u4e2a\u57fa\u672c\u65b9\u9762\u63d0\u4f9b\u4e86\u6f5c\u5728\u8fdb\u5c55\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u638c\u63e1\u8de8\u4e0d\u540c\u4efb\u52a1\u7684\u590d\u6742\u5de5\u5177\u64cd\u4f5c\u827a\u672f\u3002"}}
{"id": "2510.26623", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26623", "abs": "https://arxiv.org/abs/2510.26623", "authors": ["Spencer Teetaert", "Sven Lilge", "Jessica Burgner-Kahrs", "Timothy D. Barfoot"], "title": "A Sliding-Window Filter for Online Continuous-Time Continuum Robot State Estimation", "comment": "8 pages, 6 figures. Submitted to IEEE-RAS International Conference on\n  Soft Robotics 2026", "summary": "Stochastic state estimation methods for continuum robots (CRs) often struggle\nto balance accuracy and computational efficiency. While several recent works\nhave explored sliding-window formulations for CRs, these methods are limited to\nsimplified, discrete-time approximations and do not provide stochastic\nrepresentations. In contrast, current stochastic filter methods must run at the\nspeed of measurements, limiting their full potential. Recent works in\ncontinuous-time estimation techniques for CRs show a principled approach to\naddressing this runtime constraint, but are currently restricted to offline\noperation. In this work, we present a sliding-window filter (SWF) for\ncontinuous-time state estimation of CRs that improves upon the accuracy of a\nfilter approach while enabling continuous-time methods to operate online, all\nwhile running at faster-than-real-time speeds. This represents the first\nstochastic SWF specifically designed for CRs, providing a promising direction\nfor future research in this area.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u8fde\u7eed\u65f6\u95f4\u72b6\u6001\u4f30\u8ba1\u7684\u6ed1\u52a8\u7a97\u53e3\u6ee4\u6ce2\u5668\uff0c\u5728\u4fdd\u6301\u6ee4\u6ce2\u5668\u65b9\u6cd5\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u5728\u7ebf\u64cd\u4f5c\uff0c\u4e14\u8fd0\u884c\u901f\u5ea6\u5feb\u4e8e\u5b9e\u65f6\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u968f\u673a\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\u4ec5\u9650\u4e8e\u7b80\u5316\u79bb\u6563\u65f6\u95f4\u8fd1\u4f3c\u4e14\u7f3a\u4e4f\u968f\u673a\u8868\u793a\uff0c\u800c\u968f\u673a\u6ee4\u6ce2\u5668\u65b9\u6cd5\u53d7\u9650\u4e8e\u6d4b\u91cf\u901f\u5ea6\u8fd0\u884c\uff0c\u8fde\u7eed\u65f6\u95f4\u4f30\u8ba1\u6280\u672f\u76ee\u524d\u4ec5\u9650\u4e8e\u79bb\u7ebf\u64cd\u4f5c\u3002", "method": "\u5f00\u53d1\u4e86\u4e13\u95e8\u9488\u5bf9\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7684\u968f\u673a\u6ed1\u52a8\u7a97\u53e3\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u8fde\u7eed\u65f6\u95f4\u72b6\u6001\u4f30\u8ba1\uff0c\u7ed3\u5408\u4e86\u6ed1\u52a8\u7a97\u53e3\u548c\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6ee4\u6ce2\u5668\u65b9\u6cd5\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u4f7f\u8fde\u7eed\u65f6\u95f4\u65b9\u6cd5\u80fd\u591f\u5728\u7ebf\u8fd0\u884c\uff0c\u4e14\u8fd0\u884c\u901f\u5ea6\u5feb\u4e8e\u5b9e\u65f6\u901f\u5ea6\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u968f\u673a\u6ed1\u52a8\u7a97\u53e3\u6ee4\u6ce2\u5668\uff0c\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2510.26646", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.26646", "abs": "https://arxiv.org/abs/2510.26646", "authors": ["Xiaoyi He", "Danggui Chen", "Zhenshuo Zhang", "Zimeng Bai"], "title": "Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments", "comment": "6 pages, 5 figures; ROS+Gazebo (TurtleBot3) implementation;\n  evaluation with PathBench metrics; code (primary):\n  https://github.com/MayaCHEN-github/HierarchicalRL-robot-navigation; mirror\n  (for reproducibility): https://github.com/ShowyHe/DRL-robot-navigation", "summary": "This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8def\u5f84\u89c4\u5212\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u5c42DQN\u8fdb\u884c\u79bb\u6563\u5b50\u76ee\u6807\u9009\u62e9\u4e0e\u5e95\u5c42TD3\u63a7\u5236\u5668\u8fdb\u884c\u8fde\u7eed\u6267\u884c\uff0c\u5728\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6210\u529f\u7387\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u5355\u4e00\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u8def\u5f84\u89c4\u5212\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u7ed3\u5408\u79bb\u6563\u51b3\u7b56\u548c\u8fde\u7eed\u63a7\u5236\u7684\u4f18\u52bf\uff0c\u63d0\u9ad8\u5728\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u9ad8\u5c42Deep Q-Network\u8fdb\u884c\u884c\u4e3a\u9009\u62e9\u548c\u5b50\u76ee\u6807\u51b3\u7b56\uff0c\u5e95\u5c42Twin Delayed Deep Deterministic Policy Gradient\u63a7\u5236\u5668\u6267\u884c\u8fde\u7eed\u901f\u5ea6\u547d\u4ee4\uff0c\u8bbe\u8ba1\u4e86\u5305\u542b\u65b9\u5411\u3001\u8ddd\u79bb\u3001\u907f\u969c\u3001\u52a8\u4f5c\u5e73\u6ed1\u5ea6\u7b49\u8981\u7d20\u7684\u5956\u52b1\u673a\u5236\uff0c\u5e76\u96c6\u6210\u4e86LiDAR\u5b89\u5168\u95e8\u9632\u6b62\u4e0d\u5b89\u5168\u8fd0\u52a8\u3002", "result": "\u5728ROS + Gazebo\u73af\u5883\u4e2d\u4f7f\u7528PathBench\u6307\u6807\u8bc4\u4f30\uff0c\u76f8\u6bd4\u5355\u4e00\u7b97\u6cd5\u57fa\u51c6\uff08\u5355\u72ecDQN\u6216TD3\uff09\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u89c4\u5212\u5668\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3001\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\uff0c\u5bf9\u672a\u89c1\u8fc7\u7684\u969c\u788d\u7269\u914d\u7f6e\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u51cf\u5c11\u4e86\u7a81\u7136\u7684\u63a7\u5236\u53d8\u5316\u3002", "conclusion": "\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u5355\u4e00\u7b97\u6cd5\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u79bb\u6563\u51b3\u7b56\u548c\u8fde\u7eed\u63a7\u5236\u7684\u4f18\u52bf\uff0c\u4e3a\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.26670", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26670", "abs": "https://arxiv.org/abs/2510.26670", "authors": ["Qianyou Zhao", "Yuliang Shen", "Xuanran Zhai", "Ce Hao", "Duidi Wu", "Jin Qi", "Jie Hu", "Qiaojun Yu"], "title": "Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and Real-Time Efficiency in Robotic Manipulation", "comment": null, "summary": "In visuomotor policy learning, diffusion-based imitation learning has become\nwidely adopted for its ability to capture diverse behaviors. However,\napproaches built on ordinary and stochastic denoising processes struggle to\njointly achieve fast sampling and strong multi-modality. To address these\nchallenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short\nstochastic prefix up to an adaptive switch time, and then applies a one-step\nconsistency jump to produce the final action. To align this one-jump\ngeneration, HCP performs time-varying consistency distillation that combines a\ntrajectory-consistency objective to keep neighboring predictions coherent and a\ndenoising-matching objective to improve local fidelity. In both simulation and\non a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step\nDDPM teacher in accuracy and mode coverage while significantly reducing\nlatency. These results show that multi-modality does not require slow\ninference, and a switch time decouples mode retention from speed. It yields a\npractical accuracy efficiency trade-off for robot policies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6df7\u5408\u4e00\u81f4\u6027\u7b56\u7565\uff08HCP\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u968f\u673a\u524d\u7f00\u548c\u4e00\u6b65\u4e00\u81f4\u6027\u8df3\u8dc3\uff0c\u5728\u4fdd\u6301\u591a\u6a21\u6001\u884c\u4e3a\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u6269\u6563\u7b56\u7565\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u6355\u6349\u591a\u6837\u5316\u884c\u4e3a\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u91c7\u6837\u548c\u5f3a\u591a\u6a21\u6001\u7279\u6027\u3002", "method": "HCP\u8fd0\u884c\u4e00\u4e2a\u77ed\u968f\u673a\u524d\u7f00\u5230\u81ea\u9002\u5e94\u5207\u6362\u65f6\u95f4\uff0c\u7136\u540e\u5e94\u7528\u4e00\u6b65\u4e00\u81f4\u6027\u8df3\u8dc3\u751f\u6210\u6700\u7ec8\u52a8\u4f5c\uff0c\u91c7\u7528\u65f6\u53d8\u4e00\u81f4\u6027\u84b8\u998f\u7ed3\u5408\u8f68\u8ff9\u4e00\u81f4\u6027\u76ee\u6807\u548c\u53bb\u566a\u5339\u914d\u76ee\u6807\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\uff0cHCP\u4ec5\u970025\u6b65SDE\u52a0\u4e00\u6b21\u8df3\u8dc3\u5373\u53ef\u63a5\u8fd180\u6b65DDPM\u6559\u5e08\u7684\u51c6\u786e\u6027\u548c\u6a21\u6001\u8986\u76d6\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "\u591a\u6a21\u6001\u4e0d\u9700\u8981\u6162\u63a8\u7406\uff0c\u5207\u6362\u65f6\u95f4\u5c06\u6a21\u6001\u4fdd\u6301\u4e0e\u901f\u5ea6\u89e3\u8026\uff0c\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u51c6\u786e\u6027\u4e0e\u6548\u7387\u6743\u8861\u3002"}}
{"id": "2510.26742", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26742", "abs": "https://arxiv.org/abs/2510.26742", "authors": ["Yunchao Ma", "Yizhuang Zhou", "Yunhuan Yang", "Tiancai Wang", "Haoqiang Fan"], "title": "Running VLAs at Real-time Speed", "comment": "Code is available at https://github.com/Dexmal/realtime-vla", "summary": "In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate\nand at most 480Hz trajectory frequency using a single consumer GPU. This\nenables dynamic and real-time tasks that were previously believed to be\nunattainable by large VLA models. To achieve it, we introduce a bag of\nstrategies to eliminate the overheads in model inference. The real-world\nexperiment shows that the pi0 policy with our strategy achieves a 100% success\nrate in grasping a falling pen task. Based on the results, we further propose a\nfull streaming inference framework for real-time robot control of VLA. Code is\navailable at https://github.com/Dexmal/realtime-vla.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5728\u5355\u4e2a\u6d88\u8d39\u7ea7GPU\u4e0a\u4ee530Hz\u5e27\u7387\u548c\u6700\u9ad8480Hz\u8f68\u8ff9\u9891\u7387\u8fd0\u884cpi0\u7ea7\u591a\u89c6\u89d2VLA\uff0c\u5b9e\u73b0\u4e86\u4e4b\u524d\u8ba4\u4e3a\u5927\u578bVLA\u6a21\u578b\u65e0\u6cd5\u5b8c\u6210\u7684\u52a8\u6001\u5b9e\u65f6\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4f7f\u52a8\u6001\u5b9e\u65f6\u4efb\u52a1\u6210\u4e3a\u53ef\u80fd\u3002", "method": "\u5f15\u5165\u4e00\u7cfb\u5217\u7b56\u7565\u6765\u6d88\u9664\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5f00\u9500\uff0c\u5e76\u63d0\u51fa\u4e86\u5b8c\u6574\u7684\u6d41\u5f0f\u63a8\u7406\u6846\u67b6\u3002", "result": "\u5728\u6293\u53d6\u4e0b\u843d\u7b14\u7684\u4efb\u52a1\u4e2d\uff0c\u91c7\u7528\u672c\u6587\u7b56\u7565\u7684pi0\u7b56\u7565\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4f7fVLA\u6a21\u578b\u80fd\u591f\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4e3a\u52a8\u6001\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
