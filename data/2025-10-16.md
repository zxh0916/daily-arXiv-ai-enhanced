<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Learning to Grasp Anything by Playing with Random Toys](https://arxiv.org/abs/2510.12866)
*Dantong Niu,Yuvan Sharma,Baifeng Shi,Rachel Ding,Matteo Gioia,Haoru Xue,Henry Tsai,Konstantinos Kallidromitis,Anirudh Pai,Shankar Shastry,Trevor Darrell,Jitendra Malik,Roei Herzig*

Main category: cs.RO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Robotic manipulation policies often struggle to generalize to novel objects,
limiting their real-world utility. In contrast, cognitive science suggests that
children develop generalizable dexterous manipulation skills by mastering a
small set of simple toys and then applying that knowledge to more complex
items. Inspired by this, we study if similar generalization capabilities can
also be achieved by robots. Our results indicate robots can learn generalizable
grasping using randomly assembled objects that are composed from just four
shape primitives: spheres, cuboids, cylinders, and rings. We show that training
on these "toys" enables robust generalization to real-world objects, yielding
strong zero-shot performance. Crucially, we find the key to this generalization
is an object-centric visual representation induced by our proposed detection
pooling mechanism. Evaluated in both simulation and on physical robots, our
model achieves a 67% real-world grasping success rate on the YCB dataset,
outperforming state-of-the-art approaches that rely on substantially more
in-domain data. We further study how zero-shot generalization performance
scales by varying the number and diversity of training toys and the
demonstrations per toy. We believe this work offers a promising path to
scalable and generalizable learning in robotic manipulation. Demonstration
videos, code, checkpoints and our dataset are available on our project page:
https://lego-grasp.github.io/ .

</details>


### [2] [Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation](https://arxiv.org/abs/2510.12919)
*Mouhyemen Khan,Tatsuya Ibuki,Abhijit Chatterjee*

Main category: cs.RO

TL;DR: 提出了一种统一框架，将高斯过程隐式表面（GPIS）作为控制屏障函数（CBF），用于机器人安全控制。通过稀疏化解决计算复杂度问题，并在机械臂和四旋翼无人机上验证了碰撞避免效果。


<details>
  <summary>Details</summary>
Motivation: 结合水平集方法和控制屏障函数的安全控制技术，利用高斯过程隐式表面来表示安全边界，提供不确定性估计和鲁棒安全边界。

Method: 使用高斯过程隐式表面（GPIS）表示安全边界，通过传感器测量数据训练GP模型。GP后验均值定义安全表面，后验方差提供安全边界。开发稀疏高斯CBF解决计算复杂度问题。

Result: 在7自由度机械臂操作斯坦福兔子和四旋翼无人机绕物理椅子导航的实验中，高斯CBF（包括稀疏版本）实现了安全交互和碰撞避免，能够执行原本会与物体相交的轨迹。

Conclusion: 高斯过程隐式表面作为控制屏障函数是一种有效的安全控制方法，能够提供不确定性感知的安全边界，稀疏化方案解决了计算复杂度问题，在机器人安全导航中表现良好。

Abstract: Level set methods underpin modern safety techniques such as control barrier
functions (CBFs), while also serving as implicit surface representations for
geometric shapes via distance fields. Inspired by these two paradigms, we
propose a unified framework where the implicit surface itself acts as a CBF. We
leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety
boundaries, using safety samples which are derived from sensor measurements to
condition the GP. The GP posterior mean defines the implicit safety surface
(safety belief), while the posterior variance provides a robust safety margin.
Although GPs have favorable properties such as uncertainty estimation and
analytical tractability, they scale cubically with data. To alleviate this
issue, we develop a sparse solution called sparse Gaussian CBFs. To the best of
our knowledge, GPIS have not been explicitly used to synthesize CBFs. We
validate the approach on collision avoidance tasks in two settings: a simulated
7-DOF manipulator operating around the Stanford bunny, and a quadrotor
navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with
and without sparsity) enable safe interaction and collision-free execution of
trajectories that would otherwise intersect the objects.

</details>


### [3] [Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance](https://arxiv.org/abs/2510.12924)
*Pavel Pochobradský,Ondřej Procházka,Robert Pěnička,Vojtěch Vonásek,Martin Saska*

Main category: cs.RO

TL;DR: GMPPI是一种基于采样的控制器，结合几何SE(3)控制和动态参数调整，能够在跟踪敏捷轨迹的同时进行在线避障，在仿真和真实环境中都表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人机在复杂环境中高速飞行时既要精确跟踪敏捷轨迹又要实时避障的挑战，现有方法在低速平滑轨迹跟踪和高速避障性能方面存在不足。

Method: 提出几何模型预测路径积分(GMPPI)控制器，使用SE(3)几何控制生成部分轨迹，引入变化的仿真时间步长和动态成本噪声参数，并与立体深度相机集成实现在线避障。

Result: 在仿真中，GMPPI能够以与几何SE(3)控制器相似的精度跟踪敏捷轨迹，同时在模拟森林环境中以13m/s的速度避障，优于现有最佳避障规划器。在真实实验中，能够在10m/s速度下跟踪敏捷轨迹并避障。

Conclusion: GMPPI控制器成功实现了在高速飞行中同时保持精确轨迹跟踪和有效避障的能力，是向复杂环境中自主无人机飞行迈出的重要一步。

Abstract: In this letter, we introduce Geometric Model Predictive Path Integral
(GMPPI), a sampling-based controller capable of tracking agile trajectories
while avoiding obstacles. In each iteration, GMPPI generates a large number of
candidate rollout trajectories and then averages them to create a nominal
control to be followed by the Unmanned Aerial Vehicle (UAV). We propose using
geometric SE(3) control to generate part of the rollout trajectories,
significantly increasing precision in agile flight. Furthermore, we introduce
varying rollout simulation time step length and dynamic cost and noise
parameters, vastly improving tracking performance of smooth and low-speed
trajectories over an existing Model Predictive Path Integral (MPPI)
implementation. Finally, we propose an integration of GMPPI with a stereo depth
camera, enabling online obstacle avoidance at high speeds, a crucial step
towards autonomous UAV flights in complex environments. The proposed controller
can track simulated agile reference trajectories with position error similar to
the geometric SE(3) controller. However, the same configuration of the proposed
controller can avoid obstacles in a simulated forest environment at speeds of
up to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware
planner. In real-world experiments, GMPPI retains the capability to track agile
trajectories and avoids obstacles at speeds of up to 10m/s.

</details>


### [4] [Enhancing Sampling-based Planning with a Library of Paths](https://arxiv.org/abs/2510.12962)
*Michal Minařík,Vojtěch Vonásek,Robert Pěnička*

Main category: cs.RO

TL;DR: 提出了一种基于历史路径库的3D物体路径规划方法，通过重用过去规划经验来加速新物体的路径规划，特别针对窄通道场景。


<details>
  <summary>Details</summary>
Motivation: 传统的采样规划器在窄通道场景中效率低下，且每次规划都从头开始，无法利用历史经验。在机器人应用中，多个不同物体需要经过相同环境，重用历史路径可以显著提高规划效率。

Method: 构建历史路径库存储已规划物体的路径，当规划新物体时，在库中寻找最相似物体并使用其路径作为近似解，通过相互变换调整后沿近似路径采样配置空间。

Result: 在多种窄通道场景中测试，相比OMPL库中的先进方法，规划时间最多减少85%，且在传统规划器失败的情况下仍能找到解。

Conclusion: 基于历史路径重用的方法能显著提高3D物体路径规划效率，特别是在窄通道场景中，且已开源实现。

Abstract: Path planning for 3D solid objects is a challenging problem, requiring a
search in a six-dimensional configuration space, which is, nevertheless,
essential in many robotic applications such as bin-picking and assembly. The
commonly used sampling-based planners, such as Rapidly-exploring Random Trees,
struggle with narrow passages where the sampling probability is low, increasing
the time needed to find a solution. In scenarios like robotic bin-picking,
various objects must be transported through the same environment. However,
traditional planners start from scratch each time, losing valuable information
gained during the planning process. We address this by using a library of past
solutions, allowing the reuse of previous experiences even when planning for a
new, previously unseen object. Paths for a set of objects are stored, and when
planning for a new object, we find the most similar one in the library and use
its paths as approximate solutions, adjusting for possible mutual
transformations. The configuration space is then sampled along the approximate
paths. Our method is tested in various narrow passage scenarios and compared
with state-of-the-art methods from the OMPL library. Results show significant
speed improvements (up to 85% decrease in the required time) of our method,
often finding a solution in cases where the other planners fail. Our
implementation of the proposed method is released as an open-source package.

</details>


### [5] [The Omega Turn: A General Turning Template for Elongate Robots](https://arxiv.org/abs/2510.12970)
*Baxi Chong,Tianyu Wang,Kelimar Diaz,Christopher J. Pierce,Eva Erickson,Julian Whitman,Yuelin Deng,Esteban Flores,Ruijie Fu,Juntao He,Jianfeng Lin,Hang Lu,Guillaume Sartoretti,Howie Choset,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 该研究从线虫C. elegans的omega转弯行为中获得灵感，提出了将omega转弯描述为两个行波叠加的波方程，并开发了适用于无肢和多肢细长机器人的控制器，在实验室和复杂环境中实现了稳健有效的转弯性能。


<details>
  <summary>Details</summary>
Motivation: 细长无肢机器人在紧密空间（如搜救和工业检查）中具有应用潜力，但现有研究对这类系统的转弯策略关注有限。为实现在杂乱空间中的有效稳健转弯性能，研究者从线虫C. elegans的卓越机动性中获得启发。

Method: 采用理论-生物学比较方法，将omega转弯描述为两个行波的叠加。基于波方程设计控制器，并在实验室和复杂现场环境中测试细长机器人的转弯行为。

Result: 成功开发出能够实现稳健有效转弯行为的控制器，该控制器不仅适用于无肢细长机器人，还能推广到多肢细长机器人，展示了一种替代性的身体驱动转弯策略。

Conclusion: 基于波方程的omega转弯控制器为细长机器人（无论有无肢体）提供了一种有效的身体驱动转弯策略，在复杂环境中表现出良好的性能。

Abstract: Elongate limbless robots have the potential to locomote through tightly
packed spaces for applications such as search-and-rescue and industrial
inspections. The capability to effectively and robustly maneuver elongate
limbless robots is crucial to realize such potential. However, there has been
limited research on turning strategies for such systems. To achieve effective
and robust turning performance in cluttered spaces, we take inspiration from a
microscopic nematode, C. elegans, which exhibits remarkable maneuverability in
rheologically complex environments partially because of its ability to perform
omega turns. Despite recent efforts to analyze omega turn kinematics, it
remains unknown if there exists a wave equation sufficient to prescribe an
omega turn, let alone its reconstruction on robot platforms. Here, using a
comparative theory-biology approach, we prescribe the omega turn as a
superposition of two traveling waves. With wave equations as a guideline, we
design a controller for limbless robots enabling robust and effective turning
behaviors in lab and cluttered field environments. Finally, we show that such
omega turn controllers can also generalize to elongate multi-legged robots,
demonstrating an alternative effective body-driven turning strategy for
elongate robots, with and without limbs.

</details>


### [6] [Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation](https://arxiv.org/abs/2510.12971)
*Anran Zhang,Hanzhi Chen,Yannick Burkhardt,Yao Zhong,Johannes Betz,Helen Oleynikova,Stefan Leutenegger*

Main category: cs.RO

TL;DR: Actron3D是一个从少量单目、未标定、仅RGB的人类视频中学习可迁移6自由度操作技能的机器人框架，核心是神经功能函数表示，通过粗到细优化实现精确操作策略迁移。


<details>
  <summary>Details</summary>
Motivation: 从少量未标定的人类视频中学习可迁移的6自由度操作技能，解决传统方法需要大量标注数据或复杂传感器设置的问题。

Method: 提出神经功能函数作为紧凑的对象中心表示，从多样化未标定视频中提取几何、视觉外观和功能信息；部署时通过检索相关功能函数和粗到细优化实现策略迁移。

Result: 在仿真和真实世界实验中，Actron3D显著优于现有方法，在13个任务上平均成功率提升14.9个百分点，每个任务仅需2-3个演示视频。

Conclusion: Actron3D证明了从少量未标定人类视频中学习可迁移6自由度操作技能的可行性，为机器人技能学习提供了高效的新途径。

Abstract: We present Actron3D, a framework that enables robots to acquire transferable
6-DoF manipulation skills from just a few monocular, uncalibrated, RGB-only
human videos. At its core lies the Neural Affordance Function, a compact
object-centric representation that distills actionable cues from diverse
uncalibrated videos-geometry, visual appearance, and affordance-into a
lightweight neural network, forming a memory bank of manipulation skills.
During deployment, we adopt a pipeline that retrieves relevant affordance
functions and transfers precise 6-DoF manipulation policies via coarse-to-fine
optimization, enabled by continuous queries to the multimodal features encoded
in the neural functions. Experiments in both simulation and the real world
demonstrate that Actron3D significantly outperforms prior methods, achieving a
14.9 percentage point improvement in average success rate across 13 tasks while
requiring only 2-3 demonstration videos per task.

</details>


### [7] [UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles](https://arxiv.org/abs/2510.12992)
*Neel P. Bhatt,Po-han Li,Kushagra Gupta,Rohan Siva,Daniel Milan,Alexander T. Hogue,Sandeep P. Chinchali,David Fridovich-Keil,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: UNCAP是一种基于视觉语言模型的协同自动驾驶规划方法，通过轻量级自然语言消息进行车辆间通信，并明确考虑感知不确定性，显著降低通信带宽同时提高驾驶安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖传输高带宽原始传感器数据流，要么忽视共享数据中的感知和规划不确定性，导致系统既不可扩展也不安全。

Method: 采用两阶段通信协议：首先识别最相关的信息交换车辆子集，然后选定的CAV传输量化表达其感知不确定性的消息，通过选择性融合最大化互信息的消息来整合最相关信号。

Result: 实验显示通信带宽减少63%，驾驶安全评分提高31%，决策不确定性降低61%，近碰撞事件中的碰撞距离裕度增加四倍。

Conclusion: UNCAP通过自然语言通信和显式处理感知不确定性，实现了可扩展且安全的协同自动驾驶规划。

Abstract: Safe large-scale coordination of multiple cooperative connected autonomous
vehicles (CAVs) hinges on communication that is both efficient and
interpretable. Existing approaches either rely on transmitting high-bandwidth
raw sensor data streams or neglect perception and planning uncertainties
inherent in shared data, resulting in systems that are neither scalable nor
safe. To address these limitations, we propose Uncertainty-Guided Natural
Language Cooperative Autonomous Planning (UNCAP), a vision-language model-based
planning approach that enables CAVs to communicate via lightweight natural
language messages while explicitly accounting for perception uncertainty in
decision-making. UNCAP features a two-stage communication protocol: (i) an ego
CAV first identifies the subset of vehicles most relevant for information
exchange, and (ii) the selected CAVs then transmit messages that quantitatively
express their perception uncertainty. By selectively fusing messages that
maximize mutual information, this strategy allows the ego vehicle to integrate
only the most relevant signals into its decision-making, improving both the
scalability and reliability of cooperative planning. Experiments across diverse
driving scenarios show a 63% reduction in communication bandwidth with a 31%
increase in driving safety score, a 61% reduction in decision uncertainty, and
a four-fold increase in collision distance margin during near-miss events.
Project website: https://uncap-project.github.io/

</details>


### [8] [Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations](https://arxiv.org/abs/2510.13005)
*Robert Muldrow,Channing Ludden,Christopher Petersen*

Main category: cs.RO

TL;DR: 本文提出了一种用于模拟在轨服务、组装和制造(ISAM)操作的新型硬件在环实验测试平台，通过将6自由度UR3e机械臂安装在卫星总线上，结合1自由度导轨系统来模拟空间运动、串联机器人操作和接触力学。


<details>
  <summary>Details</summary>
Motivation: 在轨服务、组装和制造(ISAM)操作对提高空间资产的寿命、容量、机动性和可扩展性具有重要意义，但机械臂在自由飞行卫星上的运动会产生复杂的扰动力和运动，这构成了一个复杂的控制问题，需要进一步研究。

Method: 设计并开发了一个硬件在环(HIL)实验测试平台，使用6自由度UR3e机械臂安装在卫星总线上，该卫星总线安装在1自由度导轨系统上，使卫星总线和机械臂能够在一个线性方向上自由移动。

Result: 该实验ISAM仿真系统将用于探索和验证空间运动、串联机器人操作和接触力学的模型。

Conclusion: 该测试平台解决了在空间环境中实验测试和验证动力学模型的挑战，为ISAM操作的研究提供了有效的实验手段。

Abstract: In-Space Servicing, Assembly, and Manufacturing (ISAM) is a set of emerging
operations that provides several benefits to improve the longevity, capacity,
mo- bility, and expandability of existing and future space assets. Serial
robotic ma- nipulators are particularly vital in accomplishing ISAM operations,
however, the complex perturbation forces and motions associated with movement
of a robotic arm on a free-flying satellite presents a complex controls problem
requiring addi- tional study. While many dynamical models are developed,
experimentally test- ing and validating these models is challenging given that
the models operate in space, where satellites have six-degrees-of-freedom
(6-DOF). This paper attempts to resolve those challenges by presenting the
design and development of a new hardware-in-the-loop (HIL) experimental testbed
utilized to emulate ISAM. This emulation will be accomplished by means of a
6-DOF UR3e robotic arm attached to a satellite bus. This satellite bus is
mounted to a 1-DOF guide-rail system, en- abling the satellite bus and robotic
arm to move freely in one linear direction. This experimental ISAM emulation
system will explore and validate models for space motion, serial robot
manipulation, and contact mechanics.

</details>


### [9] [Kinematic Kitbashing for Modeling Functional Articulated Objects](https://arxiv.org/abs/2510.13048)
*Minghao Guo,Victor Zordan,Sheldon Andrews,Wojciech Matusik,Maneesh Agrawala,Hsueh-Ti Derek Liu*

Main category: cs.RO

TL;DR: Kinematic Kitbashing是一个自动框架，通过重用现有模型的零件来合成功能感知的铰接对象。该框架通过优化零件空间布局，确保几何连接的合理性并满足用户指定的功能目标。


<details>
  <summary>Details</summary>
Motivation: 旨在将基于零件的形状建模与功能装配设计相结合，实现交互式铰接资产的快速创建。

Method: 使用运动学感知的附着能量对齐多个铰接快照中的向量距离函数特征，并将其嵌入退火的黎曼朗之万动力学采样器中，处理不可微分的功能目标和约束。

Result: 生成了广泛的铰接形状，从垃圾桶轮子到多段灯具、齿轮驱动的划桨器和可重构家具，在几何、运动学和功能指标上显著优于现有基线方法。

Conclusion: 通过紧密耦合铰接感知的几何匹配与功能驱动的优化，Kinematic Kitbashing成功连接了基于零件的形状建模和功能装配设计。

Abstract: We introduce Kinematic Kitbashing, an automatic framework that synthesizes
functionality-aware articulated objects by reusing parts from existing models.
Given a kinematic graph with a small collection of articulated parts, our
optimizer jointly solves for the spatial placement of every part so that (i)
attachments remain geometrically sound over the entire range of motion and (ii)
the assembled object satisfies user-specified functional goals such as
collision-free actuation, reachability, or trajectory following. At its core is
a kinematics-aware attachment energy that aligns vector distance function
features sampled across multiple articulation snapshots. We embed this
attachment term within an annealed Riemannian Langevin dynamics sampler that
treats functionality objectives as additional energies, enabling robust global
exploration while accommodating non-differentiable functionality objectives and
constraints. Our framework produces a wide spectrum of assembled articulated
shapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,
gear-driven paddlers, and reconfigurable furniture, and delivers strong
quantitative improvements over state-of-the-art baselines across geometric,
kinematic, and functional metrics. By tightly coupling articulation-aware
geometry matching with functionality-driven optimization, Kinematic Kitbashing
bridges part-based shape modeling and functional assembly design, empowering
rapid creation of interactive articulated assets.

</details>


### [10] [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://arxiv.org/abs/2510.13054)
*Ankit Goyal,Hugo Hadfield,Xuning Yang,Valts Blukis,Fabio Ramos*

Main category: cs.RO

TL;DR: VLA-0展示了将动作直接表示为文本的简单方法在视觉-语言-动作模型中具有强大性能，超越了更复杂的模型设计。


<details>
  <summary>Details</summary>
Motivation: 当前构建视觉-语言-动作模型的方法往往过于复杂，而将动作直接表示为文本这一最简单策略却未被充分探索。

Method: 提出VLA-0模型，采用将动作直接表示为文本的简单设计，通过特定技术解锁其高性能。

Result: 在LIBERO基准测试中，VLA-0超越了所有现有方法，包括π0.5-KI、OpenVLA-OFT和SmolVLA等，在真实世界中也优于经过大规模预训练的SmolVLA。

Conclusion: 将动作表示为文本的简单VLA设计不仅有效，而且出人意料地强大，为构建通用机器人操作模型提供了更简洁的路径。

Abstract: Vision-Language-Action models (VLAs) hold immense promise for enabling
generalist robot manipulation. However, the best way to build them remains an
open question. Current approaches often add complexity, such as modifying the
existing vocabulary of a Vision-Language Model (VLM) with action tokens or
introducing special action heads. Curiously, the simplest strategy of
representing actions directly as text has remained largely unexplored. This
work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only
effective; it is surprisingly powerful. With the right design, VLA-0
outperforms more involved models. On LIBERO, a popular benchmark for evaluating
VLAs, VLA-0 outperforms all existing methods trained on the same robotic data,
including $\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without
large-scale robotics-specific training, it outperforms methods trained on
large-scale robotic data, like $\pi_0.5$-KI, $\pi_0$, GR00T-N1 and MolmoAct.
These findings also translate to the real world, where VLA-0 outperforms
SmolVLA, a VLA model pre-trained on large-scale real data. This paper
summarizes our unexpected findings and spells out the specific techniques
required to unlock the high performance of this simple yet potent VLA design.
Visual results, code, and trained models are provided here:
https://vla0.github.io/.

</details>


### [11] [RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation](https://arxiv.org/abs/2510.13149)
*Yangtao Chen,Zixuan Chen,Nga Teng Chan,Junting Chen,Junhui Yin,Jieqi Shi,Yang Gao,Yong-Lu Li,Jing Huo*

Main category: cs.RO

TL;DR: 提出了RoboHiMan层次化评估范式，用于系统研究长时程操作中的组合泛化能力，包括HiMan-Bench基准测试、多级训练数据集和三种评估范式，揭示了现有模型在技能组合和鲁棒性方面的能力差距。


<details>
  <summary>Details</summary>
Motivation: 现有机器人技能调度和组合方法在面临复杂扰动时表现有限，端到端VLA模型泛化能力不足，层次化方法在技能组合方面仍有局限，且现有基准主要关注任务完成而非组合泛化、鲁棒性及规划与执行的交互。

Method: 提出RoboHiMan层次化评估范式，包含HiMan-Bench基准（原子和组合任务、多样化扰动）、多级训练数据集用于渐进数据缩放分析，以及三种评估范式（vanilla、decoupled、coupled）来探究技能组合的必要性和层次架构瓶颈。

Result: 实验揭示了代表性模型和架构在能力上的明显差距，特别是在技能组合和应对扰动方面存在不足。

Conclusion: 研究结果指出了推进更适合现实世界长时程操作任务的模型发展方向，强调了在组合泛化和鲁棒性方面需要进一步改进。

Abstract: Enabling robots to flexibly schedule and compose learned skills for novel
long-horizon manipulation under diverse perturbations remains a core challenge.
Early explorations with end-to-end VLA models show limited success, as these
models struggle to generalize beyond the training distribution. Hierarchical
approaches, where high-level planners generate subgoals for low-level policies,
bring certain improvements but still suffer under complex perturbations,
revealing limited capability in skill composition. However, existing benchmarks
primarily emphasize task completion in long-horizon settings, offering little
insight into compositional generalization, robustness, and the interplay
between planning and execution. To systematically investigate these gaps, we
propose RoboHiMan, a hierarchical evaluation paradigm for compositional
generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,
a benchmark of atomic and compositional tasks under diverse perturbations,
supported by a multi-level training dataset for analyzing progressive data
scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)
that probe the necessity of skill composition and reveal bottlenecks in
hierarchical architectures. Experiments highlight clear capability gaps across
representative models and architectures, pointing to directions for advancing
models better suited to real-world long-horizon manipulation tasks. Videos and
open-source code can be found on our project website:
https://chenyt31.github.io/robo-himan.github.io/.

</details>


### [12] [ALOHA2 Robot Kitchen Application Scenario Reproduction Report](https://arxiv.org/abs/2510.13284)
*Haoyang Wu,Siheng Wu,William X. Liu,Fangui Zeng*

Main category: cs.RO

TL;DR: ALOHA2是ALOHA双臂遥操作机器人的增强版本，具有更高性能和鲁棒性，同时更符合人体工程学设计。它包含两个夹爪、两个ViperX 6自由度机械臂和两个较小的WidowX机械臂，通过反向驱动实现遥操作控制，并配备多视角摄像头用于RGB数据采集。


<details>
  <summary>Details</summary>
Motivation: 开发一个性能更高、鲁棒性更强且更符合人体工程学的双臂遥操作机器人系统，以改进原始ALOHA设计，提供更好的用户体验和数据采集能力。

Method: 采用双夹爪、双ViperX 6自由度机械臂和双WidowX机械臂的组合设计，通过反向驱动机制实现遥操作控制，集成多视角摄像头系统，并安装在带有铝制框架的桌面上，提供额外的摄像头安装点和重力补偿系统。

Result: 成功开发出ALOHA2系统，相比原始ALOHA具有更高的性能、更好的鲁棒性和改进的人体工程学设计，能够有效收集多视角RGB数据。

Conclusion: ALOHA2作为ALOHA的增强版本，在性能、鲁棒性和人体工程学方面都有显著改进，为遥操作机器人研究提供了更先进的平台。

Abstract: ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,
featuring higher performance and robustness compared to the original design,
while also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers
and two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control
the follower mechanical arms by operating the leader mechanical arms through
back-driving. The device also includes cameras that generate images from
multiple viewpoints, allowing for RGB data collection during teleoperation. The
robot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame
that provides additional mounting points for cameras and gravity compensation
systems.

</details>


### [13] [DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping](https://arxiv.org/abs/2510.13287)
*Nishant Chandna,Akshat Kaushal*

Main category: cs.RO

TL;DR: 提出了一种新颖的退化感知多度量LiDAR里程计和建图模块DAMM-LOAM，通过点云分类和退化感知ICP算法，在稀疏特征和重复几何结构环境中显著提高了里程计精度。


<details>
  <summary>Details</summary>
Motivation: 当前点对面ICP算法在结构化环境中表现良好，但在稀疏特征、重复几何结构和高频运动场景下存在6自由度姿态估计退化问题，需要改进LiDAR-only解决方案。

Method: 1. 基于表面法线和邻域分析的点云分类（地面、墙壁、屋顶、边缘和非平面点）；2. 退化感知加权最小二乘ICP算法；3. Scan Context后端支持鲁棒闭环检测。

Result: DAMM-LOAM在里程计精度方面显示出显著改进，特别是在长走廊等室内环境中表现优异。

Conclusion: 所提出的退化感知多度量方法有效解决了LiDAR SLAM在挑战性环境中的退化问题，为纯LiDAR解决方案提供了更可靠的性能。

Abstract: LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for
enabling precise navigation and environmental reconstruction across various
applications. Although current point-to-plane ICP algorithms perform effec-
tively in structured, feature-rich environments, they struggle in scenarios
with sparse features, repetitive geometric structures, and high-frequency
motion. This leads to degeneracy in 6- DOF pose estimation. Most
state-of-the-art algorithms address these challenges by incorporating
additional sensing modalities, but LiDAR-only solutions continue to face
limitations under such conditions. To address these issues, we propose a novel
Degeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.
Our system improves mapping accuracy through point cloud classification based
on surface normals and neighborhood analysis. Points are classified into
ground, walls, roof, edges, and non-planar points, enabling accurate
correspondences. A Degeneracy-based weighted least squares-based ICP algorithm
is then applied for accurate odom- etry estimation. Additionally, a Scan
Context based back-end is implemented to support robust loop closures.
DAMM-LOAM demonstrates significant improvements in odometry accuracy,
especially in indoor environments such as long corridors

</details>


### [14] [Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation](https://arxiv.org/abs/2510.13324)
*Erik Helmut,Niklas Funk,Tim Schneider,Cristiana de Farias,Jan Peters*

Main category: cs.RO

TL;DR: FARM是一个模仿学习框架，通过整合高维触觉数据来推断触觉条件化的力信号，并定义基于力的动作空间，用于接触丰富的机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法通常仅将视觉触觉反馈视为额外观察，而将施加的力视为夹爪命令的不可控后果。需要一种能够主动控制操作力的方法，特别是在处理易碎或可变形物体时。

Method: 使用集成了GelSight Mini视觉触觉传感器的改进版手持UMI夹爪收集人类演示数据，开发了与手持版本几何匹配的驱动变体用于策略部署。FARM扩散策略联合预测机器人位姿、夹持宽度和夹持力。

Result: FARM在三个具有不同力要求（高力、低力和动态力适应）的任务中优于多个基线方法，证明了利用基于力的高维触觉观测和基于力的控制空间的优势。

Conclusion: FARM框架通过整合触觉数据和力控制，在接触丰富的操作任务中表现出色，代码和设计文件已开源。

Abstract: Contact-rich manipulation depends on applying the correct grasp forces
throughout the manipulation task, especially when handling fragile or
deformable objects. Most existing imitation learning approaches often treat
visuotactile feedback only as an additional observation, leaving applied forces
as an uncontrolled consequence of gripper commands. In this work, we present
Force-Aware Robotic Manipulation (FARM), an imitation learning framework that
integrates high-dimensional tactile data to infer tactile-conditioned force
signals, which in turn define a matching force-based action space. We collect
human demonstrations using a modified version of the handheld Universal
Manipulation Interface (UMI) gripper that integrates a GelSight Mini visual
tactile sensor. For deploying the learned policies, we developed an actuated
variant of the UMI gripper with geometry matching our handheld version. During
policy rollouts, the proposed FARM diffusion policy jointly predicts robot
pose, grip width, and grip force. FARM outperforms several baselines across
three tasks with distinct force requirements -- high-force, low-force, and
dynamic force adaptation -- demonstrating the advantages of its two key
components: leveraging force-grounded, high-dimensional tactile observations
and a force-based control space. The codebase and design files are open-sourced
and available at https://tactile-farm.github.io .

</details>


### [15] [MODUR: A Modular Dual-reconfigurable Robot](https://arxiv.org/abs/2510.13356)
*Jie Gu,Tin Lun Lam,Chunxu Tian,Zhihao Xia,Yongheng Xing,Dan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为MODUR的新型模块化自重构机器人，具有双重重构能力，既能进行模块间的高层重构形成不同构型，又能改变单个模块形状执行基本运动。


<details>
  <summary>Details</summary>
Motivation: 模块化自重构机器人系统能够通过改变模块间的拓扑关系形成更高级别的机器人系统，在各种环境中提供增强的适应性和鲁棒性。

Method: MODUR的设计主要包括紧凑连接器和剪刀连杆组，形成并联机构，实现连接器运动解耦和相邻位置迁移能力，并对工作空间进行综合分析。

Result: 通过一系列实验验证了MODUR的运动能力。

Conclusion: MODUR成功实现了双重重构能力，为模块化自重构机器人的设计提供了新的理论和技术基础。

Abstract: Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots
capable of forming higher-level robotic systems by altering the topological
relationships between modules, offering enhanced adaptability and robustness in
various environments. This paper presents a novel MSRR called MODUR, featuring
dual-level reconfiguration capabilities designed to integrate reconfigurable
mechanisms into MSRR. Specifically, MODUR can perform high-level
self-reconfiguration among modules to create different configurations, while
each module is also able to change its shape to execute basic motions. The
design of MODUR primarily includes a compact connector and scissor linkage
groups that provide actuation, forming a parallel mechanism capable of
achieving both connector motion decoupling and adjacent position migration
capabilities. Furthermore, the workspace, considering the interdependent
connectors, is comprehensively analyzed, laying a theoretical foundation for
the design of the module's basic motion. Finally, the motion of MODUR is
validated through a series of experiments.

</details>


### [16] [Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control](https://arxiv.org/abs/2510.13358)
*Shingo Ayabe,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.RO

TL;DR: 该研究提出了一种离线到在线的强化学习框架，通过在离线训练的策略上注入动作空间扰动进行对抗性微调，结合性能感知课程学习策略，显著提升了策略在动作空间扰动下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习虽然能够高效地从静态数据集中学习策略，但这些策略在面对执行器故障等动作空间扰动时表现脆弱。研究旨在解决离线策略在不确定环境中的适应性问题。

Method: 采用离线到在线框架：先在干净数据上训练策略，然后通过向执行动作注入扰动进行对抗性微调，并使用基于指数移动平均信号的性能感知课程来动态调整扰动概率。

Result: 在连续控制运动任务上的实验表明，该方法相比纯离线基线显著提升了鲁棒性，比从头开始训练收敛更快。匹配微调与评估条件可获得最强的鲁棒性，自适应课程策略缓解了线性课程策略中观察到的名义性能下降。

Conclusion: 对抗性微调能够在不确定环境下实现自适应和鲁棒的控制，弥合了离线效率与在线适应性之间的差距。

Abstract: Offline reinforcement learning enables sample-efficient policy acquisition
without risky online interaction, yet policies trained on static datasets
remain brittle under action-space perturbations such as actuator faults. This
study introduces an offline-to-online framework that trains policies on clean
data and then performs adversarial fine-tuning, where perturbations are
injected into executed actions to induce compensatory behavior and improve
resilience. A performance-aware curriculum further adjusts the perturbation
probability during training via an exponential-moving-average signal, balancing
robustness and stability throughout the learning process. Experiments on
continuous-control locomotion tasks demonstrate that the proposed method
consistently improves robustness over offline-only baselines and converges
faster than training from scratch. Matching the fine-tuning and evaluation
conditions yields the strongest robustness to action-space perturbations, while
the adaptive curriculum strategy mitigates the degradation of nominal
performance observed with the linear curriculum strategy. Overall, the results
show that adversarial fine-tuning enables adaptive and robust control under
uncertain environments, bridging the gap between offline efficiency and online
adaptability.

</details>


### [17] [Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations](https://arxiv.org/abs/2510.13488)
*Maximilian Stasica,Arne Bick,Nico Bohlinger,Omid Mohseni,Max Johannes Alois Fritzsche,Clemens Hübler,Jan Peters,André Seyfarth*

Main category: cs.RO

TL;DR: 本研究通过在振荡桥梁上训练四足机器人，使用强化学习开发了15种不同的运动策略，显著提高了机器人在动态地面扰动下的稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在崎岖地形导航表现出色，但在垂直地面扰动（如振荡表面）下的性能研究不足，需要探索如何增强其在这种动态环境中的鲁棒性。

Method: 使用Unitree Go2机器人在13.24米钢混结构的振荡桥梁上进行训练，采用PPO算法在MuJoCo模拟中进行强化学习，结合5种步态和3种训练条件，并通过领域随机化实现零样本迁移。

Result: 在振荡桥梁上训练的策略比在刚性表面上训练的策略表现出更优越的稳定性和适应性，即使没有事先接触过桥梁也能产生鲁棒的步态模式。

Conclusion: 基于模拟的强化学习能够有效提高四足机器人在动态地面扰动下的运动能力，为设计能够穿越振动环境的机器人提供了重要见解。

Abstract: Legged robots, particularly quadrupeds, excel at navigating rough terrains,
yet their performance under vertical ground perturbations, such as those from
oscillating surfaces, remains underexplored. This study introduces a novel
approach to enhance quadruped locomotion robustness by training the Unitree Go2
robot on an oscillating bridge - a 13.24-meter steel-and-concrete structure
with a 2.0 Hz eigenfrequency designed to perturb locomotion. Using
Reinforcement Learning (RL) with the Proximal Policy Optimization (PPO)
algorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,
combining five gaits (trot, pace, bound, free, default) with three training
conditions: rigid bridge and two oscillating bridge setups with differing
height regulation strategies (relative to bridge surface or ground). Domain
randomization ensured zero-shot transfer to the real-world bridge. Our results
demonstrate that policies trained on the oscillating bridge exhibit superior
stability and adaptability compared to those trained on rigid surfaces. Our
framework enables robust gait patterns even without prior bridge exposure.
These findings highlight the potential of simulation-based RL to improve
quadruped locomotion during dynamic ground perturbations, offering insights for
designing robots capable of traversing vibrating environments.

</details>


### [18] [A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints](https://arxiv.org/abs/2510.13535)
*Wentao Guo,Yizhou Wang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新型欠驱动自适应机器人手Hockens-A Hand，集成了Hoeckens机构、双平行四边形连杆和专用四杆机构，实现三种自适应抓取模式：平行捏取、非对称舀取和包络抓取。仅需单个线性执行器，利用被动机械智能确保在非结构化环境中的适应性和顺应性。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在非结构化环境中自适应抓取各种形状和大小物体的机器人手，通过机械智能实现多模式抓取，同时保持结构简单和低成本。

Method: 结合Hoeckens机构提供垂直顺应性，双平行四边形连杆确保指尖线接触，四杆放大系统实现不同抓取模式间的自然过渡，并通过网格纹理硅胶指节增强包络能力。采用详细运动学分析优化推角和连杆长度设计。

Result: 仿真验证了指尖运动和抓取模式间平滑过渡，通过功率方程分析抓取力。3D打印原型实验验证了三种抓取模式在各种环境约束下的抓取稳定性和广泛适用性。

Conclusion: Hockens-A Hand成功实现了仅用单个执行器的多模式自适应抓取，在非结构化环境中表现出良好的适应性和抓取性能，验证了其设计的有效性和实用性。

Abstract: This paper presents a novel underactuated adaptive robotic hand, Hockens-A
Hand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,
and a specialized four-bar linkage to achieve three adaptive grasping modes:
parallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand
requires only a single linear actuator, leveraging passive mechanical
intelligence to ensure adaptability and compliance in unstructured
environments. Specifically, the vertical motion of the Hoeckens mechanism
introduces compliance, the double-parallelogram linkage ensures line contact at
the fingertip, and the four-bar amplification system enables natural
transitions between different grasping modes. Additionally, the inclusion of a
mesh-textured silicone phalanx further enhances the ability to envelop objects
of various shapes and sizes. This study employs detailed kinematic analysis to
optimize the push angle and design the linkage lengths for optimal performance.
Simulations validated the design by analyzing the fingertip motion and ensuring
smooth transitions between grasping modes. Furthermore, the grasping force was
analyzed using power equations to enhance the understanding of the system's
performance.Experimental validation using a 3D-printed prototype demonstrates
the three grasping modes of the hand in various scenarios under environmental
constraints, verifying its grasping stability and broad applicability.

</details>


### [19] [Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.13553)
*Wentao Guo,Wenzeng Zhang*

Main category: cs.RO

TL;DR: Hoecken-D Hand是一种欠驱动机器人夹爪，通过改进的Hoecken连杆机构和差动弹簧机构，实现线性平行夹持和自适应包络抓取，仅需单个线性驱动器即可操作。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在结构化环境中实现精确夹持，同时在非结构化环境中自适应抓取不规则物体的紧凑、低成本机器人夹爪。

Method: 重新配置Hoecken连杆机构，用差动连杆替换一个构件，结合双平行四边形保持指尖平行性，使用差动机构实现接触触发重构，无需额外执行器。

Result: 原型机实现了约200mm的线性夹持范围，在多种物体几何形状下可靠抓取，展示了紧凑、自适应和成本效益高的特点。

Conclusion: Hoecken-D Hand为在非结构化环境中的操作提供了一种紧凑、适应性强且经济有效的解决方案，结合了精确夹持和自适应包络抓取的优点。

Abstract: This paper presents the Hoecken-D Hand, an underactuated robotic gripper that
combines a modified Hoecken linkage with a differential spring mechanism to
achieve both linear parallel pinching and a mid-stroke transition to adaptive
envelope. The original Hoecken linkage is reconfigured by replacing one member
with differential links, preserving straight-line guidance while enabling
contact-triggered reconfiguration without additional actuators. A
double-parallelogram arrangement maintains fingertip parallelism during
conventional pinching, whereas the differential mechanism allows one finger to
wrap inward upon encountering an obstacle, improving stability on irregular or
thin objects. The mechanism can be driven by a single linear actuator,
minimizing complexity and cost; in our prototype, each finger is driven by its
own linear actuator for simplicity. We perform kinematic modeling and force
analysis to characterize grasp performance, including simulated grasping forces
and spring-opening behavior under varying geometric parameters. The design was
prototyped using PLA-based 3D printing, achieving a linear pinching span of
approximately 200 mm. Preliminary tests demonstrate reliable grasping in both
modes across a wide range of object geometries, highlighting the Hoecken-D Hand
as a compact, adaptable, and cost-effective solution for manipulation in
unstructured environments.

</details>


### [20] [Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots](https://arxiv.org/abs/2510.13594)
*Austin Barret,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 开发一个面向非专家用户的简单直观的人形机器人图形用户界面，用于控制机器人通过FIRA规定的障碍赛道


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人系统缺乏专门为非专家操作者设计的图形用户界面，限制了系统的可访问性和实用性

Method: 结合用户界面开发实践、人机交互概念及相关理论，开发新的非专家远程操作系统界面

Result: 开发了一个可扩展的GUI，专门为非专家操作者设计，使其能够简单直观地控制机器人

Conclusion: 成功创建了一个适合非专家用户的人形机器人操作界面，提高了系统的可用性和可访问性

Abstract: The operation of humanoid robotics is an essential field of research with
many practical and competitive applications. Many of these systems, however, do
not invest heavily in developing a non-expert-centered graphical user interface
(GUI) for operation. The focus of this research is to develop a scalable GUI
that is tailored to be simple and intuitive so non-expert operators can control
the robot through a FIRA-regulated obstacle course. Using common practices from
user interface development (UI) and understanding concepts described in
human-robot interaction (HRI) and other related concepts, we will develop a new
interface with the goal of a non-expert teleoperation system.

</details>


### [21] [Active Tactile Exploration for Rigid Body Pose and Shape Estimation](https://arxiv.org/abs/2510.13595)
*Ethan K. Gordon,Bruke Baraki,Hien Bui,Michael Posa*

Main category: cs.RO

TL;DR: 提出一种仅使用触觉数据同时确定刚性物体形状和位置的学习与探索框架，通过物理约束优化的方式高效学习物体几何形状


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作需要处理未见过的物体，学习物理精确模型在测试时能提供数据效率、可预测性和任务间重用的优势。触觉感知可以补充视觉感知的遮挡鲁棒性，但需要精心设计的在线探索来保持数据效率

Method: 基于接触丰富的系统识别最新进展，构建惩罚物理约束违反的损失函数，避免刚体接触引入的数值刚度。通过优化该损失函数学习立方体和凸多面体几何形状，探索方案旨在最大化期望信息增益

Result: 在首次接触后仅用不到10秒的随机收集数据就能学习几何形状，探索方案在仿真和真实机器人实验中都能显著加快学习速度

Conclusion: 该框架能够仅使用触觉数据高效地同时确定物体的形状和位置，为机器人操作提供了数据效率高的解决方案

Abstract: General robot manipulation requires the handling of previously unseen
objects. Learning a physically accurate model at test time can provide
significant benefits in data efficiency, predictability, and reuse between
tasks. Tactile sensing can compliment vision with its robustness to occlusion,
but its temporal sparsity necessitates careful online exploration to maintain
data efficiency. Direct contact can also cause an unrestrained object to move,
requiring both shape and location estimation. In this work, we propose a
learning and exploration framework that uses only tactile data to
simultaneously determine the shape and location of rigid objects with minimal
robot motion. We build on recent advances in contact-rich system identification
to formulate a loss function that penalizes physical constraint violation
without introducing the numerical stiffness inherent in rigid-body contact.
Optimizing this loss, we can learn cuboid and convex polyhedral geometries with
less than 10s of randomly collected data after first contact. Our exploration
scheme seeks to maximize Expected Information Gain and results in significantly
faster learning in both simulated and real-robot experiments. More information
can be found at https://dairlab.github.io/activetactile

</details>


### [22] [PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction](https://arxiv.org/abs/2510.13599)
*Jiahao Wang,Nived Chebrolu,Yifu Tao,Lintong Zhang,Ayoung Kim,Maurice Fallon*

Main category: cs.RO

TL;DR: PlanarMesh是一种新颖的增量式、基于网格的LiDAR重建系统，通过自适应调整网格分辨率实现实时紧凑详细重建，结合平面建模和网格化，在保持实时性能的同时显著减小输出文件大小。


<details>
  <summary>Details</summary>
Motivation: 构建在线3D LiDAR建图系统需要在保持计算效率的同时生成详细表面重建，这是一个具有挑战性的任务。现有方法在实时性能和重建质量之间存在权衡。

Method: 提出planar-mesh表示法，结合平面建模和网格化；采用多线程架构和BVH进行高效数据存储和快速搜索；考虑局部表面曲率和传感器测量的自由空间信息进行增量更新。

Result: 重建精度达到或超过最先进技术（包括截断符号距离函数、占用映射和基于体素的网格化）；输出文件大小比原始输入小10倍，比基于网格的方法小5倍以上；保持实时性能（64波束传感器约2Hz）。

Conclusion: PlanarMesh系统在实现高质量重建的同时，显著减少了存储需求并保持了实时性能，为在线3D LiDAR建图提供了有效的解决方案。

Abstract: Building an online 3D LiDAR mapping system that produces a detailed surface
reconstruction while remaining computationally efficient is a challenging task.
In this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR
reconstruction system that adaptively adjusts mesh resolution to achieve
compact, detailed reconstructions in real-time. It introduces a new
representation, planar-mesh, which combines plane modeling and meshing to
capture both large surfaces and detailed geometry. The planar-mesh can be
incrementally updated considering both local surface curvature and free-space
information from sensor measurements. We employ a multi-threaded architecture
with a Bounding Volume Hierarchy (BVH) for efficient data storage and fast
search operations, enabling real-time performance. Experimental results show
that our method achieves reconstruction accuracy on par with, or exceeding,
state-of-the-art techniques-including truncated signed distance functions,
occupancy mapping, and voxel-based meshing-while producing smaller output file
sizes (10 times smaller than raw input and more than 5 times smaller than
mesh-based methods) and maintaining real-time performance (around 2 Hz for a
64-beam sensor).

</details>


### [23] [Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor](https://arxiv.org/abs/2510.13616)
*Preston Fairchild,Claudia Chen,Xiaobo Tan*

Main category: cs.RO

TL;DR: 开发了一种低成本、易制造的柔性压力传感器，集成到机器人抓手中，用于处理不同形状、大小和硬度的农产品。该传感器能够提供力反馈，估计物体特性，并支持实时应用。


<details>
  <summary>Details</summary>
Motivation: 在农业收获和加工中，机器人操纵器需要正确处理易损农产品。正确的抓取力不仅确保牢固抓握，还要避免损坏或压伤产品。

Method: 将柔性压力传感器集成到刚性机器人抓手和气动软指中，提出基于瞬态响应数据加速估计传感器稳态值的算法，实现实时应用。

Result: 传感器成功集成到两种抓手中，能够为未知大小和硬度的物体提供正确抓取反馈，同时估计这些值用于识别成熟度和压伤等质量特性。

Conclusion: 该传感器技术不仅可用于农产品识别，还可用于质量控制和基于成熟度的选择性分拣任务，具有广阔的应用前景。

Abstract: Properly handling delicate produce with robotic manipulators is a major part
of the future role of automation in agricultural harvesting and processing.
Grasping with the correct amount of force is crucial in not only ensuring
proper grip on the object, but also to avoid damaging or bruising the product.
In this work, a flexible pressure sensor that is both low cost and easy to
fabricate is integrated with robotic grippers for working with produce of
varying shapes, sizes, and stiffnesses. The sensor is successfully integrated
with both a rigid robotic gripper, as well as a pneumatically actuated soft
finger. Furthermore, an algorithm is proposed for accelerated estimation of the
steady-state value of the sensor output based on the transient response data,
to enable real-time applications. The sensor is shown to be effective in
incorporating feedback to correctly grasp objects of unknown sizes and
stiffnesses. At the same time, the sensor provides estimates for these values
which can be utilized for identification of qualities such as ripeness levels
and bruising. It is also shown to be able to provide force feedback for objects
of variable stiffnesses. This enables future use not only for produce
identification, but also for tasks such as quality control and selective
distribution based on ripeness levels.

</details>


### [24] [Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization](https://arxiv.org/abs/2510.13619)
*Daniel Choate,Jason Rife*

Main category: cs.RO

TL;DR: 提出一种可视化方法，帮助分析师识别影响激光雷达扫描匹配的逆境模式，通过生成向量场图来显示配准点云之间的局部差异。


<details>
  <summary>Details</summary>
Motivation: 帮助人类分析师更好地理解激光雷达扫描匹配中的逆境模式，这些模式在原始点云数据中难以直接观察。

Method: 开发可视化方法生成向量场图，表征配准点云对之间的局部差异，用于离线分析而非实时处理。

Result: 在两个概念验证示例（仿真研究和现场实验）中，分析师能够识别一系列逆境机制，并迭代地从原始数据中移除这些机制。

Conclusion: 该方法有效帮助分析师逐步关注更小的差异，通过可视化手段增强对激光雷达扫描匹配问题的理解。

Abstract: In this paper we introduce a visualization methodology to aid a human analyst
in classifying adversity modes that impact lidar scan matching. Our methodology
is intended for offline rather than real-time analysis. The method generates a
vector-field plot that characterizes local discrepancies between a pair of
registered point clouds. The vector field plot reveals patterns that would be
difficult for the analyst to extract from raw point-cloud data. After
introducing our methodology, we apply the process to two proof-of-concept
examples: one a simulation study and the other a field experiment. For both
data sets, a human analyst was able to reason about a series of adversity
mechanisms and iteratively remove those mechanisms from the raw data, to help
focus attention on progressively smaller discrepancies.

</details>


### [25] [A Modular Object Detection System for Humanoid Robots Using YOLO](https://arxiv.org/abs/2510.13625)
*Nicolas Pottier,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 本研究提出了一种基于YOLOv9的通用视觉模块，用于机器人视觉系统，在FIRA机器人Hurocup数据集上训练，并在ROS1中实现。相比现有几何模型，YOLO模型实现了相当的精度但计算成本更高，同时提供了更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人领域中的计算机视觉仍然是进展的重要障碍，许多任务受到低效视觉系统的阻碍，需要为计算受限环境优化的解决方案。

Method: 使用YOLOv9框架构建通用视觉模块，在FIRA机器人Hurocup数据集上训练，通过ROS1中的虚拟环境实现YOLO兼容性，使用FPS和mAP等指标评估性能，并与现有几何框架在静态和动态场景下进行比较。

Result: YOLO模型在精度上与几何模型相当，但计算成本更高，同时提供了改进的鲁棒性。

Conclusion: YOLO模型为机器人视觉系统提供了一种可行的替代方案，在保持精度的同时增强了鲁棒性，尽管需要更高的计算资源。

Abstract: Within the field of robotics, computer vision remains a significant barrier
to progress, with many tasks hindered by inefficient vision systems. This
research proposes a generalized vision module leveraging YOLOv9, a
state-of-the-art framework optimized for computationally constrained
environments like robots. The model is trained on a dataset tailored to the
FIRA robotics Hurocup. A new vision module is implemented in ROS1 using a
virtual environment to enable YOLO compatibility. Performance is evaluated
using metrics such as frames per second (FPS) and Mean Average Precision (mAP).
Performance is then compared to the existing geometric framework in static and
dynamic contexts. The YOLO model achieved comparable precision at a higher
computational cost then the geometric model, while providing improved
robustness.

</details>


### [26] [LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models](https://arxiv.org/abs/2510.13626)
*Senyu Fei,Siyin Wang,Junhao Shi,Zihao Dai,Jikun Cai,Pengfang Qian,Li Ji,Xinzhe He,Shiduo Zhang,Zhaoye Fei,Jinlan Fu,Jingjing Gong,Xipeng Qiu*

Main category: cs.RO

TL;DR: 该论文对视觉-语言-动作(VLA)模型进行了系统性脆弱性分析，发现在看似高成功率下存在严重的鲁棒性问题，模型对多种扰动因素极为敏感，性能可从95%降至30%以下，且模型倾向于完全忽略语言指令。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型在机器人操作基准测试中报告了令人印象深刻的成功率，但这些结果可能掩盖了鲁棒性的根本弱点。研究者旨在通过系统性脆弱性分析揭示模型在真实世界变化下的可靠性问题。

Method: 通过引入七个维度的受控扰动进行系统性分析：物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声。对多个最先进模型进行了全面分析。

Result: 模型表现出对扰动因素的极端敏感性，特别是相机视角和机器人初始状态，性能从95%降至30%以下。令人惊讶的是，模型对语言变化不敏感，进一步实验显示模型倾向于完全忽略语言指令。

Conclusion: 研究结果挑战了高基准分数等同于真正能力的假设，强调需要评估在真实变化下的可靠性的实践方法。

Abstract: Visual-Language-Action (VLA) models report impressive success rates on
robotic manipulation benchmarks, yet these results may mask fundamental
weaknesses in robustness. We perform a systematic vulnerability analysis by
introducing controlled perturbations across seven dimensions: objects layout,
camera viewpoints, robot initial states, language instructions, light
conditions, background textures and sensor noise. We comprehensively analyzed
multiple state-of-the-art models and revealed consistent brittleness beneath
apparent competence. Our analysis exposes critical weaknesses: models exhibit
extreme sensitivity to perturbation factors, including camera viewpoints and
robot initial states, with performance dropping from 95% to below 30% under
modest perturbations. Surprisingly, models are largely insensitive to language
variations, with further experiments revealing that models tend to ignore
language instructions completely. Our findings challenge the assumption that
high benchmark scores equate to true competency and highlight the need for
evaluation practices that assess reliability under realistic variation.

</details>


### [27] [On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas](https://arxiv.org/abs/2510.13644)
*Michael Bosello,Flavio Pinzarrone,Sara Kiade,Davide Aguiari,Yvo Keuter,Aaesha AlShehhi,Gyordan Caminati,Kei Long Wong,Ka Seng Chou,Junaid Halepota,Fares Alneyadi,Jacopo Panerati,Giovanni Pau*

Main category: cs.RO

TL;DR: 本文分析了无人机视觉自主系统在受控环境和无仪器环境中的性能，证明其能够达到专业人类飞行员水平，并公开了相关飞行数据。


<details>
  <summary>Details</summary>
Motivation: 无人机技术在多个行业快速发展，视觉自主系统是关键推动力。虽然现有系统在受控环境中表现出色，但在商业和实际应用中的直接适用性仍然有限。本文旨在评估系统在受控和无仪器环境中的性能。

Method: 在受控环境中使用外部跟踪进行地面实况比较，同时在无地面实况测量的无仪器环境中进行演示。

Result: 研究表明，该方法在两种场景下都能达到专业人类飞行员的性能水平。

Conclusion: 本文证明了视觉自主系统在实际应用中的可行性，并为研究社区提供了宝贵的飞行数据资源。

Abstract: Drone technology is proliferating in many industries, including agriculture,
logistics, defense, infrastructure, and environmental monitoring. Vision-based
autonomy is one of its key enablers, particularly for real-world applications.
This is essential for operating in novel, unstructured environments where
traditional navigation methods may be unavailable. Autonomous drone racing has
become the de facto benchmark for such systems. State-of-the-art research has
shown that autonomous systems can surpass human-level performance in racing
arenas. However, direct applicability to commercial and field operations is
still limited as current systems are often trained and evaluated in highly
controlled environments. In our contribution, the system's capabilities are
analyzed within a controlled environment -- where external tracking is
available for ground-truth comparison -- but also demonstrated in a
challenging, uninstrumented environment -- where ground-truth measurements were
never available. We show that our approach can match the performance of
professional human pilots in both scenarios. We also publicly release the data
from the flights carried out by our approach and a world-class human pilot.

</details>


### [28] [Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures](https://arxiv.org/abs/2510.13686)
*Miana Smith,Paul Arthur Richard,Alexander Htet Kyaw,Neil Gershenfeld*

Main category: cs.RO

TL;DR: 该论文提出了一种使用简单机器人和互锁晶格构建块来制造可扩展宏观结构的方法，通过体素化、分层分组和移动机器人组装，实现了米级结构的自动化制造。


<details>
  <summary>Details</summary>
Motivation: 当前面向大型结构的数字制造系统通常复杂、昂贵且不可靠，需要开发更简单、可靠的大规模制造方法。

Method: 将目标结构体素化并填充架构晶格，将体素分组为更大互连块，使用标准数字制造工艺生产，然后通过移动机器人遍历结构并放置新块进行组装。

Result: 通过演示体素化、分层分组、路径规划和机器人制造过程，验证了系统能够制造米级物体。

Conclusion: 该方法利用简单机器人和互锁晶格构建块，实现了可扩展宏观结构的可靠制造，为大型数字制造提供了新途径。

Abstract: Although digital fabrication processes at the desktop scale have become
proficient and prolific, systems aimed at producing larger-scale structures are
still typically complex, expensive, and unreliable. In this work, we present an
approach for the fabrication of scalable macroscale structures using simple
robots and interlocking lattice building blocks. A target structure is first
voxelized so that it can be populated with an architected lattice. These voxels
are then grouped into larger interconnected blocks, which are produced using
standard digital fabrication processes, leveraging their capability to produce
highly complex geometries at a small scale. These blocks, on the size scale of
tens of centimeters, are then fed to mobile relative robots that are able to
traverse over the structure and place new blocks to form structures on the
meter scale. To facilitate the assembly of large structures, we introduce a
live digital twin simulation tool for controlling and coordinating assembly
robots that enables both global planning for a target structure and live user
design, interaction, or intervention. To improve assembly throughput, we
introduce a new modular assembly robot, designed for hierarchical voxel
handling. We validate this system by demonstrating the voxelization,
hierarchical blocking, path planning, and robotic fabrication of a set of
meter-scale objects.

</details>


### [29] [InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy](https://arxiv.org/abs/2510.13778)
*Xinyi Chen,Yilun Chen,Yanwei Fu,Ning Gao,Jiaya Jia,Weiyang Jin,Hao Li,Yao Mu,Jiangmiao Pang,Yu Qiao,Yang Tian,Bin Wang,Bolun Wang,Fangjing Wang,Hanqing Wang,Tai Wang,Ziqin Wang,Xueyuan Wei,Chao Wu,Shuai Yang,Jinhui Ye,Junqiu Yu,Jia Zeng,Jingjing Zhang,Jinyu Zhang,Shi Zhang,Feng Zheng,Bowen Zhou,Yangkun Zhu*

Main category: cs.RO

TL;DR: InternVLA-M1是一个用于空间定位和机器人控制的统一框架，通过空间引导的视觉-语言-动作训练，将空间定位作为指令和机器人动作之间的关键连接。该框架采用两阶段训练流程，在多个任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 推动指令跟随机器人向可扩展、通用智能方向发展，解决传统方法在空间推理和动作生成之间的脱节问题。

Method: 采用两阶段训练：(1) 在230万空间推理数据上进行空间定位预训练，确定"在哪里行动"；(2) 空间引导的动作后训练，通过即插即用的空间提示决定"如何行动"。

Result: 在多个基准测试中表现优异：在SimplerEnv Google Robot上提升14.6%，WidowX上提升17%，LIBERO Franka上提升4.3%。在真实世界集群拾取任务中提升7.3%，通过合成协同训练在未见对象上提升20.6%。

Conclusion: 空间引导训练是构建可扩展和鲁棒的通用机器人的统一原则，在长期推理密集型场景中超越了现有工作10%以上。

Abstract: We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence. Its core idea is spatially guided
vision-language-action training, where spatial grounding serves as the critical
link between instructions and robot actions. InternVLA-M1 employs a two-stage
pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
data to determine ``where to act'' by aligning instructions with visual,
embodiment-agnostic positions, and (ii) spatially guided action post-training
to decide ``how to act'' by generating embodiment-aware actions through
plug-and-play spatial prompting. This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction. To further scale instruction following, we built a
simulation engine to collect 244K generalizable pick-and-place episodes,
enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
surpassed existing works by over 10%. These results highlight spatially guided
training as a unifying principle for scalable and resilient generalist robots.
Code and models are available at
https://github.com/InternRobotics/InternVLA-M1.

</details>
