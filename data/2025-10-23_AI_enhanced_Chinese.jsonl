{"id": "2510.18986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18986", "abs": "https://arxiv.org/abs/2510.18986", "authors": ["Alberto Sanchez-Delgado", "Jo\u00e3o Carlos Virgolino Soares", "Victor Barasuol", "Claudio Semini"], "title": "Towards Proprioceptive Terrain Mapping with Quadruped Robots for Exploration in Planetary Permanently Shadowed Regions", "comment": "Published in the Proceedings of the International Conference on Space\n  Robotics (iSpaRo 2025)", "summary": "Permanently Shadowed Regions (PSRs) near the lunar poles are of interest for\nfuture exploration due to their potential to contain water ice and preserve\ngeological records. Their complex, uneven terrain favors the use of legged\nrobots, which can traverse challenging surfaces while collecting in-situ data,\nand have proven effective in Earth analogs, including dark caves, when equipped\nwith onboard lighting. While exteroceptive sensors like cameras and lidars can\ncapture terrain geometry and even semantic information, they cannot quantify\nits physical interaction with the robot, a capability provided by\nproprioceptive sensing. We propose a terrain mapping framework for quadruped\nrobots, which estimates elevation, foot slippage, energy cost, and stability\nmargins from internal sensing during locomotion. These metrics are\nincrementally integrated into a multi-layer 2.5D gridmap that reflects terrain\ninteraction from the robot's perspective. The system is evaluated in a\nsimulator that mimics a lunar environment, using the 21 kg quadruped robot\nAliengo, showing consistent mapping performance under lunar gravity and terrain\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u5730\u5f62\u6620\u5c04\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u90e8\u4f20\u611f\u5728\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u4f30\u8ba1\u9ad8\u7a0b\u3001\u8db3\u90e8\u6ed1\u79fb\u3001\u80fd\u91cf\u6210\u672c\u548c\u7a33\u5b9a\u6027\u88d5\u5ea6\uff0c\u5e76\u5728\u6a21\u62df\u6708\u7403\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u6027\u80fd\u3002", "motivation": "\u6708\u7403\u6781\u533a\u6c38\u4e45\u9634\u5f71\u533a\u57df\u5730\u5f62\u590d\u6742\uff0c\u9002\u5408\u817f\u5f0f\u673a\u5668\u4eba\u63a2\u7d22\uff0c\u4f46\u73b0\u6709\u5916\u90e8\u4f20\u611f\u5668\u65e0\u6cd5\u91cf\u5316\u5730\u5f62\u4e0e\u673a\u5668\u4eba\u7684\u7269\u7406\u4ea4\u4e92\u3002", "method": "\u4f7f\u7528\u56db\u8db3\u673a\u5668\u4eba\u5185\u90e8\u4f20\u611f\u6570\u636e\uff0c\u589e\u91cf\u5f0f\u6784\u5efa\u53cd\u6620\u5730\u5f62\u4ea4\u4e92\u7684\u591a\u5c422.5D\u7f51\u683c\u5730\u56fe\uff0c\u5305\u542b\u9ad8\u7a0b\u3001\u6ed1\u79fb\u3001\u80fd\u8017\u548c\u7a33\u5b9a\u6027\u6307\u6807\u3002", "result": "\u5728\u6a21\u62df\u6708\u7403\u73af\u5883\u4e2d\u4f7f\u752821\u516c\u65a4\u56db\u8db3\u673a\u5668\u4ebaAliengo\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5728\u6708\u7403\u91cd\u529b\u548c\u5730\u5f62\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u6620\u5c04\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u91cf\u5316\u5730\u5f62\u4e0e\u673a\u5668\u4eba\u7684\u7269\u7406\u4ea4\u4e92\uff0c\u4e3a\u6708\u7403\u63a2\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5730\u5f62\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2510.18991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18991", "abs": "https://arxiv.org/abs/2510.18991", "authors": ["Chinmay Burgul", "Yewei Huang", "Michalis Chatzispyrou", "Ioannis Rekleitis", "Alberto Quattrini Li", "Marios Xanthidis"], "title": "Underwater Dense Mapping with the First Compact 3D Sonar", "comment": "8 pages, 12 figures", "summary": "In the past decade, the adoption of compact 3D range sensors, such as LiDARs,\nhas driven the developments of robust state-estimation pipelines, making them a\nstandard sensor for aerial, ground, and space autonomy. Unfortunately, poor\npropagation of electromagnetic waves underwater, has limited the\nvisibility-independent sensing options of underwater state-estimation to\nacoustic range sensors, which provide 2D information including, at-best,\nspatially ambiguous information. This paper, to the best of our knowledge, is\nthe first study examining the performance, capacity, and opportunities arising\nfrom the recent introduction of the first compact 3D sonar. Towards that\npurpose, we introduce calibration procedures for extracting the extrinsics\nbetween the 3D sonar and a camera and we provide a study on acoustic response\nin different surfaces and materials. Moreover, we provide novel mapping and\nSLAM pipelines tested in deployments in underwater cave systems and other\ngeometrically and acoustically challenging underwater environments. Our\nassessment showcases the unique capacity of 3D sonars to capture consistent\nspatial information allowing for detailed reconstructions and localization in\ndatasets expanding to hundreds of meters. At the same time it highlights\nremaining challenges related to acoustic propagation, as found also in other\nacoustic sensors. Datasets collected for our evaluations would be released and\nshared with the community to enable further research advancements.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u7d27\u51d1\u578b3D\u58f0\u7eb3\u7684\u6027\u80fd\u3001\u80fd\u529b\u548c\u5e94\u7528\u673a\u4f1a\uff0c\u63d0\u51fa\u4e863D\u58f0\u7eb3\u4e0e\u76f8\u673a\u7684\u6807\u5b9a\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86\u65b0\u9896\u7684\u6c34\u4e0b\u5efa\u56fe\u548cSLAM\u6d41\u7a0b\uff0c\u5e76\u5728\u6c34\u4e0b\u6d1e\u7a74\u7b49\u6311\u6218\u6027\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "motivation": "\u4f20\u7edf\u6c34\u4e0b\u72b6\u6001\u4f30\u8ba1\u4e3b\u8981\u4f9d\u8d562D\u58f0\u5b66\u4f20\u611f\u5668\uff0c\u4fe1\u606f\u6709\u9650\u4e14\u5b58\u5728\u7a7a\u95f4\u6a21\u7cca\u6027\u3002\u968f\u7740\u9996\u4e2a\u7d27\u51d1\u578b3D\u58f0\u7eb3\u7684\u51fa\u73b0\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e863D\u58f0\u7eb3\u4e0e\u76f8\u673a\u7684\u5916\u90e8\u53c2\u6570\u6807\u5b9a\u7a0b\u5e8f\uff1b\u7814\u7a76\u4e86\u4e0d\u540c\u8868\u9762\u548c\u6750\u6599\u7684\u58f0\u5b66\u54cd\u5e94\u7279\u6027\uff1b\u5f00\u53d1\u4e86\u65b0\u9896\u7684\u6c34\u4e0b\u5efa\u56fe\u548cSLAM\u6d41\u7a0b\uff1b\u5728\u51e0\u4f55\u548c\u58f0\u5b66\u6311\u6218\u6027\u6c34\u4e0b\u73af\u5883\u4e2d\u8fdb\u884c\u90e8\u7f72\u6d4b\u8bd5\u3002", "result": "3D\u58f0\u7eb3\u80fd\u591f\u6355\u83b7\u4e00\u81f4\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u5b9e\u73b0\u6570\u767e\u7c73\u8303\u56f4\u5185\u8be6\u7ec6\u7684\u91cd\u5efa\u548c\u5b9a\u4f4d\uff1b\u540c\u65f6\u63ed\u793a\u4e86\u58f0\u6ce2\u4f20\u64ad\u76f8\u5173\u7684\u6311\u6218\uff1b\u6536\u96c6\u7684\u6570\u636e\u96c6\u5c06\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "3D\u58f0\u7eb3\u4e3a\u6c34\u4e0b\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u80fd\u591f\u5728\u6c34\u4e0b\u6d1e\u7a74\u7b49\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u7a7a\u95f4\u611f\u77e5\uff0c\u4f46\u4ecd\u9762\u4e34\u58f0\u5b66\u4f20\u64ad\u76f8\u5173\u7684\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2510.18996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.18996", "abs": "https://arxiv.org/abs/2510.18996", "authors": ["Susheel Vadakkekuruppath", "Herman B. Amundsen", "Jason M. O'Kane", "Marios Xanthidis"], "title": "SHRUMS: Sensor Hallucination for Real-time Underwater Motion Planning with a Compact 3D Sonar", "comment": "8 pages, 5 figures", "summary": "Autonomous navigation in 3D is a fundamental problem for autonomy. Despite\nmajor advancements in terrestrial and aerial settings due to improved range\nsensors including LiDAR, compact sensors with similar capabilities for\nunderwater robots have only recently become available, in the form of 3D\nsonars. This paper introduces a novel underwater 3D navigation pipeline, called\nSHRUMS (Sensor Hallucination for Robust Underwater Motion planning with 3D\nSonar). To the best of the authors' knowledge, SHRUMS is the first underwater\nautonomous navigation stack to integrate a 3D sonar. The proposed pipeline\nexhibits strong robustness while operating in complex 3D environments in spite\nof extremely poor visibility conditions. To accommodate the intricacies of the\nnovel sensor data stream while achieving real-time locally optimal performance,\nSHRUMS introduces the concept of hallucinating sensor measurements from\nnon-existent sensors with convenient arbitrary parameters, tailored to\napplication specific requirements. The proposed concepts are validated with\nreal 3D sonar sensor data, utilizing real inputs in challenging settings and\nlocal maps constructed in real-time. Field deployments validating the proposed\napproach in full are planned in the very near future.", "AI": {"tldr": "SHRUMS\u662f\u9996\u4e2a\u96c6\u62103D\u58f0\u7eb3\u7684\u6c34\u4e0b\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u865a\u62df\u4f20\u611f\u5668\u6d4b\u91cf\u6280\u672f\uff0c\u5728\u80fd\u89c1\u5ea6\u6781\u5dee\u7684\u6c34\u4e0b\u590d\u67423D\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u5bfc\u822a\u3002", "motivation": "\u6c34\u4e0b3D\u81ea\u4e3b\u5bfc\u822a\u9762\u4e34\u4f20\u611f\u5668\u9650\u5236\uff0c\u4f20\u7edfLiDAR\u5728\u6c34\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u65b0\u578b3D\u58f0\u7eb3\u4f20\u611f\u5668\u521a\u521a\u53ef\u7528\uff0c\u9700\u8981\u5f00\u53d1\u76f8\u5e94\u7684\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4f20\u611f\u5668\u865a\u62df\u5316\u6982\u5ff5\uff0c\u4ece\u4e0d\u5b58\u5728\u4f46\u53c2\u6570\u53ef\u5b9a\u5236\u7684\u4f20\u611f\u5668\u4e2d\u865a\u62df\u751f\u6210\u6d4b\u91cf\u6570\u636e\uff0c\u4ee5\u9002\u5e94\u65b0\u578b\u4f20\u611f\u5668\u6570\u636e\u6d41\u5e76\u5b9e\u73b0\u5b9e\u65f6\u5c40\u90e8\u6700\u4f18\u6027\u80fd\u3002", "result": "\u4f7f\u7528\u771f\u5b9e3D\u58f0\u7eb3\u4f20\u611f\u5668\u6570\u636e\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5b9e\u65f6\u6784\u5efa\u5c40\u90e8\u5730\u56fe\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "SHRUMS\u6210\u529f\u89e3\u51b3\u4e86\u6c34\u4e0b3D\u5bfc\u822a\u95ee\u9898\uff0c\u4e3a\u6c34\u4e0b\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b8c\u6574\u7684\u73b0\u573a\u90e8\u7f72\u9a8c\u8bc1\u5373\u5c06\u8fdb\u884c\u3002"}}
{"id": "2510.19054", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19054", "abs": "https://arxiv.org/abs/2510.19054", "authors": ["Shiyu Liu", "Ilija Hadzic", "Akshay Gupta", "Aliasghar Arab"], "title": "Motion Planning and Control of an Overactuated 4-Wheel Drive with Constrained Independent Steering", "comment": "7 pages, 5 figures, 3 tables, video available at\n  https://youtu.be/8l9s2Wb_vec, To appear at IEEE 2025 International Conference\n  on Advanced Robotics", "summary": "This paper addresses motion planning and con- trol of an overactuated 4-wheel\ndrive train with independent steering (4WIS) where mechanical constraints\nprevent the wheels from executing full 360-degree rotations (swerve). The\nconfiguration space of such a robot is constrained and contains discontinuities\nthat affect the smoothness of the robot motion. We introduce a mathematical\nformulation of the steering constraints and derive discontinuity planes that\npartition the velocity space into regions of smooth and efficient motion. We\nfurther design the motion planner for path tracking and ob- stacle avoidance\nthat explicitly accounts for swerve constraints and the velocity transition\nsmoothness. The motion controller uses local feedback to generate actuation\nfrom the desired velocity, while properly handling the discontinuity crossing\nby temporarily stopping the motion and repositioning the wheels. We implement\nthe proposed motion planner as an extension to ROS Navigation package and\nevaluate the system in simulation and on a physical robot.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5177\u6709\u72ec\u7acb\u8f6c\u5411\u7684\u8fc7\u9a71\u52a8\u56db\u8f6e\u9a71\u52a8\u7cfb\u7edf\uff084WIS\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u4e0e\u63a7\u5236\u95ee\u9898\uff0c\u5176\u4e2d\u673a\u68b0\u7ea6\u675f\u9650\u5236\u4e86\u8f66\u8f6e\u65e0\u6cd5\u5b9e\u73b0360\u5ea6\u5168\u5411\u65cb\u8f6c\u3002\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u8f6c\u5411\u7ea6\u675f\uff0c\u8bbe\u8ba1\u8003\u8651\u8f6c\u5411\u7ea6\u675f\u548c\u901f\u5ea6\u8fc7\u6e21\u5e73\u6ed1\u6027\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u5e76\u5f00\u53d1\u80fd\u591f\u6b63\u786e\u5904\u7406\u4e0d\u8fde\u7eed\u6027\u7a7f\u8d8a\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u3002", "motivation": "\u89e3\u51b3\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u7cfb\u7edf\u4e2d\u7531\u4e8e\u673a\u68b0\u7ea6\u675f\u5bfc\u81f4\u8f66\u8f6e\u65e0\u6cd5360\u5ea6\u65cb\u8f6c\u800c\u4ea7\u751f\u7684\u914d\u7f6e\u7a7a\u95f4\u7ea6\u675f\u548c\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u8fd9\u4e9b\u4e0d\u8fde\u7eed\u6027\u4f1a\u5f71\u54cd\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u5e73\u6ed1\u6027\u3002", "method": "\u5f15\u5165\u8f6c\u5411\u7ea6\u675f\u7684\u6570\u5b66\u516c\u5f0f\u5316\uff0c\u63a8\u5bfc\u5212\u5206\u901f\u5ea6\u7a7a\u95f4\u7684\u4e0d\u8fde\u7eed\u6027\u5e73\u9762\uff0c\u8bbe\u8ba1\u8003\u8651\u8f6c\u5411\u7ea6\u675f\u548c\u901f\u5ea6\u8fc7\u6e21\u5e73\u6ed1\u6027\u7684\u8def\u5f84\u8ddf\u8e2a\u548c\u907f\u969c\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u4f7f\u7528\u5c40\u90e8\u53cd\u9988\u751f\u6210\u9a71\u52a8\u5e76\u6b63\u786e\u5904\u7406\u4e0d\u8fde\u7eed\u6027\u7a7f\u8d8a\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u3002", "result": "\u5c06\u63d0\u51fa\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u5b9e\u73b0\u4e3aROS\u5bfc\u822a\u5305\u7684\u6269\u5c55\uff0c\u5e76\u5728\u4eff\u771f\u548c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u56db\u8f6e\u72ec\u7acb\u8f6c\u5411\u7cfb\u7edf\u4e2d\u7684\u8f6c\u5411\u7ea6\u675f\u548c\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u5e73\u6ed1\u9ad8\u6548\u7684\u8fd0\u52a8\u63a7\u5236\u3002"}}
{"id": "2510.19058", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19058", "abs": "https://arxiv.org/abs/2510.19058", "authors": ["Fausto Vega", "Jon Arrizabalaga", "Ryan Watson", "Zachary Manchester"], "title": "Convex Maneuver Planning for Spacecraft Collision Avoidance", "comment": "8 pages, 6 figures, Accepted to International Space Robotics\n  Conference", "summary": "Conjunction analysis and maneuver planning for spacecraft collision avoidance\nremains a manual and time-consuming process, typically involving repeated\nforward simulations of hand-designed maneuvers. With the growing density of\nsatellites in low-Earth orbit (LEO), autonomy is becoming essential for\nefficiently evaluating and mitigating collisions. In this work, we present an\nalgorithm to design low-thrust collision-avoidance maneuvers for short-term\nconjunction events. We first formulate the problem as a nonconvex\nquadratically-constrained quadratic program (QCQP), which we then relax into a\nconvex semidefinite program (SDP) using Shor's relaxation. We demonstrate\nempirically that the relaxation is tight, which enables the recovery of\nglobally optimal solutions to the original nonconvex problem. Our formulation\nproduces a minimum-energy solution while ensuring a desired probability of\ncollision at the time of closest approach. Finally, if the desired probability\nof collision cannot be satisfied, we relax this constraint into a penalty,\nyielding a minimum-risk solution. We validate our algorithm with a\nhigh-fidelity simulation of a satellite conjunction in low-Earth orbit with a\nsimulated conjunction data message (CDM), demonstrating its effectiveness in\nreducing collision risk.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u77ed\u671f\u4ea4\u4f1a\u4e8b\u4ef6\u7684\u4f4e\u63a8\u529b\u78b0\u649e\u89c4\u907f\u673a\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u51f8\u534a\u5b9a\u89c4\u5212\uff0c\u5b9e\u73b0\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u786e\u4fdd\u5728\u6700\u8fd1\u63a5\u8fd1\u70b9\u8fbe\u5230\u671f\u671b\u7684\u78b0\u649e\u6982\u7387\u3002", "motivation": "\u968f\u7740\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u5bc6\u5ea6\u589e\u52a0\uff0c\u4f20\u7edf\u624b\u52a8\u78b0\u649e\u89c4\u907f\u89c4\u5212\u8fc7\u7a0b\u8017\u65f6\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u8bc4\u4f30\u548c\u7f13\u89e3\u78b0\u649e\u7684\u6548\u7387\u3002", "method": "\u9996\u5148\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u975e\u51f8\u4e8c\u6b21\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212(QCQP)\uff0c\u7136\u540e\u4f7f\u7528Shor\u677e\u5f1b\u5c06\u5176\u8f6c\u5316\u4e3a\u51f8\u534a\u5b9a\u89c4\u5212(SDP)\uff0c\u7ecf\u9a8c\u8bc1\u660e\u8be5\u677e\u5f1b\u662f\u7d27\u7684\uff0c\u53ef\u4ee5\u6062\u590d\u539f\u59cb\u95ee\u9898\u7684\u5168\u5c40\u6700\u4f18\u89e3\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u6700\u5c0f\u80fd\u91cf\u89e3\uff0c\u786e\u4fdd\u5728\u6700\u8fd1\u63a5\u8fd1\u70b9\u8fbe\u5230\u671f\u671b\u7684\u78b0\u649e\u6982\u7387\uff0c\u5982\u679c\u65e0\u6cd5\u6ee1\u8db3\u7ea6\u675f\u5219\u8f6c\u5316\u4e3a\u6700\u5c0f\u98ce\u9669\u89e3\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u822a\u5929\u5668\u78b0\u649e\u89c4\u907f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u78b0\u649e\u98ce\u9669\uff0c\u9002\u5e94\u65e5\u76ca\u62e5\u6324\u7684\u4f4e\u5730\u7403\u8f68\u9053\u73af\u5883\u3002"}}
{"id": "2510.19074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19074", "abs": "https://arxiv.org/abs/2510.19074", "authors": ["Yilang Liu", "Haoxiang You", "Ian Abraham"], "title": "Sample-Based Hybrid Mode Control: Asymptotically Optimal Switching of Algorithmic and Non-Differentiable Control Modes", "comment": null, "summary": "This paper investigates a sample-based solution to the hybrid mode control\nproblem across non-differentiable and algorithmic hybrid modes. Our approach\nreasons about a set of hybrid control modes as an integer-based optimization\nproblem where we select what mode to apply, when to switch to another mode, and\nthe duration for which we are in a given control mode. A sample-based variation\nis derived to efficiently search the integer domain for optimal solutions. We\nfind our formulation yields strong performance guarantees that can be applied\nto a number of robotics-related tasks. In addition, our approach is able to\nsynthesize complex algorithms and policies to compound behaviors and achieve\nchallenging tasks. Last, we demonstrate the effectiveness of our approach in\nreal-world robotic examples that require reactive switching between long-term\nplanning and high-frequency control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u6df7\u5408\u6a21\u5f0f\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5c06\u6df7\u5408\u63a7\u5236\u6a21\u5f0f\u5efa\u6a21\u4e3a\u6574\u6570\u4f18\u5316\u95ee\u9898\uff0c\u6709\u6548\u9009\u62e9\u63a7\u5236\u6a21\u5f0f\u3001\u5207\u6362\u65f6\u673a\u548c\u6301\u7eed\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u975e\u53ef\u5fae\u548c\u7b97\u6cd5\u6df7\u5408\u6a21\u5f0f\u4e2d\u7684\u6df7\u5408\u63a7\u5236\u95ee\u9898\uff0c\u9700\u8981\u5728\u957f\u671f\u89c4\u5212\u548c\u9ad8\u9891\u63a7\u5236\u4e4b\u95f4\u8fdb\u884c\u53cd\u5e94\u6027\u5207\u6362\u3002", "method": "\u5c06\u6df7\u5408\u63a7\u5236\u6a21\u5f0f\u5efa\u6a21\u4e3a\u6574\u6570\u4f18\u5316\u95ee\u9898\uff0c\u91c7\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u53d8\u4f53\u5728\u6574\u6570\u57df\u4e2d\u9ad8\u6548\u641c\u7d22\u6700\u4f18\u89e3\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u673a\u5668\u4eba\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\u4fdd\u8bc1\uff0c\u80fd\u591f\u5408\u6210\u590d\u6742\u7b97\u6cd5\u548c\u7b56\u7565\u6765\u5b9e\u73b0\u590d\u5408\u884c\u4e3a\u548c\u6311\u6218\u6027\u4efb\u52a1\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u9700\u8981\u957f\u671f\u89c4\u5212\u548c\u9ad8\u9891\u63a7\u5236\u4e4b\u95f4\u53cd\u5e94\u6027\u5207\u6362\u7684\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2510.19081", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19081", "abs": "https://arxiv.org/abs/2510.19081", "authors": ["Shifa Sulaiman", "Tobias Busk Jensen", "Stefan Hein Bengtson", "Simon B\u00f8gh"], "title": "Kinematic Analysis and Integration of Vision Algorithms for a Mobile Manipulator Employed Inside a Self-Driving Laboratory", "comment": "International Journal of Intelligent Robotics and Applications 2025", "summary": "Recent advances in robotics and autonomous systems have broadened the use of\nrobots in laboratory settings, including automated synthesis, scalable reaction\nworkflows, and collaborative tasks in self-driving laboratories (SDLs). This\npaper presents a comprehensive development of a mobile manipulator designed to\nassist human operators in such autonomous lab environments. Kinematic modeling\nof the manipulator is carried out based on the Denavit Hartenberg (DH)\nconvention and inverse kinematics solution is determined to enable precise and\nadaptive manipulation capabilities. A key focus of this research is enhancing\nthe manipulator ability to reliably grasp textured objects as a critical\ncomponent of autonomous handling tasks. Advanced vision-based algorithms are\nimplemented to perform real-time object detection and pose estimation, guiding\nthe manipulator in dynamic grasping and following tasks. In this work, we\nintegrate a vision method that combines feature-based detection with\nhomography-driven pose estimation, leveraging depth information to represent an\nobject pose as a $2$D planar projection within $3$D space. This adaptive\ncapability enables the system to accommodate variations in object orientation\nand supports robust autonomous manipulation across diverse environments. By\nenabling autonomous experimentation and human-robot collaboration, this work\ncontributes to the scalability and reproducibility of next-generation chemical\nlaboratories", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8e\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u73af\u5883\u7684\u79fb\u52a8\u673a\u68b0\u81c2\uff0c\u901a\u8fc7DH\u5efa\u6a21\u548c\u9006\u8fd0\u52a8\u5b66\u5b9e\u73b0\u7cbe\u786e\u64cd\u4f5c\uff0c\u7ed3\u5408\u89c6\u89c9\u7b97\u6cd5\u8fdb\u884c\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u589e\u5f3a\u7eb9\u7406\u7269\u4f53\u7684\u6293\u53d6\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u548c\u81ea\u4e3b\u7cfb\u7edf\u7684\u53d1\u5c55\uff0c\u5b9e\u9a8c\u5ba4\u4e2d\u673a\u5668\u4eba\u7684\u5e94\u7528\u8303\u56f4\u6269\u5927\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u534f\u52a9\u4eba\u7c7b\u64cd\u4f5c\u5458\u7684\u79fb\u52a8\u673a\u68b0\u81c2\uff0c\u4ee5\u652f\u6301\u81ea\u4e3b\u5b9e\u9a8c\u548c\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\u3002", "method": "\u57fa\u4e8eDenavit Hartenberg\u7ea6\u5b9a\u8fdb\u884c\u673a\u68b0\u81c2\u8fd0\u52a8\u5b66\u5efa\u6a21\uff0c\u786e\u5b9a\u9006\u8fd0\u52a8\u5b66\u89e3\uff1b\u5b9e\u73b0\u7ed3\u5408\u7279\u5f81\u68c0\u6d4b\u548c\u5355\u5e94\u6027\u9a71\u52a8\u7684\u59ff\u6001\u4f30\u8ba1\u7684\u89c6\u89c9\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u57283D\u7a7a\u95f4\u4e2d\u8868\u793a\u7269\u4f53\u59ff\u6001\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u9002\u5e94\u7269\u4f53\u65b9\u5411\u53d8\u5316\uff0c\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u81ea\u4e3b\u64cd\u4f5c\uff0c\u652f\u6301\u52a8\u6001\u6293\u53d6\u548c\u8ddf\u968f\u4efb\u52a1\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5b9e\u73b0\u81ea\u4e3b\u5b9e\u9a8c\u548c\u4eba\u7c7b-\u673a\u5668\u4eba\u534f\u4f5c\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5316\u5b66\u5b9e\u9a8c\u5ba4\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u590d\u6027\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.19101", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19101", "abs": "https://arxiv.org/abs/2510.19101", "authors": ["Matthew Jiang", "Shipeng Liu", "Feifei Qian"], "title": "Safe Active Navigation and Exploration for Planetary Environments Using Proprioceptive Measurements", "comment": null, "summary": "Legged robots can sense terrain through force interactions during locomotion,\noffering more reliable traversability estimates than remote sensing and serving\nas scouts for guiding wheeled rovers in challenging environments. However, even\nlegged scouts face challenges when traversing highly deformable or unstable\nterrain. We present Safe Active Exploration for Granular Terrain (SAEGT), a\nnavigation framework that enables legged robots to safely explore unknown\ngranular environments using proprioceptive sensing, particularly where visual\ninput fails to capture terrain deformability. SAEGT estimates the safe region\nand frontier region online from leg-terrain interactions using Gaussian Process\nregression for traversability assessment, with a reactive controller for\nreal-time safe exploration and navigation. SAEGT demonstrated its ability to\nsafely explore and navigate toward a specified goal using only proprioceptively\nestimated traversability in simulation.", "AI": {"tldr": "SAEGT\u662f\u4e00\u4e2a\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u5728\u672a\u77e5\u9897\u7c92\u5730\u5f62\u4e2d\u5b89\u5168\u63a2\u7d22\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u77e5\u6765\u8bc4\u4f30\u5730\u5f62\u53ef\u901a\u884c\u6027", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u867d\u7136\u80fd\u901a\u8fc7\u529b\u4ea4\u4e92\u611f\u77e5\u5730\u5f62\uff0c\u4f46\u5728\u9ad8\u5ea6\u53ef\u53d8\u5f62\u6216\u4e0d\u7a33\u5b9a\u5730\u5f62\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u7684\u5b89\u5168\u63a2\u7d22\u65b9\u6cd5", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u4ece\u817f-\u5730\u5f62\u4ea4\u4e92\u4e2d\u5728\u7ebf\u4f30\u8ba1\u5b89\u5168\u533a\u57df\u548c\u8fb9\u754c\u533a\u57df\uff0c\u7ed3\u5408\u53cd\u5e94\u5f0f\u63a7\u5236\u5668\u5b9e\u73b0\u5b9e\u65f6\u5b89\u5168\u63a2\u7d22\u548c\u5bfc\u822a", "result": "\u5728\u4eff\u771f\u4e2d\u8bc1\u660e\u4e86SAEGT\u80fd\u591f\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u77e5\u4f30\u8ba1\u7684\u53ef\u901a\u884c\u6027\u5b89\u5168\u63a2\u7d22\u5e76\u5bfc\u822a\u5230\u6307\u5b9a\u76ee\u6807", "conclusion": "SAEGT\u6846\u67b6\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u5728\u89c6\u89c9\u8f93\u5165\u65e0\u6cd5\u6355\u6349\u5730\u5f62\u53ef\u53d8\u5f62\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b89\u5168\u63a2\u7d22\u80fd\u529b"}}
{"id": "2510.19128", "categories": ["cs.RO", "cs.AI", "68T40 (Primary), 70Q05 (Secondary)"], "pdf": "https://arxiv.org/pdf/2510.19128", "abs": "https://arxiv.org/abs/2510.19128", "authors": ["Mehran Ghafarian Tamizi", "Homayoun Honari", "Amir Mehdi Soufi Enayati", "Aleksey Nozdryn-Plotnicki", "Homayoun Najjaran"], "title": "A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model", "comment": "20 pages, 9 figures", "summary": "Path planning for a robotic system in high-dimensional cluttered environments\nneeds to be efficient, safe, and adaptable for different environments and\nhardware. Conventional methods face high computation time and require extensive\nparameter tuning, while prior learning-based methods still fail to generalize\neffectively. The primary goal of this research is to develop a path planning\nframework capable of generalizing to unseen environments and new robotic\nmanipulators without the need for retraining. We present GADGET (Generalizable\nand Adaptive Diffusion-Guided Environment-aware Trajectory generation), a\ndiffusion-based planning model that generates joint-space trajectories\nconditioned on voxelized scene representations as well as start and goal\nconfigurations. A key innovation is GADGET's hybrid dual-conditioning mechanism\nthat combines classifier-free guidance via learned scene encoding with\nclassifier-guided Control Barrier Function (CBF) safety shaping, integrating\nenvironment awareness with real-time collision avoidance directly in the\ndenoising process. This design supports zero-shot transfer to new environments\nand robotic embodiments without retraining. Experimental results show that\nGADGET achieves high success rates with low collision intensity in\nspherical-obstacle, bin-picking, and shelf environments, with CBF guidance\nfurther improving safety. Moreover, comparative evaluations indicate strong\nperformance relative to both sampling-based and learning-based baselines.\nFurthermore, GADGET provides transferability across Franka Panda, Kinova Gen3\n(6/7-DoF), and UR5 robots, and physical execution on a Kinova Gen3 demonstrates\nits ability to generate safe, collision-free trajectories in real-world\nsettings.", "AI": {"tldr": "\u63d0\u51faGADGET\u6846\u67b6\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u673a\u5668\u4eba\u5173\u8282\u7a7a\u95f4\u8f68\u8ff9\uff0c\u7ed3\u5408\u573a\u666f\u8868\u793a\u548c\u8d77\u59cb\u76ee\u6807\u914d\u7f6e\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u65b0\u73af\u5883\u548c\u673a\u5668\u4eba\u786c\u4ef6\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u4ee5\u53ca\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u53cc\u6761\u4ef6\u673a\u5236\uff0c\u7ed3\u5408\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u7684\u573a\u666f\u7f16\u7801\u548c\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u6574\u5f62\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u96c6\u6210\u73af\u5883\u611f\u77e5\u548c\u5b9e\u65f6\u78b0\u649e\u907f\u514d\u3002", "result": "\u5728\u7403\u5f62\u969c\u788d\u7269\u3001\u7bb1\u62e3\u9009\u548c\u8d27\u67b6\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u3001\u4f4e\u78b0\u649e\u5f3a\u5ea6\uff0cCBF\u5f15\u5bfc\u8fdb\u4e00\u6b65\u63d0\u5347\u5b89\u5168\u6027\uff0c\u5728\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5c55\u793a\u826f\u597d\u8fc1\u79fb\u6027\u3002", "conclusion": "GADGET\u6846\u67b6\u80fd\u591f\u751f\u6210\u5b89\u5168\u3001\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\uff0c\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\u5230\u65b0\u73af\u5883\u548c\u673a\u5668\u4eba\u786c\u4ef6\uff0c\u5728\u771f\u5b9e\u7269\u7406\u6267\u884c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2510.19200", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19200", "abs": "https://arxiv.org/abs/2510.19200", "authors": ["Matteo Bortolon", "Nuno Ferreira Duarte", "Plinio Moreno", "Fabio Poiesi", "Jos\u00e9 Santos-Victor", "Alessio Del Bue"], "title": "GRASPLAT: Enabling dexterous grasping through novel view synthesis", "comment": "Accepted IROS 2025", "summary": "Achieving dexterous robotic grasping with multi-fingered hands remains a\nsignificant challenge. While existing methods rely on complete 3D scans to\npredict grasp poses, these approaches face limitations due to the difficulty of\nacquiring high-quality 3D data in real-world scenarios. In this paper, we\nintroduce GRASPLAT, a novel grasping framework that leverages consistent 3D\ninformation while being trained solely on RGB images. Our key insight is that\nby synthesizing physically plausible images of a hand grasping an object, we\ncan regress the corresponding hand joints for a successful grasp. To achieve\nthis, we utilize 3D Gaussian Splatting to generate high-fidelity novel views of\nreal hand-object interactions, enabling end-to-end training with RGB data.\nUnlike prior methods, our approach incorporates a photometric loss that refines\ngrasp predictions by minimizing discrepancies between rendered and real images.\nWe conduct extensive experiments on both synthetic and real-world grasping\ndatasets, demonstrating that GRASPLAT improves grasp success rates up to 36.9%\nover existing image-based methods. Project page:\nhttps://mbortolon97.github.io/grasplat/", "AI": {"tldr": "GRASPLAT\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6293\u53d6\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528RGB\u56fe\u50cf\u8bad\u7ec3\uff0c\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u751f\u6210\u624b-\u7269\u4f53\u4ea4\u4e92\u7684\u9ad8\u4fdd\u771f\u65b0\u89c6\u56fe\uff0c\u7ed3\u5408\u5149\u5ea6\u635f\u5931\u4f18\u5316\u6293\u53d6\u9884\u6d4b\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u56fe\u50cf\u65b9\u6cd5\u63d0\u5347\u6293\u53d6\u6210\u529f\u7387\u9ad8\u8fbe36.9%\u3002", "motivation": "\u591a\u6307\u673a\u5668\u4eba\u6293\u53d6\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u6574\u76843D\u626b\u63cf\u6765\u9884\u6d4b\u6293\u53d6\u59ff\u6001\uff0c\u4f46\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u83b7\u53d6\u9ad8\u8d28\u91cf3D\u6570\u636e\u5f88\u56f0\u96be\u3002", "method": "\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\u751f\u6210\u771f\u5b9e\u624b-\u7269\u4f53\u4ea4\u4e92\u7684\u9ad8\u4fdd\u771f\u65b0\u89c6\u56fe\uff0c\u901a\u8fc7\u5408\u6210\u7269\u7406\u4e0a\u5408\u7406\u7684\u624b\u6293\u53d6\u7269\u4f53\u56fe\u50cf\u6765\u56de\u5f52\u5bf9\u5e94\u7684\u624b\u5173\u8282\uff0c\u7ed3\u5408\u5149\u5ea6\u635f\u5931\u6700\u5c0f\u5316\u6e32\u67d3\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6293\u53d6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGRASPLAT\u6bd4\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u63d0\u9ad8\u6293\u53d6\u6210\u529f\u7387\u9ad8\u8fbe36.9%\u3002", "conclusion": "GRASPLAT\u5c55\u793a\u4e86\u4ec5\u4f7f\u7528RGB\u56fe\u50cf\u8bad\u7ec3\u5c31\u80fd\u5b9e\u73b0\u6709\u6548\u6293\u53d6\u9884\u6d4b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u6293\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.19268", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19268", "abs": "https://arxiv.org/abs/2510.19268", "authors": ["Mingen Li", "Houjian Yu", "Yixuan Huang", "Youngjin Hong", "Changhyun Choi"], "title": "Hierarchical DLO Routing with Reinforcement Learning and In-Context Vision-language Models", "comment": "8 pages, 6 figures, 3 tables", "summary": "Long-horizon routing tasks of deformable linear objects (DLOs), such as\ncables and ropes, are common in industrial assembly lines and everyday life.\nThese tasks are particularly challenging because they require robots to\nmanipulate DLO with long-horizon planning and reliable skill execution.\nSuccessfully completing such tasks demands adapting to their nonlinear\ndynamics, decomposing abstract routing goals, and generating multi-step plans\ncomposed of multiple skills, all of which require accurate high-level reasoning\nduring execution. In this paper, we propose a fully autonomous hierarchical\nframework for solving challenging DLO routing tasks. Given an implicit or\nexplicit routing goal expressed in language, our framework leverages\nvision-language models~(VLMs) for in-context high-level reasoning to synthesize\nfeasible plans, which are then executed by low-level skills trained via\nreinforcement learning. To improve robustness in long horizons, we further\nintroduce a failure recovery mechanism that reorients the DLO into\ninsertion-feasible states. Our approach generalizes to diverse scenes involving\nobject attributes, spatial descriptions, as well as implicit language commands.\nIt outperforms the next best baseline method by nearly 50% and achieves an\noverall success rate of 92.5% across long-horizon routing scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u89e3\u51b3\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08\u5982\u7535\u7f06\u548c\u7ef3\u7d22\uff09\u957f\u65f6\u7a0b\u8def\u7531\u4efb\u52a1\u7684\u81ea\u4e3b\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u4f4e\u5c42\u6280\u80fd\u6267\u884c\uff0c\u5728\u957f\u65f6\u7a0b\u8def\u7531\u573a\u666f\u4e2d\u8fbe\u523092.5%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5de5\u4e1a\u88c5\u914d\u7ebf\u548c\u65e5\u5e38\u751f\u6d3b\u4e2d\u5e38\u89c1\u7684\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u8def\u7531\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u957f\u65f6\u7a0b\u89c4\u5212\u3001\u53ef\u9760\u6280\u80fd\u6267\u884c\uff0c\u5e76\u9002\u5e94\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u3001\u5206\u89e3\u62bd\u8c61\u8def\u7531\u76ee\u6807\u3001\u751f\u6210\u591a\u6b65\u9aa4\u8ba1\u5212\uff0c\u8fd9\u4e9b\u90fd\u9700\u8981\u51c6\u786e\u7684\u9ad8\u5c42\u63a8\u7406\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff1a\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e0a\u4e0b\u6587\u9ad8\u5c42\u63a8\u7406\u5408\u6210\u53ef\u884c\u8ba1\u5212\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4f4e\u5c42\u6280\u80fd\u6267\u884c\u8ba1\u5212\uff0c\u5e76\u5f15\u5165\u6545\u969c\u6062\u590d\u673a\u5236\u5c06\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u91cd\u65b0\u5b9a\u5411\u5230\u53ef\u63d2\u5165\u72b6\u6001\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6d89\u53ca\u7269\u4f53\u5c5e\u6027\u3001\u7a7a\u95f4\u63cf\u8ff0\u4ee5\u53ca\u9690\u5f0f\u8bed\u8a00\u547d\u4ee4\u7684\u591a\u6837\u5316\u573a\u666f\u4e2d\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u6bd4\u6b21\u4f18\u57fa\u7ebf\u65b9\u6cd5\u6027\u80fd\u63d0\u9ad8\u8fd150%\uff0c\u5728\u957f\u65f6\u7a0b\u8def\u7531\u573a\u666f\u4e2d\u603b\u4f53\u6210\u529f\u7387\u8fbe\u523092.5%\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u5c42\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5177\u6709\u6311\u6218\u6027\u7684\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\u8def\u7531\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u5c42\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u4f4e\u5c42\u6280\u80fd\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u957f\u65f6\u7a0b\u573a\u666f\u4e2d\u7684\u9ad8\u6210\u529f\u7387\u6267\u884c\u3002"}}
{"id": "2510.19289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19289", "abs": "https://arxiv.org/abs/2510.19289", "authors": ["Kefeng Huang", "Jonathon Pipe", "Alice E. Martin", "Tianyuan Wang", "Barnabas A. Franklin", "Andy M. Tyrrell", "Ian J. S. Fairlamb", "Jihong Zhu"], "title": "TARMAC: A Taxonomy for Robot Manipulation in Chemistry", "comment": null, "summary": "Chemistry laboratory automation aims to increase throughput, reproducibility,\nand safety, yet many existing systems still depend on frequent human\nintervention. Advances in robotics have reduced this dependency, but without a\nstructured representation of the required skills, autonomy remains limited to\nbespoke, task-specific solutions with little capacity to transfer beyond their\ninitial design. Current experiment abstractions typically describe\nprotocol-level steps without specifying the robotic actions needed to execute\nthem. This highlights the lack of a systematic account of the manipulation\nskills required for robots in chemistry laboratories. To address this gap, we\nintroduce TARMAC - a Taxonomy for Robot Manipulation in Chemistry - a\ndomain-specific framework that defines and organizes the core manipulations\nneeded in laboratory practice. Based on annotated teaching-lab demonstrations\nand supported by experimental validation, TARMAC categorizes actions according\nto their functional role and physical execution requirements. Beyond serving as\na descriptive vocabulary, TARMAC can be instantiated as robot-executable\nprimitives and composed into higher-level macros, enabling skill reuse and\nsupporting scalable integration into long-horizon workflows. These\ncontributions provide a structured foundation for more flexible and autonomous\nlaboratory automation. More information is available at\nhttps://tarmac-paper.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TARMAC\uff08\u5316\u5b66\u673a\u5668\u4eba\u64cd\u4f5c\u5206\u7c7b\u6cd5\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u5316\u5b66\u5b9e\u9a8c\u5ba4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5206\u7c7b\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u7cfb\u7edf\u7f3a\u4e4f\u7ed3\u6784\u5316\u6280\u80fd\u8868\u793a\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u6027\u548c\u6280\u80fd\u590d\u7528\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5316\u5b66\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u7cfb\u7edf\u867d\u7136\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u6240\u9700\u64cd\u4f5c\u6280\u80fd\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u81ea\u4e3b\u6027\u4ecd\u7136\u6709\u9650\uff0c\u5927\u591a\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u96be\u4ee5\u5b9e\u73b0\u6280\u80fd\u8fc1\u79fb\u3002\u73b0\u6709\u7684\u5b9e\u9a8c\u62bd\u8c61\u901a\u5e38\u53ea\u63cf\u8ff0\u534f\u8bae\u7ea7\u6b65\u9aa4\uff0c\u800c\u6ca1\u6709\u6307\u5b9a\u6267\u884c\u8fd9\u4e9b\u6b65\u9aa4\u6240\u9700\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "method": "\u57fa\u4e8e\u6ce8\u91ca\u7684\u6559\u5b66\u5b9e\u9a8c\u5ba4\u6f14\u793a\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0cTARMAC\u6839\u636e\u529f\u80fd\u89d2\u8272\u548c\u7269\u7406\u6267\u884c\u8981\u6c42\u5bf9\u64cd\u4f5c\u8fdb\u884c\u5206\u7c7b\uff0c\u5c06\u52a8\u4f5c\u5b9a\u4e49\u4e3a\u673a\u5668\u4eba\u53ef\u6267\u884c\u7684\u539f\u8bed\uff0c\u5e76\u53ef\u7ec4\u5408\u6210\u66f4\u9ad8\u7ea7\u7684\u5b8f\u64cd\u4f5c\u3002", "result": "TARMAC\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b9a\u4e49\u548c\u7ec4\u7ec7\u5b9e\u9a8c\u5ba4\u5b9e\u8df5\u4e2d\u6240\u9700\u7684\u6838\u5fc3\u64cd\u4f5c\uff0c\u652f\u6301\u6280\u80fd\u590d\u7528\uff0c\u5e76\u80fd\u591f\u6269\u5c55\u5230\u957f\u671f\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002", "conclusion": "TARMAC\u4e3a\u66f4\u7075\u6d3b\u548c\u81ea\u4e3b\u7684\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u7684\u64cd\u4f5c\u5206\u7c7b\u548c\u53ef\u7ec4\u5408\u7684\u539f\u8bed\uff0c\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u6280\u80fd\u7684\u590d\u7528\u548c\u7cfb\u7edf\u96c6\u6210\u3002"}}
{"id": "2510.19356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19356", "abs": "https://arxiv.org/abs/2510.19356", "authors": ["Yu Fang", "Xinyu Wang", "Xuehe Zhang", "Wanli Xue", "Mingwei Zhang", "Shengyong Chen", "Jie Zhao"], "title": "Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model", "comment": null, "summary": "The wide application of flow-matching methods has greatly promoted the\ndevelopment of robot imitation learning. However, these methods all face the\nproblem of high inference time. To address this issue, researchers have\nproposed distillation methods and consistency methods, but the performance of\nthese methods still struggles to compete with that of the original diffusion\nmodels and flow-matching models. In this article, we propose a one-step\nshortcut method with multi-step integration for robot imitation learning. To\nbalance the inference speed and performance, we extend the multi-step\nconsistency loss on the basis of the shortcut model, split the one-step loss\ninto multi-step losses, and improve the performance of one-step inference.\nSecondly, to solve the problem of unstable optimization of the multi-step loss\nand the original flow-matching loss, we propose an adaptive gradient allocation\nmethod to enhance the stability of the learning process. Finally, we evaluate\nthe proposed method in two simulation benchmarks and five real-world\nenvironment tasks. The experimental results verify the effectiveness of the\nproposed algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u4e00\u6b65\u6377\u5f84\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6b65\u96c6\u6210\u6765\u5e73\u8861\u63a8\u7406\u901f\u5ea6\u548c\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u591a\u6b65\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5c06\u4e00\u6b65\u635f\u5931\u62c6\u5206\u4e3a\u591a\u6b65\u635f\u5931\uff0c\u5e76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u68af\u5ea6\u5206\u914d\u65b9\u6cd5\u6765\u89e3\u51b3\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u6d41\u5339\u914d\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u9762\u4e34\u63a8\u7406\u65f6\u95f4\u9ad8\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u84b8\u998f\u65b9\u6cd5\u548c\u4e00\u81f4\u6027\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u96be\u4ee5\u4e0e\u539f\u59cb\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6a21\u578b\u7ade\u4e89\u3002", "method": "1. \u5728\u6377\u5f84\u6a21\u578b\u57fa\u7840\u4e0a\u6269\u5c55\u591a\u6b65\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5c06\u4e00\u6b65\u635f\u5931\u62c6\u5206\u4e3a\u591a\u6b65\u635f\u5931\uff1b2. \u63d0\u51fa\u81ea\u9002\u5e94\u68af\u5ea6\u5206\u914d\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u6b65\u635f\u5931\u548c\u539f\u59cb\u6d41\u5339\u914d\u635f\u5931\u7684\u4f18\u5316\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "result": "\u5728\u4e24\u4e2a\u6a21\u62df\u57fa\u51c6\u548c\u4e94\u4e2a\u771f\u5b9e\u73af\u5883\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5e73\u8861\u4e86\u63a8\u7406\u901f\u5ea6\u548c\u6027\u80fd\uff0c\u901a\u8fc7\u591a\u6b65\u96c6\u6210\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u5206\u914d\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\u3002"}}
{"id": "2510.19364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19364", "abs": "https://arxiv.org/abs/2510.19364", "authors": ["Golnaz Raja", "Ruslan Agishev", "Milo\u0161 Pr\u00e1gr", "Joni Pajarinen", "Karel Zimmermann", "Arun Kumar Singh", "Reza Ghabcheloo"], "title": "ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling", "comment": "This paper is submitted to IEEE International Conference on Robotics\n  and Automation (ICRA) 2026", "summary": "Uncertainty-aware robot motion prediction is crucial for downstream\ntraversability estimation and safe autonomous navigation in unstructured,\noff-road environments, where terrain is heterogeneous and perceptual\nuncertainty is high. Most existing methods assume deterministic or spatially\nindependent terrain uncertainties, ignoring the inherent local correlations of\n3D spatial data and often producing unreliable predictions. In this work, we\nintroduce an efficient probabilistic framework that explicitly models spatially\ncorrelated aleatoric uncertainty over terrain parameters as a probabilistic\nworld model and propagates this uncertainty through a differentiable physics\nengine for probabilistic trajectory forecasting. By leveraging structured\nconvolutional operators, our approach provides high-resolution multivariate\npredictions at manageable computational cost. Experimental evaluation on a\npublicly available dataset shows significantly improved uncertainty estimation\nand trajectory prediction accuracy over aleatoric uncertainty estimation\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8fd0\u52a8\u9884\u6d4b\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7a7a\u95f4\u76f8\u5173\u7684\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u5e76\u4f20\u64ad\u5230\u53ef\u5fae\u5206\u7269\u7406\u5f15\u64ce\u4e2d\uff0c\u5b9e\u73b0\u6982\u7387\u8f68\u8ff9\u9884\u6d4b\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u8d8a\u91ce\u73af\u5883\u4e2d\uff0c\u5730\u5f62\u5f02\u8d28\u6027\u548c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u9ad8\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u786e\u5b9a\u6027\u6216\u7a7a\u95f4\u72ec\u7acb\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5ffd\u7565\u4e863D\u7a7a\u95f4\u6570\u636e\u7684\u5c40\u90e8\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u4e0d\u53ef\u9760\u7684\u9884\u6d4b\u3002", "method": "\u4f7f\u7528\u7ed3\u6784\u5316\u5377\u79ef\u7b97\u5b50\u663e\u5f0f\u5efa\u6a21\u5730\u5f62\u53c2\u6570\u7684\u7a7a\u95f4\u76f8\u5173\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u6982\u7387\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u7269\u7406\u5f15\u64ce\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u6982\u7387\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u8d8a\u91ce\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u76f8\u5173\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u4e0b\u6e38\u53ef\u7a7f\u8d8a\u6027\u4f30\u8ba1\u548c\u5b89\u5168\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u9884\u6d4b\u3002"}}
{"id": "2510.19373", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19373", "abs": "https://arxiv.org/abs/2510.19373", "authors": ["Basavasagar Patil", "Sydney Belt", "Jayjun Lee", "Nima Fazeli", "Bernadette Bucher"], "title": "Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets", "comment": null, "summary": "Increasingly large datasets of robot actions and sensory observations are\nbeing collected to train ever-larger neural networks. These datasets are\ncollected based on tasks and while these tasks may be distinct in their\ndescriptions, many involve very similar physical action sequences (e.g., 'pick\nup an apple' versus 'pick up an orange'). As a result, many datasets of robotic\ntasks are substantially imbalanced in terms of the physical robotic actions\nthey represent. In this work, we propose a simple sampling strategy for policy\ntraining that mitigates this imbalance. Our method requires only a few lines of\ncode to integrate into existing codebases and improves generalization. We\nevaluate our method in both pre-training small models and fine-tuning large\nfoundational models. Our results show substantial improvements on low-resource\ntasks compared to prior state-of-the-art methods, without degrading performance\non high-resource tasks. This enables more effective use of model capacity for\nmulti-task policies. We also further validate our approach in a real-world\nsetup on a Franka Panda robot arm across a diverse set of tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u91c7\u6837\u7b56\u7565\u6765\u7f13\u89e3\u673a\u5668\u4eba\u4efb\u52a1\u6570\u636e\u96c6\u4e2d\u7269\u7406\u52a8\u4f5c\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u4ee3\u7801\u5e93\u4e2d\uff0c\u80fd\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4f4e\u8d44\u6e90\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u4e0d\u5f71\u54cd\u9ad8\u8d44\u6e90\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u4efb\u52a1\u6570\u636e\u96c6\u901a\u5e38\u57fa\u4e8e\u4efb\u52a1\u63cf\u8ff0\u6536\u96c6\uff0c\u4f46\u8bb8\u591a\u4efb\u52a1\u6d89\u53ca\u76f8\u4f3c\u7684\u7269\u7406\u52a8\u4f5c\u5e8f\u5217\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u5728\u7269\u7406\u673a\u5668\u4eba\u52a8\u4f5c\u8868\u793a\u4e0a\u5b58\u5728\u4e25\u91cd\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u7684\u91c7\u6837\u7b56\u7565\uff0c\u53ea\u9700\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709\u4ee3\u7801\u5e93\u4e2d\uff0c\u7528\u4e8e\u7b56\u7565\u8bad\u7ec3\u4ee5\u7f13\u89e3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u9884\u8bad\u7ec3\u5c0f\u6a21\u578b\u548c\u5fae\u8c03\u5927\u578b\u57fa\u7840\u6a21\u578b\u4e2d\u90fd\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u4f4e\u8d44\u6e90\u4efb\u52a1\u4e0a\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb\uff0c\u4e14\u4e0d\u5f71\u54cd\u9ad8\u8d44\u6e90\u4efb\u52a1\u6027\u80fd\uff0c\u4f7f\u591a\u4efb\u52a1\u7b56\u7565\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u6a21\u578b\u5bb9\u91cf\u3002\u5728Franka Panda\u673a\u5668\u4eba\u81c2\u4e0a\u7684\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u91c7\u6837\u7b56\u7565\u80fd\u6709\u6548\u89e3\u51b3\u673a\u5668\u4eba\u4efb\u52a1\u6570\u636e\u96c6\u4e2d\u7684\u52a8\u4f5c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6613\u4e8e\u5b9e\u73b0\u548c\u96c6\u6210\u3002"}}
{"id": "2510.19415", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.19415", "abs": "https://arxiv.org/abs/2510.19415", "authors": ["Abdelrahman Sayed Sayed"], "title": "Risk Assessment of an Autonomous Underwater Snake Robot in Confined Operations", "comment": "9 pages, 6 figures, Accepted for publication in OCEANS 2023 -\n  Limerick", "summary": "The growing interest in ocean discovery imposes a need for inspection and\nintervention in confined and demanding environments. Eely's slender shape, in\naddition to its ability to change its body configurations, makes articulated\nunderwater robots an adequate option for such environments. However, operation\nof Eely in such environments imposes demanding requirements on the system, as\nit must deal with uncertain and unstructured environments, extreme\nenvironmental conditions, and reduced navigational capabilities. This paper\nproposes a Bayesian approach to assess the risks of losing Eely during two\nmission scenarios. The goal of this work is to improve Eely's performance and\nthe likelihood of mission success. Sensitivity analysis results are presented\nin order to demonstrate the causes having the highest impact on losing Eely.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u65b9\u6cd5\u6765\u8bc4\u4f30Eely\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u4e24\u79cd\u4efb\u52a1\u573a\u666f\u4e2d\u4e22\u5931\u7684\u98ce\u9669\uff0c\u65e8\u5728\u63d0\u9ad8Eely\u7684\u6027\u80fd\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u8bc6\u522b\u5bf9\u4e22\u5931\u98ce\u9669\u5f71\u54cd\u6700\u5927\u7684\u56e0\u7d20\u3002", "motivation": "\u6d77\u6d0b\u63a2\u6d4b\u9700\u6c42\u7684\u589e\u957f\u8981\u6c42\u5728\u53d7\u9650\u548c\u82db\u523b\u73af\u5883\u4e2d\u8fdb\u884c\u68c0\u67e5\u548c\u5e72\u9884\uff0cEely\u7684\u7ec6\u957f\u5f62\u72b6\u548c\u6539\u53d8\u8eab\u4f53\u914d\u7f6e\u7684\u80fd\u529b\u4f7f\u5176\u6210\u4e3a\u9002\u5408\u6b64\u7c7b\u73af\u5883\u7684\u9009\u9879\uff0c\u4f46\u64cd\u4f5cEely\u9762\u4e34\u4e0d\u786e\u5b9a\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u3001\u6781\u7aef\u73af\u5883\u6761\u4ef6\u548c\u964d\u4f4e\u7684\u5bfc\u822a\u80fd\u529b\u7b49\u6311\u6218\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u6765\u8bc4\u4f30Eely\u5728\u4e24\u79cd\u4efb\u52a1\u573a\u666f\u4e2d\u7684\u4e22\u5931\u98ce\u9669\uff0c\u5e76\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u4ee5\u8bc6\u522b\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u901a\u8fc7\u654f\u611f\u6027\u5206\u6790\u5c55\u793a\u4e86\u5bfc\u81f4Eely\u4e22\u5931\u5f71\u54cd\u6700\u5927\u7684\u539f\u56e0\u3002", "conclusion": "\u8d1d\u53f6\u65af\u98ce\u9669\u8bc4\u4f30\u65b9\u6cd5\u6709\u52a9\u4e8e\u63d0\u9ad8Eely\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u6027\u80fd\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2510.19430", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19430", "abs": "https://arxiv.org/abs/2510.19430", "authors": ["GigaBrain Team", "Angen Ye", "Boyuan Wang", "Chaojun Ni", "Guan Huang", "Guosheng Zhao", "Haoyun Li", "Jie Li", "Jiagang Zhu", "Lv Feng", "Peng Li", "Qiuping Deng", "Runqi Ouyang", "Wenkang Qin", "Xinze Chen", "Xiaofeng Wang", "Yang Wang", "Yifan Li", "Yilong Li", "Yiran Ding", "Yuan Xu", "Yun Ye", "Yukun Zhou", "Zhehao Dong", "Zhenan Wang", "Zhichao Liu", "Zheng Zhu"], "title": "GigaBrain-0: A World Model-Powered Vision-Language-Action Model", "comment": "https://gigabrain0.github.io/", "summary": "Training Vision-Language-Action (VLA) models for generalist robots typically\nrequires large-scale real-world robot data, which is expensive and\ntime-consuming to collect. The inefficiency of physical data collection\nseverely limits the scalability, and generalization capacity of current VLA\nsystems. To address this challenge, we introduce GigaBrain-0, a novel VLA\nfoundation model empowered by world model-generated data (e.g., video\ngeneration, real2real transfer, human transfer, view transfer, sim2real\ntransfer data). By leveraging world models to generate diverse data at scale,\nGigaBrain-0 significantly reduces reliance on real robot data while improving\ncross-task generalization. Our approach further improves policy robustness\nthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,\nenabling the model to reason about spatial geometry, object states, and\nlong-horizon dependencies during task execution. This leads to substantial\ngains in real-world performance on dexterous, long-horizon, and mobile\nmanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves\nsuperior generalization across variations in appearances (e.g., textures,\ncolors), object placements, and camera viewpoints. Additionally, we present\nGigaBrain-0-Small, an optimized lightweight variant designed to run efficiently\non devices such as the NVIDIA Jetson AGX Orin.", "AI": {"tldr": "GigaBrain-0\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u751f\u6210\u6570\u636e\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u6570\u636e\u51cf\u5c11\u5bf9\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8bad\u7ec3\u9700\u8981\u5927\u89c4\u6a21\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u9650\u5236\u4e86VLA\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528\u4e16\u754c\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u6570\u636e\uff08\u89c6\u9891\u751f\u6210\u3001\u771f\u5b9e\u5230\u771f\u5b9e\u4f20\u8f93\u3001\u4eba\u7c7b\u4f20\u8f93\u3001\u89c6\u89d2\u4f20\u8f93\u3001\u4eff\u771f\u5230\u771f\u5b9e\u4f20\u8f93\u6570\u636e\uff09\uff0c\u7ed3\u5408RGBD\u8f93\u5165\u5efa\u6a21\u548c\u5177\u8eab\u601d\u7ef4\u94fe\u76d1\u7763\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u7a7a\u95f4\u51e0\u4f55\u3001\u7269\u4f53\u72b6\u6001\u548c\u957f\u65f6\u4f9d\u8d56\u5173\u7cfb\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u7075\u5de7\u64cd\u4f5c\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5728\u5916\u89c2\u3001\u7269\u4f53\u653e\u7f6e\u548c\u76f8\u673a\u89c6\u89d2\u53d8\u5316\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u7248\u672cGigaBrain-0-Small\u3002", "conclusion": "GigaBrain-0\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u751f\u6210\u6570\u636e\u6709\u6548\u89e3\u51b3\u4e86\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.19495", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19495", "abs": "https://arxiv.org/abs/2510.19495", "authors": ["Kevin Huang", "Rosario Scalise", "Cleah Winston", "Ayush Agrawal", "Yunchu Zhang", "Rohan Baijal", "Markus Grotz", "Byron Boots", "Benjamin Burchfiel", "Hongkai Dai", "Masha Itkina", "Paarth Shah", "Abhishek Gupta"], "title": "Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning", "comment": null, "summary": "Imitation learning has proven effective for training robots to perform\ncomplex tasks from expert human demonstrations. However, it remains limited by\nits reliance on high-quality, task-specific data, restricting adaptability to\nthe diverse range of real-world object configurations and scenarios. In\ncontrast, non-expert data -- such as play data, suboptimal demonstrations,\npartial task completions, or rollouts from suboptimal policies -- can offer\nbroader coverage and lower collection costs. However, conventional imitation\nlearning approaches fail to utilize this data effectively. To address these\nchallenges, we posit that with right design decisions, offline reinforcement\nlearning can be used as a tool to harness non-expert data to enhance the\nperformance of imitation learning policies. We show that while standard offline\nRL approaches can be ineffective at actually leveraging non-expert data under\nthe sparse data coverage settings typically encountered in the real world,\nsimple algorithmic modifications can allow for the utilization of this data,\nwithout significant additional assumptions. Our approach shows that broadening\nthe support of the policy distribution can allow imitation algorithms augmented\nby offline RL to solve tasks robustly, showing considerably enhanced recovery\nand generalization behavior. In manipulation tasks, these innovations\nsignificantly increase the range of initial conditions where learned policies\nare successful when non-expert data is incorporated. Moreover, we show that\nthese methods are able to leverage all collected data, including partial or\nsuboptimal demonstrations, to bolster task-directed policy performance. This\nunderscores the importance of algorithmic techniques for using non-expert data\nfor robust policy learning in robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u5229\u7528\u975e\u4e13\u5bb6\u6570\u636e\u6765\u589e\u5f3a\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u5bf9\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6570\u636e\u4f9d\u8d56\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u4f9d\u8d56\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6570\u636e\uff0c\u9650\u5236\u4e86\u5728\u591a\u6837\u5316\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u3002\u975e\u4e13\u5bb6\u6570\u636e\uff08\u5982\u6e38\u620f\u6570\u636e\u3001\u6b21\u4f18\u6f14\u793a\u7b49\uff09\u5177\u6709\u66f4\u5e7f\u8986\u76d6\u548c\u66f4\u4f4e\u6536\u96c6\u6210\u672c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5229\u7528\u3002", "method": "\u91c7\u7528\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u5de5\u5177\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u7b97\u6cd5\u4fee\u6539\u6765\u5229\u7528\u975e\u4e13\u5bb6\u6570\u636e\uff0c\u5305\u62ec\u6269\u5c55\u7b56\u7565\u5206\u5e03\u652f\u6301\u8303\u56f4\u7b49\u6280\u672f\uff0c\u65e0\u9700\u989d\u5916\u5f3a\u5047\u8bbe\u3002", "result": "\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u7b56\u7565\u5728\u66f4\u5e7f\u6cdb\u521d\u59cb\u6761\u4ef6\u4e0b\u7684\u6210\u529f\u7387\uff0c\u589e\u5f3a\u4e86\u6062\u590d\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u6240\u6709\u6536\u96c6\u6570\u636e\uff08\u5305\u62ec\u90e8\u5206\u6216\u6b21\u4f18\u6f14\u793a\uff09\u6765\u63d0\u5347\u4efb\u52a1\u5bfc\u5411\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "\u7b97\u6cd5\u6280\u672f\u5bf9\u4e8e\u5229\u7528\u975e\u4e13\u5bb6\u6570\u636e\u8fdb\u884c\u9c81\u68d2\u7b56\u7565\u5b66\u4e60\u5728\u673a\u5668\u4eba\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6210\u4e3a\u6709\u6548\u5229\u7528\u975e\u4e13\u5bb6\u6570\u636e\u7684\u5de5\u5177\u3002"}}
{"id": "2510.19541", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19541", "abs": "https://arxiv.org/abs/2510.19541", "authors": ["Francesco Schetter", "Shifa Sulaiman", "Shoby George", "Paolino De Risi", "Fanny Ficuciello"], "title": "Optimizing Prosthetic Wrist Movement: A Model Predictive Control Approach", "comment": "International Conference on Social Robotics + AI 2025", "summary": "The integration of advanced control strategies into prosthetic hands is\nessential to improve their adaptability and performance. In this study, we\npresent an implementation of a Model Predictive Control (MPC) strategy to\nregulate the motions of a soft continuum wrist section attached to a\ntendon-driven prosthetic hand with less computational effort. MPC plays a\ncrucial role in enhancing the functionality and responsiveness of prosthetic\nhands. By leveraging predictive modeling, this approach enables precise\nmovement adjustments while accounting for dynamic user interactions. This\nadvanced control strategy allows for the anticipation of future movements and\nadjustments based on the current state of the prosthetic device and the\nintentions of the user. Kinematic and dynamic modelings are performed using\nEuler-Bernoulli beam and Lagrange methods respectively. Through simulation and\nexperimental validations, we demonstrate the effectiveness of MPC in optimizing\nwrist articulation and user control. Our findings suggest that this technique\nsignificantly improves the prosthetic hand dexterity, making movements more\nnatural and intuitive. This research contributes to the field of robotics and\nbiomedical engineering by offering a promising direction for intelligent\nprosthetic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u8c03\u8282\u8fde\u63a5\u5230\u808c\u8171\u9a71\u52a8\u5047\u80a2\u624b\u7684\u8f6f\u8fde\u7eed\u8155\u90e8\u8fd0\u52a8\uff0c\u901a\u8fc7\u9884\u6d4b\u5efa\u6a21\u5b9e\u73b0\u7cbe\u786e\u8fd0\u52a8\u8c03\u6574\uff0c\u663e\u8457\u63d0\u9ad8\u5047\u80a2\u624b\u7684\u7075\u6d3b\u6027\u548c\u81ea\u7136\u5ea6\u3002", "motivation": "\u5c06\u5148\u8fdb\u63a7\u5236\u7b56\u7565\u96c6\u6210\u5230\u5047\u80a2\u624b\u4e2d\u5bf9\u4e8e\u63d0\u9ad8\u5176\u9002\u5e94\u6027\u548c\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cMPC\u5728\u589e\u5f3a\u5047\u80a2\u624b\u529f\u80fd\u548c\u54cd\u5e94\u6027\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528\u3002", "method": "\u4f7f\u7528Euler-Bernoulli\u6881\u8fdb\u884c\u8fd0\u52a8\u5b66\u5efa\u6a21\uff0cLagrange\u65b9\u6cd5\u8fdb\u884c\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u5b9e\u65bd\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\u6765\u8c03\u8282\u8f6f\u8fde\u7eed\u8155\u90e8\u8fd0\u52a8\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660eMPC\u5728\u4f18\u5316\u8155\u90e8\u5173\u8282\u548c\u7528\u6237\u63a7\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u5047\u80a2\u624b\u7684\u7075\u6d3b\u6027\uff0c\u4f7f\u8fd0\u52a8\u66f4\u52a0\u81ea\u7136\u76f4\u89c2\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u4e3a\u667a\u80fd\u5047\u80a2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5bf9\u673a\u5668\u4eba\u6280\u672f\u548c\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u9886\u57df\u505a\u51fa\u4e86\u8d21\u732e\u3002"}}
{"id": "2510.19655", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19655", "abs": "https://arxiv.org/abs/2510.19655", "authors": ["Hongyu Ding", "Ziming Xu", "Yudong Fang", "You Wu", "Zixuan Chen", "Jieqi Shi", "Jing Huo", "Yifan Zhang", "Yang Gao"], "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments", "comment": null, "summary": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)\nrequires an agent to navigate unseen environments based on natural language\ninstructions without any prior training. Current methods face a critical\ntrade-off: either rely on environment-specific waypoint predictors that limit\nscene generalization, or underutilize the reasoning capabilities of large\nmodels during navigation. We introduce LaViRA, a simple yet effective zero-shot\nframework that addresses this dilemma by decomposing action into a\ncoarse-to-fine hierarchy: Language Action for high-level planning, Vision\nAction for perceptual grounding, and Robot Action for robust navigation. This\nmodular decomposition allows us to leverage the distinct strengths of different\nscales of Multimodal Large Language Models (MLLMs) at each stage, creating a\nsystem that is powerful in its reasoning, grounding and practical control.\nLaViRA significantly outperforms existing state-of-the-art methods on the\nVLN-CE benchmark, demonstrating superior generalization capabilities in unseen\nenvironments, while maintaining transparency and efficiency for real-world\ndeployment.", "AI": {"tldr": "LaViRA\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u52a8\u4f5c\u5206\u89e3\u4e3a\u8bed\u8a00\u52a8\u4f5c\u3001\u89c6\u89c9\u52a8\u4f5c\u548c\u673a\u5668\u4eba\u52a8\u4f5c\u4e09\u4e2a\u5c42\u6b21\uff0c\u5229\u7528\u4e0d\u540c\u89c4\u6a21\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u81ea\u9636\u6bb5\u7684\u4f18\u52bf\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5b9e\u73b0\u5353\u8d8a\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u73af\u5883\u7279\u5b9a\u8def\u5f84\u70b9\u9884\u6d4b\u5668\u9650\u5236\u573a\u666f\u6cdb\u5316\u80fd\u529b\uff0c\u4ee5\u53ca\u5927\u6a21\u578b\u5728\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u63a8\u7406\u80fd\u529b\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLaViRA\u6846\u67b6\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u52a8\u4f5c\u5c42\u6b21\u5206\u89e3\uff1a\u8bed\u8a00\u52a8\u4f5c\u7528\u4e8e\u9ad8\u5c42\u89c4\u5212\uff0c\u89c6\u89c9\u52a8\u4f5c\u7528\u4e8e\u611f\u77e5\u63a5\u5730\uff0c\u673a\u5668\u4eba\u52a8\u4f5c\u7528\u4e8e\u9c81\u68d2\u5bfc\u822a\uff0c\u5229\u7528\u4e0d\u540c\u89c4\u6a21MLLM\u5728\u5404\u9636\u6bb5\u7684\u4f18\u52bf\u3002", "result": "\u5728VLN-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "LaViRA\u901a\u8fc7\u6a21\u5757\u5316\u5206\u89e3\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u3001\u63a5\u5730\u548c\u5b9e\u9645\u63a7\u5236\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u900f\u660e\u5ea6\u548c\u6548\u7387\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2510.19663", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19663", "abs": "https://arxiv.org/abs/2510.19663", "authors": ["Vojt\u011bch Vrba", "Viktor Walter", "Petr \u0160t\u011bp\u00e1n", "Martin Saska"], "title": "Fast Marker Detection for UV-Based Visual Relative Localisation in Agile UAV Swarms", "comment": null, "summary": "A novel approach for the fast onboard detection of isolated markers for\nvisual relative localisation of multiple teammates in agile UAV swarms is\nintroduced in this paper. As the detection forms a key component of real-time\nlocalisation systems, a three-fold innovation is presented, consisting of an\noptimised procedure for CPUs, a GPU shader program, and a functionally\nequivalent FPGA streaming architecture. For the proposed CPU and GPU solutions,\nthe mean processing time per pixel of input camera frames was accelerated by\ntwo to three orders of magnitude compared to the state of the art. For the\nlocalisation task, the proposed FPGA architecture offered the most significant\noverall acceleration by minimising the total delay from camera exposure to\ndetection results. Additionally, the proposed solutions were evaluated on\nvarious 32-bit and 64-bit embedded platforms to demonstrate their efficiency,\nas well as their feasibility for applications using low-end UAVs and MAVs.\nThus, it has become a crucial enabling technology for agile UAV swarming.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u654f\u6377\u65e0\u4eba\u673a\u7fa4\u4e2d\u591a\u961f\u53cb\u89c6\u89c9\u76f8\u5bf9\u5b9a\u4f4d\u7684\u5feb\u901f\u673a\u8f7d\u5b64\u7acb\u6807\u8bb0\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5305\u542bCPU\u4f18\u5316\u7a0b\u5e8f\u3001GPU\u7740\u8272\u5668\u7a0b\u5e8f\u548cFPGA\u6d41\u67b6\u6784\u4e09\u79cd\u521b\u65b0\u65b9\u6848\uff0c\u5904\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u6280\u672f\u5feb2-3\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u654f\u6377\u65e0\u4eba\u673a\u7fa4\u5b9e\u65f6\u5b9a\u4f4d\u7cfb\u7edf\u7684\u5173\u952e\u9700\u6c42\uff0c\u9700\u8981\u5feb\u901f\u68c0\u6d4b\u5b64\u7acb\u6807\u8bb0\u4ee5\u5b9e\u73b0\u591a\u961f\u53cb\u4e4b\u95f4\u7684\u89c6\u89c9\u76f8\u5bf9\u5b9a\u4f4d\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u521b\u65b0\u65b9\u6848\uff1aCPU\u4f18\u5316\u7a0b\u5e8f\u3001GPU\u7740\u8272\u5668\u7a0b\u5e8f\u548c\u529f\u80fd\u7b49\u6548\u7684FPGA\u6d41\u67b6\u6784\uff0c\u5e76\u5728\u591a\u79cd32\u4f4d\u548c64\u4f4d\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "CPU\u548cGPU\u89e3\u51b3\u65b9\u6848\u7684\u6bcf\u50cf\u7d20\u5e73\u5747\u5904\u7406\u65f6\u95f4\u6bd4\u73b0\u6709\u6280\u672f\u5feb2-3\u4e2a\u6570\u91cf\u7ea7\uff0cFPGA\u67b6\u6784\u901a\u8fc7\u6700\u5c0f\u5316\u4ece\u76f8\u673a\u66dd\u5149\u5230\u68c0\u6d4b\u7ed3\u679c\u7684\u603b\u5ef6\u8fdf\u63d0\u4f9b\u4e86\u6700\u663e\u8457\u7684\u603b\u4f53\u52a0\u901f\u3002", "conclusion": "\u8be5\u6280\u672f\u5df2\u6210\u4e3a\u654f\u6377\u65e0\u4eba\u673a\u7fa4\u7684\u5173\u952e\u4f7f\u80fd\u6280\u672f\uff0c\u8bc1\u660e\u4e86\u5728\u4f4e\u7aef\u65e0\u4eba\u673a\u548cMAV\u5e94\u7528\u4e2d\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2510.19752", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9; I.2.8"], "pdf": "https://arxiv.org/pdf/2510.19752", "abs": "https://arxiv.org/abs/2510.19752", "authors": ["Ameesh Shah", "William Chen", "Adwait Godbole", "Federico Mora", "Sanjit A. Seshia", "Sergey Levine"], "title": "Learning Affordances at Inference-Time for Vision-Language-Action Models", "comment": "7 pages and appendix", "summary": "Solving complex real-world control tasks often takes multiple tries: if we\nfail at first, we reflect on what went wrong, and change our strategy\naccordingly to avoid making the same mistake. In robotics,\nVision-Language-Action models (VLAs) offer a promising path towards solving\ncomplex control tasks, but lack the ability to contextually and dynamically\nreadjust behavior when they fail to accomplish a task. In this work, we\nintroduce Learning from Inference-Time Execution (LITEN), which connects a VLA\nlow-level policy to a high-level VLM that conditions on past experiences by\nincluding them in-context, allowing it to learn the affordances and\ncapabilities of the low-level VLA. Our approach iterates between a reasoning\nphase that generates and executes plans for the low-level VLA, and an\nassessment phase that reflects on the resulting execution and draws useful\nconclusions to be included in future reasoning contexts. Unlike similar\napproaches to self-refinement in non-robotics domains, LITEN must reflect on\nunstructured real-world robot trajectories (e.g., raw videos), which requires\nstructured guiderails during assessment. Our experimental results demonstrate\nLITEN is able to effectively learn from past experience to generate plans that\nuse high-affordance instructions to accomplish long-horizon tasks.", "AI": {"tldr": "LITEN\u65b9\u6cd5\u901a\u8fc7\u8fde\u63a5VLA\u4f4e\u7ea7\u7b56\u7565\u548c\u9ad8\u7ea7VLM\uff0c\u5229\u7528\u8fc7\u53bb\u6267\u884c\u7ecf\u9a8c\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u4efb\u52a1\u5931\u8d25\u540e\u7684\u52a8\u6001\u884c\u4e3a\u8c03\u6574\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7f3a\u4e4f\u5728\u4efb\u52a1\u5931\u8d25\u65f6\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u65e0\u6cd5\u50cf\u4eba\u7c7b\u4e00\u6837\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u5e76\u6539\u8fdb\u7b56\u7565\u3002", "method": "\u5c06VLA\u4f4e\u7ea7\u7b56\u7565\u8fde\u63a5\u5230\u9ad8\u7ea7VLM\uff0c\u901a\u8fc7\u63a8\u7406\u9636\u6bb5\u751f\u6210\u6267\u884c\u8ba1\u5212\uff0c\u8bc4\u4f30\u9636\u6bb5\u53cd\u601d\u6267\u884c\u7ed3\u679c\u5e76\u63d0\u53d6\u6709\u7528\u7ed3\u8bba\uff0c\u5c06\u7ecf\u9a8c\u7eb3\u5165\u540e\u7eed\u63a8\u7406\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aLITEN\u80fd\u591f\u6709\u6548\u4ece\u8fc7\u53bb\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u751f\u6210\u4f7f\u7528\u9ad8\u53ef\u7528\u6027\u6307\u4ee4\u5b8c\u6210\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u8ba1\u5212\u3002", "conclusion": "LITEN\u901a\u8fc7\u8fde\u63a5\u9ad8\u4f4e\u7ea7\u6a21\u578b\u5e76\u5229\u7528\u6267\u884c\u65f6\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5904\u7406\u590d\u6742\u63a7\u5236\u4efb\u52a1\u7684\u80fd\u529b\u3002"}}
{"id": "2510.19766", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.19766", "abs": "https://arxiv.org/abs/2510.19766", "authors": ["Hongyu Ding", "Xinyue Liang", "Yudong Fang", "You Wu", "Jieqi Shi", "Jing Huo", "Wenbin Li", "Jing Wu", "Yu-Kun Lai", "Yang Gao"], "title": "SEA: Semantic Map Prediction for Active Exploration of Uncertain Areas", "comment": null, "summary": "In this paper, we propose SEA, a novel approach for active robot exploration\nthrough semantic map prediction and a reinforcement learning-based hierarchical\nexploration policy. Unlike existing learning-based methods that rely on\none-step waypoint prediction, our approach enhances the agent's long-term\nenvironmental understanding to facilitate more efficient exploration. We\npropose an iterative prediction-exploration framework that explicitly predicts\nthe missing areas of the map based on current observations. The difference\nbetween the actual accumulated map and the predicted global map is then used to\nguide exploration. Additionally, we design a novel reward mechanism that\nleverages reinforcement learning to update the long-term exploration\nstrategies, enabling us to construct an accurate semantic map within limited\nsteps. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art exploration strategies, achieving superior\ncoverage ares of the global map within the same time constraints.", "AI": {"tldr": "\u63d0\u51faSEA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u9884\u6d4b\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5c42\u6b21\u63a2\u7d22\u7b56\u7565\u8fdb\u884c\u4e3b\u52a8\u673a\u5668\u4eba\u63a2\u7d22\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u4f9d\u8d56\u5355\u6b65\u8def\u5f84\u70b9\u9884\u6d4b\uff0c\u7f3a\u4e4f\u957f\u671f\u73af\u5883\u7406\u89e3\uff0c\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4e0d\u9ad8", "method": "\u91c7\u7528\u8fed\u4ee3\u9884\u6d4b-\u63a2\u7d22\u6846\u67b6\uff0c\u57fa\u4e8e\u5f53\u524d\u89c2\u6d4b\u9884\u6d4b\u5730\u56fe\u7f3a\u5931\u533a\u57df\uff0c\u5229\u7528\u5b9e\u9645\u5730\u56fe\u4e0e\u9884\u6d4b\u5168\u5c40\u5730\u56fe\u7684\u5dee\u5f02\u6307\u5bfc\u63a2\u7d22\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5956\u52b1\u673a\u5236\u66f4\u65b0\u957f\u671f\u63a2\u7d22\u7b56\u7565", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u76f8\u540c\u65f6\u95f4\u9650\u5236\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u63a2\u7d22\u7b56\u7565\uff0c\u83b7\u5f97\u66f4\u4f18\u7684\u5168\u5c40\u5730\u56fe\u8986\u76d6\u8303\u56f4", "conclusion": "SEA\u65b9\u6cd5\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u5c42\u6b21\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u63a2\u7d22\u7684\u6548\u7387\u548c\u51c6\u786e\u6027"}}
