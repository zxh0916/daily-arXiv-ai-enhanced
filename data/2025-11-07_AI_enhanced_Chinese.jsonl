{"id": "2511.03931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03931", "abs": "https://arxiv.org/abs/2511.03931", "authors": ["Iman Adibnazari", "Harsh Sharma", "Myungsun Park", "Jacobo Cervera-Torralba", "Boris Kramer", "Michael T. Tolley"], "title": "Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction", "comment": "20 Pages, 8 Figures", "summary": "Soft robots have shown immense promise in settings where they can leverage\ndynamic control of their entire bodies. However, effective dynamic shape\ncontrol requires a controller that accounts for the robot's high-dimensional\ndynamics--a challenge exacerbated by a lack of general-purpose tools for\nmodeling soft robots amenably for control. In this work, we conduct a\ncomparative study of data-driven model reduction techniques for generating\nlinear models amendable to dynamic shape control. We focus on three\nmethods--the eigensystem realization algorithm, dynamic mode decomposition with\ncontrol, and the Lagrangian operator inference (LOpInf) method. Using each\nclass of model, we explored their efficacy in model predictive control policies\nfor the dynamic shape control of a simulated eel-inspired soft robot in three\nexperiments: 1) tracking simulated reference trajectories guaranteed to be\nfeasible, 2) tracking reference trajectories generated from a biological model\nof eel kinematics, and 3) tracking reference trajectories generated by a\nreduced-scale physical analog. In all experiments, the LOpInf-based policies\ngenerated lower tracking errors than policies based on other models.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u6570\u636e\u9a71\u52a8\u6a21\u578b\u964d\u9636\u6280\u672f\uff08ERA\u3001DMDc\u3001LOpInf\uff09\u5728\u9cd7\u9c7c\u4eff\u751f\u8f6f\u4f53\u673a\u5668\u4eba\u52a8\u6001\u5f62\u72b6\u63a7\u5236\u4e2d\u7684\u6548\u679c\uff0c\u53d1\u73b0LOpInf\u65b9\u6cd5\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\u90fd\u80fd\u4ea7\u751f\u6700\u4f4e\u7684\u8ddf\u8e2a\u8bef\u5dee\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u9700\u8981\u5229\u7528\u5168\u8eab\u52a8\u6001\u63a7\u5236\u7684\u573a\u666f\u4e2d\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u6709\u6548\u7684\u52a8\u6001\u5f62\u72b6\u63a7\u5236\u9700\u8981\u80fd\u591f\u5904\u7406\u9ad8\u7ef4\u52a8\u529b\u5b66\u7684\u63a7\u5236\u5668\uff0c\u800c\u7f3a\u4e4f\u9002\u7528\u4e8e\u63a7\u5236\u7684\u901a\u7528\u5efa\u6a21\u5de5\u5177\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u6570\u636e\u9a71\u52a8\u6a21\u578b\u964d\u9636\u6280\u672f\uff08\u7279\u5f81\u7cfb\u7edf\u5b9e\u73b0\u7b97\u6cd5\u3001\u5e26\u63a7\u5236\u7684\u52a8\u6001\u6a21\u6001\u5206\u89e3\u3001\u62c9\u683c\u6717\u65e5\u7b97\u5b50\u63a8\u65ad\u65b9\u6cd5\uff09\u751f\u6210\u9002\u7528\u4e8e\u52a8\u6001\u5f62\u72b6\u63a7\u5236\u7684\u7ebf\u6027\u6a21\u578b\uff0c\u5e76\u5728\u6a21\u62df\u7684\u9cd7\u9c7c\u4eff\u751f\u8f6f\u4f53\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7b56\u7565\u6d4b\u8bd5\u3002", "result": "\u5728\u6240\u6709\u4e09\u4e2a\u5b9e\u9a8c\u4e2d\uff08\u8ddf\u8e2a\u4fdd\u8bc1\u53ef\u884c\u7684\u6a21\u62df\u53c2\u8003\u8f68\u8ff9\u3001\u8ddf\u8e2a\u57fa\u4e8e\u751f\u7269\u9cd7\u9c7c\u8fd0\u52a8\u5b66\u6a21\u578b\u751f\u6210\u7684\u53c2\u8003\u8f68\u8ff9\u3001\u8ddf\u8e2a\u7531\u7f29\u5c0f\u7269\u7406\u6a21\u62df\u5668\u751f\u6210\u7684\u53c2\u8003\u8f68\u8ff9\uff09\uff0c\u57fa\u4e8eLOpInf\u7684\u7b56\u7565\u4ea7\u751f\u7684\u8ddf\u8e2a\u8bef\u5dee\u5747\u4f4e\u4e8e\u57fa\u4e8e\u5176\u4ed6\u6a21\u578b\u7684\u7b56\u7565\u3002", "conclusion": "\u62c9\u683c\u6717\u65e5\u7b97\u5b50\u63a8\u65ad\u65b9\u6cd5\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u52a8\u6001\u5f62\u72b6\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u5176\u4ed6\u6570\u636e\u9a71\u52a8\u6a21\u578b\u964d\u9636\u6280\u672f\u7684\u6027\u80fd\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u52a8\u6001\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5efa\u6a21\u5de5\u5177\u3002"}}
{"id": "2511.03996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.03996", "abs": "https://arxiv.org/abs/2511.03996", "authors": ["Yushi Wang", "Changsheng Luo", "Penghui Chen", "Jianran Liu", "Weijian Sun", "Tong Guo", "Kechang Yang", "Biao Hu", "Yangang Zhang", "Mingguo Zhao"], "title": "Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots", "comment": "Project page: https://humanoid-kick.github.io", "summary": "Humanoid soccer poses a representative challenge for embodied intelligence,\nrequiring robots to operate within a tightly coupled perception-action loop.\nHowever, existing systems typically rely on decoupled modules, resulting in\ndelayed responses and incoherent behaviors in dynamic environments, while\nreal-world perceptual limitations further exacerbate these issues. In this\nwork, we present a unified reinforcement learning-based controller that enables\nhumanoid robots to acquire reactive soccer skills through the direct\nintegration of visual perception and motion control. Our approach extends\nAdversarial Motion Priors to perceptual settings in real-world dynamic\nenvironments, bridging motion imitation and visually grounded dynamic control.\nWe introduce an encoder-decoder architecture combined with a virtual perception\nsystem that models real-world visual characteristics, allowing the policy to\nrecover privileged states from imperfect observations and establish active\ncoordination between perception and action. The resulting controller\ndemonstrates strong reactivity, consistently executing coherent and robust\nsoccer behaviors across various scenarios, including real RoboCup matches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u63a7\u5236\u5668\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u89c6\u89c9\u611f\u77e5\u548c\u8fd0\u52a8\u63a7\u5236\u7684\u76f4\u63a5\u96c6\u6210\u6765\u83b7\u5f97\u53cd\u5e94\u5f0f\u8db3\u7403\u6280\u80fd\u3002\u8be5\u65b9\u6cd5\u5c06\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u6269\u5c55\u5230\u771f\u5b9e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u611f\u77e5\u8bbe\u7f6e\uff0c\u7ed3\u5408\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u865a\u62df\u611f\u77e5\u7cfb\u7edf\uff0c\u5728RoboCup\u6bd4\u8d5b\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u53cd\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u89e3\u8026\u6a21\u5757\u5bfc\u81f4\u7684\u5ef6\u8fdf\u54cd\u5e94\u548c\u4e0d\u8fde\u8d2f\u884c\u4e3a\u95ee\u9898\uff0c\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u611f\u77e5\u9650\u5236\u5bf9\u52a8\u6001\u73af\u5883\u7684\u8fdb\u4e00\u6b65\u5f71\u54cd\u3002", "method": "\u6269\u5c55\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u5230\u611f\u77e5\u8bbe\u7f6e\uff0c\u5f15\u5165\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u548c\u865a\u62df\u611f\u77e5\u7cfb\u7edf\uff0c\u4ece\u975e\u5b8c\u7f8e\u89c2\u5bdf\u4e2d\u6062\u590d\u7279\u6743\u72b6\u6001\uff0c\u5efa\u7acb\u611f\u77e5\u4e0e\u52a8\u4f5c\u7684\u4e3b\u52a8\u534f\u8c03\u3002", "result": "\u63a7\u5236\u5668\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u53cd\u5e94\u6027\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e2d\u6301\u7eed\u6267\u884c\u8fde\u8d2f\u4e14\u9c81\u68d2\u7684\u8db3\u7403\u884c\u4e3a\uff0c\u5305\u62ec\u771f\u5b9e\u7684RoboCup\u6bd4\u8d5b\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\u76f4\u63a5\u96c6\u6210\u89c6\u89c9\u611f\u77e5\u548c\u8fd0\u52a8\u63a7\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53cd\u5e94\u5f0f\u8db3\u7403\u6280\u80fd\u83b7\u53d6\u3002"}}
{"id": "2511.04009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04009", "abs": "https://arxiv.org/abs/2511.04009", "authors": ["Chenzui Li", "Yiming Chen", "Xi Wu", "Giacinto Barresi", "Fei Chen"], "title": "Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration", "comment": "7 pages, 7 figures, IROS 2025 accepted", "summary": "This paper introduces an upper limb postural optimization method for\nenhancing physical ergonomics and force manipulability during bimanual\nhuman-robot co-carrying tasks. Existing research typically emphasizes human\nsafety or manipulative efficiency, whereas our proposed method uniquely\nintegrates both aspects to strengthen collaboration across diverse conditions\n(e.g., different grasping postures of humans, and different shapes of objects).\nSpecifically, the joint angles of a simplified human skeleton model are\noptimized by minimizing the cost function to prioritize safety and manipulative\ncapability. To guide humans towards the optimized posture, the reference\nend-effector poses of the robot are generated through a transformation module.\nA bimanual model predictive impedance controller (MPIC) is proposed for our\nhuman-like robot, CURI, to recalibrate the end effector poses through planned\ntrajectories. The proposed method has been validated through various subjects\nand objects during human-human collaboration (HHC) and human-robot\ncollaboration (HRC). The experimental results demonstrate significant\nimprovement in muscle conditions by comparing the activation of target muscles\nbefore and after optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u53cc\u4eba\u534f\u4f5c\u642c\u8fd0\u4efb\u52a1\u7684\u4e0a\u80a2\u59ff\u6001\u4f18\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u4eba\u4f53\u5de5\u7a0b\u5b66\u548c\u529b\u64cd\u7eb5\u6027\uff0c\u901a\u8fc7\u4f18\u5316\u7b80\u5316\u4eba\u4f53\u9aa8\u9abc\u6a21\u578b\u7684\u5173\u8282\u89d2\u5ea6\u6765\u63d0\u5347\u5b89\u5168\u6027\u548c\u64cd\u4f5c\u80fd\u529b\uff0c\u5e76\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u963b\u6297\u63a7\u5236\u5668\u5f15\u5bfc\u673a\u5668\u4eba\u8c03\u6574\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u53ea\u5173\u6ce8\u4eba\u7c7b\u5b89\u5168\u6216\u64cd\u4f5c\u6548\u7387\uff0c\u800c\u8be5\u65b9\u6cd5\u72ec\u7279\u5730\u5c06\u8fd9\u4e24\u4e2a\u65b9\u9762\u7ed3\u5408\u8d77\u6765\uff0c\u4ee5\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\uff08\u5982\u4e0d\u540c\u7684\u6293\u63e1\u59ff\u52bf\u548c\u7269\u4f53\u5f62\u72b6\uff09\u52a0\u5f3a\u534f\u4f5c\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u6210\u672c\u51fd\u6570\u4f18\u5316\u7b80\u5316\u4eba\u4f53\u9aa8\u9abc\u6a21\u578b\u7684\u5173\u8282\u89d2\u5ea6\uff0c\u4f18\u5148\u8003\u8651\u5b89\u5168\u6027\u548c\u64cd\u7eb5\u80fd\u529b\uff1b\u4f7f\u7528\u53d8\u6362\u6a21\u5757\u751f\u6210\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u53c2\u8003\u59ff\u6001\uff1b\u63d0\u51fa\u53cc\u624b\u673a\u5668\u4eba\u7684\u6a21\u578b\u9884\u6d4b\u963b\u6297\u63a7\u5236\u5668\u6765\u91cd\u65b0\u6821\u51c6\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eba\u4e0e\u4eba\u534f\u4f5c\u548c\u4eba\u673a\u534f\u4f5c\u4e2d\u901a\u8fc7\u591a\u4e2a\u53d7\u8bd5\u8005\u548c\u7269\u4f53\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u901a\u8fc7\u6bd4\u8f83\u4f18\u5316\u524d\u540e\u76ee\u6807\u808c\u8089\u7684\u6fc0\u6d3b\u60c5\u51b5\uff0c\u808c\u8089\u72b6\u51b5\u6709\u663e\u8457\u6539\u5584\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6539\u5584\u53cc\u4eba\u534f\u4f5c\u642c\u8fd0\u4efb\u52a1\u4e2d\u7684\u808c\u8089\u72b6\u51b5\uff0c\u63d0\u5347\u7269\u7406\u4eba\u4f53\u5de5\u7a0b\u5b66\u548c\u529b\u64cd\u7eb5\u6027\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u59ff\u6001\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04042", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.04042", "abs": "https://arxiv.org/abs/2511.04042", "authors": ["Kailun Ji", "Xiaoyu Hu", "Xinyu Zhang", "Jun Chen"], "title": "An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue", "comment": null, "summary": "Large-scale disaster Search And Rescue (SAR) operations are persistently\nchallenged by complex terrain and disrupted communications. While Unmanned\nAerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area\nsearch and supply delivery, yet their effective coordination places a\nsignificant cognitive burden on human operators. The core human-machine\ncollaboration bottleneck lies in the ``intention-to-action gap'', which is an\nerror-prone process of translating a high-level rescue objective into a\nlow-level swarm command under high intensity and pressure. To bridge this gap,\nthis study proposes a novel LLM-CRF system that leverages Large Language Models\n(LLMs) to model and augment human-swarm teaming cognition. The proposed\nframework initially captures the operator's intention through natural and\nmulti-modal interactions with the device via voice or graphical annotations. It\nthen employs the LLM as a cognitive engine to perform intention comprehension,\nhierarchical task decomposition, and mission planning for the UAV swarm. This\nclosed-loop framework enables the swarm to act as a proactive partner,\nproviding active feedback in real-time while reducing the need for manual\nmonitoring and control, which considerably advances the efficacy of the SAR\ntask. We evaluate the proposed framework in a simulated SAR scenario.\nExperimental results demonstrate that, compared to traditional order and\ncommand-based interfaces, the proposed LLM-driven approach reduced task\ncompletion time by approximately $64.2\\%$ and improved task success rate by\n$7\\%$. It also leads to a considerable reduction in subjective cognitive\nworkload, with NASA-TLX scores dropping by $42.9\\%$. This work establishes the\npotential of LLMs to create more intuitive and effective human-swarm\ncollaborations in high-stakes scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cdLLM-CRF\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u5efa\u6a21\u548c\u589e\u5f3a\u4eba-\u7fa4\u534f\u4f5c\u8ba4\u77e5\uff0c\u901a\u8fc7\u81ea\u7136\u4ea4\u4e92\u65b9\u5f0f\u6355\u83b7\u64cd\u4f5c\u5458\u610f\u56fe\uff0c\u5e76\u81ea\u52a8\u5206\u89e3\u4efb\u52a1\u548c\u89c4\u5212\u65e0\u4eba\u673a\u7fa4\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u641c\u6551\u4efb\u52a1\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u5927\u89c4\u6a21\u707e\u96be\u641c\u6551\u884c\u52a8\u9762\u4e34\u590d\u6742\u5730\u5f62\u548c\u901a\u4fe1\u4e2d\u65ad\u7684\u6311\u6218\uff0c\u65e0\u4eba\u673a\u7fa4\u867d\u7136\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u6709\u6548\u534f\u8c03\u7ed9\u4eba\u7c7b\u64cd\u4f5c\u5458\u5e26\u6765\u4e86\u5de8\u5927\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002\u6838\u5fc3\u7684\u4eba\u673a\u534f\u4f5c\u74f6\u9888\u5728\u4e8e\"\u610f\u56fe\u5230\u884c\u52a8\u5dee\u8ddd\"\uff0c\u5373\u5728\u9ad8\u538b\u73af\u5883\u4e0b\u5c06\u9ad8\u7ea7\u6551\u63f4\u76ee\u6807\u8f6c\u5316\u4e3a\u4f4e\u7ea7\u7fa4\u547d\u4ee4\u7684\u6613\u51fa\u9519\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faLLM-CRF\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u97f3\u6216\u56fe\u5f62\u6807\u6ce8\u7b49\u81ea\u7136\u591a\u6a21\u6001\u4ea4\u4e92\u6355\u83b7\u64cd\u4f5c\u5458\u610f\u56fe\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u8ba4\u77e5\u5f15\u64ce\u8fdb\u884c\u610f\u56fe\u7406\u89e3\u3001\u5206\u5c42\u4efb\u52a1\u5206\u89e3\u548c\u65e0\u4eba\u673a\u7fa4\u4efb\u52a1\u89c4\u5212\uff0c\u5f62\u6210\u95ed\u73af\u6846\u67b6\u4f7f\u7fa4\u7cfb\u7edf\u6210\u4e3a\u4e3b\u52a8\u5408\u4f5c\u4f19\u4f34\u3002", "result": "\u5728\u6a21\u62df\u641c\u6551\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u547d\u4ee4\u5f0f\u63a5\u53e3\u76f8\u6bd4\uff0cLLM\u9a71\u52a8\u65b9\u6cd5\u5c06\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea664.2%\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e867%\uff0cNASA-TLX\u4e3b\u89c2\u8ba4\u77e5\u8d1f\u8377\u8bc4\u5206\u4e0b\u964d\u4e8642.9%\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u786e\u7acb\u4e86LLM\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u521b\u5efa\u66f4\u76f4\u89c2\u6709\u6548\u7684\u4eba-\u7fa4\u534f\u4f5c\u7684\u6f5c\u529b\uff0c\u4f7f\u65e0\u4eba\u673a\u7fa4\u80fd\u591f\u4f5c\u4e3a\u4e3b\u52a8\u5408\u4f5c\u4f19\u4f34\u63d0\u4f9b\u5b9e\u65f6\u53cd\u9988\uff0c\u540c\u65f6\u51cf\u5c11\u624b\u52a8\u76d1\u63a7\u548c\u63a7\u5236\u7684\u9700\u6c42\u3002"}}
{"id": "2511.04052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04052", "abs": "https://arxiv.org/abs/2511.04052", "authors": ["Kyongsik Yun", "David Bayard", "Gerik Kubiak", "Austin Owens", "Andrew Johnson", "Ryan Johnson", "Dan Scharf", "Thomas Lu"], "title": "Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors", "comment": null, "summary": "Future planetary exploration missions demand high-performance, fault-tolerant\ncomputing to enable autonomous Guidance, Navigation, and Control (GNC) and\nLander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).\nThis paper evaluates the deployment of GNC and LVS algorithms on\nnext-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx\nVersal--demonstrating up to 15x speedup for LVS image processing and over 250x\nspeedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory\noptimization compared to legacy spaceflight hardware. To ensure computational\nreliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for\nTrusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that\nperforms real-time fault detection and correction across redundant cores.\nARBITER is validated in both static optimization tasks (GFOLD) and dynamic\nclosed-loop control (Attitude Control System). A fault injection study further\nidentifies the gradient computation stage in GFOLD as the most sensitive to\nbit-level errors, motivating selective protection strategies and vector-based\noutput arbitration. This work establishes a scalable and energy-efficient\narchitecture for future missions, including Mars Sample Return, Enceladus\nOrbilander, and Ceres Sample Return, where onboard autonomy, low latency, and\nfault resilience are critical.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u5728\u4e0b\u4e00\u4ee3\u591a\u6838\u5904\u7406\u5668\u4e0a\u90e8\u7f72GNC\u548cLVS\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u76f8\u6bd4\u4f20\u7edf\u822a\u5929\u786c\u4ef6\u9ad8\u8fbe15\u500d\u7684LVS\u56fe\u50cf\u5904\u7406\u52a0\u901f\u548c250\u500d\u7684GFOLD\u8f68\u8ff9\u4f18\u5316\u52a0\u901f\uff0c\u5e76\u63d0\u51fa\u4e86ARBITER\u591a\u6838\u6295\u7968\u673a\u5236\u8fdb\u884c\u5b9e\u65f6\u6545\u969c\u68c0\u6d4b\u548c\u7ea0\u6b63\u3002", "motivation": "\u672a\u6765\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u9700\u8981\u9ad8\u6027\u80fd\u3001\u5bb9\u9519\u7684\u8ba1\u7b97\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u5728\u8fdb\u5165\u3001\u4e0b\u964d\u548c\u7740\u9646\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u5236\u5bfc\u3001\u5bfc\u822a\u4e0e\u63a7\u5236\u4ee5\u53ca\u7740\u9646\u89c6\u89c9\u7cfb\u7edf\u64cd\u4f5c\u3002", "method": "\u5728HPSC\u3001Snapdragon VOXL2\u548cAMD Xilinx Versal\u7b49\u591a\u6838\u5904\u7406\u5668\u4e0a\u90e8\u7f72GNC\u548cLVS\u7b97\u6cd5\uff0c\u5e76\u5f00\u53d1ARBITER\u591a\u6838\u6295\u7968\u673a\u5236\u8fdb\u884c\u5b9e\u65f6\u6545\u969c\u68c0\u6d4b\u548c\u7ea0\u6b63\u3002", "result": "LVS\u56fe\u50cf\u5904\u7406\u5b9e\u73b015\u500d\u52a0\u901f\uff0cGFOLD\u8f68\u8ff9\u4f18\u5316\u5b9e\u73b0250\u500d\u52a0\u901f\uff1b\u6545\u969c\u6ce8\u5165\u7814\u7a76\u8bc6\u522b\u51faGFOLD\u4e2d\u7684\u68af\u5ea6\u8ba1\u7b97\u9636\u6bb5\u5bf9\u4f4d\u7ea7\u9519\u8bef\u6700\u654f\u611f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u672a\u6765\u4efb\u52a1\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u4e14\u8282\u80fd\u7684\u67b6\u6784\uff0c\u5305\u62ec\u706b\u661f\u6837\u672c\u8fd4\u56de\u3001\u571f\u536b\u4e8c\u8f68\u9053\u7740\u9646\u5668\u548c\u8c37\u795e\u661f\u6837\u672c\u8fd4\u56de\u7b49\u4efb\u52a1\uff0c\u5176\u4e2d\u673a\u8f7d\u81ea\u4e3b\u6027\u3001\u4f4e\u5ef6\u8fdf\u548c\u5bb9\u9519\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2511.04131", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04131", "abs": "https://arxiv.org/abs/2511.04131", "authors": ["Yitang Li", "Zhengyi Luo", "Tonghe Zhang", "Cunxi Dai", "Anssi Kanervisto", "Andrea Tirinzoni", "Haoyang Weng", "Kris Kitani", "Mateusz Guzek", "Ahmed Touati", "Alessandro Lazaric", "Matteo Pirotta", "Guanya Shi"], "title": "BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning", "comment": null, "summary": "Building Behavioral Foundation Models (BFMs) for humanoid robots has the\npotential to unify diverse control tasks under a single, promptable generalist\npolicy. However, existing approaches are either exclusively deployed on\nsimulated humanoid characters, or specialized to specific tasks such as\ntracking. We propose BFM-Zero, a framework that learns an effective shared\nlatent representation that embeds motions, goals, and rewards into a common\nspace, enabling a single policy to be prompted for multiple downstream tasks\nwithout retraining. This well-structured latent space in BFM-Zero enables\nversatile and robust whole-body skills on a Unitree G1 humanoid in the real\nworld, via diverse inference methods, including zero-shot motion tracking, goal\nreaching, and reward optimization, and few-shot optimization-based adaptation.\nUnlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds\nupon recent advancements in unsupervised RL and Forward-Backward (FB) models,\nwhich offer an objective-centric, explainable, and smooth latent representation\nof whole-body motions. We further extend BFM-Zero with critical reward shaping,\ndomain randomization, and history-dependent asymmetric learning to bridge the\nsim-to-real gap. Those key design choices are quantitatively ablated in\nsimulation. A first-of-its-kind model, BFM-Zero establishes a step toward\nscalable, promptable behavioral foundation models for whole-body humanoid\ncontrol.", "AI": {"tldr": "BFM-Zero\u662f\u4e00\u4e2a\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u884c\u4e3a\u57fa\u7840\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6f5c\u5728\u8868\u793a\u7edf\u4e00\u591a\u79cd\u63a7\u5236\u4efb\u52a1\uff0c\u652f\u6301\u96f6\u6837\u672c\u8fd0\u52a8\u8ddf\u8e2a\u3001\u76ee\u6807\u5230\u8fbe\u548c\u5956\u52b1\u4f18\u5316\u7b49\u529f\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5168\u8eab\u6280\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4ec5\u9650\u4e8e\u6a21\u62df\u4eba\u5f62\u89d2\u8272\uff0c\u8981\u4e48\u4e13\u95e8\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u5982\u8ddf\u8e2a\u3002BFM-Zero\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u5355\u4e00\u3001\u53ef\u63d0\u793a\u7684\u901a\u7528\u7b56\u7565\uff0c\u7edf\u4e00\u4e0d\u540c\u7684\u63a7\u5236\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u548c\u524d\u5411-\u540e\u5411\u6a21\u578b\uff0c\u5b66\u4e60\u5d4c\u5165\u8fd0\u52a8\u3001\u76ee\u6807\u548c\u5956\u52b1\u7684\u5171\u4eab\u6f5c\u5728\u8868\u793a\uff0c\u7ed3\u5408\u5173\u952e\u5956\u52b1\u5851\u9020\u3001\u9886\u57df\u968f\u673a\u5316\u548c\u5386\u53f2\u4f9d\u8d56\u975e\u5bf9\u79f0\u5b66\u4e60\u6765\u5f25\u5408\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "result": "BFM-Zero\u5728\u771f\u5b9e\u4e16\u754c\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5305\u62ec\u96f6\u6837\u672c\u8fd0\u52a8\u8ddf\u8e2a\u3001\u76ee\u6807\u5230\u8fbe\u548c\u5956\u52b1\u4f18\u5316\uff0c\u4ee5\u53ca\u5c11\u91cf\u6837\u672c\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u9002\u5e94\u3002", "conclusion": "BFM-Zero\u662f\u9996\u4e2a\u6b64\u7c7b\u6a21\u578b\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u63d0\u793a\u7684\u5168\u8eab\u4eba\u5f62\u63a7\u5236\u884c\u4e3a\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2511.04180", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04180", "abs": "https://arxiv.org/abs/2511.04180", "authors": ["Yizhen Yin", "Dapeng Feng", "Hongbo Chen", "Yuhua Qi"], "title": "PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration", "comment": null, "summary": "Existing Active SLAM methodologies face issues such as slow exploration speed\nand suboptimal paths. To address these limitations, we propose a hybrid\nframework combining a Path-Uncertainty Co-Optimization Deep Reinforcement\nLearning framework and a Lightweight Stagnation Detection mechanism. The\nPath-Uncertainty Co-Optimization framework jointly optimizes travel distance\nand map uncertainty through a dual-objective reward function, balancing\nexploration and exploitation. The Lightweight Stagnation Detection reduces\nredundant exploration through Lidar Static Anomaly Detection and Map Update\nStagnation Detection, terminating episodes on low expansion rates. Experimental\nresults show that compared with the frontier-based method and RRT method, our\napproach shortens exploration time by up to 65% and reduces path distance by up\nto 42%, significantly improving exploration efficiency in complex environments\nwhile maintaining reliable map completeness. Ablation studies confirm that the\ncollaborative mechanism accelerates training convergence. Empirical validation\non a physical robotic platform demonstrates the algorithm's practical\napplicability and its successful transferability from simulation to real-world\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8def\u5f84-\u4e0d\u786e\u5b9a\u6027\u534f\u540c\u4f18\u5316\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7\u505c\u6ede\u68c0\u6d4b\u673a\u5236\u7684\u6df7\u5408\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e3b\u52a8SLAM\u7684\u63a2\u7d22\u6548\u7387\u548c\u8def\u5f84\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8SLAM\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u901f\u5ea6\u6162\u548c\u8def\u5f84\u6b21\u4f18\u7684\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u4ee5\u53ca\u5197\u4f59\u63a2\u7d22\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8def\u5f84-\u4e0d\u786e\u5b9a\u6027\u534f\u540c\u4f18\u5316\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u76ee\u6807\u5956\u52b1\u51fd\u6570\u540c\u65f6\u4f18\u5316\u65c5\u884c\u8ddd\u79bb\u548c\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\uff1b\u7ed3\u5408\u8f7b\u91cf\u7ea7\u505c\u6ede\u68c0\u6d4b\u673a\u5236\uff0c\u5305\u62ec\u6fc0\u5149\u96f7\u8fbe\u9759\u6001\u5f02\u5e38\u68c0\u6d4b\u548c\u5730\u56fe\u66f4\u65b0\u505c\u6ede\u68c0\u6d4b\uff0c\u5728\u4f4e\u6269\u5c55\u7387\u65f6\u7ec8\u6b62\u63a2\u7d22\u3002", "result": "\u76f8\u6bd4\u524d\u6cbf\u65b9\u6cd5\u548cRRT\u65b9\u6cd5\uff0c\u63a2\u7d22\u65f6\u95f4\u7f29\u77ed\u8fbe65%\uff0c\u8def\u5f84\u8ddd\u79bb\u51cf\u5c11\u8fbe42%\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u9760\u7684\u5730\u56fe\u5b8c\u6574\u6027\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u534f\u4f5c\u673a\u5236\u52a0\u901f\u8bad\u7ec3\u6536\u655b\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u7269\u7406\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5f97\u5230\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u73af\u5883\u7684\u6210\u529f\u53ef\u8fc1\u79fb\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.04199", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04199", "abs": "https://arxiv.org/abs/2511.04199", "authors": ["Shenglin Wang", "Mingtong Dai", "Jingxuan Su", "Lingbo Liu", "Chunjie Chen", "Xinyu Wu", "Liang Lin"], "title": "GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments", "comment": null, "summary": "Robotic grasping is a fundamental capability for autonomous manipulation, yet\nremains highly challenging in cluttered environments where occlusion, poor\nperception quality, and inconsistent 3D reconstructions often lead to unstable\nor failed grasps. Conventional pipelines have widely relied on RGB-D cameras to\nprovide geometric information, which fail on transparent or glossy objects and\ndegrade at close range. We present GraspView, an RGB-only robotic grasping\npipeline that achieves accurate manipulation in cluttered environments without\ndepth sensors. Our framework integrates three key components: (i) global\nperception scene reconstruction, which provides locally consistent, up-to-scale\ngeometry from a single RGB view and fuses multi-view projections into a\ncoherent global 3D scene; (ii) a render-and-score active perception strategy,\nwhich dynamically selects next-best-views to reveal occluded regions; and (iii)\nan online metric alignment module that calibrates VGGT predictions against\nrobot kinematics to ensure physical scale consistency. Building on these\ntailor-designed modules, GraspView performs best-view global grasping, fusing\nmulti-view reconstructions and leveraging GraspNet for robust execution.\nExperiments on diverse tabletop objects demonstrate that GraspView\nsignificantly outperforms both RGB-D and single-view RGB baselines, especially\nunder heavy occlusion, near-field sensing, and with transparent objects. These\nresults highlight GraspView as a practical and versatile alternative to RGB-D\npipelines, enabling reliable grasping in unstructured real-world environments.", "AI": {"tldr": "GraspView\u662f\u4e00\u4e2a\u4ec5\u4f7f\u7528RGB\u76f8\u673a\u7684\u673a\u5668\u4eba\u6293\u53d6\u7cfb\u7edf\uff0c\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u64cd\u4f5c\uff0c\u65e0\u9700\u6df1\u5ea6\u4f20\u611f\u5668\uff0c\u7279\u522b\u64c5\u957f\u5904\u7406\u906e\u6321\u3001\u8fd1\u8ddd\u79bb\u611f\u77e5\u548c\u900f\u660e\u7269\u4f53\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eRGB-D\u76f8\u673a\u7684\u6293\u53d6\u7cfb\u7edf\u5728\u900f\u660e\u6216\u53cd\u5149\u7269\u4f53\u4e0a\u4f1a\u5931\u8d25\uff0c\u4e14\u5728\u8fd1\u8ddd\u79bb\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u4ec5\u4f7f\u7528RGB\u76f8\u673a\u7684\u53ef\u9760\u6293\u53d6\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u96c6\u6210\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u5168\u5c40\u611f\u77e5\u573a\u666f\u91cd\u5efa\uff08\u4ece\u5355RGB\u89c6\u56fe\u63d0\u4f9b\u5c40\u90e8\u4e00\u81f4\u51e0\u4f55\uff09\u3001\u6e32\u67d3\u8bc4\u5206\u4e3b\u52a8\u611f\u77e5\u7b56\u7565\uff08\u52a8\u6001\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u63ed\u793a\u906e\u6321\u533a\u57df\uff09\u3001\u5728\u7ebf\u5ea6\u91cf\u5bf9\u9f50\u6a21\u5757\uff08\u6821\u51c6\u6293\u53d6\u9884\u6d4b\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\uff09\u3002", "result": "\u5728\u591a\u6837\u5316\u684c\u9762\u7269\u4f53\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGraspView\u663e\u8457\u4f18\u4e8eRGB-D\u548c\u5355\u89c6\u89d2RGB\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4e25\u91cd\u906e\u6321\u3001\u8fd1\u8ddd\u79bb\u611f\u77e5\u548c\u900f\u660e\u7269\u4f53\u60c5\u51b5\u4e0b\u3002", "conclusion": "GraspView\u662fRGB-D\u7ba1\u9053\u7684\u5b9e\u7528\u4e14\u591a\u529f\u80fd\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u6293\u53d6\u3002"}}
{"id": "2511.04249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04249", "abs": "https://arxiv.org/abs/2511.04249", "authors": ["Marco Iannotta", "Yuxuan Yang", "Johannes A. Stork", "Erik Schaffernicht", "Todor Stoyanov"], "title": "Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies", "comment": null, "summary": "Sim-to-real transfer remains a major challenge in reinforcement learning (RL)\nfor robotics, as policies trained in simulation often fail to generalize to the\nreal world due to discrepancies in environment dynamics. Domain Randomization\n(DR) mitigates this issue by exposing the policy to a wide range of randomized\ndynamics during training, yet leading to a reduction in performance. While\nstandard approaches typically train policies agnostic to these variations, we\ninvestigate whether sim-to-real transfer can be improved by conditioning the\npolicy on an estimate of the dynamics parameters -- referred to as context. To\nthis end, we integrate a context estimation module into a DR-based RL framework\nand systematically compare SOTA supervision strategies. We evaluate the\nresulting context-aware policies in both a canonical control benchmark and a\nreal-world pushing task using a Franka Emika Panda robot. Results show that\ncontext-aware policies outperform the context-agnostic baseline across all\nsettings, although the best supervision strategy depends on the task.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u4e2d\u901a\u8fc7\u6761\u4ef6\u5316\u7b56\u7565\u4e8e\u52a8\u6001\u53c2\u6570\u4f30\u8ba1\uff08\u4e0a\u4e0b\u6587\uff09\u6765\u6539\u8fdb\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u5728\u6807\u51c6\u63a7\u5236\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u63a8\u52a8\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565\u4f18\u4e8e\u4e0a\u4e0b\u6587\u65e0\u5173\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5373\u7531\u4e8e\u73af\u5883\u52a8\u6001\u5dee\u5f02\u5bfc\u81f4\u4eff\u771f\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6cdb\u5316\u5931\u8d25\u7684\u95ee\u9898\u3002", "method": "\u5c06\u4e0a\u4e0b\u6587\u4f30\u8ba1\u6a21\u5757\u96c6\u6210\u5230\u57fa\u4e8e\u57df\u968f\u673a\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u6700\u5148\u8fdb\u7684\u76d1\u7763\u7b56\u7565\uff0c\u4f7f\u7b56\u7565\u80fd\u591f\u6839\u636e\u52a8\u6001\u53c2\u6570\u4f30\u8ba1\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "result": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7b56\u7565\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u90fd\u4f18\u4e8e\u4e0a\u4e0b\u6587\u65e0\u5173\u57fa\u7ebf\uff0c\u4f46\u6700\u4f73\u76d1\u7763\u7b56\u7565\u53d6\u51b3\u4e8e\u5177\u4f53\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u6761\u4ef6\u5316\u7b56\u7565\u4e8e\u52a8\u6001\u53c2\u6570\u4f30\u8ba1\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u4f46\u9700\u8981\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u5408\u9002\u7684\u76d1\u7763\u7b56\u7565\u3002"}}
{"id": "2511.04251", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04251", "abs": "https://arxiv.org/abs/2511.04251", "authors": ["Jinfeng Liang", "Haocheng Guo", "Ximin Lyu"], "title": "Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism", "comment": "8 pages 12 figures", "summary": "The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to\nits lower dead weight, which eliminates the actuators and mechanisms for\ntilting. However, the tailsitter UAV is susceptible to wind disturbances in\nmulti-rotor mode, as it exposes a large frontal fuselage area. To address this\nissue, our tailsitter UAV features a reconfigurable wing design, allowing wings\nto retract in multi-rotor mode and extend in fixed- wing mode. Considering\npower efficiency, we design a coaxial heterogeneous dual-rotor configuration,\nwhich significantly re- duces the total power consumption. To reduce structural\nweight and simplify structural complexity, we employ a swashplateless mechanism\nwith an improved design to control pitch and roll in multi-rotor mode. We\noptimize the structure of the swashplateless mechanism by adding flapping\nhinges, which reduces vibration during cyclic acceleration and deceleration.\nFinally, we perform comprehensive transition flight tests to validate stable\nflight performance across the entire flight envelope of the tailsitter UAV.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u6784\u673a\u7ffc\u7684\u5c3e\u5750\u5f0f\u5782\u76f4\u8d77\u964d\u65e0\u4eba\u673a\uff0c\u901a\u8fc7\u673a\u7ffc\u4f38\u7f29\u8bbe\u8ba1\u3001\u540c\u8f74\u5f02\u6784\u53cc\u65cb\u7ffc\u914d\u7f6e\u548c\u65e0\u659c\u76d8\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5c3e\u5750\u5f0f\u65e0\u4eba\u673a\u5728\u591a\u65cb\u7ffc\u6a21\u5f0f\u4e0b\u6613\u53d7\u98ce\u6270\u3001\u529f\u8017\u9ad8\u548c\u7ed3\u6784\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5c3e\u5750\u5f0f\u65e0\u4eba\u673a\u5728\u591a\u65cb\u7ffc\u6a21\u5f0f\u4e0b\u7531\u4e8e\u66b4\u9732\u8f83\u5927\u7684\u673a\u8eab\u9762\u79ef\u800c\u5bb9\u6613\u53d7\u5230\u98ce\u6270\uff0c\u540c\u65f6\u5b58\u5728\u529f\u8017\u9ad8\u548c\u7ed3\u6784\u590d\u6742\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53ef\u4f38\u7f29\u673a\u7ffc\u8bbe\u8ba1\uff08\u591a\u65cb\u7ffc\u6a21\u5f0f\u6536\u7f29\u3001\u56fa\u5b9a\u7ffc\u6a21\u5f0f\u5c55\u5f00\uff09\u3001\u540c\u8f74\u5f02\u6784\u53cc\u65cb\u7ffc\u914d\u7f6e\u4ee5\u63d0\u5347\u80fd\u6548\u3001\u6539\u8fdb\u7684\u65e0\u659c\u76d8\u673a\u5236\u63a7\u5236\u4fef\u4ef0\u548c\u6eda\u8f6c\uff0c\u5e76\u901a\u8fc7\u6dfb\u52a0\u6325\u821e\u94f0\u94fe\u4f18\u5316\u7ed3\u6784\u51cf\u5c11\u632f\u52a8\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u603b\u529f\u8017\uff0c\u51cf\u5c11\u4e86\u7ed3\u6784\u91cd\u91cf\u548c\u590d\u6742\u6027\uff0c\u5728\u8fc7\u6e21\u98de\u884c\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65e0\u4eba\u673a\u5728\u6574\u4e2a\u98de\u884c\u5305\u7ebf\u5185\u7684\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u53ef\u91cd\u6784\u673a\u7ffc\u5c3e\u5750\u5f0f\u65e0\u4eba\u673a\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u98ce\u6270\u3001\u529f\u8017\u548c\u7ed3\u6784\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u98de\u884c\u6027\u80fd\u3002"}}
{"id": "2511.04320", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04320", "abs": "https://arxiv.org/abs/2511.04320", "authors": ["Kuankuan Sima", "Longbin Tang", "Haozhe Ma", "Lin Zhao"], "title": "MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments", "comment": null, "summary": "Autonomous navigation in unknown environments requires compact yet expressive\nspatial understanding under partial observability to support high-level\ndecision making. Existing approaches struggle to balance rich contextual\nrepresentation with navigation efficiency. We present MacroNav, a\nlearning-based navigation framework featuring two key components: (1) a\nlightweight context encoder trained via multi-task self-supervised learning to\ncapture multi-scale, navigation-centric spatial representations; and (2) a\nreinforcement learning policy that seamlessly integrates these representations\nwith graph-based reasoning for efficient action selection. Extensive\nexperiments demonstrate the context encoder's efficient and robust\nenvironmental understanding. Real-world deployments further validate MacroNav's\neffectiveness, yielding significant gains over state-of-the-art navigation\nmethods in both Success Rate (SR) and Success weighted by Path Length (SPL),\nwhile maintaining low computational cost. Code will be released upon\nacceptance.", "AI": {"tldr": "MacroNav\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u8def\u5f84\u6548\u7387\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u9700\u8981\u7d27\u51d1\u800c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u4e0e\u5bfc\u822a\u6548\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51faMacroNav\u6846\u67b6\uff0c\u5305\u542b\uff1a(1)\u901a\u8fc7\u591a\u4efb\u52a1\u81ea\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u7684\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff0c\u6355\u83b7\u591a\u5c3a\u5ea6\u5bfc\u822a\u4e2d\u5fc3\u7a7a\u95f4\u8868\u793a\uff1b(2)\u7ed3\u5408\u56fe\u63a8\u7406\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5c06\u8fd9\u4e9b\u8868\u793a\u65e0\u7f1d\u96c6\u6210\u4ee5\u8fdb\u884c\u9ad8\u6548\u52a8\u4f5c\u9009\u62e9\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u5177\u6709\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u73af\u5883\u7406\u89e3\u80fd\u529b\u3002\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86MacroNav\u7684\u6709\u6548\u6027\uff0c\u5728\u6210\u529f\u7387\u548c\u8def\u5f84\u957f\u5ea6\u52a0\u6743\u6210\u529f\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "MacroNav\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6548\u7387\u548c\u8868\u793a\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04357", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04357", "abs": "https://arxiv.org/abs/2511.04357", "authors": ["Ma\u00eblic Neau", "Zoe Falomir", "Paulo E. Santos", "Anne-Gwenn Bosser", "C\u00e9dric Buche"], "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies", "comment": null, "summary": "Deploying autonomous robots that can learn new skills from demonstrations is\nan important challenge of modern robotics. Existing solutions often apply\nend-to-end imitation learning with Vision-Language Action (VLA) models or\nsymbolic approaches with Action Model Learning (AML). On the one hand, current\nVLA models are limited by the lack of high-level symbolic planning, which\nhinders their abilities in long-horizon tasks. On the other hand, symbolic\napproaches in AML lack generalization and scalability perspectives. In this\npaper we present a new neuro-symbolic approach, GraSP-VLA, a framework that\nuses a Continuous Scene Graph representation to generate a symbolic\nrepresentation of human demonstrations. This representation is used to generate\nnew planning domains during inference and serves as an orchestrator for\nlow-level VLA policies, scaling up the number of actions that can be reproduced\nin a row. Our results show that GraSP-VLA is effective for modeling symbolic\nrepresentations on the task of automatic planning domain generation from\nobservations. In addition, results on real-world experiments show the potential\nof our Continuous Scene Graph representation to orchestrate low-level VLA\npolicies in long-horizon tasks.", "AI": {"tldr": "GraSP-VLA\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u4f7f\u7528\u8fde\u7eed\u573a\u666f\u56fe\u8868\u793a\u6765\u751f\u6210\u4eba\u7c7b\u6f14\u793a\u7684\u7b26\u53f7\u8868\u793a\uff0c\u7528\u4e8e\u63a8\u7406\u65f6\u751f\u6210\u65b0\u7684\u89c4\u5212\u57df\uff0c\u5e76\u534f\u8c03\u4f4e\u5c42VLA\u7b56\u7565\u4ee5\u6269\u5c55\u8fde\u7eed\u6267\u884c\u52a8\u4f5c\u7684\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u7aef\u5230\u7aef\u6a21\u4eff\u5b66\u4e60\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u7f3a\u4e4f\u9ad8\u5c42\u7b26\u53f7\u89c4\u5212\u80fd\u529b\uff0c\u800c\u7b26\u53f7\u65b9\u6cd5\u4e2d\u7684\u52a8\u4f5c\u6a21\u578b\u5b66\u4e60\u7f3a\u4e4f\u6cdb\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u6765\u89e3\u51b3\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faGraSP-VLA\u6846\u67b6\uff0c\u4f7f\u7528\u8fde\u7eed\u573a\u666f\u56fe\u8868\u793a\u751f\u6210\u7b26\u53f7\u8868\u793a\uff0c\u5728\u63a8\u7406\u65f6\u751f\u6210\u65b0\u7684\u89c4\u5212\u57df\uff0c\u5e76\u4f5c\u4e3a\u4f4e\u5c42VLA\u7b56\u7565\u7684\u534f\u8c03\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGraSP-VLA\u5728\u4ece\u89c2\u5bdf\u81ea\u52a8\u751f\u6210\u89c4\u5212\u57df\u7684\u4efb\u52a1\u4e2d\u80fd\u6709\u6548\u5efa\u6a21\u7b26\u53f7\u8868\u793a\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u663e\u793a\u8fde\u7eed\u573a\u666f\u56fe\u8868\u793a\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u534f\u8c03\u4f4e\u5c42VLA\u7b56\u7565\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "GraSP-VLA\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u7b26\u53f7\u89c4\u5212\u548c\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4e3a\u957f\u65f6\u7a0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04555", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04555", "abs": "https://arxiv.org/abs/2511.04555", "authors": ["Tao Lin", "Yilei Zhong", "Yuxin Du", "Jingjing Zhang", "Jiting Liu", "Yinxinyu Chen", "Encheng Gu", "Ziyan Liu", "Hongyi Cai", "Yanwen Zou", "Lixing Zou", "Zhaoye Zhou", "Gen Li", "Bo Zhao"], "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment", "comment": "Github: https://github.com/MINT-SJTU/Evo-1", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models.", "AI": {"tldr": "Evo-1\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u8de8\u8c03\u5236\u6269\u6563\u53d8\u6362\u5668\u548c\u4f18\u5316\u7684\u96c6\u6210\u6a21\u5757\uff0c\u5728\u4ec50.77\u4ebf\u53c2\u6570\u4e0b\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u65e0\u9700\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\u3002", "motivation": "\u89e3\u51b3\u5f53\u524dVLA\u6a21\u578b\u53c2\u6570\u5e9e\u5927\u3001\u4f9d\u8d56\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u9884\u8bad\u7ec3\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u90e8\u7f72\u6548\u7387\u4f4e\u4ee5\u53ca\u611f\u77e5\u8868\u793a\u9000\u5316\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u539f\u751f\u591a\u6a21\u6001VLM\uff0c\u5f15\u5165\u8de8\u8c03\u5236\u6269\u6563\u53d8\u6362\u5668\u548c\u4f18\u5316\u96c6\u6210\u6a21\u5757\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u9010\u6b65\u5bf9\u9f50\u52a8\u4f5c\u4e0e\u611f\u77e5\u3002", "result": "\u5728Meta-World\u548cRoboTwin\u5957\u4ef6\u4e0a\u5206\u522b\u8d85\u8d8a\u4e4b\u524d\u6700\u4f73\u6a21\u578b12.4%\u548c6.9%\uff0cLIBERO\u4e0a\u8fbe\u523094.8%\uff0c\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u6210\u529f\u738778%\uff0c\u63a8\u7406\u9891\u7387\u9ad8\u4e14\u5185\u5b58\u5f00\u9500\u4f4e\u3002", "conclusion": "Evo-1\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7VLA\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u90e8\u7f72\u6548\u7387\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u9ad8\u6548VLA\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.04664", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.04664", "abs": "https://arxiv.org/abs/2511.04664", "authors": ["Phat Nguyen", "Erfan Aasi", "Shiva Sreeram", "Guy Rosman", "Andrew Silva", "Sertac Karaman", "Daniela Rus"], "title": "SAFe-Copilot: Unified Shared Autonomy Framework", "comment": null, "summary": "Autonomous driving systems remain brittle in rare, ambiguous, and\nout-of-distribution scenarios, where human driver succeed through contextual\nreasoning. Shared autonomy has emerged as a promising approach to mitigate such\nfailures by incorporating human input when autonomy is uncertain. However, most\nexisting methods restrict arbitration to low-level trajectories, which\nrepresent only geometric paths and therefore fail to preserve the underlying\ndriving intent. We propose a unified shared autonomy framework that integrates\nhuman input and autonomous planners at a higher level of abstraction. Our\nmethod leverages Vision Language Models (VLMs) to infer driver intent from\nmulti-modal cues -- such as driver actions and environmental context -- and to\nsynthesize coherent strategies that mediate between human and autonomous\ncontrol. We first study the framework in a mock-human setting, where it\nachieves perfect recall alongside high accuracy and precision. A human-subject\nsurvey further shows strong alignment, with participants agreeing with\narbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive\nbenchmark demonstrates a substantial reduction in collision rate and\nimprovement in overall performance compared to pure autonomy. Arbitration at\nthe level of semantic, language-based representations emerges as a design\nprinciple for shared autonomy, enabling systems to exercise common-sense\nreasoning and maintain continuity with human intent.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u5171\u4eab\u81ea\u6cbb\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u591a\u6a21\u6001\u7ebf\u7d22\u63a8\u65ad\u9a7e\u9a76\u5458\u610f\u56fe\uff0c\u5728\u8bed\u4e49\u5c42\u9762\u534f\u8c03\u4eba\u7c7b\u8f93\u5165\u548c\u81ea\u4e3b\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5728\u7f55\u89c1\u548c\u6a21\u7cca\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u7f55\u89c1\u3001\u6a21\u7cca\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u800c\u4eba\u7c7b\u9a7e\u9a76\u5458\u901a\u8fc7\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u591f\u6210\u529f\u5e94\u5bf9\u3002\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u4f4e\u5c42\u8f68\u8ff9\u534f\u8c03\uff0c\u65e0\u6cd5\u4fdd\u6301\u9a7e\u9a76\u610f\u56fe\u7684\u8fde\u7eed\u6027\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u9a7e\u9a76\u5458\u52a8\u4f5c\u548c\u73af\u5883\u4e0a\u4e0b\u6587\u7b49\u591a\u6a21\u6001\u7ebf\u7d22\u63a8\u65ad\u9a7e\u9a76\u610f\u56fe\uff0c\u5728\u66f4\u9ad8\u62bd\u8c61\u5c42\u9762\u5408\u6210\u534f\u8c03\u4eba\u7c7b\u548c\u81ea\u4e3b\u63a7\u5236\u7684\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u4eba\u7c7b\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u5b8c\u7f8e\u53ec\u56de\u7387\u548c\u9ad8\u7cbe\u5ea6\uff1b\u4eba\u7c7b\u4e3b\u4f53\u8c03\u67e5\u663e\u793a92%\u7684\u53c2\u4e0e\u8005\u540c\u610f\u4ef2\u88c1\u7ed3\u679c\uff1b\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u78b0\u649e\u7387\u5e76\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u4e49\u3001\u8bed\u8a00\u8868\u793a\u7684\u4ef2\u88c1\u6210\u4e3a\u5171\u4eab\u81ea\u6cbb\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u8fd0\u7528\u5e38\u8bc6\u63a8\u7406\u5e76\u4fdd\u6301\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u8fde\u7eed\u6027\u3002"}}
{"id": "2511.04665", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.04665", "abs": "https://arxiv.org/abs/2511.04665", "authors": ["Kaifeng Zhang", "Shuo Sha", "Hanxiao Jiang", "Matthew Loper", "Hyunjong Song", "Guangyan Cai", "Zhuo Xu", "Xiaochen Hu", "Changxi Zheng", "Yunzhu Li"], "title": "Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions", "comment": "Website: https://real2sim-eval.github.io/", "summary": "Robotic manipulation policies are advancing rapidly, but their direct\nevaluation in the real world remains costly, time-consuming, and difficult to\nreproduce, particularly for tasks involving deformable objects. Simulation\nprovides a scalable and systematic alternative, yet existing simulators often\nfail to capture the coupled visual and physical complexity of soft-body\ninteractions. We present a real-to-sim policy evaluation framework that\nconstructs soft-body digital twins from real-world videos and renders robots,\nobjects, and environments with photorealistic fidelity using 3D Gaussian\nSplatting. We validate our approach on representative deformable manipulation\ntasks, including plush toy packing, rope routing, and T-block pushing,\ndemonstrating that simulated rollouts correlate strongly with real-world\nexecution performance and reveal key behavioral patterns of learned policies.\nOur results suggest that combining physics-informed reconstruction with\nhigh-quality rendering enables reproducible, scalable, and accurate evaluation\nof robotic manipulation policies. Website: https://real2sim-eval.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u6784\u5efa\u8f6f\u4f53\u6570\u5b57\u5b6a\u751f\u7684\u771f\u5b9e\u5230\u4eff\u771f\u7b56\u7565\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5b9e\u73b0\u903c\u771f\u6e32\u67d3\uff0c\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u4eba\u5bf9\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u64cd\u4f5c\u7b56\u7565\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u4e2d\u76f4\u63a5\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u96be\u4ee5\u590d\u73b0\uff0c\u7279\u522b\u662f\u6d89\u53ca\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u4efb\u52a1\u3002\u73b0\u6709\u6a21\u62df\u5668\u96be\u4ee5\u6355\u6349\u8f6f\u4f53\u4ea4\u4e92\u7684\u89c6\u89c9\u548c\u7269\u7406\u590d\u6742\u6027\u3002", "method": "\u6784\u5efa\u8f6f\u4f53\u6570\u5b57\u5b6a\u751f\uff0c\u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u5b9e\u73b0\u673a\u5668\u4eba\u3001\u7269\u4f53\u548c\u73af\u5883\u7684\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u6e32\u67d3\uff0c\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u91cd\u5efa\u6280\u672f\u3002", "result": "\u5728\u4ee3\u8868\u6027\u53ef\u53d8\u5f62\u64cd\u4f5c\u4efb\u52a1\uff08\u6bdb\u7ed2\u73a9\u5177\u6253\u5305\u3001\u7ef3\u7d22\u5e03\u7ebf\u3001T\u5f62\u5757\u63a8\u52a8\uff09\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u62df\u8fd0\u884c\u4e0e\u771f\u5b9e\u4e16\u754c\u6267\u884c\u6027\u80fd\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u80fd\u63ed\u793a\u5b66\u4e60\u7b56\u7565\u7684\u5173\u952e\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u5c06\u7269\u7406\u4fe1\u606f\u91cd\u5efa\u4e0e\u9ad8\u8d28\u91cf\u6e32\u67d3\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u548c\u51c6\u786e\u8bc4\u4f30\u3002"}}
{"id": "2511.04671", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.04671", "abs": "https://arxiv.org/abs/2511.04671", "authors": ["Maximus A. Pace", "Prithwish Dan", "Chuanruo Ning", "Atiksh Bhardwaj", "Audrey Du", "Edward W. Duan", "Wei-Chiu Ma", "Kushal Kedia"], "title": "X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations", "comment": null, "summary": "Human videos can be recorded quickly and at scale, making them an appealing\nsource of training data for robot learning. However, humans and robots differ\nfundamentally in embodiment, resulting in mismatched action execution. Direct\nkinematic retargeting of human hand motion can therefore produce actions that\nare physically infeasible for robots. Despite these low-level differences,\nhuman demonstrations provide valuable motion cues about how to manipulate and\ninteract with objects. Our key idea is to exploit the forward diffusion\nprocess: as noise is added to actions, low-level execution differences fade\nwhile high-level task guidance is preserved. We present X-Diffusion, a\nprincipled framework for training diffusion policies that maximally leverages\nhuman data without learning dynamically infeasible motions. X-Diffusion first\ntrains a classifier to predict whether a noisy action is executed by a human or\nrobot. Then, a human action is incorporated into policy training only after\nadding sufficient noise such that the classifier cannot discern its embodiment.\nActions consistent with robot execution supervise fine-grained denoising at low\nnoise levels, while mismatched human actions provide only coarse guidance at\nhigher noise levels. Our experiments show that naive co-training under\nexecution mismatches degrades policy performance, while X-Diffusion\nconsistently improves it. Across five manipulation tasks, X-Diffusion achieves\na 16% higher average success rate than the best baseline. The project website\nis available at https://portal-cornell.github.io/X-Diffusion/.", "AI": {"tldr": "X-Diffusion\u662f\u4e00\u4e2a\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u673a\u5668\u4eba\u6269\u6563\u7b56\u7565\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6dfb\u52a0\u566a\u58f0\u6765\u6d88\u9664\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u6267\u884c\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u7559\u9ad8\u5c42\u6b21\u4efb\u52a1\u6307\u5bfc\u3002", "motivation": "\u4eba\u7c7b\u89c6\u9891\u6570\u636e\u4e30\u5bcc\u6613\u5f97\uff0c\u4f46\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u5728\u6267\u884c\u52a8\u4f5c\u4e0a\u5b58\u5728\u6839\u672c\u5dee\u5f02\uff0c\u76f4\u63a5\u4f7f\u7528\u4f1a\u5bfc\u81f4\u7269\u7406\u4e0d\u53ef\u884c\u7684\u52a8\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u5229\u7528\u4eba\u7c7b\u6570\u636e\uff0c\u53c8\u907f\u514d\u5b66\u4e60\u4e0d\u53ef\u884c\u7684\u52a8\u4f5c\u3002", "method": "\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u6765\u533a\u5206\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u6267\u884c\u7684\u52a8\u4f5c\uff0c\u7136\u540e\u5728\u7b56\u7565\u8bad\u7ec3\u4e2d\uff0c\u53ea\u6709\u5f53\u6dfb\u52a0\u8db3\u591f\u566a\u58f0\u4f7f\u5f97\u5206\u7c7b\u5668\u65e0\u6cd5\u533a\u5206\u6267\u884c\u4e3b\u4f53\u65f6\uff0c\u624d\u5c06\u4eba\u7c7b\u52a8\u4f5c\u7eb3\u5165\u8bad\u7ec3\u3002\u4e0e\u673a\u5668\u4eba\u6267\u884c\u4e00\u81f4\u7684\u52a8\u4f5c\u5728\u4f4e\u566a\u58f0\u6c34\u5e73\u4e0b\u8fdb\u884c\u7cbe\u7ec6\u53bb\u566a\u76d1\u7763\uff0c\u4e0d\u5339\u914d\u7684\u4eba\u7c7b\u52a8\u4f5c\u4ec5\u5728\u9ad8\u566a\u58f0\u6c34\u5e73\u4e0b\u63d0\u4f9b\u7c97\u7565\u6307\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5b58\u5728\u6267\u884c\u4e0d\u5339\u914d\u7684\u60c5\u51b5\u4e0b\uff0c\u7b80\u5355\u7684\u5171\u540c\u8bad\u7ec3\u4f1a\u964d\u4f4e\u7b56\u7565\u6027\u80fd\uff0c\u800cX-Diffusion\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002\u5728\u4e94\u4e2a\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cX-Diffusion\u6bd4\u6700\u4f73\u57fa\u7ebf\u5b9e\u73b0\u4e8616%\u7684\u5e73\u5747\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "X-Diffusion\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u80fd\u591f\u6700\u5927\u9650\u5ea6\u5730\u5229\u7528\u4eba\u7c7b\u6570\u636e\u8bad\u7ec3\u6269\u6563\u7b56\u7565\uff0c\u540c\u65f6\u907f\u514d\u5b66\u4e60\u52a8\u6001\u4e0d\u53ef\u884c\u7684\u52a8\u4f5c\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2511.04679", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.04679", "abs": "https://arxiv.org/abs/2511.04679", "authors": ["Qingzhou Lu", "Yao Feng", "Baiyu Shi", "Michael Piseno", "Zhenan Bao", "C. Karen Liu"], "title": "GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction", "comment": "Home page: https://gentle-humanoid.axell.top", "summary": "Humanoid robots are expected to operate in human-centered environments where\nsafe and natural physical interaction is essential. However, most recent\nreinforcement learning (RL) policies emphasize rigid tracking and suppress\nexternal forces. Existing impedance-augmented approaches are typically\nrestricted to base or end-effector control and focus on resisting extreme\nforces rather than enabling compliance. We introduce GentleHumanoid, a\nframework that integrates impedance control into a whole-body motion tracking\npolicy to achieve upper-body compliance. At its core is a unified spring-based\nformulation that models both resistive contacts (restoring forces when pressing\nagainst surfaces) and guiding contacts (pushes or pulls sampled from human\nmotion data). This formulation ensures kinematically consistent forces across\nthe shoulder, elbow, and wrist, while exposing the policy to diverse\ninteraction scenarios. Safety is further supported through task-adjustable\nforce thresholds. We evaluate our approach in both simulation and on the\nUnitree G1 humanoid across tasks requiring different levels of compliance,\nincluding gentle hugging, sit-to-stand assistance, and safe object\nmanipulation. Compared to baselines, our policy consistently reduces peak\ncontact forces while maintaining task success, resulting in smoother and more\nnatural interactions. These results highlight a step toward humanoid robots\nthat can safely and effectively collaborate with humans and handle objects in\nreal-world environments.", "AI": {"tldr": "GentleHumanoid\u6846\u67b6\u5c06\u963b\u6297\u63a7\u5236\u96c6\u6210\u5230\u5168\u8eab\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\u4e2d\uff0c\u5b9e\u73b0\u4e0a\u534a\u8eab\u67d4\u987a\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u5f39\u7c27\u6a21\u578b\u5904\u7406\u963b\u529b\u548c\u5f15\u5bfc\u63a5\u89e6\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a5\u89e6\u529b\u5cf0\u503c\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u9700\u8981\u5728\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u5b89\u5168\u81ea\u7136\u5730\u7269\u7406\u4ea4\u4e92\uff0c\u4f46\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8fc7\u4e8e\u5f3a\u8c03\u521a\u6027\u8ddf\u8e2a\u800c\u6291\u5236\u5916\u529b\uff0c\u73b0\u6709\u963b\u6297\u63a7\u5236\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u57fa\u5ea7\u6216\u672b\u7aef\u6267\u884c\u5668\u63a7\u5236\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u62b5\u6297\u6781\u7aef\u529b\u800c\u975e\u5b9e\u73b0\u67d4\u987a\u6027\u3002", "method": "\u63d0\u51faGentleHumanoid\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u7edf\u4e00\u7684\u5f39\u7c27\u6a21\u578b\uff0c\u540c\u65f6\u5efa\u6a21\u963b\u529b\u63a5\u89e6\uff08\u6309\u538b\u8868\u9762\u65f6\u7684\u6062\u590d\u529b\uff09\u548c\u5f15\u5bfc\u63a5\u89e6\uff08\u4ece\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u91c7\u6837\u7684\u63a8\u62c9\u52a8\u4f5c\uff09\uff0c\u786e\u4fdd\u80a9\u90e8\u3001\u8098\u90e8\u548c\u8155\u90e8\u7684\u8fd0\u52a8\u5b66\u4e00\u81f4\u6027\u529b\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u53ef\u8c03\u529b\u9608\u503c\u589e\u5f3a\u5b89\u5168\u6027\u3002", "result": "\u5728\u4eff\u771f\u548cUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8bc4\u4f30\uff0c\u5728\u9700\u8981\u4e0d\u540c\u67d4\u987a\u6027\u6c34\u5e73\u7684\u4efb\u52a1\u4e2d\uff08\u6e29\u67d4\u62e5\u62b1\u3001\u5750\u7ad9\u8f85\u52a9\u3001\u5b89\u5168\u7269\u4f53\u64cd\u4f5c\uff09\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u7b56\u7565\u5728\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u7684\u540c\u65f6\u6301\u7eed\u964d\u4f4e\u5cf0\u503c\u63a5\u89e6\u529b\uff0c\u5b9e\u73b0\u66f4\u5e73\u6ed1\u81ea\u7136\u7684\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u7814\u7a76\u5411\u80fd\u591f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b89\u5168\u6709\u6548\u4e0e\u4eba\u7c7b\u534f\u4f5c\u548c\u5904\u7406\u7269\u4f53\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fc8\u51fa\u4e86\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u963b\u6297\u63a7\u5236\u5b9e\u73b0\u67d4\u987a\u4ea4\u4e92\u7684\u53ef\u884c\u6027\u3002"}}
