<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Leveraging Foundation Models for Enhancing Robot Perception and Action](https://arxiv.org/abs/2510.26855)
*Reihaneh Mirjalili*

Main category: cs.RO

TL;DR: 该论文研究如何系统性地利用基础模型增强机器人能力，使其在非结构化环境中实现更有效的定位、交互和操作。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学中的基础挑战，构建语义感知的机器人智能框架。

Method: 围绕四个核心研究方向展开，每个方向针对机器人学中的一个基本挑战。

Result: 提出了一个增强机器人定位、交互和操作能力的系统性方法。

Conclusion: 基础模型能够为机器人提供语义感知智能，提升其在非结构化环境中的性能。

Abstract: This thesis investigates how foundation models can be systematically
leveraged to enhance robotic capabilities, enabling more effective
localization, interaction, and manipulation in unstructured environments. The
work is structured around four core lines of inquiry, each addressing a
fundamental challenge in robotics while collectively contributing to a cohesive
framework for semantics-aware robotic intelligence.

</details>


### [2] [NaviTrace: Evaluating Embodied Navigation of Vision-Language Models](https://arxiv.org/abs/2510.26909)
*Tim Windecker,Manthan Patel,Moritz Reuss,Richard Schwarzkopf,Cesar Cadena,Rudolf Lioutikov,Marco Hutter,Jonas Frey*

Main category: cs.RO

TL;DR: 提出了NaviTrace基准测试，用于评估视觉语言模型在机器人导航任务中的能力，包含1000个场景和3000多个专家轨迹，通过语义感知轨迹评分系统评估了8个最先进的VLM模型。


<details>
  <summary>Details</summary>
Motivation: 当前评估视觉语言模型导航能力面临成本高昂的真实世界测试、过度简化的模拟和有限基准的问题，需要建立可扩展且可复现的基准测试。

Method: 开发了NaviTrace基准测试，模型接收指令和体现类型（人类、腿式机器人、轮式机器人、自行车），输出2D导航轨迹，使用结合动态时间规整距离、目标端点误差和基于像素语义的体现条件惩罚的语义感知轨迹评分。

Result: 评估显示现有VLM在空间基础和目标定位方面与人类性能存在明显差距，新评分指标与人类偏好相关。

Conclusion: NaviTrace为真实世界机器人导航建立了可扩展且可复现的基准测试，揭示了当前模型在导航能力上的局限性。

Abstract: Vision-language models demonstrate unprecedented performance and
generalization across a wide range of tasks and scenarios. Integrating these
foundation models into robotic navigation systems opens pathways toward
building general-purpose robots. Yet, evaluating these models' navigation
capabilities remains constrained by costly real-world trials, overly simplified
simulations, and limited benchmarks. We introduce NaviTrace, a high-quality
Visual Question Answering benchmark where a model receives an instruction and
embodiment type (human, legged robot, wheeled robot, bicycle) and must output a
2D navigation trace in image space. Across 1000 scenarios and more than 3000
expert traces, we systematically evaluate eight state-of-the-art VLMs using a
newly introduced semantic-aware trace score. This metric combines Dynamic Time
Warping distance, goal endpoint error, and embodiment-conditioned penalties
derived from per-pixel semantics and correlates with human preferences. Our
evaluation reveals consistent gap to human performance caused by poor spatial
grounding and goal localization. NaviTrace establishes a scalable and
reproducible benchmark for real-world robotic navigation. The benchmark and
leaderboard can be found at
https://leggedrobotics.github.io/navitrace_webpage/.

</details>


### [3] [Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence](https://arxiv.org/abs/2510.26915)
*Zachary Ravichandran,Fernando Cladera,Ankit Prabhu,Jason Hughes,Varun Murali,Camillo Taylor,George J. Pappas,Vijay Kumar*

Main category: cs.RO

TL;DR: SPINE-HT是一个用于异构机器人团队的框架，通过三阶段过程将大语言模型的推理能力与机器人能力相结合，在非结构化环境中实现基于自然语言的任务执行。


<details>
  <summary>Details</summary>
Motivation: 异构机器人团队在非结构化环境中执行复杂任务时，需要协作和适应在线获取的信息。现有基于大语言模型的团队方法通常假设环境结构良好且已知，限制了在非结构化环境中的部署。

Method: 采用三阶段过程：1）基于语言描述的任务目标和团队能力，由大语言模型生成经过验证的可行子任务；2）根据机器人能力（如可通行性、感知能力）分配子任务；3）基于在线操作收集的反馈细化子任务。

Result: 在具有闭环感知和控制的仿真实验中，该框架的成功率比现有基于大语言模型的异构团队方法提高近一倍。在真实世界实验中，使用多种机器人平台实现了87%的成功率。

Conclusion: SPINE-HT框架通过将大语言模型推理能力与机器人能力相结合，有效解决了异构机器人团队在非结构化环境中执行复杂任务的问题，显著提高了任务成功率。

Abstract: Heterogeneous robot teams operating in realistic settings often must
accomplish complex missions requiring collaboration and adaptation to
information acquired online. Because robot teams frequently operate in
unstructured environments -- uncertain, open-world settings without prior maps
-- subtasks must be grounded in robot capabilities and the physical world.
While heterogeneous teams have typically been designed for fixed
specifications, generative intelligence opens the possibility of teams that can
accomplish a wide range of missions described in natural language. However,
current large language model (LLM)-enabled teaming methods typically assume
well-structured and known environments, limiting deployment in unstructured
environments. We present SPINE-HT, a framework that addresses these limitations
by grounding the reasoning abilities of LLMs in the context of a heterogeneous
robot team through a three-stage process. Given language specifications
describing mission goals and team capabilities, an LLM generates grounded
subtasks which are validated for feasibility. Subtasks are then assigned to
robots based on capabilities such as traversability or perception and refined
given feedback collected during online operation. In simulation experiments
with closed-loop perception and control, our framework achieves nearly twice
the success rate compared to prior LLM-enabled heterogeneous teaming
approaches. In real-world experiments with a Clearpath Jackal, a Clearpath
Husky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an
87\% success rate in missions requiring reasoning about robot capabilities and
refining subtasks with online feedback. More information is provided at
https://zacravichandran.github.io/SPINE-HT.

</details>


### [4] [RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification](https://arxiv.org/abs/2510.26935)
*Yunhao Yang,Neel P. Bhatt,Pranay Samineni,Rohan Siva,Zhanyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: RepV是一种神经符号验证器，通过在潜在空间中学习安全与不安全计划的线性可分性，实现了对自然语言规则的合规性验证，并提供概率保证。


<details>
  <summary>Details</summary>
Motivation: 在AI系统应用于安全关键领域时，验证其行为是否符合规则是一个挑战。传统形式化方法需要手工编写时态逻辑规范，表达能力有限；深度学习方法虽然能处理自然语言约束，但决策过程不透明可能导致严重误判。

Method: RepV结合神经符号方法，从少量模型检查器标记的计划开始，训练轻量级投影器将计划和语言模型生成的原理嵌入低维空间，使用冻结的线性边界对未见过的自然语言规则进行一次性前向验证。

Result: 实验评估显示，RepV相比基线方法将合规预测准确率提高了15%，同时仅增加不到0.2M参数。其精炼框架在各种规划领域都优于普通微调基线。

Conclusion: 安全可分的潜在空间为可靠的神经符号计划验证提供了可扩展、即插即用的基础组件。

Abstract: As AI systems migrate to safety-critical domains, verifying that their
actions comply with well-defined rules remains a challenge. Formal methods
provide provable guarantees but demand hand-crafted temporal-logic
specifications, offering limited expressiveness and accessibility. Deep
learning approaches enable evaluation of plans against natural-language
constraints, yet their opaque decision process invites misclassifications with
potentially severe consequences. We introduce RepV, a neurosymbolic verifier
that unifies both views by learning a latent space where safe and unsafe plans
are linearly separable. Starting from a modest seed set of plans labeled by an
off-the-shelf model checker, RepV trains a lightweight projector that embeds
each plan, together with a language model-generated rationale, into a
low-dimensional space; a frozen linear boundary then verifies compliance for
unseen natural-language rules in a single forward pass.
  Beyond binary classification, RepV provides a probabilistic guarantee on the
likelihood of correct verification based on its position in the latent space.
This guarantee enables a guarantee-driven refinement of the planner, improving
rule compliance without human annotations. Empirical evaluations show that RepV
improves compliance prediction accuracy by up to 15% compared to baseline
methods while adding fewer than 0.2M parameters. Furthermore, our refinement
framework outperforms ordinary fine-tuning baselines across various planning
domains. These results show that safety-separable latent spaces offer a
scalable, plug-and-play primitive for reliable neurosymbolic plan verification.
Code and data are available at: https://repv-project.github.io/.

</details>


### [5] [A Hermetic, Transparent Soft Growing Vine Robot System for Pipe Inspection](https://arxiv.org/abs/2510.27010)
*William E. Heap,Yimeng Qin,Kai Hammond,Anish Bayya,Haonon Kong,Allison M. Okamura*

Main category: cs.RO

TL;DR: 本文提出了一种用于管道内部视觉状况评估和测绘的密封透明藤蔓机器人系统，通过将机械和电气组件封装在机器人软体内，保护其免受环境干扰，并在真实废水管道中验证了该系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 老化管道的修复需要准确评估管道内部状况，现有软藤蔓机器人系统在复杂管道导航方面有潜力，但受到复杂子系统和缺乏真实工业环境验证的限制。

Method: 开发了密封透明的藤蔓机器人系统，将所有组件封装在软体内部；设计了被动适应的封闭式尖端安装座来运输传感器；在真实废水管道中进行了验证测试。

Result: 成功实现了管道内部视觉状况评估和测绘，系统能够保护内部组件免受环境干扰，同时保持视觉感知能力。

Conclusion: 这项工作推进了软藤蔓机器人在管道检测中的应用，开发并展示了一个稳健、简化、经过现场验证的系统，适合持续开发和部署。

Abstract: Rehabilitation of aging pipes requires accurate condition assessment and
mapping far into the pipe interiors. Soft growing vine robot systems are
particularly promising for navigating confined, sinuous paths such as in pipes,
but are currently limited by complex subsystems and a lack of validation in
real-world industrial settings. In this paper, we introduce the concept and
implementation of a hermetic and transparent vine robot system for visual
condition assessment and mapping within non-branching pipes. This design
encloses all mechanical and electrical components within the vine robot's soft,
airtight, and transparent body, protecting them from environmental interference
while enabling visual sensing. Because this approach requires an enclosed
mechanism for transporting sensors, we developed, modeled, and tested a
passively adapting enclosed tip mount. Finally, we validated the hermetic and
transparent vine robot system concept through a real-world condition assessment
and mapping task in a wastewater pipe. This work advances the use of
soft-growing vine robots in pipe inspection by developing and demonstrating a
robust, streamlined, field-validated system suitable for continued development
and deployment.

</details>


### [6] [A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics](https://arxiv.org/abs/2510.27033)
*Simindokht Jahangard,Mehrzad Mohammadi,Abhinav Dhall,Hamid Rezatofighi*

Main category: cs.RO

TL;DR: 提出了一种结合全景图像和3D点云的神经符号框架，通过显式建模空间和逻辑关系来改进视觉空间推理能力，在拥挤的人造环境中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在感知任务上表现出色，但在细粒度空间推理方面存在困难，主要原因是它们依赖隐式的相关性推理且仅使用图像数据。

Method: 开发了一个神经符号框架，包含感知模块（检测实体和提取属性）和推理模块（构建结构化场景图），结合全景图像和3D点云信息进行显式空间关系建模。

Result: 在JRDB-Reasoning数据集上的评估显示，该方法在拥挤的人造环境中表现出优越的性能和可靠性。

Conclusion: 该框架为机器人和具身AI应用提供了一种轻量级、可解释的空间推理解决方案，成功结合了神经感知和符号推理的优势。

Abstract: Visual reasoning, particularly spatial reasoning, is a challenging cognitive
task that requires understanding object relationships and their interactions
within complex environments, especially in robotics domain. Existing
vision_language models (VLMs) excel at perception tasks but struggle with
fine-grained spatial reasoning due to their implicit, correlation-driven
reasoning and reliance solely on images. We propose a novel neuro_symbolic
framework that integrates both panoramic-image and 3D point cloud information,
combining neural perception with symbolic reasoning to explicitly model spatial
and logical relationships. Our framework consists of a perception module for
detecting entities and extracting attributes, and a reasoning module that
constructs a structured scene graph to support precise, interpretable queries.
Evaluated on the JRDB-Reasoning dataset, our approach demonstrates superior
performance and reliability in crowded, human_built environments while
maintaining a lightweight design suitable for robotics and embodied AI
applications.

</details>


### [7] [SpikeATac: A Multimodal Tactile Finger with Taxelized Dynamic Sensing for Dexterous Manipulation](https://arxiv.org/abs/2510.27048)
*Eric T. Chang,Peter Ballentine,Zhanpeng He,Do-Gon Kim,Kai Jiang,Hua-Hsuan Liang,Joaquin Palacios,William Wang,Pedro Piacenza,Ioannis Kymissis,Matei Ciocarlie*

Main category: cs.RO

TL;DR: SpikeATac是一种结合PVDF动态响应和电容静态传感的多模态触觉手指，能够快速检测接触开始和断开，用于精细抓取易碎物体，并通过强化学习实现灵巧手对易碎物体的在手机器人操作。


<details>
  <summary>Details</summary>
Motivation: 开发能够精细感知接触动态和静态信息的多模态触觉传感器，以实现对易碎、可变形物体的安全抓取和灵巧操作。

Method: 设计16个触点的PVDF薄膜传感器，采样频率4kHz，结合电容式静态传感，使用基于人类反馈的强化学习和触觉奖励来微调策略以调节抓取力。

Result: SpikeATac能够快速停止并精细抓取易碎物体，在灵巧多指机器人手上实现了对易碎物体的在手机器人操作这一困难任务。

Conclusion: SpikeATac硬件平台和学习管道相结合，成功实现了以前未达到的灵巧接触密集型任务——在手机器人操作易碎物体。

Abstract: In this work, we introduce SpikeATac, a multimodal tactile finger combining a
taxelized and highly sensitive dynamic response (PVDF) with a static
transduction method (capacitive) for multimodal touch sensing. Named for its
`spiky' response, SpikeATac's 16-taxel PVDF film sampled at 4 kHz provides
fast, sensitive dynamic signals to the very onset and breaking of contact. We
characterize the sensitivity of the different modalities, and show that
SpikeATac provides the ability to stop quickly and delicately when grasping
fragile, deformable objects. Beyond parallel grasping, we show that SpikeATac
can be used in a learning-based framework to achieve new capabilities on a
dexterous multifingered robot hand. We use a learning recipe that combines
reinforcement learning from human feedback with tactile-based rewards to
fine-tune the behavior of a policy to modulate force. Our hardware platform and
learning pipeline together enable a difficult dexterous and contact-rich task
that has not previously been achieved: in-hand manipulation of fragile objects.
Videos are available at
\href{https://roamlab.github.io/spikeatac/}{roamlab.github.io/spikeatac}.

</details>


### [8] [Learning Generalizable Visuomotor Policy through Dynamics-Alignment](https://arxiv.org/abs/2510.27114)
*Dohyeok Lee,Jung Min Lee,Munkyung Kim,Seokhun Ju,Jin Woo Koo,Kyungjae Lee,Dohyeong Kim,TaeHyun Cho,Jungwoo Lee*

Main category: cs.RO

TL;DR: 提出了一种名为Dynamics-Aligned Flow Matching Policy (DAP)的新方法，通过将动力学预测集成到策略学习中，解决了行为克隆方法泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 行为克隆方法由于仅限于专家演示数据而泛化能力差，现有视频预测模型虽然能从大规模数据中学习时空表示，但学习的是动作无关的动力学，无法区分不同控制输入，限制了在精确操作任务中的应用。

Method: 提出DAP方法，采用新颖架构使策略和动力学模型在动作生成过程中提供相互纠正反馈，实现自我纠正和改进的泛化能力。

Result: 实证验证表明，该方法在真实世界机器人操作任务上的泛化性能优于基线方法，在包括视觉干扰和光照变化在内的OOD场景中表现出特别的鲁棒性。

Conclusion: DAP方法通过集成动力学预测和策略学习，有效提升了机器人学习的泛化能力和鲁棒性。

Abstract: Behavior cloning methods for robot learning suffer from poor generalization
due to limited data support beyond expert demonstrations. Recent approaches
leveraging video prediction models have shown promising results by learning
rich spatiotemporal representations from large-scale datasets. However, these
models learn action-agnostic dynamics that cannot distinguish between different
control inputs, limiting their utility for precise manipulation tasks and
requiring large pretraining datasets. We propose a Dynamics-Aligned Flow
Matching Policy (DAP) that integrates dynamics prediction into policy learning.
Our method introduces a novel architecture where policy and dynamics models
provide mutual corrective feedback during action generation, enabling
self-correction and improved generalization. Empirical validation demonstrates
generalization performance superior to baseline methods on real-world robotic
manipulation tasks, showing particular robustness in OOD scenarios including
visual distractions and lighting variations.

</details>


### [9] [Confined Space Underwater Positioning Using Collaborative Robots](https://arxiv.org/abs/2510.27151)
*Xueliang Cheng,Kanzhong Yao,Andrew West,Ognjen Marjanovic,Barry Lennox,Keir Groves*

Main category: cs.RO

TL;DR: 本文提出了协作水下定位系统(CAP)，通过水面机器人作为移动领导者协助水下机器人定位，在GPS受限和受限环境中实现实时定位，平均欧几里得距离误差为70毫米。


<details>
  <summary>Details</summary>
Motivation: 水下机器人在受限和杂乱空间中的定位是现场操作的关键挑战，现有系统主要针对大型开放水域设计，在工业环境中表现不佳，存在覆盖范围差、依赖外部基础设施、需要特征丰富环境等问题。多径效应进一步降低信号质量，影响精度和可靠性。

Method: CAP系统集成了协作机器人和传感器融合技术，采用"母船"概念，水面车辆作为移动领导者协助水下机器人定位，即使在GPS受限和高度受限环境中也能实现定位。系统通过CAP位置估计进行实时轨迹控制，在大型测试池中通过可重复的自主任务进行验证。

Result: 实验结果显示，系统实现了70毫米的平均欧几里得距离误差，无需固定基础设施、广泛校准或环境特征即可实时实现。

Conclusion: CAP系统利用移动机器人传感和领导者-跟随者控制的进步，在准确、实用和无基础设施的水下定位方面实现了重大突破。

Abstract: Positioning of underwater robots in confined and cluttered spaces remains a
key challenge for field operations. Existing systems are mostly designed for
large, open-water environments and struggle in industrial settings due to poor
coverage, reliance on external infrastructure, and the need for feature-rich
surroundings. Multipath effects from continuous sound reflections further
degrade signal quality, reducing accuracy and reliability. Accurate and easily
deployable positioning is essential for repeatable autonomous missions;
however, this requirement has created a technological bottleneck limiting
underwater robotic deployment. This paper presents the Collaborative Aquatic
Positioning (CAP) system, which integrates collaborative robotics and sensor
fusion to overcome these limitations. Inspired by the "mother-ship" concept,
the surface vehicle acts as a mobile leader to assist in positioning a
submerged robot, enabling localization even in GPS-denied and highly
constrained environments. The system is validated in a large test tank through
repeatable autonomous missions using CAP's position estimates for real-time
trajectory control. Experimental results demonstrate a mean Euclidean distance
(MED) error of 70 mm, achieved in real time without requiring fixed
infrastructure, extensive calibration, or environmental features. CAP leverages
advances in mobile robot sensing and leader-follower control to deliver a step
change in accurate, practical, and infrastructure-free underwater localization.

</details>


### [10] [MobiDock: Design and Control of A Modular Self Reconfigurable Bimanual Mobile Manipulator via Robotic Docking](https://arxiv.org/abs/2510.27178)
*Xuan-Thuan Nguyen,Khac Nam Nguyen,Ngoc Duy Tran,Thi Thoa Mac,Anh Nguyen,Hoang Hiep Ly,Tung D. Ta*

Main category: cs.RO

TL;DR: MobiDock是一个模块化自重构移动机械臂系统，允许两个独立机器人物理连接形成统一的移动双手平台，将复杂多机器人控制问题简化为单一系统管理。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人系统（特别是移动机械臂）在协同工作时面临的协调控制和动态稳定性挑战。

Method: 采用基于计算机视觉和AprilTag标记的自主对接策略，以及新型螺纹螺钉锁定机制，实现两个独立机器人的物理连接。

Result: 对接配置在动态稳定性和操作效率方面表现更优：RMS加速度和急动度值更低，角度精度更高，任务完成时间显著缩短。

Conclusion: 物理重构是简化协同控制的有效设计原则，能提高复杂任务在真实环境中的稳定性和性能。

Abstract: Multi-robot systems, particularly mobile manipulators, face challenges in
control coordination and dynamic stability when working together. To address
this issue, this study proposes MobiDock, a modular self-reconfigurable mobile
manipulator system that allows two independent robots to physically connect and
form a unified mobile bimanual platform. This process helps transform a complex
multi-robot control problem into the management of a simpler, single system.
The system utilizes an autonomous docking strategy based on computer vision
with AprilTag markers and a new threaded screw-lock mechanism. Experimental
results show that the docked configuration demonstrates better performance in
dynamic stability and operational efficiency compared to two independently
cooperating robots. Specifically, the unified system has lower Root Mean Square
(RMS) Acceleration and Jerk values, higher angular precision, and completes
tasks significantly faster. These findings confirm that physical
reconfiguration is a powerful design principle that simplifies cooperative
control, improving stability and performance for complex tasks in real-world
environments.

</details>


### [11] [Hybrid Gripper Finger Enabling In-Grasp Friction Modulation Using Inflatable Silicone Pockets](https://arxiv.org/abs/2510.27184)
*Hoang Hiep Ly,Cong-Nhat Nguyen,Doan-Quang Tran,Quoc-Khanh Dang,Ngoc Duy Tran,Thi Thoa Mac,Anh Nguyen,Xuan-Thuan Nguyen,Tung D. Ta*

Main category: cs.RO

TL;DR: 提出了一种结合刚性外壳和可充气硅胶袋的混合夹爪手指，通过控制内部气压主动调节表面摩擦系数，实现稳定抓取重物、易滑物体和易碎物品而无需增加夹持力。


<details>
  <summary>Details</summary>
Motivation: 传统夹爪依赖高法向力容易损坏物体，需要一种能安全处理具有不同机械特性（如重、滑、脆）物体的解决方案。

Method: 设计混合夹爪手指，刚性结构外壳配软性可充气硅胶袋，通过控制硅胶袋内部气压来主动调节表面摩擦系数。

Result: 基础实验表明增加内部压力会成比例增加有效摩擦系数，使夹爪能稳定抓取重滑物体而不增加夹持力，并能安全处理易碎物品如鸡蛋、水果和纸杯。

Conclusion: 具有可调摩擦功能的混合夹爪手指为依赖高法向力的传统方法提供了更稳健、更安全的替代方案，增强了处理精细、易碎和多样化物体的灵活性。

Abstract: Grasping objects with diverse mechanical properties, such as heavy, slippery,
or fragile items, remains a significant challenge in robotics. Conventional
grippers often rely on applying high normal forces, which can cause damage to
objects. To address this limitation, we present a hybrid gripper finger that
combines a rigid structural shell with a soft, inflatable silicone pocket. The
gripper finger can actively modulate its surface friction by controlling the
internal air pressure of the silicone pocket. Results from fundamental
experiments indicate that increasing the internal pressure results in a
proportional increase in the effective coefficient of friction. This enables
the gripper to stably lift heavy and slippery objects without increasing the
gripping force and to handle fragile or deformable objects, such as eggs,
fruits, and paper cups, with minimal damage by increasing friction rather than
applying excessive force. The experimental results demonstrate that the hybrid
gripper finger with adaptable friction provides a robust and safer alternative
to relying solely on high normal forces, thereby enhancing the gripper
flexibility in handling delicate, fragile, and diverse objects.

</details>


### [12] [Vectorized Online POMDP Planning](https://arxiv.org/abs/2510.27191)
*Marcus Hoerger,Muhammad Sudrajat,Hanna Kurniawati*

Main category: cs.RO

TL;DR: 提出了一种名为VOPP的并行在线POMDP求解器，通过向量化计算和消除同步瓶颈，实现了至少20倍的计算效率提升。


<details>
  <summary>Details</summary>
Motivation: 部分可观测环境下的规划是自主机器人的关键能力，POMDP框架虽然强大，但现有并行求解器存在依赖关系和同步瓶颈，限制了硬件并行化的优势。

Method: 采用向量化的POMDP规划方法，将所有规划数据结构表示为张量集合，并将所有规划步骤实现为完全向量化的计算，消除了并行计算间的依赖和同步瓶颈。

Result: 实验结果表明，VOPP在计算近优解方面比现有最先进的并行在线求解器至少高效20倍。

Conclusion: VOPP通过向量化计算和消除同步瓶颈，成功实现了POMDP求解的大规模并行化，显著提升了计算效率。

Abstract: Planning under partial observability is an essential capability of autonomous
robots. The Partially Observable Markov Decision Process (POMDP) provides a
powerful framework for planning under partial observability problems, capturing
the stochastic effects of actions and the limited information available through
noisy observations. POMDP solving could benefit tremendously from massive
parallelization of today's hardware, but parallelizing POMDP solvers has been
challenging. They rely on interleaving numerical optimization over actions with
the estimation of their values, which creates dependencies and synchronization
bottlenecks between parallel processes that can quickly offset the benefits of
parallelization. In this paper, we propose Vectorized Online POMDP Planner
(VOPP), a novel parallel online solver that leverages a recent POMDP
formulation that analytically solves part of the optimization component,
leaving only the estimation of expectations for numerical computation. VOPP
represents all data structures related to planning as a collection of tensors
and implements all planning steps as fully vectorized computations over this
representation. The result is a massively parallel solver with no dependencies
and synchronization bottlenecks between parallel computations. Experimental
results indicate that VOPP is at least 20X more efficient in computing
near-optimal solutions compared to an existing state-of-the-art parallel online
solver.

</details>


### [13] [A Modular and Scalable System Architecture for Heterogeneous UAV Swarms Using ROS 2 and PX4-Autopilot](https://arxiv.org/abs/2510.27327)
*Robert Pommeranz,Kevin Tebbe,Ralf Heynicke,Gerd Scholl*

Main category: cs.RO

TL;DR: 提出基于PX4-Autopilot和ROS 2的模块化、可扩展异构无人机群反无人机系统架构，支持硬件无缝集成、多种通信技术、编队飞行和计算机视觉算法。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够集成异构硬件组件、支持多种通信技术、实现无人机群协同作战的反无人机系统架构。

Method: 采用模块化设计，为每个无人机组件创建独立的ROS 2节点，通过软件抽象实现通信技术无关性，集成计算机视觉算法和地面站控制系统。

Result: 在Gazebo仿真环境和真实世界演示中验证了该无人机群架构的可行性和有效性。

Conclusion: 该架构为异构无人机群反无人机系统提供了实用的解决方案，具有良好的可扩展性和适应性。

Abstract: In this paper a modular and scalable architecture for heterogeneous
swarm-based Counter Unmanned Aerial Systems (C-UASs) built on PX4-Autopilot and
Robot Operating System 2 (ROS 2) framework is presented. The proposed
architecture emphasizes seamless integration of hardware components by
introducing independent ROS 2 nodes for each component of a Unmanned Aerial
Vehicle (UAV). Communication between swarm participants is abstracted in
software, allowing the use of various technologies without architectural
changes. Key functionalities are supported, e.g. leader following and formation
flight to maneuver the swarm. The system also allows computer vision algorithms
to be integrated for the detection and tracking of UAVs. Additionally, a ground
station control is integrated for the coordination of swarm operations.
Swarm-based Unmanned Aerial System (UAS) architecture is verified within a
Gazebo simulation environment but also in real-world demonstrations.

</details>


### [14] [Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict](https://arxiv.org/abs/2510.27333)
*Hao Cheng,Yanbo Jiang,Qingyuan Shi,Qingwen Meng,Keyu Chen,Wenhao Yu,Jianqiang Wang,Sifa Zheng*

Main category: cs.RO

TL;DR: 本文提出了改进的紧急指数（MEI）来量化横向冲突中的规避努力，相比现有指标能更准确地评估城市环境中自动驾驶的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的关键性指标主要针对纵向冲突，难以准确量化城市环境中常见的横向冲突风险，需要开发更有效的评估方法。

Method: 提出MEI指标，改进原始紧急指数中对规避机动可用时间的估计，实现更精确的风险量化。在基于Argoverse-2的公共横向冲突数据集上进行验证，提取1500多个高质量AV冲突案例。

Result: MEI在准确量化关键性和捕捉风险演变方面持续优于既有的ACT和PET指标。

Conclusion: MEI是评估城市冲突和增强自动驾驶安全评估框架的有前景指标。

Abstract: Effective, reliable, and efficient evaluation of autonomous driving safety is
essential to demonstrate its trustworthiness. Criticality metrics provide an
objective means of assessing safety. However, as existing metrics primarily
target longitudinal conflicts, accurately quantifying the risks of lateral
conflicts - prevalent in urban settings - remains challenging. This paper
proposes the Modified-Emergency Index (MEI), a metric designed to quantify
evasive effort in lateral conflicts. Compared to the original Emergency Index
(EI), MEI refines the estimation of the time available for evasive maneuvers,
enabling more precise risk quantification. We validate MEI on a public lateral
conflict dataset based on Argoverse-2, from which we extract over 1,500
high-quality AV conflict cases, including more than 500 critical events. MEI is
then compared with the well-established ACT and the widely used PET metrics.
Results show that MEI consistently outperforms them in accurately quantifying
criticality and capturing risk evolution. Overall, these findings highlight MEI
as a promising metric for evaluating urban conflicts and enhancing the safety
assessment framework for autonomous driving. The open-source implementation is
available at https://github.com/AutoChengh/MEI.

</details>


### [15] [Towards a Multi-Embodied Grasping Agent](https://arxiv.org/abs/2510.27420)
*Roman Freiberg,Alexander Qualmann,Ngo Anh Vien,Gerhard Neumann*

Main category: cs.RO

TL;DR: 提出了一种数据高效的、基于流的等变抓取合成架构，能够处理不同自由度类型的夹爪，仅从夹爪和场景几何中推断必要信息，并在JAX中实现所有模块，支持场景、夹爪和抓取的批处理。


<details>
  <summary>Details</summary>
Motivation: 现有多体抓取方法通常隐式学习机器人运动学结构，且面临大规模数据获取困难的问题，需要开发更数据高效的方法。

Method: 开发基于流的等变抓取合成架构，利用JAX实现所有模块，支持对场景、夹爪和抓取的批处理，仅从几何信息推断运动学模型。

Result: 构建了包含从人形手到平行偏转夹爪的25,000个场景和2000万次抓取的数据集，实现了更平滑的学习、改进的性能和更快的推理时间。

Conclusion: 该方法能够高效处理不同类型的夹爪，仅依赖几何信息即可推断运动学模型，在数据效率和性能方面优于现有方法。

Abstract: Multi-embodiment grasping focuses on developing approaches that exhibit
generalist behavior across diverse gripper designs. Existing methods often
learn the kinematic structure of the robot implicitly and face challenges due
to the difficulty of sourcing the required large-scale data. In this work, we
present a data-efficient, flow-based, equivariant grasp synthesis architecture
that can handle different gripper types with variable degrees of freedom and
successfully exploit the underlying kinematic model, deducing all necessary
information solely from the gripper and scene geometry. Unlike previous
equivariant grasping methods, we translated all modules from the ground up to
JAX and provide a model with batching capabilities over scenes, grippers, and
grasps, resulting in smoother learning, improved performance and faster
inference time. Our dataset encompasses grippers ranging from humanoid hands to
parallel yaw grippers and includes 25,000 scenes and 20 million grasps.

</details>


### [16] [Learning Soft Robotic Dynamics with Active Exploration](https://arxiv.org/abs/2510.27428)
*Hehui Zheng,Bhavya Sukhija,Chenhao Li,Klemens Iten,Andreas Krause,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: SoftAE是一个不确定性感知的主动探索框架，能够自主学习软体机器人系统的任务无关且可泛化的动力学模型，通过概率集成模型估计认知不确定性，并主动引导探索状态-动作空间中代表性不足的区域。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有出色的适应性和安全性，但其柔顺、高维和非线性的动力学特性使得建模控制变得困难。现有数据驱动方法往往泛化能力不足，受限于狭窄的任务演示或低效的随机探索。

Method: 采用概率集成模型估计认知不确定性，主动引导探索状态-动作空间中代表性不足的区域，无需任务特定监督即可高效覆盖多样化行为。

Result: 在三个模拟软体机器人平台和真实气动连续软臂上评估，相比随机探索和任务特定模型强化学习，SoftAE产生更准确的动力学模型，在未见任务上实现更优的零样本控制，并在感知噪声、驱动延迟和非线性材料效应下保持鲁棒性。

Conclusion: 不确定性驱动的主动探索能够为多样化软体机器人形态生成可扩展、可重用的动力学模型，朝着更自主、适应性强和数据高效的控制迈进一步。

Abstract: Soft robots offer unmatched adaptability and safety in unstructured
environments, yet their compliant, high-dimensional, and nonlinear dynamics
make modeling for control notoriously difficult. Existing data-driven
approaches often fail to generalize, constrained by narrowly focused task
demonstrations or inefficient random exploration. We introduce SoftAE, an
uncertainty-aware active exploration framework that autonomously learns
task-agnostic and generalizable dynamics models of soft robotic systems. SoftAE
employs probabilistic ensemble models to estimate epistemic uncertainty and
actively guides exploration toward underrepresented regions of the state-action
space, achieving efficient coverage of diverse behaviors without task-specific
supervision. We evaluate SoftAE on three simulated soft robotic platforms -- a
continuum arm, an articulated fish in fluid, and a musculoskeletal leg with
hybrid actuation -- and on a pneumatically actuated continuum soft arm in the
real world. Compared with random exploration and task-specific model-based
reinforcement learning, SoftAE produces more accurate dynamics models, enables
superior zero-shot control on unseen tasks, and maintains robustness under
sensing noise, actuation delays, and nonlinear material effects. These results
demonstrate that uncertainty-driven active exploration can yield scalable,
reusable dynamics models across diverse soft robotic morphologies, representing
a step toward more autonomous, adaptable, and data-efficient control in
compliant robots.

</details>


### [17] [Preliminary Prototyping of Avoidance Behaviors Triggered by a User's Physical Approach to a Robot](https://arxiv.org/abs/2510.27436)
*Tomoko Yonezawa,Hirotake Yamazoe,Atsuo Fujino,Daigo Suhara,Takaya Tamamoto,Yuto Nishiguchi*

Main category: cs.RO

TL;DR: 本文研究机器人如何通过内部状态和回避行为来拒绝人类的接近，基于人际距离建模不适感的积累和衰减，并在机械臂上实现耐受和极限回避行为。


<details>
  <summary>Details</summary>
Motivation: 在人类-机器人交互中，物理接近或接触很常见。人类会根据关系和情境灵活地接受、拒绝或容忍这种接近，因此需要设计机器人能够表达拒绝内部状态和相应回避行为的能力。

Method: 基于人际距离函数建模不适感的积累和衰减，利用PAD情感模型的支配性轴实现耐受（忍耐）和极限超出的回避行为，并在机械臂上实现这些行为及其强度。

Result: 结果展示了从内部状态参数到分级耐受动作，再到极限被超越时的回避动作的连贯流程。

Conclusion: 该研究成功实现了机器人拒绝内部状态和回避行为的设计，为人类-机器人交互中的物理接近提供了更自然的应对机制。

Abstract: Human-robot interaction frequently involves physical proximity or contact. In
human-human settings, people flexibly accept, reject, or tolerate such
approaches depending on the relationship and context. We explore the design of
a robot's rejective internal state and corresponding avoidance behaviors, such
as withdrawing or pushing away, when a person approaches. We model the
accumulation and decay of discomfort as a function of interpersonal distance,
and implement tolerance (endurance) and limit-exceeding avoidance driven by the
Dominance axis of the PAD affect model. The behaviors and their intensities are
realized on an arm robot. Results illustrate a coherent pipeline from internal
state parameters to graded endurance motions and, once a limit is crossed, to
avoidance actions.

</details>


### [18] [Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs](https://arxiv.org/abs/2510.27558)
*Sushil Samuel Dinesh,Shinkyu Park*

Main category: cs.RO

TL;DR: 提出一个利用预训练基础模型实现机器人操作的框架，无需领域特定训练，结合多模态感知和通用推理模型，通过场景图提供空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 旨在利用现成的基础模型构建机器人操作系统，避免领域特定训练的需求，提高系统的通用性和部署效率。

Method: 整合现成模型，将基础模型的多模态感知能力与通用推理模型相结合，使用动态维护的场景图提供空间感知和环境一致性推理。

Result: 通过桌面机器人操作实验验证了框架的有效性，展示了直接在现成基础模型上构建机器人操作系统的潜力。

Conclusion: 该框架证明了利用预训练基础模型构建机器人操作系统的可行性，为无需领域特定训练的机器人系统开发提供了新途径。

Abstract: This paper presents a framework that leverages pre-trained foundation models
for robotic manipulation without domain-specific training. The framework
integrates off-the-shelf models, combining multimodal perception from
foundation models with a general-purpose reasoning model capable of robust task
sequencing. Scene graphs, dynamically maintained within the framework, provide
spatial awareness and enable consistent reasoning about the environment. The
framework is evaluated through a series of tabletop robotic manipulation
experiments, and the results highlight its potential for building robotic
manipulation systems directly on top of off-the-shelf foundation models.

</details>
