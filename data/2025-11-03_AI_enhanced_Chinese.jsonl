{"id": "2510.26855", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26855", "abs": "https://arxiv.org/abs/2510.26855", "authors": ["Reihaneh Mirjalili"], "title": "Leveraging Foundation Models for Enhancing Robot Perception and Action", "comment": "Doctoral thesis", "summary": "This thesis investigates how foundation models can be systematically\nleveraged to enhance robotic capabilities, enabling more effective\nlocalization, interaction, and manipulation in unstructured environments. The\nwork is structured around four core lines of inquiry, each addressing a\nfundamental challenge in robotics while collectively contributing to a cohesive\nframework for semantics-aware robotic intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u7cfb\u7edf\u6027\u5730\u5229\u7528\u57fa\u7840\u6a21\u578b\u589e\u5f3a\u673a\u5668\u4eba\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5b9a\u4f4d\u3001\u4ea4\u4e92\u548c\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u57fa\u7840\u6311\u6218\uff0c\u6784\u5efa\u8bed\u4e49\u611f\u77e5\u7684\u673a\u5668\u4eba\u667a\u80fd\u6846\u67b6\u3002", "method": "\u56f4\u7ed5\u56db\u4e2a\u6838\u5fc3\u7814\u7a76\u65b9\u5411\u5c55\u5f00\uff0c\u6bcf\u4e2a\u65b9\u5411\u9488\u5bf9\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u589e\u5f3a\u673a\u5668\u4eba\u5b9a\u4f4d\u3001\u4ea4\u4e92\u548c\u64cd\u4f5c\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u80fd\u591f\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u8bed\u4e49\u611f\u77e5\u667a\u80fd\uff0c\u63d0\u5347\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2510.26909", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26909", "abs": "https://arxiv.org/abs/2510.26909", "authors": ["Tim Windecker", "Manthan Patel", "Moritz Reuss", "Richard Schwarzkopf", "Cesar Cadena", "Rudolf Lioutikov", "Marco Hutter", "Jonas Frey"], "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models", "comment": "9 pages, 6 figures, under review at IEEE conference", "summary": "Vision-language models demonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models into robotic navigation systems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduce NaviTrace, a high-quality\nVisual Question Answering benchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introduced semantic-aware trace score. This metric combines Dynamic Time\nWarping distance, goal endpoint error, and embodiment-conditioned penalties\nderived from per-pixel semantics and correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization. NaviTrace establishes a scalable and\nreproducible benchmark for real-world robotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/.", "AI": {"tldr": "\u63d0\u51fa\u4e86NaviTrace\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u542b1000\u4e2a\u573a\u666f\u548c3000\u591a\u4e2a\u4e13\u5bb6\u8f68\u8ff9\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u8f68\u8ff9\u8bc4\u5206\u7cfb\u7edf\u8bc4\u4f30\u4e868\u4e2a\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bfc\u822a\u80fd\u529b\u9762\u4e34\u6210\u672c\u9ad8\u6602\u7684\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u3001\u8fc7\u5ea6\u7b80\u5316\u7684\u6a21\u62df\u548c\u6709\u9650\u57fa\u51c6\u7684\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f00\u53d1\u4e86NaviTrace\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u578b\u63a5\u6536\u6307\u4ee4\u548c\u4f53\u73b0\u7c7b\u578b\uff08\u4eba\u7c7b\u3001\u817f\u5f0f\u673a\u5668\u4eba\u3001\u8f6e\u5f0f\u673a\u5668\u4eba\u3001\u81ea\u884c\u8f66\uff09\uff0c\u8f93\u51fa2D\u5bfc\u822a\u8f68\u8ff9\uff0c\u4f7f\u7528\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u8ddd\u79bb\u3001\u76ee\u6807\u7aef\u70b9\u8bef\u5dee\u548c\u57fa\u4e8e\u50cf\u7d20\u8bed\u4e49\u7684\u4f53\u73b0\u6761\u4ef6\u60e9\u7f5a\u7684\u8bed\u4e49\u611f\u77e5\u8f68\u8ff9\u8bc4\u5206\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709VLM\u5728\u7a7a\u95f4\u57fa\u7840\u548c\u76ee\u6807\u5b9a\u4f4d\u65b9\u9762\u4e0e\u4eba\u7c7b\u6027\u80fd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u65b0\u8bc4\u5206\u6307\u6807\u4e0e\u4eba\u7c7b\u504f\u597d\u76f8\u5173\u3002", "conclusion": "NaviTrace\u4e3a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5bfc\u822a\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5bfc\u822a\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.26915", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26915", "abs": "https://arxiv.org/abs/2510.26915", "authors": ["Zachary Ravichandran", "Fernando Cladera", "Ankit Prabhu", "Jason Hughes", "Varun Murali", "Camillo Taylor", "George J. Pappas", "Vijay Kumar"], "title": "Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence", "comment": null, "summary": "Heterogeneous robot teams operating in realistic settings often must\naccomplish complex missions requiring collaboration and adaptation to\ninformation acquired online. Because robot teams frequently operate in\nunstructured environments -- uncertain, open-world settings without prior maps\n-- subtasks must be grounded in robot capabilities and the physical world.\nWhile heterogeneous teams have typically been designed for fixed\nspecifications, generative intelligence opens the possibility of teams that can\naccomplish a wide range of missions described in natural language. However,\ncurrent large language model (LLM)-enabled teaming methods typically assume\nwell-structured and known environments, limiting deployment in unstructured\nenvironments. We present SPINE-HT, a framework that addresses these limitations\nby grounding the reasoning abilities of LLMs in the context of a heterogeneous\nrobot team through a three-stage process. Given language specifications\ndescribing mission goals and team capabilities, an LLM generates grounded\nsubtasks which are validated for feasibility. Subtasks are then assigned to\nrobots based on capabilities such as traversability or perception and refined\ngiven feedback collected during online operation. In simulation experiments\nwith closed-loop perception and control, our framework achieves nearly twice\nthe success rate compared to prior LLM-enabled heterogeneous teaming\napproaches. In real-world experiments with a Clearpath Jackal, a Clearpath\nHusky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an\n87\\% success rate in missions requiring reasoning about robot capabilities and\nrefining subtasks with online feedback. More information is provided at\nhttps://zacravichandran.github.io/SPINE-HT.", "AI": {"tldr": "SPINE-HT\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8fc7\u7a0b\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u534f\u4f5c\u548c\u9002\u5e94\u5728\u7ebf\u83b7\u53d6\u7684\u4fe1\u606f\u3002\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56e2\u961f\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u73af\u5883\u7ed3\u6784\u826f\u597d\u4e14\u5df2\u77e5\uff0c\u9650\u5236\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8fc7\u7a0b\uff1a1\uff09\u57fa\u4e8e\u8bed\u8a00\u63cf\u8ff0\u7684\u4efb\u52a1\u76ee\u6807\u548c\u56e2\u961f\u80fd\u529b\uff0c\u7531\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u53ef\u884c\u5b50\u4efb\u52a1\uff1b2\uff09\u6839\u636e\u673a\u5668\u4eba\u80fd\u529b\uff08\u5982\u53ef\u901a\u884c\u6027\u3001\u611f\u77e5\u80fd\u529b\uff09\u5206\u914d\u5b50\u4efb\u52a1\uff1b3\uff09\u57fa\u4e8e\u5728\u7ebf\u64cd\u4f5c\u6536\u96c6\u7684\u53cd\u9988\u7ec6\u5316\u5b50\u4efb\u52a1\u3002", "result": "\u5728\u5177\u6709\u95ed\u73af\u611f\u77e5\u548c\u63a7\u5236\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u7684\u6210\u529f\u7387\u6bd4\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f02\u6784\u56e2\u961f\u65b9\u6cd5\u63d0\u9ad8\u8fd1\u4e00\u500d\u3002\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u5b9e\u73b0\u4e8687%\u7684\u6210\u529f\u7387\u3002", "conclusion": "SPINE-HT\u6846\u67b6\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002"}}
{"id": "2510.26935", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.26935", "abs": "https://arxiv.org/abs/2510.26935", "authors": ["Yunhao Yang", "Neel P. Bhatt", "Pranay Samineni", "Rohan Siva", "Zhanyang Wang", "Ufuk Topcu"], "title": "RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification", "comment": "Code and data are available at: https://repv-project.github.io/", "summary": "As AI systems migrate to safety-critical domains, verifying that their\nactions comply with well-defined rules remains a challenge. Formal methods\nprovide provable guarantees but demand hand-crafted temporal-logic\nspecifications, offering limited expressiveness and accessibility. Deep\nlearning approaches enable evaluation of plans against natural-language\nconstraints, yet their opaque decision process invites misclassifications with\npotentially severe consequences. We introduce RepV, a neurosymbolic verifier\nthat unifies both views by learning a latent space where safe and unsafe plans\nare linearly separable. Starting from a modest seed set of plans labeled by an\noff-the-shelf model checker, RepV trains a lightweight projector that embeds\neach plan, together with a language model-generated rationale, into a\nlow-dimensional space; a frozen linear boundary then verifies compliance for\nunseen natural-language rules in a single forward pass.\n  Beyond binary classification, RepV provides a probabilistic guarantee on the\nlikelihood of correct verification based on its position in the latent space.\nThis guarantee enables a guarantee-driven refinement of the planner, improving\nrule compliance without human annotations. Empirical evaluations show that RepV\nimproves compliance prediction accuracy by up to 15% compared to baseline\nmethods while adding fewer than 0.2M parameters. Furthermore, our refinement\nframework outperforms ordinary fine-tuning baselines across various planning\ndomains. These results show that safety-separable latent spaces offer a\nscalable, plug-and-play primitive for reliable neurosymbolic plan verification.\nCode and data are available at: https://repv-project.github.io/.", "AI": {"tldr": "RepV\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u8ba1\u5212\u7684\u7ebf\u6027\u53ef\u5206\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u7684\u5408\u89c4\u6027\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\u3002", "motivation": "\u5728AI\u7cfb\u7edf\u5e94\u7528\u4e8e\u5b89\u5168\u5173\u952e\u9886\u57df\u65f6\uff0c\u9a8c\u8bc1\u5176\u884c\u4e3a\u662f\u5426\u7b26\u5408\u89c4\u5219\u662f\u4e00\u4e2a\u6311\u6218\u3002\u4f20\u7edf\u5f62\u5f0f\u5316\u65b9\u6cd5\u9700\u8981\u624b\u5de5\u7f16\u5199\u65f6\u6001\u903b\u8f91\u89c4\u8303\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff1b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u5904\u7406\u81ea\u7136\u8bed\u8a00\u7ea6\u675f\uff0c\u4f46\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u8bef\u5224\u3002", "method": "RepV\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u4ece\u5c11\u91cf\u6a21\u578b\u68c0\u67e5\u5668\u6807\u8bb0\u7684\u8ba1\u5212\u5f00\u59cb\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6295\u5f71\u5668\u5c06\u8ba1\u5212\u548c\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u539f\u7406\u5d4c\u5165\u4f4e\u7ef4\u7a7a\u95f4\uff0c\u4f7f\u7528\u51bb\u7ed3\u7684\u7ebf\u6027\u8fb9\u754c\u5bf9\u672a\u89c1\u8fc7\u7684\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u8fdb\u884c\u4e00\u6b21\u6027\u524d\u5411\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cRepV\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c06\u5408\u89c4\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8615%\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u4e0d\u52300.2M\u53c2\u6570\u3002\u5176\u7cbe\u70bc\u6846\u67b6\u5728\u5404\u79cd\u89c4\u5212\u9886\u57df\u90fd\u4f18\u4e8e\u666e\u901a\u5fae\u8c03\u57fa\u7ebf\u3002", "conclusion": "\u5b89\u5168\u53ef\u5206\u7684\u6f5c\u5728\u7a7a\u95f4\u4e3a\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7\u8ba1\u5212\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5373\u63d2\u5373\u7528\u7684\u57fa\u7840\u7ec4\u4ef6\u3002"}}
{"id": "2510.27010", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27010", "abs": "https://arxiv.org/abs/2510.27010", "authors": ["William E. Heap", "Yimeng Qin", "Kai Hammond", "Anish Bayya", "Haonon Kong", "Allison M. Okamura"], "title": "A Hermetic, Transparent Soft Growing Vine Robot System for Pipe Inspection", "comment": "8 pages, 7 figures", "summary": "Rehabilitation of aging pipes requires accurate condition assessment and\nmapping far into the pipe interiors. Soft growing vine robot systems are\nparticularly promising for navigating confined, sinuous paths such as in pipes,\nbut are currently limited by complex subsystems and a lack of validation in\nreal-world industrial settings. In this paper, we introduce the concept and\nimplementation of a hermetic and transparent vine robot system for visual\ncondition assessment and mapping within non-branching pipes. This design\nencloses all mechanical and electrical components within the vine robot's soft,\nairtight, and transparent body, protecting them from environmental interference\nwhile enabling visual sensing. Because this approach requires an enclosed\nmechanism for transporting sensors, we developed, modeled, and tested a\npassively adapting enclosed tip mount. Finally, we validated the hermetic and\ntransparent vine robot system concept through a real-world condition assessment\nand mapping task in a wastewater pipe. This work advances the use of\nsoft-growing vine robots in pipe inspection by developing and demonstrating a\nrobust, streamlined, field-validated system suitable for continued development\nand deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ba1\u9053\u5185\u90e8\u89c6\u89c9\u72b6\u51b5\u8bc4\u4f30\u548c\u6d4b\u7ed8\u7684\u5bc6\u5c01\u900f\u660e\u85e4\u8513\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u673a\u68b0\u548c\u7535\u6c14\u7ec4\u4ef6\u5c01\u88c5\u5728\u673a\u5668\u4eba\u8f6f\u4f53\u5185\uff0c\u4fdd\u62a4\u5176\u514d\u53d7\u73af\u5883\u5e72\u6270\uff0c\u5e76\u5728\u771f\u5b9e\u5e9f\u6c34\u7ba1\u9053\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "motivation": "\u8001\u5316\u7ba1\u9053\u7684\u4fee\u590d\u9700\u8981\u51c6\u786e\u8bc4\u4f30\u7ba1\u9053\u5185\u90e8\u72b6\u51b5\uff0c\u73b0\u6709\u8f6f\u85e4\u8513\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u590d\u6742\u7ba1\u9053\u5bfc\u822a\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u53d7\u5230\u590d\u6742\u5b50\u7cfb\u7edf\u548c\u7f3a\u4e4f\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u9a8c\u8bc1\u7684\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u5bc6\u5c01\u900f\u660e\u7684\u85e4\u8513\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5c06\u6240\u6709\u7ec4\u4ef6\u5c01\u88c5\u5728\u8f6f\u4f53\u5185\u90e8\uff1b\u8bbe\u8ba1\u4e86\u88ab\u52a8\u9002\u5e94\u7684\u5c01\u95ed\u5f0f\u5c16\u7aef\u5b89\u88c5\u5ea7\u6765\u8fd0\u8f93\u4f20\u611f\u5668\uff1b\u5728\u771f\u5b9e\u5e9f\u6c34\u7ba1\u9053\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u6d4b\u8bd5\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u7ba1\u9053\u5185\u90e8\u89c6\u89c9\u72b6\u51b5\u8bc4\u4f30\u548c\u6d4b\u7ed8\uff0c\u7cfb\u7edf\u80fd\u591f\u4fdd\u62a4\u5185\u90e8\u7ec4\u4ef6\u514d\u53d7\u73af\u5883\u5e72\u6270\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u8f6f\u85e4\u8513\u673a\u5668\u4eba\u5728\u7ba1\u9053\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5f00\u53d1\u5e76\u5c55\u793a\u4e86\u4e00\u4e2a\u7a33\u5065\u3001\u7b80\u5316\u3001\u7ecf\u8fc7\u73b0\u573a\u9a8c\u8bc1\u7684\u7cfb\u7edf\uff0c\u9002\u5408\u6301\u7eed\u5f00\u53d1\u548c\u90e8\u7f72\u3002"}}
{"id": "2510.27033", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.27033", "abs": "https://arxiv.org/abs/2510.27033", "authors": ["Simindokht Jahangard", "Mehrzad Mohammadi", "Abhinav Dhall", "Hamid Rezatofighi"], "title": "A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics", "comment": null, "summary": "Visual reasoning, particularly spatial reasoning, is a challenging cognitive\ntask that requires understanding object relationships and their interactions\nwithin complex environments, especially in robotics domain. Existing\nvision_language models (VLMs) excel at perception tasks but struggle with\nfine-grained spatial reasoning due to their implicit, correlation-driven\nreasoning and reliance solely on images. We propose a novel neuro_symbolic\nframework that integrates both panoramic-image and 3D point cloud information,\ncombining neural perception with symbolic reasoning to explicitly model spatial\nand logical relationships. Our framework consists of a perception module for\ndetecting entities and extracting attributes, and a reasoning module that\nconstructs a structured scene graph to support precise, interpretable queries.\nEvaluated on the JRDB-Reasoning dataset, our approach demonstrates superior\nperformance and reliability in crowded, human_built environments while\nmaintaining a lightweight design suitable for robotics and embodied AI\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5168\u666f\u56fe\u50cf\u548c3D\u70b9\u4e91\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7a7a\u95f4\u548c\u903b\u8f91\u5173\u7cfb\u6765\u6539\u8fdb\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u62e5\u6324\u7684\u4eba\u9020\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5b83\u4eec\u4f9d\u8d56\u9690\u5f0f\u7684\u76f8\u5173\u6027\u63a8\u7406\u4e14\u4ec5\u4f7f\u7528\u56fe\u50cf\u6570\u636e\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5305\u542b\u611f\u77e5\u6a21\u5757\uff08\u68c0\u6d4b\u5b9e\u4f53\u548c\u63d0\u53d6\u5c5e\u6027\uff09\u548c\u63a8\u7406\u6a21\u5757\uff08\u6784\u5efa\u7ed3\u6784\u5316\u573a\u666f\u56fe\uff09\uff0c\u7ed3\u5408\u5168\u666f\u56fe\u50cf\u548c3D\u70b9\u4e91\u4fe1\u606f\u8fdb\u884c\u663e\u5f0f\u7a7a\u95f4\u5173\u7cfb\u5efa\u6a21\u3002", "result": "\u5728JRDB-Reasoning\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u62e5\u6324\u7684\u4eba\u9020\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u548c\u5177\u8eabAI\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u63a8\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u6210\u529f\u7ed3\u5408\u4e86\u795e\u7ecf\u611f\u77e5\u548c\u7b26\u53f7\u63a8\u7406\u7684\u4f18\u52bf\u3002"}}
{"id": "2510.27048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27048", "abs": "https://arxiv.org/abs/2510.27048", "authors": ["Eric T. Chang", "Peter Ballentine", "Zhanpeng He", "Do-Gon Kim", "Kai Jiang", "Hua-Hsuan Liang", "Joaquin Palacios", "William Wang", "Pedro Piacenza", "Ioannis Kymissis", "Matei Ciocarlie"], "title": "SpikeATac: A Multimodal Tactile Finger with Taxelized Dynamic Sensing for Dexterous Manipulation", "comment": "9 pages, 8 figures, under review", "summary": "In this work, we introduce SpikeATac, a multimodal tactile finger combining a\ntaxelized and highly sensitive dynamic response (PVDF) with a static\ntransduction method (capacitive) for multimodal touch sensing. Named for its\n`spiky' response, SpikeATac's 16-taxel PVDF film sampled at 4 kHz provides\nfast, sensitive dynamic signals to the very onset and breaking of contact. We\ncharacterize the sensitivity of the different modalities, and show that\nSpikeATac provides the ability to stop quickly and delicately when grasping\nfragile, deformable objects. Beyond parallel grasping, we show that SpikeATac\ncan be used in a learning-based framework to achieve new capabilities on a\ndexterous multifingered robot hand. We use a learning recipe that combines\nreinforcement learning from human feedback with tactile-based rewards to\nfine-tune the behavior of a policy to modulate force. Our hardware platform and\nlearning pipeline together enable a difficult dexterous and contact-rich task\nthat has not previously been achieved: in-hand manipulation of fragile objects.\nVideos are available at\n\\href{https://roamlab.github.io/spikeatac/}{roamlab.github.io/spikeatac}.", "AI": {"tldr": "SpikeATac\u662f\u4e00\u79cd\u7ed3\u5408PVDF\u52a8\u6001\u54cd\u5e94\u548c\u7535\u5bb9\u9759\u6001\u4f20\u611f\u7684\u591a\u6a21\u6001\u89e6\u89c9\u624b\u6307\uff0c\u80fd\u591f\u5feb\u901f\u68c0\u6d4b\u63a5\u89e6\u5f00\u59cb\u548c\u65ad\u5f00\uff0c\u7528\u4e8e\u7cbe\u7ec6\u6293\u53d6\u6613\u788e\u7269\u4f53\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u7075\u5de7\u624b\u5bf9\u6613\u788e\u7269\u4f53\u7684\u5728\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u7cbe\u7ec6\u611f\u77e5\u63a5\u89e6\u52a8\u6001\u548c\u9759\u6001\u4fe1\u606f\u7684\u591a\u6a21\u6001\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u6613\u788e\u3001\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u5b89\u5168\u6293\u53d6\u548c\u7075\u5de7\u64cd\u4f5c\u3002", "method": "\u8bbe\u8ba116\u4e2a\u89e6\u70b9\u7684PVDF\u8584\u819c\u4f20\u611f\u5668\uff0c\u91c7\u6837\u9891\u73874kHz\uff0c\u7ed3\u5408\u7535\u5bb9\u5f0f\u9759\u6001\u4f20\u611f\uff0c\u4f7f\u7528\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u89e6\u89c9\u5956\u52b1\u6765\u5fae\u8c03\u7b56\u7565\u4ee5\u8c03\u8282\u6293\u53d6\u529b\u3002", "result": "SpikeATac\u80fd\u591f\u5feb\u901f\u505c\u6b62\u5e76\u7cbe\u7ec6\u6293\u53d6\u6613\u788e\u7269\u4f53\uff0c\u5728\u7075\u5de7\u591a\u6307\u673a\u5668\u4eba\u624b\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u6613\u788e\u7269\u4f53\u7684\u5728\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u8fd9\u4e00\u56f0\u96be\u4efb\u52a1\u3002", "conclusion": "SpikeATac\u786c\u4ef6\u5e73\u53f0\u548c\u5b66\u4e60\u7ba1\u9053\u76f8\u7ed3\u5408\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u4ee5\u524d\u672a\u8fbe\u5230\u7684\u7075\u5de7\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u2014\u2014\u5728\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u6613\u788e\u7269\u4f53\u3002"}}
{"id": "2510.27114", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27114", "abs": "https://arxiv.org/abs/2510.27114", "authors": ["Dohyeok Lee", "Jung Min Lee", "Munkyung Kim", "Seokhun Ju", "Jin Woo Koo", "Kyungjae Lee", "Dohyeong Kim", "TaeHyun Cho", "Jungwoo Lee"], "title": "Learning Generalizable Visuomotor Policy through Dynamics-Alignment", "comment": "9 pages, 6 figures", "summary": "Behavior cloning methods for robot learning suffer from poor generalization\ndue to limited data support beyond expert demonstrations. Recent approaches\nleveraging video prediction models have shown promising results by learning\nrich spatiotemporal representations from large-scale datasets. However, these\nmodels learn action-agnostic dynamics that cannot distinguish between different\ncontrol inputs, limiting their utility for precise manipulation tasks and\nrequiring large pretraining datasets. We propose a Dynamics-Aligned Flow\nMatching Policy (DAP) that integrates dynamics prediction into policy learning.\nOur method introduces a novel architecture where policy and dynamics models\nprovide mutual corrective feedback during action generation, enabling\nself-correction and improved generalization. Empirical validation demonstrates\ngeneralization performance superior to baseline methods on real-world robotic\nmanipulation tasks, showing particular robustness in OOD scenarios including\nvisual distractions and lighting variations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDynamics-Aligned Flow Matching Policy (DAP)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u52a8\u529b\u5b66\u9884\u6d4b\u96c6\u6210\u5230\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u89e3\u51b3\u4e86\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u7531\u4e8e\u4ec5\u9650\u4e8e\u4e13\u5bb6\u6f14\u793a\u6570\u636e\u800c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u73b0\u6709\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u867d\u7136\u80fd\u4ece\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u5b66\u4e60\u65f6\u7a7a\u8868\u793a\uff0c\u4f46\u5b66\u4e60\u7684\u662f\u52a8\u4f5c\u65e0\u5173\u7684\u52a8\u529b\u5b66\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u63a7\u5236\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5728\u7cbe\u786e\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faDAP\u65b9\u6cd5\uff0c\u91c7\u7528\u65b0\u9896\u67b6\u6784\u4f7f\u7b56\u7565\u548c\u52a8\u529b\u5b66\u6a21\u578b\u5728\u52a8\u4f5c\u751f\u6210\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u76f8\u4e92\u7ea0\u6b63\u53cd\u9988\uff0c\u5b9e\u73b0\u81ea\u6211\u7ea0\u6b63\u548c\u6539\u8fdb\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5305\u62ec\u89c6\u89c9\u5e72\u6270\u548c\u5149\u7167\u53d8\u5316\u5728\u5185\u7684OOD\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7279\u522b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DAP\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u52a8\u529b\u5b66\u9884\u6d4b\u548c\u7b56\u7565\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.27151", "categories": ["cs.RO", "/"], "pdf": "https://arxiv.org/pdf/2510.27151", "abs": "https://arxiv.org/abs/2510.27151", "authors": ["Xueliang Cheng", "Kanzhong Yao", "Andrew West", "Ognjen Marjanovic", "Barry Lennox", "Keir Groves"], "title": "Confined Space Underwater Positioning Using Collaborative Robots", "comment": "31 pages including appendix, 24 figures", "summary": "Positioning of underwater robots in confined and cluttered spaces remains a\nkey challenge for field operations. Existing systems are mostly designed for\nlarge, open-water environments and struggle in industrial settings due to poor\ncoverage, reliance on external infrastructure, and the need for feature-rich\nsurroundings. Multipath effects from continuous sound reflections further\ndegrade signal quality, reducing accuracy and reliability. Accurate and easily\ndeployable positioning is essential for repeatable autonomous missions;\nhowever, this requirement has created a technological bottleneck limiting\nunderwater robotic deployment. This paper presents the Collaborative Aquatic\nPositioning (CAP) system, which integrates collaborative robotics and sensor\nfusion to overcome these limitations. Inspired by the \"mother-ship\" concept,\nthe surface vehicle acts as a mobile leader to assist in positioning a\nsubmerged robot, enabling localization even in GPS-denied and highly\nconstrained environments. The system is validated in a large test tank through\nrepeatable autonomous missions using CAP's position estimates for real-time\ntrajectory control. Experimental results demonstrate a mean Euclidean distance\n(MED) error of 70 mm, achieved in real time without requiring fixed\ninfrastructure, extensive calibration, or environmental features. CAP leverages\nadvances in mobile robot sensing and leader-follower control to deliver a step\nchange in accurate, practical, and infrastructure-free underwater localization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u534f\u4f5c\u6c34\u4e0b\u5b9a\u4f4d\u7cfb\u7edf(CAP)\uff0c\u901a\u8fc7\u6c34\u9762\u673a\u5668\u4eba\u4f5c\u4e3a\u79fb\u52a8\u9886\u5bfc\u8005\u534f\u52a9\u6c34\u4e0b\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u5728GPS\u53d7\u9650\u548c\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5b9a\u4f4d\uff0c\u5e73\u5747\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u8bef\u5dee\u4e3a70\u6beb\u7c73\u3002", "motivation": "\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u53d7\u9650\u548c\u6742\u4e71\u7a7a\u95f4\u4e2d\u7684\u5b9a\u4f4d\u662f\u73b0\u573a\u64cd\u4f5c\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u5927\u578b\u5f00\u653e\u6c34\u57df\u8bbe\u8ba1\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u8986\u76d6\u8303\u56f4\u5dee\u3001\u4f9d\u8d56\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u3001\u9700\u8981\u7279\u5f81\u4e30\u5bcc\u73af\u5883\u7b49\u95ee\u9898\u3002\u591a\u5f84\u6548\u5e94\u8fdb\u4e00\u6b65\u964d\u4f4e\u4fe1\u53f7\u8d28\u91cf\uff0c\u5f71\u54cd\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "method": "CAP\u7cfb\u7edf\u96c6\u6210\u4e86\u534f\u4f5c\u673a\u5668\u4eba\u548c\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\uff0c\u91c7\u7528\"\u6bcd\u8239\"\u6982\u5ff5\uff0c\u6c34\u9762\u8f66\u8f86\u4f5c\u4e3a\u79fb\u52a8\u9886\u5bfc\u8005\u534f\u52a9\u6c34\u4e0b\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u5373\u4f7f\u5728GPS\u53d7\u9650\u548c\u9ad8\u5ea6\u53d7\u9650\u73af\u5883\u4e2d\u4e5f\u80fd\u5b9e\u73b0\u5b9a\u4f4d\u3002\u7cfb\u7edf\u901a\u8fc7CAP\u4f4d\u7f6e\u4f30\u8ba1\u8fdb\u884c\u5b9e\u65f6\u8f68\u8ff9\u63a7\u5236\uff0c\u5728\u5927\u578b\u6d4b\u8bd5\u6c60\u4e2d\u901a\u8fc7\u53ef\u91cd\u590d\u7684\u81ea\u4e3b\u4efb\u52a1\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e8670\u6beb\u7c73\u7684\u5e73\u5747\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u8bef\u5dee\uff0c\u65e0\u9700\u56fa\u5b9a\u57fa\u7840\u8bbe\u65bd\u3001\u5e7f\u6cdb\u6821\u51c6\u6216\u73af\u5883\u7279\u5f81\u5373\u53ef\u5b9e\u65f6\u5b9e\u73b0\u3002", "conclusion": "CAP\u7cfb\u7edf\u5229\u7528\u79fb\u52a8\u673a\u5668\u4eba\u4f20\u611f\u548c\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u63a7\u5236\u7684\u8fdb\u6b65\uff0c\u5728\u51c6\u786e\u3001\u5b9e\u7528\u548c\u65e0\u57fa\u7840\u8bbe\u65bd\u7684\u6c34\u4e0b\u5b9a\u4f4d\u65b9\u9762\u5b9e\u73b0\u4e86\u91cd\u5927\u7a81\u7834\u3002"}}
{"id": "2510.27178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27178", "abs": "https://arxiv.org/abs/2510.27178", "authors": ["Xuan-Thuan Nguyen", "Khac Nam Nguyen", "Ngoc Duy Tran", "Thi Thoa Mac", "Anh Nguyen", "Hoang Hiep Ly", "Tung D. Ta"], "title": "MobiDock: Design and Control of A Modular Self Reconfigurable Bimanual Mobile Manipulator via Robotic Docking", "comment": "ICRA2026 submited", "summary": "Multi-robot systems, particularly mobile manipulators, face challenges in\ncontrol coordination and dynamic stability when working together. To address\nthis issue, this study proposes MobiDock, a modular self-reconfigurable mobile\nmanipulator system that allows two independent robots to physically connect and\nform a unified mobile bimanual platform. This process helps transform a complex\nmulti-robot control problem into the management of a simpler, single system.\nThe system utilizes an autonomous docking strategy based on computer vision\nwith AprilTag markers and a new threaded screw-lock mechanism. Experimental\nresults show that the docked configuration demonstrates better performance in\ndynamic stability and operational efficiency compared to two independently\ncooperating robots. Specifically, the unified system has lower Root Mean Square\n(RMS) Acceleration and Jerk values, higher angular precision, and completes\ntasks significantly faster. These findings confirm that physical\nreconfiguration is a powerful design principle that simplifies cooperative\ncontrol, improving stability and performance for complex tasks in real-world\nenvironments.", "AI": {"tldr": "MobiDock\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u81ea\u91cd\u6784\u79fb\u52a8\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u5141\u8bb8\u4e24\u4e2a\u72ec\u7acb\u673a\u5668\u4eba\u7269\u7406\u8fde\u63a5\u5f62\u6210\u7edf\u4e00\u7684\u79fb\u52a8\u53cc\u624b\u5e73\u53f0\uff0c\u5c06\u590d\u6742\u591a\u673a\u5668\u4eba\u63a7\u5236\u95ee\u9898\u7b80\u5316\u4e3a\u5355\u4e00\u7cfb\u7edf\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u7279\u522b\u662f\u79fb\u52a8\u673a\u68b0\u81c2\uff09\u5728\u534f\u540c\u5de5\u4f5c\u65f6\u9762\u4e34\u7684\u534f\u8c03\u63a7\u5236\u548c\u52a8\u6001\u7a33\u5b9a\u6027\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAprilTag\u6807\u8bb0\u7684\u81ea\u4e3b\u5bf9\u63a5\u7b56\u7565\uff0c\u4ee5\u53ca\u65b0\u578b\u87ba\u7eb9\u87ba\u9489\u9501\u5b9a\u673a\u5236\uff0c\u5b9e\u73b0\u4e24\u4e2a\u72ec\u7acb\u673a\u5668\u4eba\u7684\u7269\u7406\u8fde\u63a5\u3002", "result": "\u5bf9\u63a5\u914d\u7f6e\u5728\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff1aRMS\u52a0\u901f\u5ea6\u548c\u6025\u52a8\u5ea6\u503c\u66f4\u4f4e\uff0c\u89d2\u5ea6\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u663e\u8457\u7f29\u77ed\u3002", "conclusion": "\u7269\u7406\u91cd\u6784\u662f\u7b80\u5316\u534f\u540c\u63a7\u5236\u7684\u6709\u6548\u8bbe\u8ba1\u539f\u5219\uff0c\u80fd\u63d0\u9ad8\u590d\u6742\u4efb\u52a1\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.27184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27184", "abs": "https://arxiv.org/abs/2510.27184", "authors": ["Hoang Hiep Ly", "Cong-Nhat Nguyen", "Doan-Quang Tran", "Quoc-Khanh Dang", "Ngoc Duy Tran", "Thi Thoa Mac", "Anh Nguyen", "Xuan-Thuan Nguyen", "Tung D. Ta"], "title": "Hybrid Gripper Finger Enabling In-Grasp Friction Modulation Using Inflatable Silicone Pockets", "comment": "Submitted to ICRA 2026", "summary": "Grasping objects with diverse mechanical properties, such as heavy, slippery,\nor fragile items, remains a significant challenge in robotics. Conventional\ngrippers often rely on applying high normal forces, which can cause damage to\nobjects. To address this limitation, we present a hybrid gripper finger that\ncombines a rigid structural shell with a soft, inflatable silicone pocket. The\ngripper finger can actively modulate its surface friction by controlling the\ninternal air pressure of the silicone pocket. Results from fundamental\nexperiments indicate that increasing the internal pressure results in a\nproportional increase in the effective coefficient of friction. This enables\nthe gripper to stably lift heavy and slippery objects without increasing the\ngripping force and to handle fragile or deformable objects, such as eggs,\nfruits, and paper cups, with minimal damage by increasing friction rather than\napplying excessive force. The experimental results demonstrate that the hybrid\ngripper finger with adaptable friction provides a robust and safer alternative\nto relying solely on high normal forces, thereby enhancing the gripper\nflexibility in handling delicate, fragile, and diverse objects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u521a\u6027\u5916\u58f3\u548c\u53ef\u5145\u6c14\u7845\u80f6\u888b\u7684\u6df7\u5408\u5939\u722a\u624b\u6307\uff0c\u901a\u8fc7\u63a7\u5236\u5185\u90e8\u6c14\u538b\u4e3b\u52a8\u8c03\u8282\u8868\u9762\u6469\u64e6\u7cfb\u6570\uff0c\u5b9e\u73b0\u7a33\u5b9a\u6293\u53d6\u91cd\u7269\u3001\u6613\u6ed1\u7269\u4f53\u548c\u6613\u788e\u7269\u54c1\u800c\u65e0\u9700\u589e\u52a0\u5939\u6301\u529b\u3002", "motivation": "\u4f20\u7edf\u5939\u722a\u4f9d\u8d56\u9ad8\u6cd5\u5411\u529b\u5bb9\u6613\u635f\u574f\u7269\u4f53\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5b89\u5168\u5904\u7406\u5177\u6709\u4e0d\u540c\u673a\u68b0\u7279\u6027\uff08\u5982\u91cd\u3001\u6ed1\u3001\u8106\uff09\u7269\u4f53\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u6df7\u5408\u5939\u722a\u624b\u6307\uff0c\u521a\u6027\u7ed3\u6784\u5916\u58f3\u914d\u8f6f\u6027\u53ef\u5145\u6c14\u7845\u80f6\u888b\uff0c\u901a\u8fc7\u63a7\u5236\u7845\u80f6\u888b\u5185\u90e8\u6c14\u538b\u6765\u4e3b\u52a8\u8c03\u8282\u8868\u9762\u6469\u64e6\u7cfb\u6570\u3002", "result": "\u57fa\u7840\u5b9e\u9a8c\u8868\u660e\u589e\u52a0\u5185\u90e8\u538b\u529b\u4f1a\u6210\u6bd4\u4f8b\u589e\u52a0\u6709\u6548\u6469\u64e6\u7cfb\u6570\uff0c\u4f7f\u5939\u722a\u80fd\u7a33\u5b9a\u6293\u53d6\u91cd\u6ed1\u7269\u4f53\u800c\u4e0d\u589e\u52a0\u5939\u6301\u529b\uff0c\u5e76\u80fd\u5b89\u5168\u5904\u7406\u6613\u788e\u7269\u54c1\u5982\u9e21\u86cb\u3001\u6c34\u679c\u548c\u7eb8\u676f\u3002", "conclusion": "\u5177\u6709\u53ef\u8c03\u6469\u64e6\u529f\u80fd\u7684\u6df7\u5408\u5939\u722a\u624b\u6307\u4e3a\u4f9d\u8d56\u9ad8\u6cd5\u5411\u529b\u7684\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u3001\u66f4\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u5904\u7406\u7cbe\u7ec6\u3001\u6613\u788e\u548c\u591a\u6837\u5316\u7269\u4f53\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.27191", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27191", "abs": "https://arxiv.org/abs/2510.27191", "authors": ["Marcus Hoerger", "Muhammad Sudrajat", "Hanna Kurniawati"], "title": "Vectorized Online POMDP Planning", "comment": "8 pages, 3 figures. Submitted to ICRA 2026", "summary": "Planning under partial observability is an essential capability of autonomous\nrobots. The Partially Observable Markov Decision Process (POMDP) provides a\npowerful framework for planning under partial observability problems, capturing\nthe stochastic effects of actions and the limited information available through\nnoisy observations. POMDP solving could benefit tremendously from massive\nparallelization of today's hardware, but parallelizing POMDP solvers has been\nchallenging. They rely on interleaving numerical optimization over actions with\nthe estimation of their values, which creates dependencies and synchronization\nbottlenecks between parallel processes that can quickly offset the benefits of\nparallelization. In this paper, we propose Vectorized Online POMDP Planner\n(VOPP), a novel parallel online solver that leverages a recent POMDP\nformulation that analytically solves part of the optimization component,\nleaving only the estimation of expectations for numerical computation. VOPP\nrepresents all data structures related to planning as a collection of tensors\nand implements all planning steps as fully vectorized computations over this\nrepresentation. The result is a massively parallel solver with no dependencies\nand synchronization bottlenecks between parallel computations. Experimental\nresults indicate that VOPP is at least 20X more efficient in computing\nnear-optimal solutions compared to an existing state-of-the-art parallel online\nsolver.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVOPP\u7684\u5e76\u884c\u5728\u7ebfPOMDP\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5411\u91cf\u5316\u8ba1\u7b97\u548c\u6d88\u9664\u540c\u6b65\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u81f3\u5c1120\u500d\u7684\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u89c4\u5212\u662f\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u5173\u952e\u80fd\u529b\uff0cPOMDP\u6846\u67b6\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u73b0\u6709\u5e76\u884c\u6c42\u89e3\u5668\u5b58\u5728\u4f9d\u8d56\u5173\u7cfb\u548c\u540c\u6b65\u74f6\u9888\uff0c\u9650\u5236\u4e86\u786c\u4ef6\u5e76\u884c\u5316\u7684\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u5411\u91cf\u5316\u7684POMDP\u89c4\u5212\u65b9\u6cd5\uff0c\u5c06\u6240\u6709\u89c4\u5212\u6570\u636e\u7ed3\u6784\u8868\u793a\u4e3a\u5f20\u91cf\u96c6\u5408\uff0c\u5e76\u5c06\u6240\u6709\u89c4\u5212\u6b65\u9aa4\u5b9e\u73b0\u4e3a\u5b8c\u5168\u5411\u91cf\u5316\u7684\u8ba1\u7b97\uff0c\u6d88\u9664\u4e86\u5e76\u884c\u8ba1\u7b97\u95f4\u7684\u4f9d\u8d56\u548c\u540c\u6b65\u74f6\u9888\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cVOPP\u5728\u8ba1\u7b97\u8fd1\u4f18\u89e3\u65b9\u9762\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5e76\u884c\u5728\u7ebf\u6c42\u89e3\u5668\u81f3\u5c11\u9ad8\u654820\u500d\u3002", "conclusion": "VOPP\u901a\u8fc7\u5411\u91cf\u5316\u8ba1\u7b97\u548c\u6d88\u9664\u540c\u6b65\u74f6\u9888\uff0c\u6210\u529f\u5b9e\u73b0\u4e86POMDP\u6c42\u89e3\u7684\u5927\u89c4\u6a21\u5e76\u884c\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2510.27327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27327", "abs": "https://arxiv.org/abs/2510.27327", "authors": ["Robert Pommeranz", "Kevin Tebbe", "Ralf Heynicke", "Gerd Scholl"], "title": "A Modular and Scalable System Architecture for Heterogeneous UAV Swarms Using ROS 2 and PX4-Autopilot", "comment": null, "summary": "In this paper a modular and scalable architecture for heterogeneous\nswarm-based Counter Unmanned Aerial Systems (C-UASs) built on PX4-Autopilot and\nRobot Operating System 2 (ROS 2) framework is presented. The proposed\narchitecture emphasizes seamless integration of hardware components by\nintroducing independent ROS 2 nodes for each component of a Unmanned Aerial\nVehicle (UAV). Communication between swarm participants is abstracted in\nsoftware, allowing the use of various technologies without architectural\nchanges. Key functionalities are supported, e.g. leader following and formation\nflight to maneuver the swarm. The system also allows computer vision algorithms\nto be integrated for the detection and tracking of UAVs. Additionally, a ground\nstation control is integrated for the coordination of swarm operations.\nSwarm-based Unmanned Aerial System (UAS) architecture is verified within a\nGazebo simulation environment but also in real-world demonstrations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePX4-Autopilot\u548cROS 2\u7684\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u5f02\u6784\u65e0\u4eba\u673a\u7fa4\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u67b6\u6784\uff0c\u652f\u6301\u786c\u4ef6\u65e0\u7f1d\u96c6\u6210\u3001\u591a\u79cd\u901a\u4fe1\u6280\u672f\u3001\u7f16\u961f\u98de\u884c\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u96c6\u6210\u5f02\u6784\u786c\u4ef6\u7ec4\u4ef6\u3001\u652f\u6301\u591a\u79cd\u901a\u4fe1\u6280\u672f\u3001\u5b9e\u73b0\u65e0\u4eba\u673a\u7fa4\u534f\u540c\u4f5c\u6218\u7684\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u67b6\u6784\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4e3a\u6bcf\u4e2a\u65e0\u4eba\u673a\u7ec4\u4ef6\u521b\u5efa\u72ec\u7acb\u7684ROS 2\u8282\u70b9\uff0c\u901a\u8fc7\u8f6f\u4ef6\u62bd\u8c61\u5b9e\u73b0\u901a\u4fe1\u6280\u672f\u65e0\u5173\u6027\uff0c\u96c6\u6210\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u548c\u5730\u9762\u7ad9\u63a7\u5236\u7cfb\u7edf\u3002", "result": "\u5728Gazebo\u4eff\u771f\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65e0\u4eba\u673a\u7fa4\u67b6\u6784\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u5f02\u6784\u65e0\u4eba\u673a\u7fa4\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2510.27333", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27333", "abs": "https://arxiv.org/abs/2510.27333", "authors": ["Hao Cheng", "Yanbo Jiang", "Qingyuan Shi", "Qingwen Meng", "Keyu Chen", "Wenhao Yu", "Jianqiang Wang", "Sifa Zheng"], "title": "Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict", "comment": null, "summary": "Effective, reliable, and efficient evaluation of autonomous driving safety is\nessential to demonstrate its trustworthiness. Criticality metrics provide an\nobjective means of assessing safety. However, as existing metrics primarily\ntarget longitudinal conflicts, accurately quantifying the risks of lateral\nconflicts - prevalent in urban settings - remains challenging. This paper\nproposes the Modified-Emergency Index (MEI), a metric designed to quantify\nevasive effort in lateral conflicts. Compared to the original Emergency Index\n(EI), MEI refines the estimation of the time available for evasive maneuvers,\nenabling more precise risk quantification. We validate MEI on a public lateral\nconflict dataset based on Argoverse-2, from which we extract over 1,500\nhigh-quality AV conflict cases, including more than 500 critical events. MEI is\nthen compared with the well-established ACT and the widely used PET metrics.\nResults show that MEI consistently outperforms them in accurately quantifying\ncriticality and capturing risk evolution. Overall, these findings highlight MEI\nas a promising metric for evaluating urban conflicts and enhancing the safety\nassessment framework for autonomous driving. The open-source implementation is\navailable at https://github.com/AutoChengh/MEI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u7d27\u6025\u6307\u6570\uff08MEI\uff09\u6765\u91cf\u5316\u6a2a\u5411\u51b2\u7a81\u4e2d\u7684\u89c4\u907f\u52aa\u529b\uff0c\u76f8\u6bd4\u73b0\u6709\u6307\u6807\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u57ce\u5e02\u73af\u5883\u4e2d\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5173\u952e\u6027\u6307\u6807\u4e3b\u8981\u9488\u5bf9\u7eb5\u5411\u51b2\u7a81\uff0c\u96be\u4ee5\u51c6\u786e\u91cf\u5316\u57ce\u5e02\u73af\u5883\u4e2d\u5e38\u89c1\u7684\u6a2a\u5411\u51b2\u7a81\u98ce\u9669\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMEI\u6307\u6807\uff0c\u6539\u8fdb\u539f\u59cb\u7d27\u6025\u6307\u6570\u4e2d\u5bf9\u89c4\u907f\u673a\u52a8\u53ef\u7528\u65f6\u95f4\u7684\u4f30\u8ba1\uff0c\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u98ce\u9669\u91cf\u5316\u3002\u5728\u57fa\u4e8eArgoverse-2\u7684\u516c\u5171\u6a2a\u5411\u51b2\u7a81\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u63d0\u53d61500\u591a\u4e2a\u9ad8\u8d28\u91cfAV\u51b2\u7a81\u6848\u4f8b\u3002", "result": "MEI\u5728\u51c6\u786e\u91cf\u5316\u5173\u952e\u6027\u548c\u6355\u6349\u98ce\u9669\u6f14\u53d8\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u65e2\u6709\u7684ACT\u548cPET\u6307\u6807\u3002", "conclusion": "MEI\u662f\u8bc4\u4f30\u57ce\u5e02\u51b2\u7a81\u548c\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u7684\u6709\u524d\u666f\u6307\u6807\u3002"}}
{"id": "2510.27420", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.27420", "abs": "https://arxiv.org/abs/2510.27420", "authors": ["Roman Freiberg", "Alexander Qualmann", "Ngo Anh Vien", "Gerhard Neumann"], "title": "Towards a Multi-Embodied Grasping Agent", "comment": "9 pages, 3 figures", "summary": "Multi-embodiment grasping focuses on developing approaches that exhibit\ngeneralist behavior across diverse gripper designs. Existing methods often\nlearn the kinematic structure of the robot implicitly and face challenges due\nto the difficulty of sourcing the required large-scale data. In this work, we\npresent a data-efficient, flow-based, equivariant grasp synthesis architecture\nthat can handle different gripper types with variable degrees of freedom and\nsuccessfully exploit the underlying kinematic model, deducing all necessary\ninformation solely from the gripper and scene geometry. Unlike previous\nequivariant grasping methods, we translated all modules from the ground up to\nJAX and provide a model with batching capabilities over scenes, grippers, and\ngrasps, resulting in smoother learning, improved performance and faster\ninference time. Our dataset encompasses grippers ranging from humanoid hands to\nparallel yaw grippers and includes 25,000 scenes and 20 million grasps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u3001\u57fa\u4e8e\u6d41\u7684\u7b49\u53d8\u6293\u53d6\u5408\u6210\u67b6\u6784\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u81ea\u7531\u5ea6\u7c7b\u578b\u7684\u5939\u722a\uff0c\u4ec5\u4ece\u5939\u722a\u548c\u573a\u666f\u51e0\u4f55\u4e2d\u63a8\u65ad\u5fc5\u8981\u4fe1\u606f\uff0c\u5e76\u5728JAX\u4e2d\u5b9e\u73b0\u6240\u6709\u6a21\u5757\uff0c\u652f\u6301\u573a\u666f\u3001\u5939\u722a\u548c\u6293\u53d6\u7684\u6279\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u591a\u4f53\u6293\u53d6\u65b9\u6cd5\u901a\u5e38\u9690\u5f0f\u5b66\u4e60\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u7ed3\u6784\uff0c\u4e14\u9762\u4e34\u5927\u89c4\u6a21\u6570\u636e\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u6d41\u7684\u7b49\u53d8\u6293\u53d6\u5408\u6210\u67b6\u6784\uff0c\u5229\u7528JAX\u5b9e\u73b0\u6240\u6709\u6a21\u5757\uff0c\u652f\u6301\u5bf9\u573a\u666f\u3001\u5939\u722a\u548c\u6293\u53d6\u7684\u6279\u5904\u7406\uff0c\u4ec5\u4ece\u51e0\u4f55\u4fe1\u606f\u63a8\u65ad\u8fd0\u52a8\u5b66\u6a21\u578b\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b\u4ece\u4eba\u5f62\u624b\u5230\u5e73\u884c\u504f\u8f6c\u5939\u722a\u768425,000\u4e2a\u573a\u666f\u548c2000\u4e07\u6b21\u6293\u53d6\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u5b66\u4e60\u3001\u6539\u8fdb\u7684\u6027\u80fd\u548c\u66f4\u5feb\u7684\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u5939\u722a\uff0c\u4ec5\u4f9d\u8d56\u51e0\u4f55\u4fe1\u606f\u5373\u53ef\u63a8\u65ad\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u5728\u6570\u636e\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.27428", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27428", "abs": "https://arxiv.org/abs/2510.27428", "authors": ["Hehui Zheng", "Bhavya Sukhija", "Chenhao Li", "Klemens Iten", "Andreas Krause", "Robert K. Katzschmann"], "title": "Learning Soft Robotic Dynamics with Active Exploration", "comment": null, "summary": "Soft robots offer unmatched adaptability and safety in unstructured\nenvironments, yet their compliant, high-dimensional, and nonlinear dynamics\nmake modeling for control notoriously difficult. Existing data-driven\napproaches often fail to generalize, constrained by narrowly focused task\ndemonstrations or inefficient random exploration. We introduce SoftAE, an\nuncertainty-aware active exploration framework that autonomously learns\ntask-agnostic and generalizable dynamics models of soft robotic systems. SoftAE\nemploys probabilistic ensemble models to estimate epistemic uncertainty and\nactively guides exploration toward underrepresented regions of the state-action\nspace, achieving efficient coverage of diverse behaviors without task-specific\nsupervision. We evaluate SoftAE on three simulated soft robotic platforms -- a\ncontinuum arm, an articulated fish in fluid, and a musculoskeletal leg with\nhybrid actuation -- and on a pneumatically actuated continuum soft arm in the\nreal world. Compared with random exploration and task-specific model-based\nreinforcement learning, SoftAE produces more accurate dynamics models, enables\nsuperior zero-shot control on unseen tasks, and maintains robustness under\nsensing noise, actuation delays, and nonlinear material effects. These results\ndemonstrate that uncertainty-driven active exploration can yield scalable,\nreusable dynamics models across diverse soft robotic morphologies, representing\na step toward more autonomous, adaptable, and data-efficient control in\ncompliant robots.", "AI": {"tldr": "SoftAE\u662f\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4efb\u52a1\u65e0\u5173\u4e14\u53ef\u6cdb\u5316\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u7387\u96c6\u6210\u6a21\u578b\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u533a\u57df\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5177\u6709\u51fa\u8272\u7684\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\uff0c\u4f46\u5176\u67d4\u987a\u3001\u9ad8\u7ef4\u548c\u975e\u7ebf\u6027\u7684\u52a8\u529b\u5b66\u7279\u6027\u4f7f\u5f97\u5efa\u6a21\u63a7\u5236\u53d8\u5f97\u56f0\u96be\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5f80\u5f80\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u53d7\u9650\u4e8e\u72ed\u7a84\u7684\u4efb\u52a1\u6f14\u793a\u6216\u4f4e\u6548\u7684\u968f\u673a\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u6982\u7387\u96c6\u6210\u6a21\u578b\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u533a\u57df\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u76d1\u7763\u5373\u53ef\u9ad8\u6548\u8986\u76d6\u591a\u6837\u5316\u884c\u4e3a\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u62df\u8f6f\u4f53\u673a\u5668\u4eba\u5e73\u53f0\u548c\u771f\u5b9e\u6c14\u52a8\u8fde\u7eed\u8f6f\u81c2\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u968f\u673a\u63a2\u7d22\u548c\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff0cSoftAE\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u66f4\u4f18\u7684\u96f6\u6837\u672c\u63a7\u5236\uff0c\u5e76\u5728\u611f\u77e5\u566a\u58f0\u3001\u9a71\u52a8\u5ef6\u8fdf\u548c\u975e\u7ebf\u6027\u6750\u6599\u6548\u5e94\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u4e3b\u52a8\u63a2\u7d22\u80fd\u591f\u4e3a\u591a\u6837\u5316\u8f6f\u4f53\u673a\u5668\u4eba\u5f62\u6001\u751f\u6210\u53ef\u6269\u5c55\u3001\u53ef\u91cd\u7528\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u671d\u7740\u66f4\u81ea\u4e3b\u3001\u9002\u5e94\u6027\u5f3a\u548c\u6570\u636e\u9ad8\u6548\u7684\u63a7\u5236\u8fc8\u8fdb\u4e00\u6b65\u3002"}}
{"id": "2510.27436", "categories": ["cs.RO", "cs.HC", "I.2.9; I.3.6"], "pdf": "https://arxiv.org/pdf/2510.27436", "abs": "https://arxiv.org/abs/2510.27436", "authors": ["Tomoko Yonezawa", "Hirotake Yamazoe", "Atsuo Fujino", "Daigo Suhara", "Takaya Tamamoto", "Yuto Nishiguchi"], "title": "Preliminary Prototyping of Avoidance Behaviors Triggered by a User's Physical Approach to a Robot", "comment": "Workshop on Socially Aware and Cooperative Intelligent Systems in HAI\n  2025", "summary": "Human-robot interaction frequently involves physical proximity or contact. In\nhuman-human settings, people flexibly accept, reject, or tolerate such\napproaches depending on the relationship and context. We explore the design of\na robot's rejective internal state and corresponding avoidance behaviors, such\nas withdrawing or pushing away, when a person approaches. We model the\naccumulation and decay of discomfort as a function of interpersonal distance,\nand implement tolerance (endurance) and limit-exceeding avoidance driven by the\nDominance axis of the PAD affect model. The behaviors and their intensities are\nrealized on an arm robot. Results illustrate a coherent pipeline from internal\nstate parameters to graded endurance motions and, once a limit is crossed, to\navoidance actions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u5185\u90e8\u72b6\u6001\u548c\u56de\u907f\u884c\u4e3a\u6765\u62d2\u7edd\u4eba\u7c7b\u7684\u63a5\u8fd1\uff0c\u57fa\u4e8e\u4eba\u9645\u8ddd\u79bb\u5efa\u6a21\u4e0d\u9002\u611f\u7684\u79ef\u7d2f\u548c\u8870\u51cf\uff0c\u5e76\u5728\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0\u8010\u53d7\u548c\u6781\u9650\u56de\u907f\u884c\u4e3a\u3002", "motivation": "\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\uff0c\u7269\u7406\u63a5\u8fd1\u6216\u63a5\u89e6\u5f88\u5e38\u89c1\u3002\u4eba\u7c7b\u4f1a\u6839\u636e\u5173\u7cfb\u548c\u60c5\u5883\u7075\u6d3b\u5730\u63a5\u53d7\u3001\u62d2\u7edd\u6216\u5bb9\u5fcd\u8fd9\u79cd\u63a5\u8fd1\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u673a\u5668\u4eba\u80fd\u591f\u8868\u8fbe\u62d2\u7edd\u5185\u90e8\u72b6\u6001\u548c\u76f8\u5e94\u56de\u907f\u884c\u4e3a\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u4eba\u9645\u8ddd\u79bb\u51fd\u6570\u5efa\u6a21\u4e0d\u9002\u611f\u7684\u79ef\u7d2f\u548c\u8870\u51cf\uff0c\u5229\u7528PAD\u60c5\u611f\u6a21\u578b\u7684\u652f\u914d\u6027\u8f74\u5b9e\u73b0\u8010\u53d7\uff08\u5fcd\u8010\uff09\u548c\u6781\u9650\u8d85\u51fa\u7684\u56de\u907f\u884c\u4e3a\uff0c\u5e76\u5728\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0\u8fd9\u4e9b\u884c\u4e3a\u53ca\u5176\u5f3a\u5ea6\u3002", "result": "\u7ed3\u679c\u5c55\u793a\u4e86\u4ece\u5185\u90e8\u72b6\u6001\u53c2\u6570\u5230\u5206\u7ea7\u8010\u53d7\u52a8\u4f5c\uff0c\u518d\u5230\u6781\u9650\u88ab\u8d85\u8d8a\u65f6\u7684\u56de\u907f\u52a8\u4f5c\u7684\u8fde\u8d2f\u6d41\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u62d2\u7edd\u5185\u90e8\u72b6\u6001\u548c\u56de\u907f\u884c\u4e3a\u7684\u8bbe\u8ba1\uff0c\u4e3a\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u7269\u7406\u63a5\u8fd1\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u5e94\u5bf9\u673a\u5236\u3002"}}
{"id": "2510.27558", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27558", "abs": "https://arxiv.org/abs/2510.27558", "authors": ["Sushil Samuel Dinesh", "Shinkyu Park"], "title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs", "comment": null, "summary": "This paper presents a framework that leverages pre-trained foundation models\nfor robotic manipulation without domain-specific training. The framework\nintegrates off-the-shelf models, combining multimodal perception from\nfoundation models with a general-purpose reasoning model capable of robust task\nsequencing. Scene graphs, dynamically maintained within the framework, provide\nspatial awareness and enable consistent reasoning about the environment. The\nframework is evaluated through a series of tabletop robotic manipulation\nexperiments, and the results highlight its potential for building robotic\nmanipulation systems directly on top of off-the-shelf foundation models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6846\u67b6\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u611f\u77e5\u548c\u901a\u7528\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u573a\u666f\u56fe\u63d0\u4f9b\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u5229\u7528\u73b0\u6210\u7684\u57fa\u7840\u6a21\u578b\u6784\u5efa\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff0c\u907f\u514d\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u7cfb\u7edf\u7684\u901a\u7528\u6027\u548c\u90e8\u7f72\u6548\u7387\u3002", "method": "\u6574\u5408\u73b0\u6210\u6a21\u578b\uff0c\u5c06\u57fa\u7840\u6a21\u578b\u7684\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\u4e0e\u901a\u7528\u63a8\u7406\u6a21\u578b\u76f8\u7ed3\u5408\uff0c\u4f7f\u7528\u52a8\u6001\u7ef4\u62a4\u7684\u573a\u666f\u56fe\u63d0\u4f9b\u7a7a\u95f4\u611f\u77e5\u548c\u73af\u5883\u4e00\u81f4\u6027\u63a8\u7406\u3002", "result": "\u901a\u8fc7\u684c\u9762\u673a\u5668\u4eba\u64cd\u4f5c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u76f4\u63a5\u5728\u73b0\u6210\u57fa\u7840\u6a21\u578b\u4e0a\u6784\u5efa\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u6784\u5efa\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
