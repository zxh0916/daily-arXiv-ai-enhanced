<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards](https://arxiv.org/abs/2510.09543)
*Chenghao Wang,Arjun Viswanathan,Eric Sihite,Alireza Ramezani*

Main category: cs.RO

TL;DR: 该论文提出了一种结合冲击缓解因子(IMF)和对抗运动先验(AMP)的方法，使强化学习策略能够同时学习动物的显性运动轨迹和隐性被动动力学，实现了高达32%的能源效率提升。


<details>
  <summary>Details</summary>
Motivation: 动物通过其隐含的被动动力学实现节能运动，但现有的模仿学习方法主要捕捉显性步态模式，而忽略了隐性被动动力学。

Method: 通过引入冲击缓解因子(IMF)这一基于物理的指标来量化机器人被动缓解冲击的能力，并将其与对抗运动先验(AMP)和强化学习相结合。

Result: 在AMP和手工设计的奖励结构中，能量效率（通过运输成本CoT衡量）提高了高达32%。

Conclusion: 该方法成功地将显性运动轨迹学习和隐性被动动力学学习相结合，显著提升了机器人的能源效率。

Abstract: Animals achieve energy-efficient locomotion by their implicit passive
dynamics, a marvel that has captivated roboticists for decades.Recently,
methods incorporated Adversarial Motion Prior (AMP) and Reinforcement learning
(RL) shows promising progress to replicate Animals' naturalistic motion.
However, such imitation learning approaches predominantly capture explicit
kinematic patterns, so-called gaits, while overlooking the implicit passive
dynamics. This work bridges this gap by incorporating a reward term guided by
Impact Mitigation Factor (IMF), a physics-informed metric that quantifies a
robot's ability to passively mitigate impacts. By integrating IMF with AMP, our
approach enables RL policies to learn both explicit motion trajectories from
animal reference motion and the implicit passive dynamic. We demonstrate energy
efficiency improvements of up to 32%, as measured by the Cost of Transport
(CoT), across both AMP and handcrafted reward structure.

</details>
