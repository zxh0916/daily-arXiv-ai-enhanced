<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling](https://arxiv.org/abs/2511.04758)
*Caelan Garrett,Fabio Ramos*

Main category: cs.RO

TL;DR: 本文提出了ScheduleStream框架，这是第一个用于规划与调度的通用框架，能够处理采样操作。该框架通过混合持续动作建模时间动态，允许异步启动动作并根据参数持续不同时间，解决了双臂和类人机器人并行运动规划的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 双臂和类人机器人因其能够利用多个臂高效完成任务而具有吸引力，但由于混合离散-连续动作空间的增长，同时控制多个臂在计算上具有挑战性。传统的任务与运动规划算法通常只能规划单个臂依次运动，无法生成允许并行臂运动的调度方案。

Method: 提出了ScheduleStream框架，使用混合持续动作建模时间动态，这些动作可以异步启动，其持续时间是参数的函数。开发了领域无关的算法来解决ScheduleStream问题，无需任何应用特定机制。在任务与运动规划与调度中，使用GPU加速采样器来加快规划过程。

Result: 在仿真中，ScheduleStream算法与多个消融实验相比，能够产生更高效的解决方案。在多个真实世界双臂机器人任务上进行了演示验证。

Conclusion: ScheduleStream成功扩展了任务与运动规划以生成调度方案，解决了双臂机器人并行运动规划的计算挑战，通过GPU加速和通用框架设计实现了高效的并行运动规划。

Abstract: Bimanual and humanoid robots are appealing because of their human-like
ability to leverage multiple arms to efficiently complete tasks. However,
controlling multiple arms at once is computationally challenging due to the
growth in the hybrid discrete-continuous action space. Task and Motion Planning
(TAMP) algorithms can efficiently plan in hybrid spaces but generally produce
plans, where only one arm is moving at a time, rather than schedules that allow
for parallel arm motion. In order to extend TAMP to produce schedules, we
present ScheduleStream, the first general-purpose framework for planning &
scheduling with sampling operations. ScheduleStream models temporal dynamics
using hybrid durative actions, which can be started asynchronously and persist
for a duration that's a function of their parameters. We propose
domain-independent algorithms that solve ScheduleStream problems without any
application-specific mechanisms. We apply ScheduleStream to Task and Motion
Planning & Scheduling (TAMPAS), where we use GPU acceleration within samplers
to expedite planning. We compare ScheduleStream algorithms to several ablations
in simulation and find that they produce more efficient solutions. We
demonstrate ScheduleStream on several real-world bimanual robot tasks at
https://schedulestream.github.io.

</details>


### [2] [ReGen: Generative Robot Simulation via Inverse Design](https://arxiv.org/abs/2511.04769)
*Phat Nguyen,Tsun-Hsuan Wang,Zhang-Wei Hong,Erfan Aasi,Andrew Silva,Guy Rosman,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: ReGen是一个生成式仿真框架，通过逆向设计自动生成机器人仿真场景。给定机器人行为轨迹或目标函数及其文本描述，ReGen能够推断出可能导致该行为的合理场景和环境。


<details>
  <summary>Details</summary>
Motivation: 构建仿真环境是机器人学习中的劳动密集型过程，现有仿真设计方法效率低下，限制了机器人策略的验证和泛化能力。

Method: 利用大语言模型通过扩展编码因果关系的定向图来合成场景，然后将结构化图转换为符号程序，配置和执行机器人仿真环境。

Result: 在自动驾驶和机器人操作任务中，ReGen生成的仿真环境比现有方法更复杂多样，成功率高，并能可控地生成边界情况。

Conclusion: 该方法增强了机器人策略的验证能力，支持数据和仿真增强，推进了可扩展的机器人学习，提高了泛化性和鲁棒性。

Abstract: Simulation plays a key role in scaling robot learning and validating
policies, but constructing simulations remains a labor-intensive process. This
paper introduces ReGen, a generative simulation framework that automates
simulation design via inverse design. Given a robot's behavior -- such as a
motion trajectory or an objective function -- and its textual description,
ReGen infers plausible scenarios and environments that could have caused the
behavior. ReGen leverages large language models to synthesize scenarios by
expanding a directed graph that encodes cause-and-effect relationships,
relevant entities, and their properties. This structured graph is then
translated into a symbolic program, which configures and executes a robot
simulation environment. Our framework supports (i) augmenting simulations based
on ego-agent behaviors, (ii) controllable, counterfactual scenario generation,
(iii) reasoning about agent cognition and mental states, and (iv) reasoning
with distinct sensing modalities, such as braking due to faulty GPS signals. We
demonstrate ReGen in autonomous driving and robot manipulation tasks,
generating more diverse, complex simulated environments compared to existing
simulations with high success rates, and enabling controllable generation for
corner cases. This approach enhances the validation of robot policies and
supports data or simulation augmentation, advancing scalable robot learning for
improved generalization and robustness. We provide code and example videos at:
https://regen-sim.github.io/

</details>


### [3] [Unified Multimodal Diffusion Forcing for Forceful Manipulation](https://arxiv.org/abs/2511.04812)
*Zixuan Huang,Huaidian Hou,Dmitry Berenson*

Main category: cs.RO

TL;DR: 本文提出了多模态扩散强制（MDF）框架，用于从多模态机器人轨迹中学习，超越单纯的动作生成，能够学习时间依赖性和跨模态关系。


<details>
  <summary>Details</summary>
Motivation: 标准模仿学习方法通常只学习从观察到动作的直接映射，忽略了不同模态（感官输入、动作、奖励）之间的丰富交互作用，这对于建模机器人行为和理解任务结果至关重要。

Method: MDF采用随机部分掩码并训练扩散模型来重建轨迹，而不是建模固定分布。这种训练目标鼓励模型学习时间依赖性和跨模态依赖关系。

Result: 在模拟和真实环境的接触丰富、强力操作任务中评估MDF，结果显示MDF不仅提供多功能性，还实现了强大的性能和噪声观察下的鲁棒性。

Conclusion: MDF框架能够有效学习多模态机器人轨迹中的复杂依赖关系，在强力操作任务中表现出色，具有多功能性和鲁棒性。

Abstract: Given a dataset of expert trajectories, standard imitation learning
approaches typically learn a direct mapping from observations (e.g., RGB
images) to actions. However, such methods often overlook the rich interplay
between different modalities, i.e., sensory inputs, actions, and rewards, which
is crucial for modeling robot behavior and understanding task outcomes. In this
work, we propose Multimodal Diffusion Forcing, a unified framework for learning
from multimodal robot trajectories that extends beyond action generation.
Rather than modeling a fixed distribution, MDF applies random partial masking
and trains a diffusion model to reconstruct the trajectory. This training
objective encourages the model to learn temporal and cross-modal dependencies,
such as predicting the effects of actions on force signals or inferring states
from partial observations. We evaluate MDF on contact-rich, forceful
manipulation tasks in simulated and real-world environments. Our results show
that MDF not only delivers versatile functionalities, but also achieves strong
performance, and robustness under noisy observations. More visualizations can
be found on our website https://unified-df.github.io

</details>


### [4] [Pixi: Unified Software Development and Distribution for Robotics and AI](https://arxiv.org/abs/2511.04827)
*Tobias Fischer,Wolf Vollprecht,Bas Zalmstra,Ruben Arts,Tim de Jager,Alejandro Fontan,Adam D Hines,Michael Milford,Silvio Traversaro,Daniel Claes,Scarlett Raine*

Main category: cs.RO

TL;DR: Pixi是一个统一的包管理框架，解决了机器人学研究中软件环境复现困难的问题，通过项目级锁定文件确保跨平台比特级复现性，将设置时间从数小时缩短到数分钟。


<details>
  <summary>Details</summary>
Motivation: 机器人学研究面临复现性危机，高达70%的算法无法被独立团队复现，多语言、硬件-软件工具链碎片化导致依赖地狱，阻碍了可部署软件的创建。

Method: 开发Pixi包管理框架，使用高性能SAT求解器实现快速依赖解析，集成conda-forge和PyPI生态系统，通过项目级锁定文件捕获精确依赖状态。

Result: Pixi的依赖解析速度比同类工具快10倍，自2023年以来已在5300多个项目中采用，显著降低了研究人员的技朮门槛。

Conclusion: Pixi通过提供可扩展、可复现的协作研究基础设施，加速了机器人和AI领域的进展。

Abstract: The reproducibility crisis in scientific computing constrains robotics
research. Existing studies reveal that up to 70% of robotics algorithms cannot
be reproduced by independent teams, while many others fail to reach deployment
because creating shareable software environments remains prohibitively complex.
These challenges stem from fragmented, multi-language, and hardware-software
toolchains that lead to dependency hell. We present Pixi, a unified
package-management framework that addresses these issues by capturing exact
dependency states in project-level lockfiles, ensuring bit-for-bit
reproducibility across platforms. Its high-performance SAT solver achieves up
to 10x faster dependency resolution than comparable tools, while integration of
the conda-forge and PyPI ecosystems removes the need for multiple managers.
Adopted in over 5,300 projects since 2023, Pixi reduces setup times from hours
to minutes and lowers technical barriers for researchers worldwide. By enabling
scalable, reproducible, collaborative research infrastructure, Pixi accelerates
progress in robotics and AI.

</details>


### [5] [Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning](https://arxiv.org/abs/2511.04831)
*NVIDIA,:,Mayank Mittal,Pascal Roth,James Tigue,Antoine Richard,Octi Zhang,Peter Du,Antonio Serrano-Muñoz,Xinjie Yao,René Zurbrügg,Nikita Rudin,Lukasz Wawrzyniak,Milad Rakhsha,Alain Denzler,Eric Heiden,Ales Borovicka,Ossama Ahmed,Iretiayo Akinola,Abrar Anwar,Mark T. Carlson,Ji Yuan Feng,Animesh Garg,Renato Gasoto,Lionel Gulich,Yijie Guo,M. Gussert,Alex Hansen,Mihir Kulkarni,Chenran Li,Wei Liu,Viktor Makoviychuk,Grzegorz Malczyk,Hammad Mazhar,Masoud Moghani,Adithyavairavan Murali,Michael Noseworthy,Alexander Poddubny,Nathan Ratliff,Welf Rehberg,Clemens Schwarke,Ritvik Singh,James Latham Smith,Bingjie Tang,Ruchik Thaker,Matthew Trepte,Karl Van Wyk,Fangzhou Yu,Alex Millane,Vikram Ramasamy,Remo Steiner,Sangeeta Subramanian,Clemens Volk,CY Chen,Neel Jawale,Ashwin Varghese Kuruttukulam,Michael A. Lin,Ajay Mandlekar,Karsten Patzwaldt,John Welsh,Huihua Zhao,Fatima Anes,Jean-Francois Lafleche,Nicolas Moënne-Loccoz,Soowan Park,Rob Stepinski,Dirk Van Gelder,Chris Amevor,Jan Carius,Jumyung Chang,Anka He Chen,Pablo de Heras Ciechomski,Gilles Daviet,Mohammad Mohajerani,Julia von Muralt,Viktor Reutskyy,Michael Sauter,Simon Schirm,Eric L. Shi,Pierre Terdiman,Kenny Vilella,Tobias Widmer,Gordon Yeoman,Tiffany Chen,Sergey Grizan,Cathy Li,Lotus Li,Connor Smith,Rafael Wiltz,Kostas Alexis,Yan Chang,David Chu,Linxi "Jim" Fan,Farbod Farshidian,Ankur Handa,Spencer Huang,Marco Hutter,Yashraj Narang,Soha Pouya,Shiwei Sheng,Yuke Zhu,Miles Macklin,Adam Moravanszky,Philipp Reist,Yunrong Guo,David Hoeller,Gavriel State*

Main category: cs.RO

TL;DR: Isaac Lab是Isaac Gym的继任者，扩展了GPU原生机器人模拟到大规模多模态学习时代，集成了高保真GPU并行物理、逼真渲染、模块化架构以及强化学习和模仿学习工具。


<details>
  <summary>Details</summary>
Motivation: 推动机器人模拟进入大规模多模态学习时代，为机器人学习提供统一、可扩展的平台，整合物理模拟、渲染、传感器模拟和数据收集等最佳实践。

Method: 采用GPU并行物理引擎、逼真渲染技术、模块化架构设计，集成执行器模型、多频传感器模拟、数据收集管道和领域随机化工具。

Result: 成功应用于全身控制、跨体现移动、接触丰富和灵巧操作等多样化挑战，并整合人类演示进行技能获取。

Conclusion: Isaac Lab结合先进模拟能力、丰富感知和数据中心级执行，有望推动机器人研究的下一代突破，即将集成可微分GPU加速牛顿物理引擎，为可扩展、数据高效和基于梯度的机器人学习方法提供新机会。

Abstract: We present Isaac Lab, the natural successor to Isaac Gym, which extends the
paradigm of GPU-native robotics simulation into the era of large-scale
multi-modal learning. Isaac Lab combines high-fidelity GPU parallel physics,
photorealistic rendering, and a modular, composable architecture for designing
environments and training robot policies. Beyond physics and rendering, the
framework integrates actuator models, multi-frequency sensor simulation, data
collection pipelines, and domain randomization tools, unifying best practices
for reinforcement and imitation learning at scale within a single extensible
platform. We highlight its application to a diverse set of challenges,
including whole-body control, cross-embodiment mobility, contact-rich and
dexterous manipulation, and the integration of human demonstrations for skill
acquisition. Finally, we discuss upcoming integration with the differentiable,
GPU-accelerated Newton physics engine, which promises new opportunities for
scalable, data-efficient, and gradient-based approaches to robot learning. We
believe Isaac Lab's combination of advanced simulation capabilities, rich
sensing, and data-center scale execution will help unlock the next generation
of breakthroughs in robotics research.

</details>


### [6] [Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning](https://arxiv.org/abs/2511.04835)
*Shubham Natraj,Bruno Sinopoli,Yiannis Kantaros*

Main category: cs.RO

TL;DR: 提出了一种用于基于采样的运动规划器的新型非均匀采样策略，通过偏向经过认证的区域来加速规划过程，并提供概率正确的保证。


<details>
  <summary>Details</summary>
Motivation: 基于采样的运动规划器依赖均匀采样，在复杂环境中效率低下且规划缓慢。需要一种能够提供概率保证的非均匀采样方法来提高效率。

Method: 集成到现有SBMP中的非均匀采样策略：(i)使用任何启发式路径预测器生成初始路径；(ii)应用共形预测量化预测器不确定性，生成包含最优解的概率保证预测集。

Result: 广泛评估表明，该方法比现有基线方法更快地找到可行路径，并且在未见环境中具有更好的泛化能力。

Conclusion: 这是第一个为SBMP提供概率正确保证的非均匀采样方法，显著提高了规划效率和在复杂环境中的性能。

Abstract: Sampling-based motion planners (SBMPs) are widely used to compute dynamically
feasible robot paths. However, their reliance on uniform sampling often leads
to poor efficiency and slow planning in complex environments. We introduce a
novel non-uniform sampling strategy that integrates into existing SBMPs by
biasing sampling toward `certified' regions. These regions are constructed by
(i) generating an initial, possibly infeasible, path using any heuristic path
predictor (e.g., A* or vision-language models) and (ii) applying conformal
prediction to quantify the predictor's uncertainty. This process yields
prediction sets around the initial-guess path that are guaranteed, with
user-specified probability, to contain the optimal solution. To our knowledge,
this is the first non-uniform sampling approach for SBMPs that provides such
probabilistically correct guarantees on the sampling regions. Extensive
evaluations demonstrate that our method consistently finds feasible paths
faster and generalizes better to unseen environments than existing baselines.

</details>


### [7] [Design Exploration for Protection and Cleaning of Solar Panels with Case Studies for Space Missions](https://arxiv.org/abs/2511.04837)
*Cameron Robinson,Ganghee Jang*

Main category: cs.RO

TL;DR: 本文研究了太阳能电池板清洁机制和防护材料，以应对灰尘和太空碎片的影响。清洁机制方面比较了刮水器系统和轨道系统，防护材料方面测试了聚碳酸酯和软硬材料分层结构。


<details>
  <summary>Details</summary>
Motivation: 太阳能用于关键任务应用，如太空探索和野火监测传感器系统。灰尘覆盖或太空碎片撞击会限制甚至终止其运行，需要解决这一问题。

Method: 设计了两种清洁机制（刮水器系统和轨道系统）并进行比较，同时测试了防护材料（聚碳酸酯）并通过碰撞测试评估软硬材料分层结构的效果。

Result: 在防护材料方面，聚碳酸酯表现良好，但最重要的因素是软材料与硬材料的分层结构。在清洁系统比较中，刮水器系统在成本、清洁速度和总功耗方面比轨道系统更高效。

Conclusion: 刮水器清洁系统比轨道系统更高效，防护材料中软硬材料分层结构是关键因素，聚碳酸酯是很有前景的防护材料。

Abstract: Solar energy is used for many mission-critical applications including space
exploration, sensor systems to monitor wildfires, etc. Their operation can be
limited or even terminated if solar panels are covered with dust or hit by
space debris. To address this issue, we designed panel cleaning mechanisms and
tested protective materials. For cleaning mechanisms, we designed and compared
a wiper system and a rail system. For protective materials, we found through
collision tests that polycarbonate was very promising, though the most
important factor was layering a soft material between the panel's surface and a
hard material. In the cleaning system comparisons, the wiper-based system was
more efficient than the rail-based system in terms of cost, cleaning speed, and
total power consumption.

</details>


### [8] [iFlyBot-VLM Technical Report](https://arxiv.org/abs/2511.04976)
*Xin Nie,Zhiyuan Cheng,Yuan Zhang,Chao Ji,Jiajia Wu,Yuhan Zhang,Jia Pan*

Main category: cs.RO

TL;DR: iFlyBot-VLM是一个通用视觉语言模型，旨在通过将复杂视觉空间信息抽象为与机器人本体无关的可转移操作语言，来弥合环境感知与机器人运动控制之间的语义鸿沟。


<details>
  <summary>Details</summary>
Motivation: 解决高维环境感知与低层机器人运动控制之间的跨模态语义鸿沟问题，推动具身智能从专用任务系统向通用认知智能体发展。

Method: 系统设计架构实现四个关键功能：空间理解与度量推理、交互式目标定位、动作抽象与控制参数生成、任务规划与技能序列化。

Result: 在10个主流具身智能VLM基准数据集上取得最优性能，同时保持模型的通用能力。

Conclusion: iFlyBot-VLM作为可扩展的具身AI基础模型，将促进从专用任务系统向通用认知智能体的演进。

Abstract: We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used
to improve the domain of Embodied Intelligence. The central objective of
iFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional
environmental perception and low-level robotic motion control. To this end, the
model abstracts complex visual and spatial information into a body-agnostic and
transferable Operational Language, thereby enabling seamless perception-action
closed-loop coordination across diverse robotic platforms. The architecture of
iFlyBot-VLM is systematically designed to realize four key functional
capabilities essential for embodied intelligence: 1) Spatial Understanding and
Metric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and
Control Parameter Generation; 4) Task Planning and Skill Sequencing. We
envision iFlyBot-VLM as a scalable and generalizable foundation model for
embodied AI, facilitating the progression from specialized task-oriented
systems toward generalist, cognitively capable agents. We conducted evaluations
on 10 current mainstream embodied intelligence-related VLM benchmark datasets,
such as Blink and Where2Place, and achieved optimal performance while
preserving the model's general capabilities. We will publicly release both the
training data and model weights to foster further research and development in
the field of Embodied Intelligence.

</details>


### [9] [A semi-analytical approach for computing the largest singularity-free spheres of a class of 6-6 Stewart-Gough platforms for specified orientation workspaces](https://arxiv.org/abs/2511.04992)
*Bibekananda Patra,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出了一种计算6-6 Stewart-Gough平台机械臂在特定方向工作空间内最大无奇点球体(SFS)的方法，通过分析计算固定方向下的SFS，并在方向工作空间内采样找到最小SFS。


<details>
  <summary>Details</summary>
Motivation: 研究Stewart-Gough平台机械臂在方向工作空间内的无奇点性能，为机械臂的分析和设计提供实用工具。

Method: 对于固定方向的移动平台，分析计算SFS；在方向工作空间内生成样本集，重复此过程，选择其中最小的SFS作为给定方向工作空间的期望SFS。

Result: 在四种不同的SGPM架构上进行了数值实验，比较了它们在相同方向工作空间内SFS体积的相对性能。

Conclusion: 该计算方法在SGPM的分析和设计中具有潜在实用价值。

Abstract: This article presents a method for computing the largest singularity-free
sphere (SFS) of a 6-6 Stewart-Gough platform manipulator (SGPM) over a
specified orientation workspace. For a fixed orientation of the moving
platform, the SFS is computed analytically. This process is repeated over a set
of samples generated within the orientation workspace, and the smallest among
them is designated as the desired SFS for the given orientation workspace.
Numerical experiments are performed on four distinct architectures of the SGPM
to understand their relative performances w.r.t. SFS volumes over the same
orientation workspace. This study demonstrates the potential utility of the
proposed computational method both in analysis and design of SGPMs.

</details>


### [10] [MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery](https://arxiv.org/abs/2511.05007)
*Baiye Cheng,Tianhai Liang,Suning Huang,Maanping Shao,Feihong Zhang,Botian Xu,Zhengrong Xue,Huazhe Xu*

Main category: cs.RO

TL;DR: 提出了MoE-DP方法，通过在视觉编码器和扩散模型之间插入混合专家层，将策略知识分解为专门处理不同任务阶段的专家，显著提升了机器人视觉运动控制的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人视觉运动控制中缺乏从子任务失败中恢复的鲁棒性，且学习到的观察表示难以解释。

Method: 在视觉编码器和扩散模型之间插入混合专家层，将策略知识分解为专门处理不同任务阶段的专家，动态激活处理不同任务阶段。

Result: 在6个长时程仿真任务中，受干扰条件下的成功率平均相对提升36%，在真实世界中也表现出显著性能提升，并学习到可解释的技能分解。

Conclusion: MoE-DP通过混合专家层增强了扩散策略的鲁棒性和可解释性，能够从干扰中恢复并学习语义任务原语，支持无需重新训练的推理时控制。

Abstract: Diffusion policies have emerged as a powerful framework for robotic
visuomotor control, yet they often lack the robustness to recover from subtask
failures in long-horizon, multi-stage tasks and their learned representations
of observations are often difficult to interpret. In this work, we propose the
Mixture of Experts-Enhanced Diffusion Policy (MoE-DP), where the core idea is
to insert a Mixture of Experts (MoE) layer between the visual encoder and the
diffusion model. This layer decomposes the policy's knowledge into a set of
specialized experts, which are dynamically activated to handle different phases
of a task. We demonstrate through extensive experiments that MoE-DP exhibits a
strong capability to recover from disturbances, significantly outperforming
standard baselines in robustness. On a suite of 6 long-horizon simulation
tasks, this leads to a 36% average relative improvement in success rate under
disturbed conditions. This enhanced robustness is further validated in the real
world, where MoE-DP also shows significant performance gains. We further show
that MoE-DP learns an interpretable skill decomposition, where distinct experts
correspond to semantic task primitives (e.g., approaching, grasping). This
learned structure can be leveraged for inference-time control, allowing for the
rearrangement of subtasks without any re-training.Our video and code are
available at the https://moe-dp-website.github.io/MoE-DP-Website/.

</details>


### [11] [Epically Powerful: An open-source software and mechatronics infrastructure for wearable robotic systems](https://arxiv.org/abs/2511.05033)
*Jennifer K. Leestma,Siddharth R. Nathella,Christoph P. O. Nuesslein,Snehil Mathur,Gregory S. Sawicki,Aaron J. Young*

Main category: cs.RO

TL;DR: Epically Powerful是一个开源机器人基础设施，简化可穿戴机器人系统的底层框架，提供通信协议、时钟管理、执行器控制、可视化、传感器数据采集等功能，并包含硬件选型、系统组装和控制器实现的完整指南。


<details>
  <summary>Details</summary>
Motivation: 降低开发和部署定制化可穿戴机器人系统的门槛，使研究人员能够快速有效地从原始硬件构建模块化、鲁棒的设备。

Method: 提供基于Python的代码库，与各种商用准直接驱动执行器、单板计算机和常见传感器无缝接口，包含示例控制器和实时可视化功能。

Result: 开发了一个完整的开源基础设施，包含推荐零件清单、兼容性指南以及详细的软硬件实现文档。

Conclusion: Epically Powerful不仅适用于可穿戴机器人，还可广泛应用于其他使用准直接驱动执行器、单板计算机和传感器进行闭环控制的机器人领域。

Abstract: Epically Powerful is an open-source robotics infrastructure that streamlines
the underlying framework of wearable robotic systems - managing communication
protocols, clocking, actuator commands, visualization, sensor data acquisition,
data logging, and more - while also providing comprehensive guides for hardware
selection, system assembly, and controller implementation. Epically Powerful
contains a code base enabling simplified user implementation via Python that
seamlessly interfaces with various commercial state-of-the-art quasi-direct
drive (QDD) actuators, single-board computers, and common sensors, provides
example controllers, and enables real-time visualization. To further support
device development, the package also includes a recommended parts list and
compatibility guide and detailed documentation on hardware and software
implementation. The goal of Epically Powerful is to lower the barrier to
developing and deploying custom wearable robotic systems without a
pre-specified form factor, enabling researchers to go from raw hardware to
modular, robust devices quickly and effectively. Though originally designed
with wearable robotics in mind, Epically Powerful is broadly applicable to
other robotic domains that utilize QDD actuators, single-board computers, and
sensors for closed-loop control.

</details>


### [12] [TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments](https://arxiv.org/abs/2511.05052)
*Zihao Li,Yiming Zhu,Zhe Zhong,Qinyuan Ren,Yijiang Huang*

Main category: cs.RO

TL;DR: TAPOM是一种拓扑感知的机器人操作规划方法，通过在任务空间进行拓扑分析来识别关键路径并生成引导关键帧，有效解决狭窄通道中细长物体操作的规划问题。


<details>
  <summary>Details</summary>
Motivation: 在复杂约束空间中，特别是狭窄通道中操作细长物体时，现有规划方法由于采样困难或局部极小值问题经常失败，需要更有效的规划策略。

Method: TAPOM采用分层规划方法：高层进行任务空间拓扑分析识别关键路径并生成引导关键帧，底层规划器利用这些关键帧寻找可行的配置空间轨迹。

Result: 实验验证表明，在低间隙操作任务上，TAPOM相比最先进方法具有显著更高的成功率和改进的效率。

Conclusion: 该方法为增强机器人在复杂现实环境中的操作能力提供了广泛的应用前景。

Abstract: Robotic manipulation in complex, constrained spaces is vital for widespread
applications but challenging, particularly when navigating narrow passages with
elongated objects. Existing planning methods often fail in these low-clearance
scenarios due to the sampling difficulties or the local minima. This work
proposes Topology-Aware Planning for Object Manipulation (TAPOM), which
explicitly incorporates task-space topological analysis to enable efficient
planning. TAPOM uses a high-level analysis to identify critical pathways and
generate guiding keyframes, which are utilized in a low-level planner to find
feasible configuration space trajectories. Experimental validation demonstrates
significantly high success rates and improved efficiency over state-of-the-art
methods on low-clearance manipulation tasks. This approach offers broad
implications for enhancing manipulation capabilities of robots in complex
real-world environments.

</details>


### [13] [Decomposed Object Manipulation via Dual-Actor Policy](https://arxiv.org/abs/2511.05129)
*Bin Fan,Jianjian Jiang,Zhuohao Li,Yixiang He,Xiaoming Wu,Yihan Yang,Shengbang Liu,Weishi Zheng*

Main category: cs.RO

TL;DR: 提出DAP双行动者策略，将物体操作分为接近和操作两个阶段，分别使用功能性和运动流视觉先验来增强各阶段性能，并在新构建的数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有物体操作方法通常使用单一策略学习整个过程，忽略了物体操作任务天然分为接近阶段和操作阶段的特性，且缺乏足够的视觉先验支持训练。

Method: 提出DAP双行动者策略：1）基于功能性的行动者定位功能部件改善接近过程；2）基于运动流的行动者捕捉部件运动促进操作过程；3）决策器确定当前阶段并选择对应行动者。同时构建了包含两种视觉先验的双先验物体操作数据集。

Result: 在自制数据集、RoboTwin基准和真实场景中，DAP方法分别比SOTA方法平均提升5.55%、14.7%和10.4%。

Conclusion: DAP通过显式考虑物体操作的不同阶段并利用异质视觉先验，有效提升了物体操作的性能，特别是在长期多阶段任务中表现优异。

Abstract: Object manipulation, which focuses on learning to perform tasks on similar
parts across different types of objects, can be divided into an approaching
stage and a manipulation stage. However, previous works often ignore this
characteristic of the task and rely on a single policy to directly learn the
whole process of object manipulation. To address this problem, we propose a
novel Dual-Actor Policy, termed DAP, which explicitly considers different
stages and leverages heterogeneous visual priors to enhance each stage.
Specifically, we introduce an affordance-based actor to locate the functional
part in the manipulation task, thereby improving the approaching process.
Following this, we propose a motion flow-based actor to capture the movement of
the component, facilitating the manipulation process. Finally, we introduce a
decision maker to determine the current stage of DAP and select the
corresponding actor. Moreover, existing object manipulation datasets contain
few objects and lack the visual priors needed to support training. To address
this, we construct a simulated dataset, the Dual-Prior Object Manipulation
Dataset, which combines the two visual priors and includes seven tasks,
including two challenging long-term, multi-stage tasks. Experimental results on
our dataset, the RoboTwin benchmark and real-world scenarios illustrate that
our method consistently outperforms the SOTA method by 5.55%, 14.7% and 10.4%
on average respectively.

</details>


### [14] [Procedimiento de auditoría de ciberseguridad para sistemas autónomos: metodología, amenazas y mitigaciones](https://arxiv.org/abs/2511.05185)
*Adrián Campazas-Vega,Claudia Álvarez-Aparicio,David Sobrín-Hidalgo,Laura Inyesto-Alonso,Francisco Javier Rodríguez-Lera,Vicente Matellán-Olivera,Ángel Manuel Guerrero-Higueras*

Main category: cs.RO

TL;DR: 本文提出了一种针对自主系统的特定安全审计程序，基于分层结构方法、适应机器人环境的威胁分类法以及具体缓解措施，并通过四个实际案例研究验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着自主系统在工业、医疗、物流和家庭环境等领域的广泛应用，其安全风险日益突出，特别是在人机交互环境中。技术发展和系统复杂性增加导致攻击面扩大，需要专门的安全审计方法。

Method: 采用分层结构方法，结合适应机器人环境的威胁分类法，并制定具体缓解措施。通过四个代表性机器人平台的实际案例研究进行验证。

Result: 提出的安全审计程序在四个实际案例中得到了验证：Ghost Robotics的Vision 60军用四足机器人、Unitree Robotics的A1机器人、Universal Robots的UR3协作臂和Aldebaran Robotics的Pepper社交机器人。

Conclusion: 该方法为自主系统提供了一种有效的安全审计框架，能够识别和缓解安全威胁，特别是在人机交互环境中具有重要意义。

Abstract: The deployment of autonomous systems has experienced remarkable growth in
recent years, driven by their integration into sectors such as industry,
medicine, logistics, and domestic environments. This expansion is accompanied
by a series of security issues that entail significant risks due to the
critical nature of autonomous systems, especially those operating in
human-interaction environments. Furthermore, technological advancement and the
high operational and architectural complexity of autonomous systems have
resulted in an increased attack surface. This article presents a specific
security auditing procedure for autonomous systems, based on a layer-structured
methodology, a threat taxonomy adapted to the robotic context, and a set of
concrete mitigation measures. The validity of the proposed approach is
demonstrated through four practical case studies applied to representative
robotic platforms: the Vision 60 military quadruped from Ghost Robotics, the A1
robot from Unitree Robotics, the UR3 collaborative arm from Universal Robots,
and the Pepper social robot from Aldebaran Robotics.

</details>


### [15] [Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation](https://arxiv.org/abs/2511.05199)
*Yichen Zhu,Feifei Feng*

Main category: cs.RO

TL;DR: 本文提出了一种通过从视频中检索（RfV）来学习机器人策略的新方法，利用人类演示的类比来解决操作任务。系统构建包含人类执行各种日常任务的视频库，提取物体功能掩码和手部运动轨迹等中级信息，通过视频检索器和策略生成器的双组件系统实现任务适应和泛化。


<details>
  <summary>Details</summary>
Motivation: 机器人在复杂不确定环境中面临挑战，传统方法依赖大量数据集学习操作任务。而人类面对陌生任务时通常通过观看视频演示来学习，因此希望开发一种类似人类学习方式的机器人学习方法。

Method: 构建人类日常任务视频库，提取物体功能掩码和手部运动轨迹等中级信息作为额外输入。采用双组件系统：视频检索器从外部视频库获取任务相关视频，策略生成器将检索到的知识整合到学习循环中。

Result: 在多个模拟和真实环境中的严格测试表明，该系统相比传统机器人系统在性能上有显著提升，能够适应各种场景并泛化到训练数据之外的任务。

Conclusion: 该方法在机器人领域取得了重大突破，通过视频检索和类比学习使机器人能够更好地适应复杂环境并实现任务泛化。

Abstract: Robots operating in complex and uncertain environments face considerable
challenges. Advanced robotic systems often rely on extensive datasets to learn
manipulation tasks. In contrast, when humans are faced with unfamiliar tasks,
such as assembling a chair, a common approach is to learn by watching video
demonstrations. In this paper, we propose a novel method for learning robot
policies by Retrieving-from-Video (RfV), using analogies from human
demonstrations to address manipulation tasks. Our system constructs a video
bank comprising recordings of humans performing diverse daily tasks. To enrich
the knowledge from these videos, we extract mid-level information, such as
object affordance masks and hand motion trajectories, which serve as additional
inputs to enhance the robot model's learning and generalization capabilities.
We further feature a dual-component system: a video retriever that taps into an
external video bank to fetch task-relevant video based on task specification,
and a policy generator that integrates this retrieved knowledge into the
learning cycle. This approach enables robots to craft adaptive responses to
various scenarios and generalize to tasks beyond those in the training data.
Through rigorous testing in multiple simulated and real-world settings, our
system demonstrates a marked improvement in performance over conventional
robotic systems, showcasing a significant breakthrough in the field of
robotics.

</details>


### [16] [TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models](https://arxiv.org/abs/2511.05275)
*Hokyun Im,Euijin Jeong,Jianlong Fu,Andrey Kolobov,Youngwoon Lee*

Main category: cs.RO

TL;DR: TwinVLA是一个模块化框架，通过组合两个预训练的单臂视觉语言动作模型来构建协调的双臂VLA，无需额外的双臂数据预训练即可在双臂任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大多数公开数据集专注于单臂演示，将VLA模型适应双臂任务通常需要大量额外的双臂数据和微调，这限制了数据效率和性能。

Method: 提出TwinVLA框架，将两个预训练的单臂VLA模型组合成一个协调的双臂VLA，采用模块化组合方法而非训练单一跨体现模型。

Result: 在真实世界和仿真环境中的多样化双臂任务中，TwinVLA优于同等规模的单体RDT-1B模型，且无需任何双臂预训练，缩小了与依赖大量专有双臂数据和计算成本的最先进模型π₀的差距。

Conclusion: 模块化组合方法为高性能双臂操作提供了一条数据高效且可扩展的路径，能够充分利用公开的单臂数据。

Abstract: Vision-language-action models (VLAs) trained on large-scale robotic datasets
have demonstrated strong performance on manipulation tasks, including bimanual
tasks. However, because most public datasets focus on single-arm
demonstrations, adapting VLAs for bimanual tasks typically requires substantial
additional bimanual data and fine-tuning. To address this challenge, we
introduce TwinVLA, a modular framework that composes two copies of a pretrained
single-arm VLA into a coordinated bimanual VLA. Unlike monolithic
cross-embodiment models trained on mixtures of single-arm and bimanual data,
TwinVLA improves both data efficiency and performance by composing pretrained
single-arm policies. Across diverse bimanual tasks in real-world and simulation
settings, TwinVLA outperforms a comparably-sized monolithic RDT-1B model
without requiring any bimanual pretraining. Furthermore, it narrows the gap to
state-of-the-art model, $\pi_0$ which rely on extensive proprietary bimanual
data and compute cost. These results establish our modular composition approach
as a data-efficient and scalable path toward high-performance bimanual
manipulation, leveraging public single-arm data.

</details>


### [17] [Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators](https://arxiv.org/abs/2511.05307)
*Akua K. Dickson,Juan C. Pacheco Garcia,Andrew P. Sabelhaus*

Main category: cs.RO

TL;DR: 本文提出了一种将软体机器人操作器的力安全标准从任务空间映射到配置空间的框架，实现了实时力安全检测，确保在接触易损障碍物时不超过最大力阈值。


<details>
  <summary>Details</summary>
Motivation: 现有的障碍物检测和避让方法未考虑软体机器人操作器与易损障碍物接触时的力限制，这限制了在精密环境中的安全部署。

Method: 通过操作器的正向运动学将任务空间中的允许接触力限制映射到配置空间，建立力安全配置的实时检测框架。

Result: 在双段气动软体机器人操作器上的仿真和硬件实验验证了该方法能准确检测与可变形障碍物交互时的力安全性。

Conclusion: 该方法为软体操作器在精密、杂乱环境中进行实时安全规划奠定了基础。

Abstract: Soft robot manipulators have the potential for deployment in delicate
environments to perform complex manipulation tasks. However, existing obstacle
detection and avoidance methods do not consider limits on the forces that
manipulators may exert upon contact with delicate obstacles. This work
introduces a framework that maps force safety criteria from task space (i.e.
positions along the robot's body) to configuration space (i.e. the robot's
joint angles) and enables real-time force safety detection. We incorporate
limits on allowable environmental contact forces for given task-space
obstacles, and map them into configuration space (C-space) through the
manipulator's forward kinematics. This formulation ensures that configurations
classified as safe are provably below the maximum force thresholds, thereby
allowing us to determine force-safe configurations of the soft robot
manipulator in real-time. We validate our approach in simulation and hardware
experiments on a two-segment pneumatic soft robot manipulator. Results
demonstrate that the proposed method accurately detects force safety during
interactions with deformable obstacles, thereby laying the foundation for
real-time safe planning of soft manipulators in delicate, cluttered
environments.

</details>


### [18] [ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality](https://arxiv.org/abs/2511.05379)
*Eric Godden,Jacquie Groenewegen,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: ETHOS是一个动态遭遇式触觉显示系统，通过集成扭矩控制机器人、可互换被动道具和安全监控，在VR中实现自然物理接触，支持握手、击掌等社交互动。


<details>
  <summary>Details</summary>
Motivation: 在虚拟现实中重现有意义的社交触觉互动，解决传统VR社交中缺乏物理接触的问题。

Method: 使用扭矩控制机器人操纵器配合可互换被动道具，结合基于标记的物理-虚拟配准和安全监控，提供静态和动态两种控制模式。

Result: 静态配准精度5.09±0.94mm，用户交互平均接触延迟28.53±31.21ms，证明在VR中重现社交触觉的可行性。

Conclusion: ETHOS通过整合关键安全和控制机制，为虚拟环境中的高保真动态人际互动建立了实用基础。

Abstract: We present ETHOS (Encountered-Type Haptics for On-demand Social Interaction),
a dynamic encountered-type haptic display (ETHD) that enables natural physical
contact in virtual reality (VR) during social interactions such as handovers,
fist bumps, and high-fives. The system integrates a torque-controlled robotic
manipulator with interchangeable passive props (silicone hand replicas and a
baton), marker-based physical-virtual registration via a ChArUco board, and a
safety monitor that gates motion based on the user's head and hand pose. We
introduce two control strategies: (i) a static mode that presents a stationary
prop aligned with its virtual counterpart, consistent with prior ETHD
baselines, and (ii) a dynamic mode that continuously updates prop position by
exponentially blending an initial mid-point trajectory with real-time hand
tracking, generating a unique contact point for each interaction. Bench tests
show static colocation accuracy of 5.09 +/- 0.94 mm, while user interactions
achieved temporal alignment with an average contact latency of 28.53 +/- 31.21
ms across all interaction and control conditions. These results demonstrate the
feasibility of recreating socially meaningful haptics in VR. By incorporating
essential safety and control mechanisms, ETHOS establishes a practical
foundation for high-fidelity, dynamic interpersonal interactions in virtual
environments.

</details>


### [19] [EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation](https://arxiv.org/abs/2511.05397)
*Samarth Chopra,Alex McMoil,Ben Carnovale,Evan Sokolson,Rajkumar Kubendran,Samuel Dickerson*

Main category: cs.RO

TL;DR: EverydayVLA是一个低成本（300美元以下）的6自由度机械臂，结合视觉-语言-动作模型，通过自适应时域集成实现安全可靠操作，在真实世界测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型依赖昂贵硬件，在复杂场景中表现不佳，需要开发低成本且可靠的机器人系统。

Method: 使用单一统一模型联合输出离散和连续动作，采用自适应时域集成监控运动不确定性并触发实时重新规划。

Result: 在LIBERO基准测试中达到最先进成功率，真实世界测试中比现有方法在分布内和分布外分别提升49%和34.9%。

Conclusion: 通过结合先进VLA模型与低成本硬件，EverydayVLA为家庭和实验室提供了经济可行的机器人基础模型解决方案。

Abstract: While Vision-Language-Action (VLA) models map visual inputs and language
instructions directly to robot actions, they often rely on costly hardware and
struggle in novel or cluttered scenes. We introduce EverydayVLA, a 6-DOF
manipulator that can be assembled for under $300, capable of modest payloads
and workspace. A single unified model jointly outputs discrete and continuous
actions, and our adaptive-horizon ensemble monitors motion uncertainty to
trigger on-the-fly re-planning for safe, reliable operation. On LIBERO,
EverydayVLA matches state-of-the-art success rates, and in real-world tests it
outperforms prior methods by 49% in-distribution and 34.9% out-of-distribution.
By combining a state-of-the-art VLA with cost-effective hardware, EverydayVLA
democratizes access to a robotic foundation model and paves the way for
economical use in homes and research labs alike. Experiment videos and details:
https://everydayvla.github.io/

</details>


### [20] [Stable and Robust SLIP Model Control via Energy Conservation-Based Feedback Cancellation for Quadrupedal Applications](https://arxiv.org/abs/2511.05402)
*Muhammad Saud Ul Hassan,Derek Vasquez,Hamza Asif,Christian Hubicki*

Main category: cs.RO

TL;DR: 提出基于能量守恒的四足机器人动态运动控制架构，使用弹簧倒立摆模型，通过跟踪稳定抛物线样条实现稳定弹跳步态，在传感器测量误差达10%时仍保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发稳定的四足机器人动态运动控制方法，模拟生物四足动物的弹跳步态特征，提高机器人在复杂环境中的运动能力。

Method: 将机器人建模为弹簧倒立摆模型，在飞行阶段控制腿方向，在支撑阶段控制腿长度，基于能量守恒原理计算稳定抛物线样条进行跟踪。

Result: 在Ghost Robotics Minitaur机器人仿真中成功生成稳定弹跳步态，控制器在传感器测量误差达10%时仍能保持稳定运动。

Conclusion: 基于能量守恒的SLIP模型控制架构能够有效实现四足机器人的稳定动态运动，具有较强的鲁棒性。

Abstract: In this paper, we present an energy-conservation based control architecture
for stable dynamic motion in quadruped robots. We model the robot as a
Spring-loaded Inverted Pendulum (SLIP), a model well-suited to represent the
bouncing motion characteristic of running gaits observed in various biological
quadrupeds and bio-inspired robotic systems. The model permits leg-orientation
control during flight and leg-length control during stance, a design choice
inspired by natural quadruped behaviors and prevalent in robotic quadruped
systems. Our control algorithm uses the reduced-order SLIP dynamics of the
quadruped to track a stable parabolic spline during stance, which is calculated
using the principle of energy conservation. Through simulations based on the
design specifications of an actual quadruped robot, Ghost Robotics Minitaur, we
demonstrate that our control algorithm generates stable bouncing gaits.
Additionally, we illustrate the robustness of our controller by showcasing its
ability to maintain stable bouncing even when faced with up to a 10% error in
sensor measurements.

</details>


### [21] [Bioinspired Soft Quadrotors Jointly Unlock Agility, Squeezability, and Collision Resilience](https://arxiv.org/abs/2511.05426)
*Luca Girardi,Gabriel Maquignaz,Stefano Mintchev*

Main category: cs.RO

TL;DR: FlexiQuad是一种软框架四旋翼设计，通过模仿生物飞行动物的各向异性刚度和分布式质量-能量结构，解决了传统刚性四旋翼在碰撞韧性和可压缩性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼的刚性框架限制了在复杂环境中的飞行能力，特别是碰撞韧性和通过狭窄通道的能力。受生物飞行动物软翅膀的启发，希望开发一种既能保持敏捷飞行又能提高环境适应性的四旋翼设计。

Method: 采用软框架设计方法，使四旋翼具有各向异性刚度和分布式质量-能量结构，实现了比传统四旋翼高出三个数量级的柔顺性。

Result: 405克的FlexiQuad原型机能够实现峰值速度超过80 km/h的杂技机动，线性加速度超过3g，角加速度超过300 rad/s²。同时具有4倍更高的碰撞韧性，能承受5 m/s的正面冲击而无损坏，并能通过仅为其标称宽度70%的缝隙。

Conclusion: FlexiQuad在0.006至0.77 N/mm的最佳结构柔软度范围内，能够同时实现敏捷性、可压缩性和碰撞韧性，扩展了悬停无人机在复杂环境中的能力，在不影响飞行性能的情况下实现稳健的物理交互。

Abstract: Natural flyers use soft wings to seamlessly enable a wide range of flight
behaviours, including agile manoeuvres, squeezing through narrow passageways,
and withstanding collisions. In contrast, conventional quadrotor designs rely
on rigid frames that support agile flight but inherently limit collision
resilience and squeezability, thereby constraining flight capabilities in
cluttered environments. Inspired by the anisotropic stiffness and distributed
mass-energy structures observed in biological organisms, we introduce
FlexiQuad, a soft-frame quadrotor design approach that limits this trade-off.
We demonstrate a 405-gram FlexiQuad prototype, three orders of magnitude more
compliant than conventional quadrotors, yet capable of acrobatic manoeuvres
with peak speeds above 80 km/h and linear and angular accelerations exceeding 3
g and 300 rad/s$^2$, respectively. Analysis demonstrates it can replicate
accelerations of rigid counterparts up to a thrust-to-weight ratio of 8.
Simultaneously, FlexiQuad exhibits fourfold higher collision resilience,
surviving frontal impacts at 5 m/s without damage and reducing destabilising
forces in glancing collisions by a factor of 39. Its frame can fully compress,
enabling flight through gaps as narrow as 70% of its nominal width. Our
analysis identifies an optimal structural softness range, from 0.006 to 0.77
N/mm, comparable to that of natural flyers' wings, whereby agility,
squeezability, and collision resilience are jointly achieved for FlexiQuad
models from 20 to 3000 grams. FlexiQuad expands hovering drone capabilities in
complex environments, enabling robust physical interactions without
compromising flight performance.

</details>
