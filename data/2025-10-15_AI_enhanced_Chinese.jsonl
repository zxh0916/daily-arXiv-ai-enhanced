{"id": "2510.12090", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2510.12090", "abs": "https://arxiv.org/abs/2510.12090", "authors": ["Hakan Ceylan", "Edoardo Sinibaldi", "Sanjay Misra", "Pankaj J. Pasricha", "Dietmar W. Hutmacher"], "title": "Translating Milli/Microrobots with A Value-Centered Readiness Framework", "comment": null, "summary": "Untethered mobile milli/microrobots hold transformative potential for\ninterventional medicine by enabling more precise and entirely non-invasive\ndiagnosis and therapy. Realizing this promise requires bridging the gap between\ngroundbreaking laboratory demonstrations and successful clinical integration.\nDespite remarkable technical progress over the past two decades, most\nmillirobots and microrobots remain confined to laboratory proof-of-concept\ndemonstrations, with limited real-world feasibility. In this Review, we\nidentify key factors that slow translation from bench to bedside, focusing on\nthe disconnect between technical innovation and real-world application. We\nargue that the long-term impact and sustainability of the field depend on\naligning development with unmet medical needs, ensuring applied feasibility,\nand integrating seamlessly into existing clinical workflows, which are\nessential pillars for delivering meaningful patient outcomes. To support this\nshift, we introduce a strategic milli/microrobot Technology Readiness Level\nframework (mTRL), which maps system development from initial conceptualization\nto clinical adoption through clearly defined milestones and their associated\nstepwise activities. The mTRL model provides a structured gauge of\ntechnological maturity, a common language for cross-disciplinary collaboration\nand actionable guidance to accelerate translational development toward new,\nsafer and more efficient interventions.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u65e0\u7f06\u79fb\u52a8\u6beb\u7c73/\u5fae\u7c73\u673a\u5668\u4eba\u5728\u4ecb\u5165\u533b\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u4ece\u5b9e\u9a8c\u5ba4\u6f14\u793a\u5230\u4e34\u5e8a\u6574\u5408\u7684\u8f6c\u5316\u969c\u788d\uff0c\u5e76\u63d0\u51fa\u4e86\u6280\u672f\u51c6\u5907\u5ea6\u6846\u67b6\u6765\u52a0\u901f\u8f6c\u5316\u3002", "motivation": "\u5c3d\u7ba1\u6beb\u7c73/\u5fae\u7c73\u673a\u5668\u4eba\u5728\u8fc7\u53bb\u4e8c\u5341\u5e74\u53d6\u5f97\u4e86\u663e\u8457\u6280\u672f\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u6570\u4ecd\u5c40\u9650\u4e8e\u5b9e\u9a8c\u5ba4\u6982\u5ff5\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u53ef\u884c\u6027\u3002\u9700\u8981\u5f25\u5408\u6280\u672f\u521b\u65b0\u4e0e\u73b0\u5b9e\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5f15\u5165\u6218\u7565\u6027\u6beb\u7c73/\u5fae\u7c73\u673a\u5668\u4eba\u6280\u672f\u51c6\u5907\u5ea6\u6846\u67b6\uff08mTRL\uff09\uff0c\u901a\u8fc7\u660e\u786e\u5b9a\u4e49\u7684\u91cc\u7a0b\u7891\u548c\u9010\u6b65\u6d3b\u52a8\uff0c\u5c06\u7cfb\u7edf\u5f00\u53d1\u4ece\u521d\u59cb\u6982\u5ff5\u5316\u6620\u5c04\u5230\u4e34\u5e8a\u91c7\u7528\u3002", "result": "mTRL\u6a21\u578b\u63d0\u4f9b\u4e86\u6280\u672f\u6210\u719f\u5ea6\u7684\u7ed3\u6784\u5316\u8861\u91cf\u6807\u51c6\u3001\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u5171\u540c\u8bed\u8a00\uff0c\u4ee5\u53ca\u52a0\u901f\u8f6c\u5316\u53d1\u5c55\u7684\u53ef\u64cd\u4f5c\u6307\u5bfc\u3002", "conclusion": "\u8be5\u9886\u57df\u7684\u957f\u671f\u5f71\u54cd\u548c\u53ef\u6301\u7eed\u6027\u53d6\u51b3\u4e8e\u5f00\u53d1\u4e0e\u672a\u6ee1\u8db3\u533b\u7597\u9700\u6c42\u7684\u5951\u5408\u3001\u786e\u4fdd\u5e94\u7528\u53ef\u884c\u6027\uff0c\u4ee5\u53ca\u4e0e\u73b0\u6709\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u7684\u65e0\u7f1d\u6574\u5408\uff0c\u8fd9\u4e9b\u662f\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u60a3\u8005\u7ed3\u679c\u7684\u57fa\u672c\u652f\u67f1\u3002"}}
{"id": "2510.12101", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12101", "abs": "https://arxiv.org/abs/2510.12101", "authors": ["Pengyu Yin", "Shenghai Yuan", "Haozhi Cao", "Xingyu Ji", "Ruofei Bai", "Siyu Chen", "Lihua Xie"], "title": "Gaussian Semantic Field for One-shot LiDAR Global Localization", "comment": null, "summary": "We present a one-shot LiDAR global localization algorithm featuring semantic\ndisambiguation ability based on a lightweight tri-layered scene graph. While\nlandmark semantic registration-based methods have shown promising performance\nimprovements in global localization compared with geometric-only methods,\nlandmarks can be repetitive and misleading for correspondence establishment. We\npropose to mitigate this problem by modeling semantic distributions with\ncontinuous functions learned from a population of Gaussian processes. Compared\nwith discrete semantic labels, the continuous functions capture finer-grained\ngeo-semantic information and also provide more detailed metric information for\ncorrespondence establishment. We insert this continuous function as the middle\nlayer between the object layer and the metric-semantic layer, forming a\ntri-layered 3D scene graph, serving as a light-weight yet performant backend\nfor one-shot localization. We term our global localization pipeline Outram-GSF\n(Gaussian semantic field) and conduct a wide range of experiments on publicly\navailable data sets, validating the superior performance against the current\nstate-of-the-art.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u4e09\u5c42\u573a\u666f\u56fe\u7684\u5355\u6b21LiDAR\u5168\u5c40\u5b9a\u4f4d\u7b97\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u8bed\u4e49\u5206\u5e03\u7684\u8fde\u7eed\u51fd\u6570\u6765\u7f13\u89e3\u5730\u6807\u91cd\u590d\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u8bed\u4e49\u6d88\u6b67\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5730\u6807\u8bed\u4e49\u6ce8\u518c\u7684\u65b9\u6cd5\u5728\u5730\u6807\u91cd\u590d\u65f6\u5bb9\u6613\u4ea7\u751f\u8bef\u5bfc\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u5730\u7406\u8bed\u4e49\u4fe1\u606f\u6765\u6539\u5584\u5bf9\u5e94\u5173\u7cfb\u5efa\u7acb\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u5b66\u4e60\u8bed\u4e49\u5206\u5e03\u7684\u8fde\u7eed\u51fd\u6570\uff0c\u6784\u5efa\u5305\u542b\u5bf9\u8c61\u5c42\u3001\u8fde\u7eed\u51fd\u6570\u5c42\u548c\u5ea6\u91cf\u8bed\u4e49\u5c42\u7684\u4e09\u5c423D\u573a\u666f\u56fe\uff0c\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5355\u6b21\u5b9a\u4f4d\u540e\u7aef\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684Outram-GSF\u5168\u5c40\u5b9a\u4f4d\u7ba1\u9053\u901a\u8fc7\u8fde\u7eed\u8bed\u4e49\u51fd\u6570\u5efa\u6a21\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8bed\u4e49\u6d88\u6b67\u548c\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2510.12169", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12169", "abs": "https://arxiv.org/abs/2510.12169", "authors": ["Akshay Naik", "William R. Norris", "Dustin Nottage", "Ahmet Soylemezoglu"], "title": "Hybrid Terrain-Aware Path Planning: Integrating VD--RRT\\(^{*}\\) Exploration and VD--D\\(^{*}\\) Lite Repair", "comment": null, "summary": "Autonomous ground vehicles operating off-road must plan curvature-feasible\npaths while accounting for spatially varying soil strength and slope hazards in\nreal time. We present a continuous state--cost metric that combines a Bekker\npressure--sinkage model with elevation-derived slope and attitude penalties.\nThe resulting terrain cost field is analytic, bounded, and monotonic in soil\nmodulus and slope, ensuring well-posed discretization and stable updates under\nsensor noise. This metric is evaluated on a lattice with exact steering\nprimitives: Dubins and Reeds--Shepp motions for differential drive and\ntime-parameterized bicycle arcs for Ackermann steering. Global exploration is\nperformed using Vehicle-Dynamics RRT\\(^{*}\\), while local repair is managed by\nVehicle-Dynamics D\\(^{*}\\) Lite, enabling millisecond-scale replanning without\nheuristic smoothing. By separating the terrain--vehicle model from the planner,\nthe framework provides a reusable basis for deterministic, sampling-based, or\nlearning-driven planning in deformable terrain. Hardware trials on an off-road\nplatform demonstrate real-time navigation across soft soil and slope\ntransitions, supporting reliable autonomy in unstructured environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8d8a\u91ce\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b9e\u65f6\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5730\u5f62\u6210\u672c\u548c\u8f66\u8f86\u52a8\u529b\u5b66\u7ea6\u675f\uff0c\u652f\u6301\u5728\u8f6f\u571f\u548c\u659c\u5761\u7b49\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u53ef\u9760\u5bfc\u822a\u3002", "motivation": "\u8d8a\u91ce\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u5728\u5b9e\u65f6\u89c4\u5212\u4e2d\u540c\u65f6\u8003\u8651\u66f2\u7387\u53ef\u884c\u7684\u8def\u5f84\u3001\u7a7a\u95f4\u53d8\u5316\u7684\u571f\u58e4\u5f3a\u5ea6\u548c\u659c\u5761\u5371\u9669\uff0c\u4ee5\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u5bfc\u822a\u3002", "method": "\u5f00\u53d1\u4e86\u8fde\u7eed\u72b6\u6001-\u6210\u672c\u5ea6\u91cf\uff0c\u7ed3\u5408Bekker\u538b\u529b-\u6c89\u9677\u6a21\u578b\u548c\u57fa\u4e8e\u9ad8\u7a0b\u7684\u659c\u5761\u60e9\u7f5a\uff1b\u5728\u683c\u7f51\u4e0a\u8bc4\u4f30\u8be5\u5ea6\u91cf\uff0c\u4f7f\u7528\u7cbe\u786e\u7684\u8f6c\u5411\u539f\u8bed\uff1b\u91c7\u7528Vehicle-Dynamics RRT*\u8fdb\u884c\u5168\u5c40\u63a2\u7d22\uff0cVehicle-Dynamics D* Lite\u8fdb\u884c\u5c40\u90e8\u4fee\u590d\u3002", "result": "\u786c\u4ef6\u8bd5\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u8f6f\u571f\u548c\u659c\u5761\u8fc7\u6e21\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5bfc\u822a\uff0c\u652f\u6301\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u53ef\u9760\u81ea\u4e3b\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5730\u5f62-\u8f66\u8f86\u6a21\u578b\u4e0e\u89c4\u5212\u5668\u5206\u79bb\uff0c\u8be5\u6846\u67b6\u4e3a\u53ef\u53d8\u5f62\u5730\u5f62\u4e2d\u7684\u786e\u5b9a\u6027\u3001\u57fa\u4e8e\u91c7\u6837\u6216\u5b66\u4e60\u9a71\u52a8\u7684\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u91cd\u7528\u7684\u57fa\u7840\u3002"}}
{"id": "2510.12206", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12206", "abs": "https://arxiv.org/abs/2510.12206", "authors": ["Pin-Lun Chen", "Chi-Hsi Kung", "Che-Han Chang", "Wei-Chen Chiu", "Yi-Ting Chen"], "title": "Controllable Collision Scenario Generation via Collision Pattern Prediction", "comment": "8 pages, 3 figures. Submitted to IEEE International Conference on\n  Robotics and Automation (ICRA) 2026", "summary": "Evaluating the safety of autonomous vehicles (AVs) requires diverse,\nsafety-critical scenarios, with collisions being especially important yet rare\nand unsafe to collect in the real world. Therefore, the community has been\nfocusing on generating safety-critical scenarios in simulation. However,\ncontrolling attributes such as collision type and time-to-accident (TTA)\nremains challenging. We introduce a new task called controllable collision\nscenario generation, where the goal is to produce trajectories that realize a\nuser-specified collision type and TTA, to investigate the feasibility of\nautomatically generating desired collision scenarios. To support this task, we\npresent COLLIDE, a large-scale collision scenario dataset constructed by\ntransforming real-world driving logs into diverse collisions, balanced across\nfive representative collision types and different TTA intervals. We propose a\nframework that predicts Collision Pattern, a compact and interpretable\nrepresentation that captures the spatial configuration of the ego and the\nadversarial vehicles at impact, before rolling out full adversarial\ntrajectories. Experiments show that our approach outperforms strong baselines\nin both collision rate and controllability. Furthermore, generated scenarios\nconsistently induce higher planner failure rates, revealing limitations of\nexisting planners. We demonstrate that these scenarios fine-tune planners for\nrobustness improvements, contributing to safer AV deployment in different\ncollision scenarios.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u63a7\u78b0\u649e\u573a\u666f\u751f\u6210\u4efb\u52a1\uff0c\u5f00\u53d1COLLIDE\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u80fd\u591f\u6309\u6307\u5b9a\u78b0\u649e\u7c7b\u578b\u548c\u65f6\u95f4\u751f\u6210\u78b0\u649e\u573a\u666f\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b89\u5168\u6027\u8bc4\u4f30\u9700\u8981\u591a\u6837\u5316\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u7279\u522b\u662f\u78b0\u649e\u573a\u666f\uff0c\u4f46\u5728\u73b0\u5b9e\u4e2d\u6536\u96c6\u65e2\u5371\u9669\u53c8\u7f55\u89c1\u3002\u4eff\u771f\u751f\u6210\u78b0\u649e\u573a\u666f\u662f\u53ef\u884c\u65b9\u6848\uff0c\u4f46\u63a7\u5236\u78b0\u649e\u7c7b\u578b\u548c\u65f6\u95f4\u7b49\u5c5e\u6027\u4ecd\u5177\u6311\u6218\u3002", "method": "\u6784\u5efaCOLLIDE\u5927\u89c4\u6a21\u78b0\u649e\u573a\u666f\u6570\u636e\u96c6\uff0c\u5c06\u771f\u5b9e\u9a7e\u9a76\u65e5\u5fd7\u8f6c\u6362\u4e3a\u591a\u6837\u5316\u78b0\u649e\uff1b\u63d0\u51fa\u9884\u6d4b\u78b0\u649e\u6a21\u5f0f\u7684\u6846\u67b6\uff0c\u8be5\u6a21\u5f0f\u662f\u7d27\u51d1\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u6355\u83b7\u78b0\u649e\u65f6\u81ea\u8f66\u548c\u5bf9\u6297\u8f66\u8f86\u7684\u7a7a\u95f4\u914d\u7f6e\uff1b\u7136\u540e\u5c55\u5f00\u5b8c\u6574\u5bf9\u6297\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u78b0\u649e\u7387\u548c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff1b\u751f\u6210\u7684\u573a\u666f\u80fd\u6301\u7eed\u8bf1\u5bfc\u66f4\u9ad8\u7684\u89c4\u5212\u5668\u5931\u8d25\u7387\uff0c\u63ed\u793a\u73b0\u6709\u89c4\u5212\u5668\u7684\u5c40\u9650\u6027\uff1b\u8fd9\u4e9b\u573a\u666f\u53ef\u7528\u4e8e\u5fae\u8c03\u89c4\u5212\u5668\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53ef\u63a7\u78b0\u649e\u573a\u666f\u751f\u6210\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u6307\u5b9a\u78b0\u649e\u7c7b\u578b\u548c\u65f6\u95f4\u7684\u573a\u666f\uff0c\u6709\u52a9\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u4e0d\u540c\u78b0\u649e\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u4fc3\u8fdb\u66f4\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u90e8\u7f72\u3002"}}
{"id": "2510.12215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12215", "abs": "https://arxiv.org/abs/2510.12215", "authors": ["Chanwoo Kim", "Jihwan Yoon", "Hyeonseong Kim", "Taemoon Jeong", "Changwoo Yoo", "Seungbeen Lee", "Soohwan Byeon", "Hoon Chung", "Matthew Pan", "Jean Oh", "Kyungjae Lee", "Sungjoon Choi"], "title": "Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications", "comment": "For more videos, see https://chanwookim971024.github.io/PioneeR/", "summary": "Mobile robot navigation in dynamic human environments requires policies that\nbalance adaptability to diverse behaviors with compliance to safety\nconstraints. We hypothesize that integrating data-driven rewards with\nrule-based objectives enables navigation policies to achieve a more effective\nbalance of adaptability and safety. To this end, we develop a framework that\nlearns a density-based reward from positive and negative demonstrations and\naugments it with rule-based objectives for obstacle avoidance and goal\nreaching. A sampling-based lookahead controller produces supervisory actions\nthat are both safe and adaptive, which are subsequently distilled into a\ncompact student policy suitable for real-time operation with uncertainty\nestimates. Experiments in synthetic and elevator co-boarding simulations show\nconsistent gains in success rate and time efficiency over baselines, and\nreal-world demonstrations with human participants confirm the practicality of\ndeployment. A video illustrating this work can be found on our project page\nhttps://chanwookim971024.github.io/PioneeR/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u5956\u52b1\u548c\u57fa\u4e8e\u89c4\u5219\u76ee\u6807\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5bc6\u5ea6\u5956\u52b1\u5b66\u4e60\u548c\u89c4\u5219\u7ea6\u675f\u5b9e\u73b0\u5b89\u5168\u4e0e\u81ea\u9002\u5e94\u7684\u5e73\u8861\uff0c\u4f7f\u7528\u91c7\u6837\u524d\u77bb\u63a7\u5236\u5668\u751f\u6210\u76d1\u7763\u52a8\u4f5c\u5e76\u84b8\u998f\u4e3a\u7d27\u51d1\u7684\u5b66\u751f\u7b56\u7565\u3002", "motivation": "\u5728\u52a8\u6001\u4eba\u7c7b\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u9700\u8981\u5e73\u8861\u5bf9\u591a\u6837\u5316\u884c\u4e3a\u7684\u9002\u5e94\u6027\u548c\u5bf9\u5b89\u5168\u7ea6\u675f\u7684\u9075\u5b88\u3002\u5047\u8bbe\u5c06\u6570\u636e\u9a71\u52a8\u5956\u52b1\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u76ee\u6807\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u9002\u5e94\u6027\u4e0e\u5b89\u5168\u6027\u5e73\u8861\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u4ece\u6b63\u8d1f\u6f14\u793a\u4e2d\u5b66\u4e60\u57fa\u4e8e\u5bc6\u5ea6\u7684\u5956\u52b1\uff0c\u5e76\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u969c\u788d\u7269\u907f\u8ba9\u548c\u76ee\u6807\u5230\u8fbe\u76ee\u6807\u8fdb\u884c\u589e\u5f3a\u3002\u4f7f\u7528\u91c7\u6837\u524d\u77bb\u63a7\u5236\u5668\u751f\u6210\u5b89\u5168\u4e14\u81ea\u9002\u5e94\u7684\u76d1\u7763\u52a8\u4f5c\uff0c\u7136\u540e\u5c06\u5176\u84b8\u998f\u4e3a\u9002\u5408\u5b9e\u65f6\u64cd\u4f5c\u7684\u7d27\u51d1\u5b66\u751f\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u548c\u7535\u68af\u5171\u4e58\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u6210\u529f\u7387\u548c\u65f6\u95f4\u6548\u7387\u65b9\u9762\u5747\u53d6\u5f97\u4e00\u81f4\u589e\u76ca\u3002\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u8bc1\u5b9e\u4e86\u90e8\u7f72\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u4eba\u7c7b\u73af\u5883\u4e2d\u5e73\u8861\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u5956\u52b1\u4e0e\u89c4\u5219\u7ea6\u675f\u7684\u7ed3\u5408\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.12276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12276", "abs": "https://arxiv.org/abs/2510.12276", "authors": ["Fuhao Li", "Wenxuan Song", "Han Zhao", "Jingbo Wang", "Pengxiang Ding", "Donglin Wang", "Long Zeng", "Haoang Li"], "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model", "comment": null, "summary": "Vision-language-action (VLA) models have recently shown strong potential in\nenabling robots to follow language instructions and execute precise actions.\nHowever, most VLAs are built upon vision-language models pretrained solely on\n2D data, which lack accurate spatial awareness and hinder their ability to\noperate in the 3D physical world. Existing solutions attempt to incorporate\nexplicit 3D sensor inputs such as depth maps or point clouds, but these\napproaches face challenges due to sensor noise, hardware heterogeneity, and\nincomplete depth coverage in existing datasets. Alternative methods that\nestimate 3D cues from 2D images also suffer from the limited performance of\ndepth estimators.We propose Spatial Forcing (SF), a simple yet effective\nalignment strategy that implicitly forces VLA models to develop spatial\ncomprehension capabilities without relying on explicit 3D inputs or depth\nestimators. SF aligns intermediate visual embeddings of VLAs with geometric\nrepresentations produced by pretrained 3D foundation models. By enforcing\nalignment at intermediate layers, SF guides VLAs to encode richer spatial\nrepresentations that enhance action precision.Extensive experiments in\nsimulation and real-world environments demonstrate that SF achieves\nstate-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further\naccelerates training by up to 3.8x and improves data efficiency across diverse\nrobotic tasks. Project page is at https://spatial-forcing.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpatial Forcing (SF)\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u7684\u4e2d\u95f4\u89c6\u89c9\u5d4c\u5165\u4e0e\u9884\u8bad\u7ec33D\u57fa\u7840\u6a21\u578b\u7684\u51e0\u4f55\u8868\u793a\u5bf9\u9f50\uff0c\u9690\u5f0f\u5730\u589e\u5f3a\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u663e\u5f0f3D\u8f93\u5165\u6216\u6df1\u5ea6\u4f30\u8ba1\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5927\u591a\u57fa\u4e8e\u4ec5\u4f7f\u75282D\u6570\u636e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7f3a\u4e4f\u51c6\u786e\u7684\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u9650\u5236\u4e86\u57283D\u7269\u7406\u4e16\u754c\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4f9d\u8d56\u6709\u566a\u58f0\u76843D\u4f20\u611f\u5668\u8f93\u5165\uff0c\u8981\u4e48\u53d7\u9650\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u3002", "method": "SF\u7b56\u7565\u5728\u4e2d\u95f4\u5c42\u5c06VLA\u6a21\u578b\u7684\u89c6\u89c9\u5d4c\u5165\u4e0e\u9884\u8bad\u7ec33D\u57fa\u7840\u6a21\u578b\u7684\u51e0\u4f55\u8868\u793a\u8fdb\u884c\u5bf9\u9f50\uff0c\u5f15\u5bfcVLA\u6a21\u578b\u7f16\u7801\u66f4\u4e30\u5bcc\u7684\u7a7a\u95f4\u8868\u793a\uff0c\u4ece\u800c\u63d0\u5347\u52a8\u4f5c\u7cbe\u5ea6\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSF\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u57fa\u4e8e2D\u548c3D\u7684VLA\u6a21\u578b\u3002SF\u8fdb\u4e00\u6b65\u5c06\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u4e86\u6700\u9ad83.8\u500d\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u3002", "conclusion": "SF\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u80fd\u591f\u9690\u5f0f\u5730\u589e\u5f3aVLA\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u9700\u663e\u5f0f3D\u8f93\u5165\uff0c\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002"}}
{"id": "2510.12332", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12332", "abs": "https://arxiv.org/abs/2510.12332", "authors": ["Mohammadreza Kasaei", "Mostafa Ghobadi", "Mohsen Khadem"], "title": "Shape-Aware Whole-Body Control for Continuum Robots with Application in Endoluminal Surgical Robotics", "comment": null, "summary": "This paper presents a shape-aware whole-body control framework for\ntendon-driven continuum robots with direct application to endoluminal surgical\nnavigation. Endoluminal procedures, such as bronchoscopy, demand precise and\nsafe navigation through tortuous, patient-specific anatomy where conventional\ntip-only control often leads to wall contact, tissue trauma, or failure to\nreach distal targets. To address these challenges, our approach combines a\nphysics-informed backbone model with residual learning through an Augmented\nNeural ODE, enabling accurate shape estimation and efficient Jacobian\ncomputation. A sampling-based Model Predictive Path Integral (MPPI) controller\nleverages this representation to jointly optimize tip tracking, backbone\nconformance, and obstacle avoidance under actuation constraints. A task manager\nfurther enhances adaptability by allowing real-time adjustment of objectives,\nsuch as wall clearance or direct advancement, during tele-operation. Extensive\nsimulation studies demonstrate millimeter-level accuracy across diverse\nscenarios, including trajectory tracking, dynamic obstacle avoidance, and\nshape-constrained reaching. Real-robot experiments on a bronchoscopy phantom\nvalidate the framework, showing improved lumen-following accuracy, reduced wall\ncontacts, and enhanced adaptability compared to joystick-only navigation and\nexisting baselines. These results highlight the potential of the proposed\nframework to increase safety, reliability, and operator efficiency in minimally\ninvasive endoluminal surgery, with broader applicability to other confined and\nsafety-critical environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u808c\u8171\u9a71\u52a8\u8fde\u7eed\u673a\u5668\u4eba\u7684\u5f62\u72b6\u611f\u77e5\u5168\u8eab\u63a7\u5236\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u8154\u5185\u624b\u672f\u5bfc\u822a\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u9aa8\u67b6\u6a21\u578b\u548c\u589e\u5f3a\u795e\u7ecfODE\u7684\u6b8b\u5dee\u5b66\u4e60\uff0c\u5b9e\u73b0\u7cbe\u786e\u5f62\u72b6\u4f30\u8ba1\u548c\u9ad8\u6548\u96c5\u53ef\u6bd4\u8ba1\u7b97\uff0c\u901a\u8fc7MPPI\u63a7\u5236\u5668\u4f18\u5316\u5c16\u7aef\u8ddf\u8e2a\u3001\u9aa8\u67b6\u987a\u5e94\u6027\u548c\u969c\u788d\u7269\u907f\u8ba9\u3002", "motivation": "\u4f20\u7edf\u4ec5\u63a7\u5236\u5c16\u7aef\u7684\u65b9\u6cd5\u5728\u8154\u5185\u624b\u672f\uff08\u5982\u652f\u6c14\u7ba1\u955c\u68c0\u67e5\uff09\u4e2d\u5bb9\u6613\u5bfc\u81f4\u58c1\u63a5\u89e6\u3001\u7ec4\u7ec7\u635f\u4f24\u6216\u65e0\u6cd5\u5230\u8fbe\u8fdc\u7aef\u76ee\u6807\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u5b89\u5168\u7684\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u9aa8\u67b6\u6a21\u578b\u4e0e\u589e\u5f3a\u795e\u7ecfODE\u7684\u6b8b\u5dee\u5b66\u4e60\u8fdb\u884c\u5f62\u72b6\u4f30\u8ba1\uff0c\u4f7f\u7528\u57fa\u4e8e\u91c7\u6837\u7684MPPI\u63a7\u5236\u5668\u8054\u5408\u4f18\u5316\u5c16\u7aef\u8ddf\u8e2a\u3001\u9aa8\u67b6\u987a\u5e94\u6027\u548c\u969c\u788d\u7269\u907f\u8ba9\uff0c\u4efb\u52a1\u7ba1\u7406\u5668\u652f\u6301\u5b9e\u65f6\u8c03\u6574\u76ee\u6807\u3002", "result": "\u4eff\u771f\u7814\u7a76\u663e\u793a\u6beb\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u5305\u62ec\u8f68\u8ff9\u8ddf\u8e2a\u3001\u52a8\u6001\u969c\u788d\u7269\u907f\u8ba9\u548c\u5f62\u72b6\u7ea6\u675f\u5230\u8fbe\u3002\u5728\u652f\u6c14\u7ba1\u955c\u6a21\u578b\u4e0a\u7684\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u663e\u793a\u6539\u8fdb\u7684\u7ba1\u8154\u8ddf\u968f\u7cbe\u5ea6\u3001\u51cf\u5c11\u7684\u58c1\u63a5\u89e6\u548c\u589e\u5f3a\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6f5c\u529b\u63d0\u9ad8\u5fae\u521b\u8154\u5185\u624b\u672f\u7684\u5b89\u5168\u6027\u3001\u53ef\u9760\u6027\u548c\u64cd\u4f5c\u6548\u7387\uff0c\u5bf9\u5176\u4ed6\u53d7\u9650\u548c\u5b89\u5168\u5173\u952e\u73af\u5883\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2510.12340", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12340", "abs": "https://arxiv.org/abs/2510.12340", "authors": ["Nicky Mol", "Luka Peternel", "Alessandro Ianniello", "Denis Zatyagov", "Auke Nachenius", "Stephan Balvert", "J. Micah Prendergast", "Sara Muscolo", "Olger Siebinga", "Eva Verhoef", "Deborah Forster", "David A. Abbink"], "title": "Achieving Meaningful Collaboration: Worker-centered Design of a Physical Human-Robot Collaborative Blending Task", "comment": "3 pages, 1 figure, ICRA@40 (Extended abstract)", "summary": "The use of robots in industrial settings continues to grow, driven by the\nneed to address complex societal challenges such as labor shortages, aging\npopulations, and ever-increasing production demands. In this abstract, we\nadvocate for (and demonstrate) a transdisciplinary approach when considering\nrobotics in the workplace. Transdisciplinarity emphasizes the integration of\nacademic research with pragmatic expertise and embodied experiential knowledge,\nthat prioritize values such as worker wellbeing and job attractiveness. In the\nfollowing, we describe an ongoing multi-pronged effort to explore the potential\nof collaborative robots in the context of airplane engine repair and\nmaintenance operations.", "AI": {"tldr": "\u672c\u6587\u4e3b\u5f20\u5e76\u5c55\u793a\u5728\u5de5\u4f5c\u573a\u6240\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u91c7\u7528\u8de8\u5b66\u79d1\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u8df5\u7ecf\u9a8c\u3001\u4f53\u9a8c\u77e5\u8bc6\u7684\u6574\u5408\uff0c\u4ee5\u63d0\u5347\u5de5\u4eba\u798f\u7949\u548c\u5de5\u4f5c\u5438\u5f15\u529b\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u5e94\u7528\u589e\u957f\u9700\u8981\u5e94\u5bf9\u52b3\u52a8\u529b\u77ed\u7f3a\u3001\u4eba\u53e3\u8001\u9f84\u5316\u548c\u751f\u4ea7\u9700\u6c42\u589e\u52a0\u7b49\u793e\u4f1a\u6311\u6218\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u6280\u672f\u5e94\u7528\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u3002", "method": "\u91c7\u7528\u8de8\u5b66\u79d1\u65b9\u6cd5\uff0c\u6574\u5408\u5b66\u672f\u7814\u7a76\u3001\u5b9e\u8df5\u4e13\u4e1a\u77e5\u8bc6\u548c\u4f53\u9a8c\u77e5\u8bc6\uff0c\u5728\u98de\u673a\u53d1\u52a8\u673a\u7ef4\u4fee\u548c\u7ef4\u62a4\u64cd\u4f5c\u4e2d\u63a2\u7d22\u534f\u4f5c\u673a\u5668\u4eba\u7684\u6f5c\u529b\u3002", "result": "\u6b63\u5728\u8fdb\u884c\u591a\u65b9\u9762\u7684\u52aa\u529b\u6765\u7814\u7a76\u534f\u4f5c\u673a\u5668\u4eba\u5728\u7279\u5b9a\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u53ef\u80fd\u6027\u3002", "conclusion": "\u8de8\u5b66\u79d1\u65b9\u6cd5\u5bf9\u4e8e\u5728\u5de5\u4f5c\u573a\u6240\u6210\u529f\u90e8\u7f72\u673a\u5668\u4eba\u6280\u672f\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u591f\u5e73\u8861\u6280\u672f\u6548\u7387\u4e0e\u4eba\u7c7b\u798f\u7949\u4ef7\u503c\u3002"}}
{"id": "2510.12346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12346", "abs": "https://arxiv.org/abs/2510.12346", "authors": ["Bingquan Li", "Ning Wang", "Tianwei Zhang", "Zhicheng He", "Yucong Wu"], "title": "PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing", "comment": null, "summary": "Recently, biped robot walking technology has been significantly developed,\nmainly in the context of a bland walking scheme. To emulate human walking,\nrobots need to step on the positions they see in unknown spaces accurately. In\nthis paper, we present PolyMap, a perception-based locomotion planning\nframework for humanoid robots to climb stairs. Our core idea is to build a\nreal-time polygonal staircase plane semantic map, followed by a footstep planar\nusing these polygonal plane segments. These plane segmentation and visual\nodometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The\nproposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz\nwhole-body motion planning output. Both indoor and outdoor real-scene\nexperiments indicate that our method is efficient and robust for humanoid robot\nstair climbing.", "AI": {"tldr": "\u63d0\u51faPolyMap\u6846\u67b6\uff0c\u57fa\u4e8e\u591a\u4f20\u611f\u5668\u878d\u5408\u6784\u5efa\u5b9e\u65f6\u591a\u8fb9\u5f62\u697c\u68af\u5e73\u9762\u8bed\u4e49\u5730\u56fe\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u722c\u697c\u68af\u7684\u611f\u77e5\u9a71\u52a8\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u6a21\u62df\u4eba\u7c7b\u884c\u8d70\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u672a\u77e5\u7a7a\u95f4\u4e2d\u51c6\u786e\u8e29\u8e0f\u4f4d\u7f6e\uff0c\u7279\u522b\u662f\u5728\u722c\u697c\u68af\u573a\u666f\u4e2d\u3002", "method": "\u4f7f\u7528\u591a\u4f20\u611f\u5668\u878d\u5408\uff08LiDAR\u3001RGB-D\u76f8\u673a\u548cIMU\uff09\u8fdb\u884c\u5e73\u9762\u5206\u5272\u548c\u89c6\u89c9\u91cc\u7a0b\u8ba1\uff0c\u6784\u5efa\u5b9e\u65f6\u591a\u8fb9\u5f62\u697c\u68af\u5e73\u9762\u8bed\u4e49\u5730\u56fe\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u591a\u8fb9\u5f62\u5e73\u9762\u6bb5\u8fdb\u884c\u811a\u6b65\u89c4\u5212\u3002", "result": "\u5728NVIDIA Orin\u4e0a\u90e8\u7f72\uff0c\u5b9e\u73b020-30Hz\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u8f93\u51fa\uff0c\u5ba4\u5185\u5916\u771f\u5b9e\u573a\u666f\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u9ad8\u6548\u4e14\u9c81\u68d2\u3002", "conclusion": "PolyMap\u6846\u67b6\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u722c\u697c\u68af\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u611f\u77e5\u9a71\u52a8\u8fd0\u52a8\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12370", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12370", "abs": "https://arxiv.org/abs/2510.12370", "authors": ["Wenli Shi", "Clemence Grislain", "Olivier Sigaud", "Mohamed Chetouani"], "title": "Controlling Intent Expressiveness in Robot Motion with Diffusion Models", "comment": "Using diffusion models trained on quality diversity datasets for\n  generating robot motions with adjustable legibility levels", "summary": "Legibility of robot motion is critical in human-robot interaction, as it\nallows humans to quickly infer a robot's intended goal. Although traditional\ntrajectory generation methods typically prioritize efficiency, they often fail\nto make the robot's intentions clear to humans. Meanwhile, existing approaches\nto legible motion usually produce only a single \"most legible\" trajectory,\noverlooking the need to modulate intent expressiveness in different contexts.\nIn this work, we propose a novel motion generation framework that enables\ncontrollable legibility across the full spectrum, from highly legible to highly\nambiguous motions. We introduce a modeling approach based on an Information\nPotential Field to assign continuous legibility scores to trajectories, and\nbuild upon it with a two-stage diffusion framework that first generates paths\nat specified legibility levels and then translates them into executable robot\nactions. Experiments in both 2D and 3D reaching tasks demonstrate that our\napproach produces diverse and controllable motions with varying degrees of\nlegibility, while achieving performance comparable to SOTA. Code and project\npage: https://legibility-modulator.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4ece\u9ad8\u5ea6\u53ef\u8bfb\u6027\u5230\u9ad8\u5ea6\u6a21\u7cca\u6027\u7684\u5168\u8c31\u7cfb\u8303\u56f4\u5185\u5b9e\u73b0\u53ef\u63a7\u7684\u53ef\u8bfb\u6027\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u4fe1\u606f\u52bf\u573a\u5efa\u6a21\uff0c\u5e76\u4f7f\u7528\u4e24\u9636\u6bb5\u6269\u6563\u6846\u67b6\u751f\u6210\u6307\u5b9a\u53ef\u8bfb\u6027\u6c34\u5e73\u7684\u8def\u5f84\u548c\u53ef\u6267\u884c\u52a8\u4f5c\u3002", "motivation": "\u4f20\u7edf\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f18\u5148\u8003\u8651\u6548\u7387\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u8ba9\u4eba\u7c7b\u6e05\u6670\u7406\u89e3\u673a\u5668\u4eba\u7684\u610f\u56fe\u3002\u73b0\u6709\u7684\u53ef\u8bfb\u6027\u8fd0\u52a8\u65b9\u6cd5\u901a\u5e38\u53ea\u4ea7\u751f\u5355\u4e00\u7684\"\u6700\u53ef\u8bfb\"\u8f68\u8ff9\uff0c\u5ffd\u89c6\u4e86\u5728\u4e0d\u540c\u60c5\u5883\u4e0b\u8c03\u8282\u610f\u56fe\u8868\u8fbe\u7684\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u4fe1\u606f\u52bf\u573a\u5efa\u6a21\u4e3a\u8f68\u8ff9\u5206\u914d\u8fde\u7eed\u53ef\u8bfb\u6027\u8bc4\u5206\uff0c\u5e76\u6784\u5efa\u4e24\u9636\u6bb5\u6269\u6563\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u6307\u5b9a\u53ef\u8bfb\u6027\u6c34\u5e73\u7684\u8def\u5f84\uff0c\u7136\u540e\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u57282D\u548c3D\u5230\u8fbe\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u5177\u6709\u4e0d\u540c\u53ef\u8bfb\u6027\u7a0b\u5ea6\u7684\u591a\u6837\u5316\u4e14\u53ef\u63a7\u7684\u8fd0\u52a8\uff0c\u540c\u65f6\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u8fd0\u52a8\u53ef\u8bfb\u6027\u7684\u8fde\u7eed\u53ef\u63a7\u8c03\u8282\uff0c\u4e3a\u4e0d\u540c\u4ea4\u4e92\u573a\u666f\u4e0b\u7684\u610f\u56fe\u8868\u8fbe\u63d0\u4f9b\u4e86\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12392", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12392", "abs": "https://arxiv.org/abs/2510.12392", "authors": ["Junhyuk So", "Chiwoong Lee", "Shinyoung Lee", "Jungseul Ok", "Eunhyeok Park"], "title": "Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking", "comment": "Accepted at NeurIPS25", "summary": "Generative Behavior Cloning (GBC) is a simple yet effective framework for\nrobot learning, particularly in multi-task settings. Recent GBC methods often\nemploy diffusion policies with open-loop (OL) control, where actions are\ngenerated via a diffusion process and executed in multi-step chunks without\nreplanning. While this approach has demonstrated strong success rates and\ngeneralization, its inherent stochasticity can result in erroneous action\nsampling, occasionally leading to unexpected task failures. Moreover, OL\ncontrol suffers from delayed responses, which can degrade performance in noisy\nor dynamic environments. To address these limitations, we propose two novel\ntechniques to enhance the consistency and reactivity of diffusion policies: (1)\nself-guidance, which improves action fidelity by leveraging past observations\nand implicitly promoting future-aware behavior; and (2) adaptive chunking,\nwhich selectively updates action sequences when the benefits of reactivity\noutweigh the need for temporal consistency. Extensive experiments show that our\napproach substantially improves GBC performance across a wide range of\nsimulated and real-world robotic manipulation tasks. Our code is available at\nhttps://github.com/junhyukso/SGAC", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6280\u672f\uff08\u81ea\u5f15\u5bfc\u548c\u81ea\u9002\u5e94\u5206\u5757\uff09\u6765\u589e\u5f3a\u6269\u6563\u7b56\u7565\u7684\u4e00\u81f4\u6027\u548c\u53cd\u5e94\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u884c\u4e3a\u514b\u9686\u5728\u591a\u4efb\u52a1\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u751f\u6210\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u4f7f\u7528\u5f00\u73af\u63a7\u5236\u7684\u6269\u6563\u7b56\u7565\uff0c\u867d\u7136\u6210\u529f\u7387\u9ad8\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4f46\u5176\u56fa\u6709\u7684\u968f\u673a\u6027\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u52a8\u4f5c\u91c7\u6837\u548c\u610f\u5916\u4efb\u52a1\u5931\u8d25\uff0c\u4e14\u5728\u566a\u58f0\u6216\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u5ef6\u8fdf\u54cd\u5e94\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6280\u672f\uff1a1\uff09\u81ea\u5f15\u5bfc\u6280\u672f\uff0c\u901a\u8fc7\u5229\u7528\u8fc7\u53bb\u89c2\u5bdf\u5e76\u9690\u5f0f\u4fc3\u8fdb\u672a\u6765\u611f\u77e5\u884c\u4e3a\u6765\u63d0\u9ad8\u52a8\u4f5c\u4fdd\u771f\u5ea6\uff1b2\uff09\u81ea\u9002\u5e94\u5206\u5757\u6280\u672f\uff0c\u5f53\u53cd\u5e94\u6027\u6536\u76ca\u8d85\u8fc7\u65f6\u95f4\u4e00\u81f4\u6027\u9700\u6c42\u65f6\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u884c\u4e3a\u514b\u9686\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u81ea\u5f15\u5bfc\u548c\u81ea\u9002\u5e94\u5206\u5757\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u7b56\u7565\u5728\u751f\u6210\u884c\u4e3a\u514b\u9686\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u53cd\u5e94\u6027\u95ee\u9898\uff0c\u4e3a\u591a\u4efb\u52a1\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12403", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12403", "abs": "https://arxiv.org/abs/2510.12403", "authors": ["Francesco Capuano", "Caroline Pascal", "Adil Zouitine", "Thomas Wolf", "Michel Aractingi"], "title": "Robot Learning: A Tutorial", "comment": "Tutorial on Robot Learning using LeRobot, the end-to-end robot\n  learning library developed by Hugging Face", "summary": "Robot learning is at an inflection point, driven by rapid advancements in\nmachine learning and the growing availability of large-scale robotics data.\nThis shift from classical, model-based methods to data-driven, learning-based\nparadigms is unlocking unprecedented capabilities in autonomous systems. This\ntutorial navigates the landscape of modern robot learning, charting a course\nfrom the foundational principles of Reinforcement Learning and Behavioral\nCloning to generalist, language-conditioned models capable of operating across\ndiverse tasks and even robot embodiments. This work is intended as a guide for\nresearchers and practitioners, and our goal is to equip the reader with the\nconceptual understanding and practical tools necessary to contribute to\ndevelopments in robot learning, with ready-to-use examples implemented in\n$\\texttt{lerobot}$.", "AI": {"tldr": "\u672c\u6559\u7a0b\u6982\u8ff0\u4e86\u73b0\u4ee3\u673a\u5668\u4eba\u5b66\u4e60\u7684\u53d1\u5c55\u5386\u7a0b\uff0c\u4ece\u5f3a\u5316\u5b66\u4e60\u548c\u884c\u4e3a\u514b\u9686\u7684\u57fa\u7840\u539f\u7406\u5230\u80fd\u591f\u8de8\u4efb\u52a1\u548c\u673a\u5668\u4eba\u5e73\u53f0\u64cd\u4f5c\u7684\u901a\u7528\u8bed\u8a00\u6761\u4ef6\u6a21\u578b\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u6b63\u5904\u4e8e\u8f6c\u6298\u70b9\uff0c\u673a\u5668\u5b66\u4e60\u5feb\u901f\u53d1\u5c55\u548c\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6570\u636e\u7684\u53ef\u7528\u6027\u63a8\u52a8\u4e86\u4ece\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u65b9\u6cd5\u5411\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u8303\u5f0f\u7684\u8f6c\u53d8\uff0c\u8fd9\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u89e3\u9501\u4e86\u524d\u6240\u672a\u6709\u7684\u80fd\u529b\u3002", "method": "\u6559\u7a0b\u4ece\u5f3a\u5316\u5b66\u4e60\u548c\u884c\u4e3a\u514b\u9686\u7684\u57fa\u7840\u539f\u7406\u51fa\u53d1\uff0c\u9010\u6b65\u4ecb\u7ecd\u5230\u901a\u7528\u8bed\u8a00\u6761\u4ef6\u6a21\u578b\uff0c\u4f7f\u7528$\texttt{lerobot}$\u5b9e\u73b0\u5373\u7528\u793a\u4f8b\u3002", "result": "\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u6982\u5ff5\u7406\u89e3\u548c\u5b9e\u8df5\u5de5\u5177\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u7684\u53d1\u5c55\u505a\u51fa\u8d21\u732e\u3002", "conclusion": "\u672c\u6559\u7a0b\u65e8\u5728\u6307\u5bfc\u8bfb\u8005\u638c\u63e1\u73b0\u4ee3\u673a\u5668\u4eba\u5b66\u4e60\u7684\u5173\u952e\u6982\u5ff5\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.12419", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12419", "abs": "https://arxiv.org/abs/2510.12419", "authors": ["Shunnosuke Yoshimura", "Kento Kawaharazuka", "Kei Okada"], "title": "M3D-skin: Multi-material 3D-printed Tactile Sensor with Hierarchical Infill Structures for Pressure Sensing", "comment": "Accepted to IROS2025, Website:\n  https://ssk-yoshimura.github.io/M3D-skin/", "summary": "Tactile sensors have a wide range of applications, from utilization in\nrobotic grippers to human motion measurement. If tactile sensors could be\nfabricated and integrated more easily, their applicability would further\nexpand. In this study, we propose a tactile sensor-M3D-skin-that can be easily\nfabricated with high versatility by leveraging the infill patterns of a\nmulti-material fused deposition modeling (FDM) 3D printer as the sensing\nprinciple. This method employs conductive and non-conductive flexible filaments\nto create a hierarchical structure with a specific infill pattern. The flexible\nhierarchical structure deforms under pressure, leading to a change in\nelectrical resistance, enabling the acquisition of tactile information. We\nmeasure the changes in characteristics of the proposed tactile sensor caused by\nmodifications to the hierarchical structure. Additionally, we demonstrate the\nfabrication and use of a multi-tile sensor. Furthermore, as applications, we\nimplement motion pattern measurement on the sole of a foot, integration with a\nrobotic hand, and tactile-based robotic operations. Through these experiments,\nwe validate the effectiveness of the proposed tactile sensor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM3D-skin\u7684\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u5229\u7528\u591a\u6750\u6599FDM 3D\u6253\u5370\u673a\u7684\u586b\u5145\u56fe\u6848\u4f5c\u4e3a\u4f20\u611f\u539f\u7406\uff0c\u901a\u8fc7\u5bfc\u7535\u548c\u975e\u5bfc\u7535\u67d4\u6027\u6750\u6599\u521b\u5efa\u5206\u5c42\u7ed3\u6784\uff0c\u5728\u538b\u529b\u4e0b\u53d8\u5f62\u5bfc\u81f4\u7535\u963b\u53d8\u5316\u6765\u83b7\u53d6\u89e6\u89c9\u4fe1\u606f\u3002", "motivation": "\u5982\u679c\u89e6\u89c9\u4f20\u611f\u5668\u80fd\u591f\u66f4\u5bb9\u6613\u5730\u5236\u9020\u548c\u96c6\u6210\uff0c\u5176\u5e94\u7528\u8303\u56f4\u5c06\u8fdb\u4e00\u6b65\u6269\u5927\u3002", "method": "\u5229\u7528\u591a\u6750\u6599FDM 3D\u6253\u5370\u6280\u672f\uff0c\u4f7f\u7528\u5bfc\u7535\u548c\u975e\u5bfc\u7535\u67d4\u6027\u6750\u6599\u521b\u5efa\u5177\u6709\u7279\u5b9a\u586b\u5145\u56fe\u6848\u7684\u5206\u5c42\u7ed3\u6784\uff0c\u901a\u8fc7\u7ed3\u6784\u53d8\u5f62\u5f15\u8d77\u7684\u7535\u963b\u53d8\u5316\u6765\u68c0\u6d4b\u89e6\u89c9\u4fe1\u606f\u3002", "result": "\u6d4b\u91cf\u4e86\u5206\u5c42\u7ed3\u6784\u4fee\u6539\u5bf9\u4f20\u611f\u5668\u7279\u6027\u7684\u5f71\u54cd\uff0c\u5c55\u793a\u4e86\u591a\u74e6\u7247\u4f20\u611f\u5668\u7684\u5236\u9020\u548c\u4f7f\u7528\uff0c\u5b9e\u73b0\u4e86\u8db3\u5e95\u8fd0\u52a8\u6a21\u5f0f\u6d4b\u91cf\u3001\u4e0e\u673a\u5668\u4eba\u624b\u7684\u96c6\u6210\u4ee5\u53ca\u57fa\u4e8e\u89e6\u89c9\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.12477", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12477", "abs": "https://arxiv.org/abs/2510.12477", "authors": ["Gaoyuan Liu", "Joris de Winter", "Kelly Merckaert", "Denis Steckelmacher", "Ann Nowe", "Bram Vanderborght"], "title": "A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation", "comment": null, "summary": "In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the\ntwo core properties to evaluate robot performance. However, safety mechanisms\nusually hinder task efficiency since human intervention will cause backup\nmotions and goal failures of the robot. Frequent motion replanning will\nincrease the computational load and the chance of failure. In this paper, we\npresent a hybrid Reinforcement Learning (RL) planning framework which is\ncomprised of an interactive motion planner and a RL task planner. The RL task\nplanner attempts to choose statistically safe and efficient task sequences\nbased on the feedback from the motion planner, while the motion planner keeps\nthe task execution process collision-free by detecting human arm motions and\ndeploying new paths when the previous path is not valid anymore. Intuitively,\nthe RL agent will learn to avoid dangerous tasks, while the motion planner\nensures that the chosen tasks are safe. The proposed framework is validated on\nthe cobot in both simulation and the real world, we compare the planner with\nhard-coded task motion planning methods. The results show that our planning\nframework can 1) react to uncertain human motions at both joint and task\nlevels; 2) reduce the times of repeating failed goal commands; 3) reduce the\ntotal number of replanning requests.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u4ea4\u4e92\u5f0f\u8fd0\u52a8\u89c4\u5212\u5668\u548cRL\u4efb\u52a1\u89c4\u5212\u5668\uff0c\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u5b89\u5168\u4e0e\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u5728\u4eba\u673a\u534f\u4f5c\u73af\u5883\u4e2d\uff0c\u5b89\u5168\u673a\u5236\u901a\u5e38\u4f1a\u5f71\u54cd\u4efb\u52a1\u6548\u7387\uff0c\u56e0\u4e3a\u4eba\u4e3a\u5e72\u9884\u4f1a\u5bfc\u81f4\u673a\u5668\u4eba\u5907\u4efd\u8fd0\u52a8\u548c\u76ee\u6807\u5931\u8d25\u3002\u9891\u7e41\u7684\u8fd0\u52a8\u91cd\u89c4\u5212\u4f1a\u589e\u52a0\u8ba1\u7b97\u8d1f\u8f7d\u548c\u5931\u8d25\u51e0\u7387\u3002", "method": "\u4f7f\u7528\u6df7\u5408RL\u89c4\u5212\u6846\u67b6\uff0c\u5305\u542b\u4ea4\u4e92\u5f0f\u8fd0\u52a8\u89c4\u5212\u5668\u548cRL\u4efb\u52a1\u89c4\u5212\u5668\u3002RL\u4efb\u52a1\u89c4\u5212\u5668\u57fa\u4e8e\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u53cd\u9988\u9009\u62e9\u7edf\u8ba1\u4e0a\u5b89\u5168\u9ad8\u6548\u7684\u4efb\u52a1\u5e8f\u5217\uff0c\u8fd0\u52a8\u89c4\u5212\u5668\u901a\u8fc7\u68c0\u6d4b\u4eba\u4f53\u624b\u81c2\u8fd0\u52a8\u5e76\u5728\u8def\u5f84\u65e0\u6548\u65f6\u90e8\u7f72\u65b0\u8def\u5f84\u6765\u4fdd\u6301\u4efb\u52a1\u6267\u884c\u8fc7\u7a0b\u65e0\u78b0\u649e\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u5bf9\u534f\u4f5c\u673a\u5668\u4eba\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4e0e\u786c\u7f16\u7801\u4efb\u52a1\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u80fd\u591f\uff1a1\uff09\u5728\u5173\u8282\u548c\u4efb\u52a1\u5c42\u9762\u54cd\u5e94\u4e0d\u786e\u5b9a\u7684\u4eba\u4f53\u8fd0\u52a8\uff1b2\uff09\u51cf\u5c11\u91cd\u590d\u5931\u8d25\u76ee\u6807\u547d\u4ee4\u7684\u6b21\u6570\uff1b3\uff09\u51cf\u5c11\u91cd\u89c4\u5212\u8bf7\u6c42\u7684\u603b\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408RL\u89c4\u5212\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e73\u8861\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\uff0c\u901a\u8fc7\u5b66\u4e60\u907f\u514d\u5371\u9669\u4efb\u52a1\u5e76\u786e\u4fdd\u6240\u9009\u4efb\u52a1\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2510.12483", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.12483", "abs": "https://arxiv.org/abs/2510.12483", "authors": ["Jingkai Jia", "Tong Yang", "Xueyao Chen", "Chenhuan Liu", "Wenqiang Zhang"], "title": "Fast Visuomotor Policy for Robotic Manipulation", "comment": null, "summary": "We present a fast and effective policy framework for robotic manipulation,\nnamed Energy Policy, designed for high-frequency robotic tasks and\nresource-constrained systems. Unlike existing robotic policies, Energy Policy\nnatively predicts multimodal actions in a single forward pass, enabling\nhigh-precision manipulation at high speed. The framework is built upon two core\ncomponents. First, we adopt the energy score as the learning objective to\nfacilitate multimodal action modeling. Second, we introduce an energy MLP to\nimplement the proposed objective while keeping the architecture simple and\nefficient. We conduct comprehensive experiments in both simulated environments\nand real-world robotic tasks to evaluate the effectiveness of Energy Policy.\nThe results show that Energy Policy matches or surpasses the performance of\nstate-of-the-art manipulation methods while significantly reducing\ncomputational overhead. Notably, on the MimicGen benchmark, Energy Policy\nachieves superior performance with at a faster inference compared to existing\napproaches.", "AI": {"tldr": "Energy Policy\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9ad8\u6548\u7b56\u7565\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u9884\u6d4b\u591a\u6a21\u6001\u52a8\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u901f\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u9488\u5bf9\u9ad8\u9891\u673a\u5668\u4eba\u4efb\u52a1\u548c\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u65e2\u5feb\u901f\u53c8\u6709\u6548\u7684\u7b56\u7565\u6846\u67b6\uff0c\u80fd\u591f\u539f\u751f\u652f\u6301\u591a\u6a21\u6001\u52a8\u4f5c\u9884\u6d4b\u3002", "method": "\u6846\u67b6\u57fa\u4e8e\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u91c7\u7528\u80fd\u91cf\u5206\u6570\u4f5c\u4e3a\u5b66\u4e60\u76ee\u6807\u4ee5\u4fc3\u8fdb\u591a\u6a21\u6001\u52a8\u4f5c\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u80fd\u91cfMLP\u6765\u5b9e\u73b0\u8be5\u76ee\u6807\u540c\u65f6\u4fdd\u6301\u67b6\u6784\u7b80\u5355\u9ad8\u6548\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEnergy Policy\u5728\u6027\u80fd\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u64cd\u4f5c\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002\u5728MimicGen\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee5\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u5b9e\u73b0\u4e86\u66f4\u4f18\u6027\u80fd\u3002", "conclusion": "Energy Policy\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u6709\u6548\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u9ad8\u9891\u548c\u8d44\u6e90\u53d7\u9650\u7684\u673a\u5668\u4eba\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2510.12509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12509", "abs": "https://arxiv.org/abs/2510.12509", "authors": ["Gaoyuan Liu", "Bas Boom", "Naftali Slob", "Yuri Durodi\u00e9", "Ann Now\u00e9", "Bram Vanderborght"], "title": "Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge", "comment": null, "summary": "Pruning is an essential agricultural practice for orchards. Proper pruning\ncan promote healthier growth and optimize fruit production throughout the\norchard's lifespan. Robot manipulators have been developed as an automated\nsolution for this repetitive task, which typically requires seasonal labor with\nspecialized skills. While previous research has primarily focused on the\nchallenges of perception, the complexities of manipulation are often\noverlooked. These challenges involve planning and control in both joint and\nCartesian spaces to guide the end-effector through intricate, obstructive\nbranches. Our work addresses the behavior planning challenge for a robotic\npruning system, which entails a multi-level planning problem in environments\nwith complex collisions. In this paper, we formulate the planning problem for a\nhigh-dimensional robotic arm in a pruning scenario, investigate the system's\nintrinsic redundancies, and propose a comprehensive pruning workflow that\nintegrates perception, modeling, and holistic planning. In our experiments, we\ndemonstrate that more comprehensive planning methods can significantly enhance\nthe performance of the robotic manipulator. Finally, we implement the proposed\nworkflow on a real-world robot. As a result, this work complements previous\nefforts on robotic pruning and motivates future research and development in\nplanning for pruning applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u679c\u56ed\u4fee\u526a\u7684\u673a\u5668\u4eba\u884c\u4e3a\u89c4\u5212\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5728\u590d\u6742\u78b0\u649e\u73af\u5883\u4e2d\u591a\u7ea7\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u611f\u77e5\u3001\u5efa\u6a21\u548c\u6574\u4f53\u89c4\u5212\u6765\u63d0\u5347\u4fee\u526a\u6027\u80fd\u3002", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u6311\u6218\uff0c\u4f46\u4fee\u526a\u64cd\u4f5c\u4e2d\u7684\u590d\u6742\u64cd\u7eb5\u95ee\u9898\u5e38\u88ab\u5ffd\u89c6\uff0c\u5305\u62ec\u5728\u5173\u8282\u7a7a\u95f4\u548c\u7b1b\u5361\u5c14\u7a7a\u95f4\u4e2d\u7684\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u4ee5\u5f15\u5bfc\u672b\u7aef\u6267\u884c\u5668\u7a7f\u8fc7\u590d\u6742\u7684\u6811\u679d\u969c\u788d\u3002", "method": "\u5236\u5b9a\u9ad8\u7ef4\u673a\u68b0\u81c2\u5728\u4fee\u526a\u573a\u666f\u4e2d\u7684\u89c4\u5212\u95ee\u9898\uff0c\u7814\u7a76\u7cfb\u7edf\u5185\u5728\u5197\u4f59\u6027\uff0c\u63d0\u51fa\u6574\u5408\u611f\u77e5\u3001\u5efa\u6a21\u548c\u6574\u4f53\u89c4\u5212\u7684\u5b8c\u6574\u4fee\u526a\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u66f4\u5168\u9762\u7684\u89c4\u5212\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u6240\u63d0\u51fa\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8865\u5145\u4e86\u5148\u524d\u5173\u4e8e\u673a\u5668\u4eba\u4fee\u526a\u7684\u7814\u7a76\uff0c\u5e76\u6fc0\u53d1\u4e86\u4fee\u526a\u5e94\u7528\u4e2d\u89c4\u5212\u95ee\u9898\u7684\u672a\u6765\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2510.12611", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12611", "abs": "https://arxiv.org/abs/2510.12611", "authors": ["Lukas Pries", "Markus Ryll"], "title": "Learning Robust Agile Flight Control with Stability Guarantees", "comment": null, "summary": "In the evolving landscape of high-speed agile quadrotor flight, achieving\nprecise trajectory tracking at the platform's operational limits is paramount.\nControllers must handle actuator constraints, exhibit robustness to\ndisturbances, and remain computationally efficient for safety-critical\napplications. In this work, we present a novel neural-augmented feedback\ncontroller for agile flight control. The controller addresses individual\nlimitations of existing state-of-the-art control paradigms and unifies their\nstrengths. We demonstrate the controller's capabilities, including the accurate\ntracking of highly aggressive trajectories that surpass the feasibility of the\nactuators. Notably, the controller provides universal stability guarantees,\nenhancing its robustness and tracking performance even in exceedingly\ndisturbance-prone settings. Its nonlinear feedback structure is highly\nefficient enabling fast computation at high update rates. Moreover, the\nlearning process in simulation is both fast and stable, and the controller's\ninherent robustness allows direct deployment to real-world platforms without\nthe need for training augmentations or fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u589e\u5f3a\u53cd\u9988\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u9ad8\u901f\u654f\u6377\u56db\u65cb\u7ffc\u98de\u884c\u63a7\u5236\uff0c\u8be5\u63a7\u5236\u5668\u7ed3\u5408\u4e86\u73b0\u6709\u63a7\u5236\u8303\u5f0f\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u901a\u7528\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff0c\u80fd\u591f\u5728\u9ad8\u6270\u52a8\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u5728\u9ad8\u901f\u654f\u6377\u56db\u65cb\u7ffc\u98de\u884c\u4e2d\uff0c\u9700\u8981\u5728\u5e73\u53f0\u64cd\u4f5c\u6781\u9650\u4e0b\u5b9e\u73b0\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u63a7\u5236\u5668\u5fc5\u987b\u5904\u7406\u6267\u884c\u5668\u7ea6\u675f\u3001\u5bf9\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u795e\u7ecf\u589e\u5f3a\u53cd\u9988\u63a7\u5236\u5668\uff0c\u8be5\u63a7\u5236\u5668\u7edf\u4e00\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u63a7\u5236\u8303\u5f0f\u7684\u4f18\u52bf\uff0c\u5177\u6709\u975e\u7ebf\u6027\u53cd\u9988\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u4eff\u771f\u4e2d\u5feb\u901f\u7a33\u5b9a\u5b66\u4e60\u3002", "result": "\u63a7\u5236\u5668\u80fd\u591f\u51c6\u786e\u8ddf\u8e2a\u9ad8\u5ea6\u6fc0\u8fdb\u7684\u8f68\u8ff9\uff0c\u751a\u81f3\u8d85\u8d8a\u6267\u884c\u5668\u7684\u53ef\u884c\u6027\u9650\u5236\uff0c\u5728\u6781\u5ea6\u6270\u52a8\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u589e\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u8ddf\u8e2a\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u652f\u6301\u9ad8\u66f4\u65b0\u7387\u3002", "conclusion": "\u8be5\u63a7\u5236\u5668\u65e0\u9700\u8bad\u7ec3\u589e\u5f3a\u6216\u5fae\u8c03\u5373\u53ef\u76f4\u63a5\u90e8\u7f72\u5230\u771f\u5b9e\u4e16\u754c\u5e73\u53f0\uff0c\u4e3a\u9ad8\u901f\u654f\u6377\u56db\u65cb\u7ffc\u98de\u884c\u63d0\u4f9b\u4e86\u5177\u6709\u901a\u7528\u7a33\u5b9a\u6027\u4fdd\u8bc1\u7684\u9ad8\u6548\u63a7\u5236\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12662", "abs": "https://arxiv.org/abs/2510.12662", "authors": ["Oz Gitelson", "Satya Prakash Nayak", "Ritam Raha", "Anne-Kathrin Schmuck"], "title": "Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop", "comment": null, "summary": "We present a novel framework for human-robot \\emph{logical} interaction that\nenables robots to reliably satisfy (infinite horizon) temporal logic tasks\nwhile effectively collaborating with humans who pursue independent and unknown\ntasks. The framework combines two key capabilities: (i) \\emph{maximal\nadaptation} enables the robot to adjust its strategy \\emph{online} to exploit\nhuman behavior for cooperation whenever possible, and (ii) \\emph{minimal\ntunable feedback} enables the robot to request cooperation by the human online\nonly when necessary to guarantee progress. This balance minimizes human-robot\ninterference, preserves human autonomy, and ensures persistent robot task\nsatisfaction even under conflicting human goals. We validate the approach in a\nreal-world block-manipulation task with a Franka Emika Panda robotic arm and in\nthe Overcooked-AI benchmark, demonstrating that our method produces rich,\n\\emph{emergent} cooperative behaviors beyond the reach of existing approaches,\nwhile maintaining strong formal guarantees.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u903b\u8f91\u4ea4\u4e92\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u53ef\u9760\u6ee1\u8db3\u65f6\u5e8f\u903b\u8f91\u4efb\u52a1\uff0c\u540c\u65f6\u4e0e\u8ffd\u6c42\u72ec\u7acb\u672a\u77e5\u4efb\u52a1\u7684\u4eba\u7c7b\u6709\u6548\u534f\u4f5c\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u673a\u5668\u4eba\u9700\u8981\u6ee1\u8db3\u957f\u671f\u65f6\u5e8f\u903b\u8f91\u4efb\u52a1\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u5bf9\u4eba\u7c7b\u81ea\u4e3b\u6027\u7684\u5e72\u6270\uff0c\u5e76\u5728\u4eba\u7c7b\u76ee\u6807\u51b2\u7a81\u65f6\u4ecd\u80fd\u4fdd\u8bc1\u4efb\u52a1\u5b8c\u6210\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6700\u5927\u9002\u5e94\u6027\u548c\u6700\u5c0f\u53ef\u8c03\u53cd\u9988\u4e24\u79cd\u80fd\u529b\uff1a\u6700\u5927\u9002\u5e94\u6027\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u7ebf\u8c03\u6574\u7b56\u7565\u4ee5\u5229\u7528\u4eba\u7c7b\u884c\u4e3a\u8fdb\u884c\u534f\u4f5c\uff1b\u6700\u5c0f\u53ef\u8c03\u53cd\u9988\u4ec5\u5728\u5fc5\u8981\u65f6\u8bf7\u6c42\u4eba\u7c7b\u5408\u4f5c\u4ee5\u4fdd\u8bc1\u8fdb\u5c55\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5757\u64cd\u4f5c\u4efb\u52a1\u548cOvercooked-AI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u4e30\u5bcc\u6d8c\u73b0\u534f\u4f5c\u884c\u4e3a\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u5f62\u5f0f\u5316\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u9002\u5e94\u6027\u548c\u5e72\u6270\u6700\u5c0f\u5316\uff0c\u5b9e\u73b0\u4e86\u5728\u4eba\u7c7b\u76ee\u6807\u51b2\u7a81\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u8bc1\u673a\u5668\u4eba\u4efb\u52a1\u5b8c\u6210\u7684\u53ef\u9760\u534f\u4f5c\u3002"}}
{"id": "2510.12684", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.12684", "abs": "https://arxiv.org/abs/2510.12684", "authors": ["Alvaro Belmonte-Baeza", "Miguel Cazorla", "Gabriel J. Garc\u00eda", "Carlos J. P\u00e9rez-Del-Pulgar", "Jorge Pomares"], "title": "Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning", "comment": "This is the authors version of the paper accepted for publication in\n  The IEEE International Conference on Space Robotics 2025. The final version\n  link will be added here after conference proceedings are published", "summary": "Robotics plays a pivotal role in planetary science and exploration, where\nautonomous and reliable systems are crucial due to the risks and challenges\ninherent to space environments. The establishment of permanent lunar bases\ndemands robotic platforms capable of navigating and manipulating in the harsh\nlunar terrain. While wheeled rovers have been the mainstay for planetary\nexploration, their limitations in unstructured and steep terrains motivate the\nadoption of legged robots, which offer superior mobility and adaptability. This\npaper introduces a constrained reinforcement learning framework designed for\nautonomous quadrupedal mobile manipulators operating in lunar environments. The\nproposed framework integrates whole-body locomotion and manipulation\ncapabilities while explicitly addressing critical safety constraints, including\ncollision avoidance, dynamic stability, and power efficiency, in order to\nensure robust performance under lunar-specific conditions, such as reduced\ngravity and irregular terrain. Experimental results demonstrate the framework's\neffectiveness in achieving precise 6D task-space end-effector pose tracking,\nachieving an average positional accuracy of 4 cm and orientation accuracy of\n8.1 degrees. The system consistently respects both soft and hard constraints,\nexhibiting adaptive behaviors optimized for lunar gravity conditions. This work\neffectively bridges adaptive learning with essential mission-critical safety\nrequirements, paving the way for advanced autonomous robotic explorers for\nfuture lunar missions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6708\u7403\u73af\u5883\u4e2d\u56db\u8db3\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u4e86\u5168\u8eab\u8fd0\u52a8\u548c\u64cd\u4f5c\u80fd\u529b\uff0c\u540c\u65f6\u6ee1\u8db3\u78b0\u649e\u907f\u514d\u3001\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u529f\u7387\u6548\u7387\u7b49\u5b89\u5168\u7ea6\u675f\u3002", "motivation": "\u8f6e\u5f0f\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u548c\u9661\u5ced\u5730\u5f62\u4e2d\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u91c7\u7528\u817f\u5f0f\u673a\u5668\u4eba\uff0c\u5176\u5728\u6708\u7403\u73af\u5883\u4e2d\u5177\u6709\u66f4\u597d\u7684\u79fb\u52a8\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u4f7f\u7528\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u96c6\u6210\u5168\u8eab\u8fd0\u52a8\u548c\u64cd\u4f5c\u80fd\u529b\uff0c\u660e\u786e\u5904\u7406\u78b0\u649e\u907f\u514d\u3001\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u529f\u7387\u6548\u7387\u7b49\u5173\u952e\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u5b9e\u73b0\u4e86\u7cbe\u786e\u76846D\u4efb\u52a1\u7a7a\u95f4\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u8ddf\u8e2a\uff0c\u5e73\u5747\u4f4d\u7f6e\u7cbe\u5ea6\u4e3a4\u5398\u7c73\uff0c\u65b9\u5411\u7cbe\u5ea6\u4e3a8.1\u5ea6\uff0c\u7cfb\u7edf\u59cb\u7ec8\u5c0a\u91cd\u8f6f\u786c\u7ea6\u675f\uff0c\u5e76\u8868\u73b0\u51fa\u9488\u5bf9\u6708\u7403\u91cd\u529b\u6761\u4ef6\u4f18\u5316\u7684\u81ea\u9002\u5e94\u884c\u4e3a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u6548\u6865\u63a5\u4e86\u81ea\u9002\u5e94\u5b66\u4e60\u4e0e\u5173\u952e\u4efb\u52a1\u5b89\u5168\u9700\u6c42\uff0c\u4e3a\u672a\u6765\u6708\u7403\u4efb\u52a1\u7684\u5148\u8fdb\u81ea\u4e3b\u673a\u5668\u4eba\u63a2\u7d22\u8005\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.12710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12710", "abs": "https://arxiv.org/abs/2510.12710", "authors": ["Baicheng Li", "Dong Wu", "Zike Yan", "Xinchen Liu", "Zecui Zeng", "Lusong Li", "Hongbin Zha"], "title": "Reflection-Based Task Adaptation for Self-Improving VLA", "comment": null, "summary": "Pre-trained Vision-Language-Action (VLA) models represent a major leap\ntowards general-purpose robots, yet efficiently adapting them to novel,\nspecific tasks in-situ remains a significant hurdle. While reinforcement\nlearning (RL) is a promising avenue for such adaptation, the process often\nsuffers from low efficiency, hindering rapid task mastery. We introduce\nReflective Self-Adaptation, a framework for rapid, autonomous task adaptation\nwithout human intervention. Our framework establishes a self-improving loop\nwhere the agent learns from its own experience to enhance both strategy and\nexecution.\n  The core of our framework is a dual-pathway architecture that addresses the\nfull adaptation lifecycle. First, a Failure-Driven Reflective RL pathway\nenables rapid learning by using the VLM's causal reasoning to automatically\nsynthesize a targeted, dense reward function from failure analysis. This\nprovides a focused learning signal that significantly accelerates policy\nexploration. However, optimizing such proxy rewards introduces a potential risk\nof \"reward hacking,\" where the agent masters the reward function but fails the\nactual task. To counteract this, our second pathway, Success-Driven\nQuality-Guided SFT, grounds the policy in holistic success. It identifies and\nselectively imitates high-quality successful trajectories, ensuring the agent\nremains aligned with the ultimate task goal. This pathway is strengthened by a\nconditional curriculum mechanism to aid initial exploration.\n  We conduct experiments in challenging manipulation tasks. The results\ndemonstrate that our framework achieves faster convergence and higher final\nsuccess rates compared to representative baselines. Our work presents a robust\nsolution for creating self-improving agents that can efficiently and reliably\nadapt to new environments.", "AI": {"tldr": "\u63d0\u51faReflective Self-Adaptation\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u67b6\u6784\u5b9e\u73b0VLA\u6a21\u578b\u5728\u65b0\u578b\u4efb\u52a1\u4e2d\u7684\u5feb\u901f\u81ea\u4e3b\u9002\u5e94\uff0c\u5305\u542b\u5931\u8d25\u9a71\u52a8\u7684\u53cd\u601dRL\u548c\u6210\u529f\u9a71\u52a8\u7684\u8d28\u91cf\u5f15\u5bfcSFT\uff0c\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u6210\u529f\u7387\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u5411\u901a\u7528\u673a\u5668\u4eba\u53d1\u5c55\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u8fdb\u5c55\uff0c\u4f46\u5728\u73b0\u573a\u9002\u5e94\u65b0\u578b\u7279\u5b9a\u4efb\u52a1\u65f6\u4ecd\u9762\u4e34\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7684\u4f4e\u6548\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u67b6\u6784\uff1a1) \u5931\u8d25\u9a71\u52a8\u7684\u53cd\u601dRL\u8def\u5f84\uff0c\u5229\u7528VLM\u7684\u56e0\u679c\u63a8\u7406\u81ea\u52a8\u4ece\u5931\u8d25\u5206\u6790\u4e2d\u5408\u6210\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\uff1b2) \u6210\u529f\u9a71\u52a8\u7684\u8d28\u91cf\u5f15\u5bfcSFT\u8def\u5f84\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6a21\u4eff\u9ad8\u8d28\u91cf\u6210\u529f\u8f68\u8ff9\u6765\u786e\u4fdd\u7b56\u7565\u4e0e\u4efb\u52a1\u76ee\u6807\u5bf9\u9f50\uff0c\u5e76\u91c7\u7528\u6761\u4ef6\u8bfe\u7a0b\u673a\u5236\u8f85\u52a9\u521d\u59cb\u63a2\u7d22\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u64cd\u4f5c\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u76f8\u6bd4\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6700\u7ec8\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u521b\u5efa\u80fd\u591f\u9ad8\u6548\u53ef\u9760\u9002\u5e94\u65b0\u73af\u5883\u7684\u81ea\u6211\u6539\u8fdb\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7a33\u5065\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.12717", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12717", "abs": "https://arxiv.org/abs/2510.12717", "authors": ["Se Hwan Jeon", "Ho Jae Lee", "Seungwoo Hong", "Sangbae Kim"], "title": "Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control", "comment": "TRO submission preprint", "summary": "Model Predictive Control (MPC) provides interpretable, tunable locomotion\ncontrollers grounded in physical models, but its robustness depends on frequent\nreplanning and is limited by model mismatch and real-time computational\nconstraints. Reinforcement Learning (RL), by contrast, can produce highly\nrobust behaviors through stochastic training but often lacks interpretability,\nsuffers from out-of-distribution failures, and requires intensive reward\nengineering. This work presents a GPU-parallelized residual architecture that\ntightly integrates MPC and RL by blending their outputs at the torque-control\nlevel. We develop a kinodynamic whole-body MPC formulation evaluated across\nthousands of agents in parallel at 100 Hz for RL training. The residual policy\nlearns to make targeted corrections to the MPC outputs, combining the\ninterpretability and constraint handling of model-based control with the\nadaptability of RL. The model-based control prior acts as a strong bias,\ninitializing and guiding the policy towards desirable behavior with a simple\nset of rewards. Compared to standalone MPC or end-to-end RL, our approach\nachieves higher sample efficiency, converges to greater asymptotic rewards,\nexpands the range of trackable velocity commands, and enables zero-shot\nadaptation to unseen gaits and uneven terrain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdGPU\u5e76\u884c\u5316\u7684\u6b8b\u5dee\u67b6\u6784\uff0c\u5c06MPC\u548cRL\u5728\u529b\u77e9\u63a7\u5236\u5c42\u9762\u7d27\u5bc6\u96c6\u6210\uff0c\u7ed3\u5408\u4e86\u6a21\u578b\u63a7\u5236\u7684\u89e3\u91ca\u6027\u548c\u7ea6\u675f\u5904\u7406\u80fd\u529b\u4e0eRL\u7684\u9002\u5e94\u6027\u3002", "motivation": "MPC\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u53ef\u8c03\u8c10\u7684\u63a7\u5236\u5668\u4f46\u53d7\u9650\u4e8e\u6a21\u578b\u5931\u914d\u548c\u5b9e\u65f6\u8ba1\u7b97\u7ea6\u675f\uff0cRL\u80fd\u4ea7\u751f\u9c81\u68d2\u884c\u4e3a\u4f46\u7f3a\u4e4f\u89e3\u91ca\u6027\u4e14\u9700\u8981\u5927\u91cf\u5956\u52b1\u5de5\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u52a8\u529b\u5b66\u5168\u8eabMPC\u516c\u5f0f\uff0c\u5728100Hz\u9891\u7387\u4e0b\u5e76\u884c\u8bc4\u4f30\u6570\u5343\u4e2a\u667a\u80fd\u4f53\u8fdb\u884cRL\u8bad\u7ec3\uff0c\u6b8b\u5dee\u7b56\u7565\u5b66\u4e60\u5bf9MPC\u8f93\u51fa\u8fdb\u884c\u9488\u5bf9\u6027\u4fee\u6b63\u3002", "result": "\u76f8\u6bd4\u72ec\u7acbMPC\u6216\u7aef\u5230\u7aefRL\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3001\u66f4\u5927\u7684\u6e10\u8fd1\u5956\u52b1\u3001\u66f4\u5e7f\u7684\u53ef\u8ddf\u8e2a\u901f\u5ea6\u547d\u4ee4\u8303\u56f4\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u9002\u5e94\u672a\u89c1\u6b65\u6001\u548c\u4e0d\u5e73\u5730\u5f62\u3002", "conclusion": "\u6a21\u578b\u63a7\u5236\u5148\u9a8c\u4f5c\u4e3a\u5f3a\u504f\u5dee\uff0c\u7528\u7b80\u5355\u5956\u52b1\u96c6\u5f15\u5bfc\u7b56\u7565\u671d\u5411\u671f\u671b\u884c\u4e3a\uff0c\u7ed3\u5408\u4e86\u6a21\u578b\u63a7\u5236\u7684\u53ef\u89e3\u91ca\u6027\u548cRL\u7684\u9002\u5e94\u6027\u4f18\u52bf\u3002"}}
{"id": "2510.12724", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.12724", "abs": "https://arxiv.org/abs/2510.12724", "authors": ["Xin Fei", "Zhixuan Xu", "Huaicong Fang", "Tianrui Zhang", "Lin Shao"], "title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping", "comment": "12 pages, 14 figures", "summary": "Dexterous grasping remains a central challenge in robotics due to the\ncomplexity of its high-dimensional state and action space. We introduce T(R,O)\nGrasp, a diffusion-based framework that efficiently generates accurate and\ndiverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,\na unified representation that models spatial transformations between robotic\nhands and objects while encoding their geometric properties. A graph diffusion\nmodel, coupled with an efficient inverse kinematics solver, supports both\nunconditioned and conditioned grasp synthesis. Extensive experiments on a\ndiverse set of dexterous hands show that T(R,O) Grasp achieves average success\nrate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per\nsecond on an NVIDIA A100 40GB GPU, substantially outperforming existing\nbaselines. In addition, our approach is robust and generalizable across\nembodiments while significantly reducing memory consumption. More importantly,\nthe high inference speed enables closed-loop dexterous manipulation,\nunderscoring the potential of T(R,O) Grasp to scale into a foundation model for\ndexterous grasping.", "AI": {"tldr": "T(R,O) Grasp\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7075\u5de7\u6293\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u7684T(R,O)\u56fe\u8868\u793a\u6cd5\u9ad8\u6548\u751f\u6210\u51c6\u786e\u591a\u6837\u7684\u6293\u53d6\u59ff\u6001\uff0c\u5728\u591a\u79cd\u673a\u5668\u4eba\u624b\u4e0a\u5b9e\u73b094.83%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe0.21\u79d2\u3002", "motivation": "\u7075\u5de7\u6293\u53d6\u5728\u673a\u5668\u4eba\u9886\u57df\u9762\u4e34\u9ad8\u7ef4\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u590d\u6742\u6027\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9ad8\u6548\u751f\u6210\u51c6\u786e\u591a\u6837\u6293\u53d6\u59ff\u6001\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faT(R,O)\u56fe\u7edf\u4e00\u8868\u793a\u6cd5\uff0c\u5efa\u6a21\u673a\u5668\u4eba\u624b\u4e0e\u7269\u4f53\u95f4\u7684\u7a7a\u95f4\u53d8\u6362\u5173\u7cfb\u5e76\u7f16\u7801\u51e0\u4f55\u5c5e\u6027\uff0c\u7ed3\u5408\u56fe\u6269\u6563\u6a21\u578b\u548c\u9ad8\u6548\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\uff0c\u652f\u6301\u65e0\u6761\u4ef6\u4e0e\u6761\u4ef6\u6293\u53d6\u5408\u6210\u3002", "result": "\u5728\u591a\u79cd\u7075\u5de7\u624b\u4e0a\u5b9e\u73b094.83%\u5e73\u5747\u6210\u529f\u7387\uff0c\u63a8\u7406\u901f\u5ea60.21\u79d2\uff0cA100 GPU\u4e0a\u541e\u5410\u91cf41\u6293\u53d6/\u79d2\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u5e76\u5177\u5907\u8de8\u5177\u8eab\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u63a8\u7406\u901f\u5ea6\uff0c\u652f\u6301\u95ed\u73af\u7075\u5de7\u64cd\u4f5c\uff0c\u6709\u6f5c\u529b\u53d1\u5c55\u4e3a\u7075\u5de7\u6293\u53d6\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2510.12733", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.12733", "abs": "https://arxiv.org/abs/2510.12733", "authors": ["Hang Yu", "Julian Jordan", "Julian Schmidt", "Silvan Lindner", "Alessandro Canevaro", "Wilhelm Stork"], "title": "HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions", "comment": null, "summary": "Safe and interpretable motion planning in complex urban environments needs to\nreason about bidirectional multi-agent interactions. This reasoning requires to\nestimate the costs of potential ego driving maneuvers. Many existing planners\ngenerate initial trajectories with sampling-based methods and refine them by\noptimizing on learned predictions of future environment states, which requires\na cost function that encodes the desired vehicle behavior. Designing such a\ncost function can be very challenging, especially if a wide range of complex\nurban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego\nproposal-conditioned predictions, a planner that integrates multimodal\ntrajectory proposals from a learned proposal model as heuristic priors into a\nMonte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,\nwe introduce an ego-conditioned occupancy prediction model, enabling\nconsistent, scene-aware reasoning. Our design significantly simplifies cost\nfunction design in refinement by considering proposal-driven guidance,\nrequiring only minimalistic grid-based cost terms. Evaluations on large-scale\nreal-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves\nstate-of-the-art performance, especially in safety and adaptability.", "AI": {"tldr": "HYPE\u662f\u4e00\u4e2a\u6df7\u5408\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5c06\u5b66\u4e60\u5230\u7684\u591a\u6a21\u6001\u8f68\u8ff9\u63d0\u6848\u4f5c\u4e3a\u542f\u53d1\u5f0f\u5148\u9a8c\u96c6\u6210\u5230\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4e2d\uff0c\u7b80\u5316\u4e86\u6210\u672c\u51fd\u6570\u8bbe\u8ba1\uff0c\u5e76\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u5b89\u5168\u548c\u53ef\u89e3\u91ca\u7684\u8fd0\u52a8\u89c4\u5212\u9700\u8981\u5904\u7406\u53cc\u5411\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u800c\u8bbe\u8ba1\u7f16\u7801\u671f\u671b\u8f66\u8f86\u884c\u4e3a\u7684\u6210\u672c\u51fd\u6570\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faHYPE\u6df7\u5408\u89c4\u5212\u5668\uff0c\u5c06\u5b66\u4e60\u5230\u7684\u63d0\u6848\u6a21\u578b\u751f\u6210\u7684\u591a\u6a21\u6001\u8f68\u8ff9\u4f5c\u4e3a\u542f\u53d1\u5f0f\u5148\u9a8c\u96c6\u6210\u5230\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7ec6\u5316\u4e2d\uff0c\u5e76\u5f15\u5165\u81ea\u6211\u6761\u4ef6\u5360\u7528\u9884\u6d4b\u6a21\u578b\u6765\u5efa\u6a21\u53cc\u5411\u4ea4\u4e92\u3002", "result": "\u5728nuPlan\u548cDeepUrban\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHYPE\u6709\u6548\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u3002", "conclusion": "HYPE\u901a\u8fc7\u63d0\u6848\u9a71\u52a8\u7684\u5f15\u5bfc\u663e\u8457\u7b80\u5316\u4e86\u7ec6\u5316\u8fc7\u7a0b\u4e2d\u7684\u6210\u672c\u51fd\u6570\u8bbe\u8ba1\uff0c\u4ec5\u9700\u6700\u5c0f\u5316\u7684\u57fa\u4e8e\u7f51\u683c\u7684\u6210\u672c\u9879\uff0c\u5c31\u80fd\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u548c\u9002\u5e94\u6027\u5f3a\u7684\u8fd0\u52a8\u89c4\u5212\u3002"}}
