<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 29]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams](https://arxiv.org/abs/2510.07417)
*Corban Rivera,Grayson Byrd,Meghan Booker,Bethany Kemp,Allison Gaines,Emma Holmes,James Uplinger,Celso M de Melo,David Handelman*

Main category: cs.RO

TL;DR: FLEET是一个混合分散式框架，将自然语言指令转化为优化的多机器人调度方案，通过LLM前端生成任务图和机器人-任务适配矩阵，结合形式化后端进行最小化完成时间优化。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人团队从自由形式自然语言指令进行协调的困难，克服纯语言规划器的长时程协调问题和幻觉，以及纯形式化方法需要封闭世界模型的限制。

Method: 使用LLM前端生成任务图（包含持续时间和优先级）和机器人-任务适配矩阵，形式化后端解决最小化完成时间问题，底层机器人通过智能闭环控制执行自由形式子任务。

Result: 在多个自由形式语言引导的自主协调基准测试中，FLEET在异构任务的两智能体团队上比最先进的生成规划器提高了成功率。消融实验显示MILP主要改善时间结构，LLM导出的适配性对能力耦合任务至关重要。

Conclusion: FLEET框架成功地将自然语言指令转化为优化的多机器人调度，在仿真和硬件实验中均表现出色，证明了混合方法在异构机器人协调中的有效性。

Abstract: Coordinating heterogeneous robot teams from free-form natural-language
instructions is hard. Language-only planners struggle with long-horizon
coordination and hallucination, while purely formal methods require
closed-world models. We present FLEET, a hybrid decentralized framework that
turns language into optimized multi-robot schedules. An LLM front-end produces
(i) a task graph with durations and precedence and (ii) a capability-aware
robot--task fitness matrix; a formal back-end solves a makespan-minimization
problem while the underlying robots execute their free-form subtasks with
agentic closed-loop control. Across multiple free-form language-guided autonomy
coordination benchmarks, FLEET improves success over state of the art
generative planners on two-agent teams across heterogeneous tasks. Ablations
show that mixed integer linear programming (MILP) primarily improves temporal
structure, while LLM-derived fitness is decisive for capability-coupled tasks;
together they deliver the highest overall performance. We demonstrate the
translation to real world challenges with hardware trials using a pair of
quadruped robots with disjoint capabilities.

</details>


### [2] [VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics](https://arxiv.org/abs/2510.07447)
*Girolamo Oddo,Roberto Nuca,Matteo Parsani*

Main category: cs.RO

TL;DR: 提出基于门控循环单元的轻量级编码器-解码器模型，用于在信息稀缺条件下预测高性能车辆的未来状态，最大平均相对误差低于2.6%，具有良好的噪声鲁棒性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 高性能车辆的动态建模需要详细的系统结构信息，但这些信息通常对非设计者不可得，这是自动驾驶应用中的典型问题。

Method: 使用基于门控循环单元(GRU)的轻量级编码器-解码器模型，通过车辆历史状态测量值和驾驶员控制动作来关联未来状态。

Result: 在极端动态条件下，模型最大平均相对误差低于2.6%；对感兴趣频率范围内的噪声输入数据具有良好的鲁棒性；输出信号（纵向和横向加速度、横摆率、纵向速度）表现出物理一致性。

Conclusion: 该数据驱动模型在信息稀缺条件下有效预测车辆动态，无需物理约束即可保持物理一致性，适用于自动驾驶应用。

Abstract: Developing a dynamic model for a high-performance vehicle is a complex
problem that requires extensive structural information about the system under
analysis. This information is often unavailable to those who did not design the
vehicle and represents a typical issue in autonomous driving applications,
which are frequently developed on top of existing vehicles; therefore, vehicle
models are developed under conditions of information scarcity. This paper
proposes a lightweight encoder-decoder model based on Gate Recurrent Unit
layers to correlate the vehicle's future state with its past states, measured
onboard, and control actions the driver performs. The results demonstrate that
the model achieves a maximum mean relative error below 2.6% in extreme dynamic
conditions. It also shows good robustness when subject to noisy input data
across the interested frequency components. Furthermore, being entirely
data-driven and free from physical constraints, the model exhibits physical
consistency in the output signals, such as longitudinal and lateral
accelerations, yaw rate, and the vehicle's longitudinal velocity.

</details>


### [3] [HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent](https://arxiv.org/abs/2510.07514)
*Cael Yasutake,Zachary Kingston,Brian Plancher*

Main category: cs.RO

TL;DR: HJCD-IK是一种基于GPU加速的采样混合逆运动学求解器，结合了方向感知的贪婪坐标下降初始化方案和基于雅可比矩阵的优化例程，在精度和速度方面优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统解析逆运动学求解器受限于低自由度和特定拓扑结构，而数值优化方法计算成本高且容易陷入局部最小值。需要开发更高效、准确的求解器。

Method: 使用GPU加速的采样混合方法，结合方向感知贪婪坐标下降初始化和雅可比矩阵优化抛光程序。

Result: 相比现有技术，该求解器在收敛速度和整体精度方面均有提升，在精度-延迟帕累托前沿上找到解决方案，通常实现数量级增益，并产生高质量样本的广泛分布。

Conclusion: HJCD-IK在逆运动学求解方面表现出色，提供了开源代码供社区使用。

Abstract: Inverse Kinematics (IK) is a core problem in robotics, in which joint
configurations are found to achieve a desired end-effector pose. Although
analytical solvers are fast and efficient, they are limited to systems with low
degrees-of-freedom and specific topological structures. Numerical
optimization-based approaches are more general, but suffer from high
computational costs and frequent convergence to spurious local minima. Recent
efforts have explored the use of GPUs to combine sampling and optimization to
enhance both the accuracy and speed of IK solvers. We build on this recent
literature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid
solver that combines an orientation-aware greedy coordinate descent
initialization scheme with a Jacobian-based polishing routine. This design
enables our solver to improve both convergence speed and overall accuracy as
compared to the state-of-the-art, consistently finding solutions along the
accuracy-latency Pareto frontier and often achieving order-of-magnitude gains.
In addition, our method produces a broad distribution of high-quality samples,
yielding the lowest maximum mean discrepancy. We release our code open-source
for the benefit of the community.

</details>


### [4] [AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation](https://arxiv.org/abs/2510.07548)
*Adam Hung,Fan Yang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 提出了摊销价值优化（AVO）方法，通过引入学习到的价值函数来预测未来任务性能，指导轨迹优化器朝向有利于后续子任务的状态，从而解决灵巧操作任务中独立优化子任务导致的性能限制和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作任务需要在不同接触模式间切换，传统方法将任务分解为独立优化的子任务，这限制了性能且计算成本高昂，因为缺乏对未来子任务的信息会导致系统陷入难以继续后续任务的状态。

Method: 提出摊销价值优化（AVO），引入学习到的价值函数预测总未来任务性能，将该价值函数纳入每个规划步骤的轨迹优化成本中，通过价值函数梯度引导优化器朝向最小化未来子任务成本的状态。

Result: 在螺丝刀抓取和转动任务的仿真和真实世界实验中验证了AVO的有效性，即使计算预算减少50%，相比没有价值函数的轨迹优化仍能获得改进的性能。

Conclusion: AVO方法有效桥接了独立优化的子任务，通过减少在线计算需求加速了优化过程，在灵巧操作任务中表现出优越性能。

Abstract: Dexterous manipulation tasks often require switching between different
contact modes, such as rolling, sliding, sticking, or non-contact contact
modes. When formulating dexterous manipulation tasks as a trajectory
optimization problem, a common approach is to decompose these tasks into
sub-tasks for each contact mode, which are each solved independently.
Optimizing each sub-task independently can limit performance, as optimizing
contact points, contact forces, or other variables without information about
future sub-tasks can place the system in a state from which it is challenging
to make progress on subsequent sub-tasks. Further, optimizing these sub-tasks
is very computationally expensive. To address these challenges, we propose
Amortized Value Optimization (AVO), which introduces a learned value function
that predicts the total future task performance. By incorporating this value
function into the cost of the trajectory optimization at each planning step,
the value function gradients guide the optimizer toward states that minimize
the cost in future sub-tasks. This effectively bridges separately optimized
sub-tasks, and accelerates the optimization by reducing the amount of online
computation needed. We validate AVO on a screwdriver grasping and turning task
in both simulation and real world experiments, and show improved performance
even with 50% less computational budget compared to trajectory optimization
without the value function.

</details>


### [5] [Inspection Planning Primitives with Implicit Models](https://arxiv.org/abs/2510.07611)
*Jingyang You,Hanna Kurniawati,Lashika Medagoda*

Main category: cs.RO

TL;DR: 提出了一套名为IPIM的原始计算工具，使基于采样的检测规划器能够完全使用神经SDF表示，在保持检测轨迹质量的同时大幅减少内存使用。


<details>
  <summary>Details</summary>
Motivation: 基础设施老化与复杂性增加使得高效检测规划更为关键。现有基于采样的检测规划器虽然快速但内存需求大，特别是对于大型复杂结构。隐式模型（如神经SDF）能高效表示这些结构，但现有规划器主要设计用于显式环境模型。

Method: 开发了IPIM（基于隐式模型的检测规划原语）工具集，使基于采样的检测规划器能够完全使用神经SDF表示，无需在隐式和显式模型间频繁转换。

Result: 在三个场景（包括具有9200万个三角网格面的复杂真实结构）的评估表明，即使使用基本的基于采样规划器配合IPIM，也能生成与最先进规划器质量相当的检测轨迹，同时内存使用减少高达70倍。

Conclusion: IPIM使得基于采样的检测规划器能够有效利用神经SDF表示，在保持检测质量的同时显著降低内存需求，为大型复杂基础设施的检测规划提供了高效解决方案。

Abstract: The aging and increasing complexity of infrastructures make efficient
inspection planning more critical in ensuring safety. Thanks to sampling-based
motion planning, many inspection planners are fast. However, they often require
huge memory. This is particularly true when the structure under inspection is
large and complex, consisting of many struts and pillars of various geometry
and sizes. Such structures can be represented efficiently using implicit
models, such as neural Signed Distance Functions (SDFs). However, most
primitive computations used in sampling-based inspection planner have been
designed to work efficiently with explicit environment models, which in turn
requires the planner to use explicit environment models or performs frequent
transformations between implicit and explicit environment models during
planning. This paper proposes a set of primitive computations, called
Inspection Planning Primitives with Implicit Models (IPIM), that enable
sampling-based inspection planners to entirely use neural SDFs representation
during planning. Evaluation on three scenarios, including inspection of a
complex real-world structure with over 92M triangular mesh faces, indicates
that even a rudimentary sampling-based planner with IPIM can generate
inspection trajectories of similar quality to those generated by the
state-of-the-art planner, while using up to 70x less memory than the
state-of-the-art inspection planner.

</details>


### [6] [GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control](https://arxiv.org/abs/2510.07625)
*Alexander Du,Emre Adabag,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: GATO是一个开源、GPU加速的批量轨迹优化求解器，专门为中等批量规模（几十到几百个求解）的实时MPC应用设计，相比CPU基准实现18-21倍加速，相比GPU基准实现1.4-16倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速方法要么并行化单个求解以满足实时要求，要么以低于实时的速率扩展到非常大的批量，要么通过限制模型通用性来实现速度。这为许多需要实时批量求解的最先进MPC应用留下了巨大的求解器性能差距。

Method: GATO在算法、软件和计算硬件上协同设计，利用块级、warp级和线程级并行性在求解内部和求解之间实现超高性能。

Result: 模拟基准测试显示，随着批量大小增加，相比CPU基准实现18-21倍加速，相比GPU基准实现1.4-16倍加速；案例研究显示改进了扰动抑制和收敛行为；在工业机械臂上进行了硬件验证。

Conclusion: GATO为中等批量规模的实时MPC应用提供了高效的求解方案，并开源以支持可重复性和采用。

Abstract: While Model Predictive Control (MPC) delivers strong performance across
robotics applications, solving the underlying (batches of) nonlinear trajectory
optimization (TO) problems online remains computationally demanding. Existing
GPU-accelerated approaches typically (i) parallelize a single solve to meet
real-time deadlines, (ii) scale to very large batches at slower-than-real-time
rates, or (iii) achieve speed by restricting model generality (e.g., point-mass
dynamics or a single linearization). This leaves a large gap in solver
performance for many state-of-the-art MPC applications that require real-time
batches of tens to low-hundreds of solves. As such, we present GATO, an open
source, GPU-accelerated, batched TO solver co-designed across algorithm,
software, and computational hardware to deliver real-time throughput for these
moderate batch size regimes. Our approach leverages a combination of block-,
warp-, and thread-level parallelism within and across solves for ultra-high
performance. We demonstrate the effectiveness of our approach through a
combination of: simulated benchmarks showing speedups of 18-21x over CPU
baselines and 1.4-16x over GPU baselines as batch size increases; case studies
highlighting improved disturbance rejection and convergence behavior; and
finally a validation on hardware using an industrial manipulator. We open
source GATO to support reproducibility and adoption.

</details>


### [7] [EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments](https://arxiv.org/abs/2510.07700)
*Raghav Mishra,Ian R. Manchester*

Main category: cs.RO

TL;DR: 提出了基于新兴障碍函数的模型扩散约束方法（EB-MBD），通过渐进引入障碍约束解决模型扩散中的性能退化问题，显著提高解的质量，无需计算昂贵的投影操作。


<details>
  <summary>Details</summary>
Motivation: 模型扩散中的约束会导致灾难性的性能退化，即使在简单的2D系统中，由于蒙特卡洛近似得分函数的样本效率低下。需要一种方法来避免这些问题。

Method: 引入新兴障碍模型扩散（EB-MBD），使用渐进引入的障碍约束，分析每轮迭代的采样活跃度来指导障碍参数调度选择。

Result: 在2D碰撞避免和3D水下机械臂系统中，该方法比模型扩散获得更低成本的解，比基于投影的方法计算时间减少数个数量级。

Conclusion: EB-MBD方法有效解决了模型扩散中的约束问题，显著提高了解决方案质量，同时大幅降低了计算成本。

Abstract: We propose enforcing constraints on Model-Based Diffusion by introducing
emerging barrier functions inspired by interior point methods. We show that
constraints on Model-Based Diffusion can lead to catastrophic performance
degradation, even on simple 2D systems due to sample inefficiency in the Monte
Carlo approximation of the score function. We introduce Emerging-Barrier
Model-Based Diffusion (EB-MBD) which uses progressively introduced barrier
constraints to avoid these problems, significantly improving solution quality,
without the need for computationally expensive operations such as projections.
We analyze the sampling liveliness of samples each iteration to inform barrier
parameter scheduling choice. We demonstrate results for 2D collision avoidance
and a 3D underwater manipulator system and show that our method achieves lower
cost solutions than Model-Based Diffusion, and requires orders of magnitude
less computation time than projection based methods.

</details>


### [8] [Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis](https://arxiv.org/abs/2510.07725)
*Kasidit Muenprasitivej,Ye Zhao,Glen Chou*

Main category: cs.RO

TL;DR: 提出一种用于双足机器人在粗糙地形上导航的概率安全规划控制框架，结合高斯过程地形估计和保形预测，确保动态可行性和质心鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人在不确定地形环境下安全导航的挑战，需要同时考虑地形不确定性和动态稳定性。

Method: 采用高斯过程回归估计地形高程，利用保形预测构建校准置信区间，设计基于收缩的可达管和飞轮扭矩控制律来稳定质心角动量。

Result: 通过MuJoCo物理仿真验证了Digit双足机器人的有效性，实现了概率安全性和目标可达性保证。

Conclusion: 该框架为双足机器人在不确定地形上的安全导航提供了理论保证和实用解决方案。

Abstract: We address the challenge of enabling bipedal robots to traverse rough terrain
by developing probabilistically safe planning and control strategies that
ensure dynamic feasibility and centroidal robustness under terrain uncertainty.
Specifically, we propose a high-level Model Predictive Control (MPC) navigation
framework for a bipedal robot with a specified confidence level of safety that
(i) enables safe traversal toward a desired goal location across a terrain map
with uncertain elevations, and (ii) formally incorporates uncertainty bounds
into the centroidal dynamics of locomotion control. To model the rough terrain,
we employ Gaussian Process (GP) regression to estimate elevation maps and
leverage Conformal Prediction (CP) to construct calibrated confidence intervals
that capture the true terrain elevation. Building on this, we formulate
contraction-based reachable tubes that explicitly account for terrain
uncertainty, ensuring state convergence and tube invariance. In addition, we
introduce a contraction-based flywheel torque control law for the reduced-order
Linear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum
about the center-of-mass (CoM). This formulation provides both probabilistic
safety and goal reachability guarantees. For a given confidence level, we
establish the forward invariance of the proposed torque control law by
demonstrating exponential stabilization of the actual CoM phase-space
trajectory and the desired trajectory prescribed by the high-level planner.
Finally, we evaluate the effectiveness of our planning framework through
physics-based simulations of the Digit bipedal robot in MuJoCo.

</details>


### [9] [Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework](https://arxiv.org/abs/2510.07749)
*Alexandre Moreira Nascimento,Gabriel Kenji Godoy Shimanuki,Lúcio Flavio Vismari,João Batista Camargo Jr,Jorge Rady de Almeida Jr,Paulo Sergio Cugnasca,Anna Carolina Muller Queiroz,Jeremy Noah Bailenson*

Main category: cs.RO

TL;DR: 该论文提出了一种可配置、组件无关的幻觉注入框架，通过将自动驾驶车辆感知故障重新定义为幻觉，在开源模拟器中注入六种可信的幻觉类型，评估其对安全性的影响。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的感知故障是导致事故的主要原因，但现有故障注入研究通常针对单一传感器或感知模块，难以推广或集成到统一模拟环境中。

Method: 将感知故障重新定义为幻觉，提出可配置、组件无关的幻觉注入框架，在开源模拟器中注入六种幻觉类型，执行超过18,350次模拟测试。

Result: 某些幻觉类型（如感知延迟和漂移）在测试场景中显著增加碰撞风险，验证了该框架能够有效评估自动驾驶系统安全性。

Conclusion: 该框架提供了一个可扩展、统计验证、组件无关且完全互操作的工具集，可简化和加速自动驾驶安全验证，为容错和弹性自动驾驶设计奠定基础。

Abstract: Perception failures in autonomous vehicles (AV) remain a major safety concern
because they are the basis for many accidents. To study how these failures
affect safety, researchers typically inject artificial faults into hardware or
software components and observe the outcomes. However, existing fault injection
studies often target a single sensor or machine perception (MP) module,
resulting in siloed frameworks that are difficult to generalize or integrate
into unified simulation environments. This work addresses that limitation by
reframing perception failures as hallucinations, false perceptions that distort
an AV situational awareness and may trigger unsafe control actions. Since
hallucinations describe only observable effects, this abstraction enables
analysis independent of specific sensors or algorithms, focusing instead on how
their faults manifest along the MP pipeline. Building on this concept, we
propose a configurable, component-agnostic hallucination injection framework
that induces six plausible hallucination types in an iterative open-source
simulator. More than 18,350 simulations were executed in which hallucinations
were injected while AVs crossed an unsignalized transverse street with traffic.
The results statistically validate the framework and quantify the impact of
each hallucination type on collisions and near misses. Certain hallucinations,
such as perceptual latency and drift, significantly increase the risk of
collision in the scenario tested, validating the proposed paradigm can stress
the AV system safety. The framework offers a scalable, statistically validated,
component agnostic, and fully interoperable toolset that simplifies and
accelerates AV safety validations, even those with novel MP architectures and
components. It can potentially reduce the time-to-market of AV and lay the
foundation for future research on fault tolerance, and resilient AV design.

</details>


### [10] [Trajectory Conditioned Cross-embodiment Skill Transfer](https://arxiv.org/abs/2510.07773)
*YuHang Tang,Yixuan Lou,Pengfei Han,Haoming Song,Xinyi Ye,Dong Wang,Bin Zhao*

Main category: cs.RO

TL;DR: TrajSkill是一个从人类演示视频直接学习机器人操作技能的框架，通过将人类运动表示为稀疏光流轨迹来实现跨形态技能迁移，无需配对数据集或手工奖励。


<details>
  <summary>Details</summary>
Motivation: 解决人类演示视频与机器人操作器之间的形态差异问题，现有方法依赖配对数据集或手工奖励，限制了可扩展性和泛化能力。

Method: 将人类运动表示为稀疏光流轨迹作为形态无关的运动线索，结合视觉和文本输入，联合合成时间一致的机器人操作视频并转换为可执行动作。

Result: 在MetaWorld仿真数据上，FVD降低39.6%，KVD降低36.6%，跨形态成功率提升16.7%；真实机器人厨房操作任务验证了方法的有效性。

Conclusion: TrajSkill实现了从人类演示到机器人的跨形态技能迁移，在仿真和真实环境中都表现出色。

Abstract: Learning manipulation skills from human demonstration videos presents a
promising yet challenging problem, primarily due to the significant embodiment
gap between human body and robot manipulators. Existing methods rely on paired
datasets or hand-crafted rewards, which limit scalability and generalization.
We propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment
Skill Transfer, enabling robots to acquire manipulation skills directly from
human demonstration videos. Our key insight is to represent human motions as
sparse optical flow trajectories, which serve as embodiment-agnostic motion
cues by removing morphological variations while preserving essential dynamics.
Conditioned on these trajectories together with visual and textual inputs,
TrajSkill jointly synthesizes temporally consistent robot manipulation videos
and translates them into executable actions, thereby achieving cross-embodiment
skill transfer. Extensive experiments are conducted, and the results on
simulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\% and KVD
by 36.6\% compared with the state-of-the-art, and improves cross-embodiment
success rate by up to 16.7\%. Real-robot experiments in kitchen manipulation
tasks further validate the effectiveness of our approach, demonstrating
practical human-to-robot skill transfer across embodiments.

</details>


### [11] [IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction](https://arxiv.org/abs/2510.07778)
*Yandu Chen,Kefan Gu,Yuqing Wen,Yucheng Zhao,Tiancai Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: IntentionVLA是一个视觉-语言-动作模型框架，通过课程训练范式赋予模型推理和感知能力，在间接指令下实现快速推理，显著提升了人机交互的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的VLA模型在多模态任务上预训练，但缺乏推理密集型预训练和推理引导的操作，无法执行复杂现实交互所需的隐式人类意图推理。

Method: 首先使用精心设计的推理数据（结合意图推理、空间定位和紧凑体现推理）进行预训练，然后在微调阶段使用紧凑推理输出作为动作生成的上下文指导。

Result: IntentionVLA显著优于π0，直接指令下成功率提高18%，意图指令下比ECoT提高28%。在分布外意图任务上，成功率是基线的两倍以上，零样本人机交互成功率40%。

Conclusion: IntentionVLA是下一代人机交互系统的有前景范式，通过推理能力增强了VLA模型在复杂交互场景中的表现。

Abstract: Vision-Language-Action (VLA) models leverage pretrained vision-language
models (VLMs) to couple perception with robotic control, offering a promising
path toward general-purpose embodied intelligence. However, current SOTA VLAs
are primarily pretrained on multimodal tasks with limited relevance to embodied
scenarios, and then finetuned to map explicit instructions to actions.
Consequently, due to the lack of reasoning-intensive pretraining and
reasoning-guided manipulation, these models are unable to perform implicit
human intention reasoning required for complex, real-world interactions. To
overcome these limitations, we propose \textbf{IntentionVLA}, a VLA framework
with a curriculum training paradigm and an efficient inference mechanism. Our
proposed method first leverages carefully designed reasoning data that combine
intention inference, spatial grounding, and compact embodied reasoning,
endowing the model with both reasoning and perception capabilities. In the
following finetuning stage, IntentionVLA employs the compact reasoning outputs
as contextual guidance for action generation, enabling fast inference under
indirect instructions. Experimental results show that IntentionVLA
substantially outperforms $\pi_0$, achieving 18\% higher success rates with
direct instructions and 28\% higher than ECoT under intention instructions. On
out-of-distribution intention tasks, IntentionVLA achieves over twice the
success rate of all baselines, and further enables zero-shot human-robot
interaction with 40\% success rate. These results highlight IntentionVLA as a
promising paradigm for next-generation human-robot interaction (HRI) systems.

</details>


### [12] [GM3: A General Physical Model for Micro-Mobility Vehicles](https://arxiv.org/abs/2510.07807)
*Grace Cai,Nithin Parepally,Laura Zheng,Ming C. Lin*

Main category: cs.RO

TL;DR: 提出了GM3（广义微移动模型），一种基于轮胎刷表示的统一物理模型，用于模拟各种微移动车辆的动力学特性，包括轮胎滑移、载荷转移和骑手/车辆倾斜等被传统模型忽略的因素。


<details>
  <summary>Details</summary>
Motivation: 现有的微移动车辆动力学建模工具主要依赖运动学自行车模型或其变体，这些模型忽略了轮胎滑移、载荷转移和骑手/车辆倾斜等重要物理效应，且缺乏统一的跨平台建模方法。

Method: 开发了基于轮胎刷表示的轮胎级公式化GM3模型，支持任意轮式配置；构建了交互式模型无关仿真框架，包含固定步长RK4积分、人在回路和脚本控制、实时轨迹跟踪和分析日志记录。

Result: 在斯坦福无人机数据集的死亡圆环（环形交叉口）场景中，对自行车、滑板车和手推车类别进行了GM3模型的实证验证。

Conclusion: GM3提供了一个统一的物理基础模型，能够准确捕捉各种微移动车辆的动力学特性，弥补了现有建模工具的不足。

Abstract: Modeling the dynamics of micro-mobility vehicles (MMV) is becoming
increasingly important for training autonomous vehicle systems and building
urban traffic simulations. However, mainstream tools rely on variants of the
Kinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,
load transfer, and rider/vehicle lean. To our knowledge, no unified,
physics-based model captures these dynamics across the full range of common
MMVs and wheel layouts. We propose the "Generalized Micro-mobility Model"
(GM3), a tire-level formulation based on the tire brush representation that
supports arbitrary wheel configurations, including single/double track and
multi-wheel platforms. We introduce an interactive model-agnostic simulation
framework that decouples vehicle/layout specification from dynamics to compare
the GM3 with the KBM and other models, consisting of fixed step RK4
integration, human-in-the-loop and scripted control, real-time trajectory
traces and logging for analysis. We also empirically validate the GM3 on the
Stanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and
cart classes.

</details>


### [13] [USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots](https://arxiv.org/abs/2510.07869)
*Junwen Gu,Zhiheng wu,Pengxuan Si,Shuang Qiu,Yukai Feng,Luoyang Sun,Laien Luo,Lianyi Yu,Jian Wang,Zhengxing Wu*

Main category: cs.RO

TL;DR: USIM是一个基于仿真的水下机器人多任务视觉-语言-动作数据集，包含超过56.1万帧数据，覆盖20个任务和9种场景。基于此提出的U0模型通过多模态融合和感知聚焦增强模块，在各项任务中达到80%的成功率，在移动操作任务中比基线方法减少21.2%的目标距离。


<details>
  <summary>Details</summary>
Motivation: 水下环境对机器人操作带来独特挑战，包括复杂流体动力学、有限能见度和受限通信。虽然数据驱动方法在陆地机器人中推动了具身智能发展，但由于缺乏大规模高质量水下数据集，开发能够自主执行多任务的水下智能系统仍然极具挑战性。

Method: 提出USIM仿真数据集和U0视觉-语言-动作模型。U0通过多模态融合整合双目视觉和其他传感器模态，并引入基于卷积注意力的感知聚焦增强模块来提升空间理解和移动操作能力。

Result: 在检查、避障、扫描和动态跟踪等任务中，框架达到80%的成功率。在具有挑战性的移动操作任务中，与基线方法相比，到目标的距离减少了21.2%。

Conclusion: USIM和U0表明视觉-语言-动作模型可以有效地应用于水下机器人应用，为可扩展数据集构建、改进任务自主性以及智能通用水下机器人的实际实现提供了基础。

Abstract: Underwater environments present unique challenges for robotic operation,
including complex hydrodynamics, limited visibility, and constrained
communication. Although data-driven approaches have advanced embodied
intelligence in terrestrial robots and enabled task-specific autonomous
underwater robots, developing underwater intelligence capable of autonomously
performing multiple tasks remains highly challenging, as large-scale,
high-quality underwater datasets are still scarce. To address these
limitations, we introduce USIM, a simulation-based multi-task
Vision-Language-Action (VLA) dataset for underwater robots. USIM comprises over
561K frames from 1,852 trajectories, totaling approximately 15.6 hours of
BlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from
visual navigation to mobile manipulation. Building upon this dataset, we
propose U0, a VLA model for general underwater robots, which integrates
binocular vision and other sensor modalities through multimodal fusion, and
further incorporates a convolution-attention-based perception focus enhancement
module (CAP) to improve spatial understanding and mobile manipulation. Across
tasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,
the framework achieves a success rate of 80%, while in challenging mobile
manipulation tasks, it reduces the distance to the target by 21.2% compared
with baseline methods, demonstrating its effectiveness. USIM and U0 show that
VLA models can be effectively applied to underwater robotic applications,
providing a foundation for scalable dataset construction, improved task
autonomy, and the practical realization of intelligent general underwater
robots.

</details>


### [14] [Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track](https://arxiv.org/abs/2510.07871)
*Erjia Xiao,Lingfeng Zhang,Yingbo Tang,Hao Cheng,Renjing Xu,Wenbo Ding,Lei Zhou,Long Chen,Hangjun Ye,Xiaoshuai Hao*

Main category: cs.RO

TL;DR: 本文介绍了参加IROS 2025 RoboSense挑战赛社会导航赛道的技术方案，通过在Falcon模型基础上增加主动风险感知模块，提升机器人在动态人群环境中的社会合规导航能力，在16支参赛队伍中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 开发能够在动态人群环境中安全、高效且符合社会规范的自主导航系统，仅使用机载RGB-D传感器和里程计，无需全局地图或特权信息。

Method: 在Falcon模型基础上引入主动风险感知模块，学习预测周围人类基于距离的碰撞风险分数，增强空间感知和主动避碰行为。

Result: 在Social-HM3D基准测试中，该方法提高了机器人在拥挤室内场景中保持个人空间合规性的能力，在挑战赛中获得了第二名。

Conclusion: 提出的主动风险感知模块有效提升了社会导航性能，使机器人能够在动态人类环境中更稳健地导航。

Abstract: In this report, we describe the technical details of our submission to the
IROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on
developing RGBD-based perception and navigation systems that enable autonomous
agents to navigate safely, efficiently, and socially compliantly in dynamic
human-populated indoor environments. The challenge requires agents to operate
from an egocentric perspective using only onboard sensors including RGB-D
observations and odometry, without access to global maps or privileged
information, while maintaining social norm compliance such as safe distances
and collision avoidance. Building upon the Falcon model, we introduce a
Proactive Risk Perception Module to enhance social navigation performance. Our
approach augments Falcon with collision risk understanding that learns to
predict distance-based collision risk scores for surrounding humans, which
enables the agent to develop more robust spatial awareness and proactive
collision avoidance behaviors. The evaluation on the Social-HM3D benchmark
demonstrates that our method improves the agent's ability to maintain personal
space compliance while navigating toward goals in crowded indoor scenes with
dynamic human agents, achieving 2nd place among 16 participating teams in the
challenge.

</details>


### [15] [Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation](https://arxiv.org/abs/2510.07975)
*Mingyang Sun,Jiude Wei,Qichen He,Donglin Wang,Cewu Lu,Jianhua Sun*

Main category: cs.RO

TL;DR: GRACE框架通过可执行分析概念（EAC）弥合视觉语言模型语义推理与机器人物理执行之间的差距，实现精确和通用的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在语义推理和任务规划方面的能力与机器人物理执行之间的"语义到物理"差距，使机器人能够在非结构化环境中进行精确和通用的操作。

Method: 引入GRACE框架，通过可执行分析概念（EAC）将自然语言指令和视觉信息转化为实例化的EAC蓝图，从中推导抓取姿态、力方向和物理可行的运动轨迹。

Result: 在模拟和真实环境中对多种铰接物体实现了强大的零样本泛化能力，无需任务特定训练。

Conclusion: GRACE提供了一个统一且可解释的接口，通过语义-物理基础有效实现了精确和可泛化的机器人操作。

Abstract: Enabling robots to perform precise and generalized manipulation in
unstructured environments remains a fundamental challenge in embodied AI. While
Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
semantic reasoning and task planning, a significant gap persists between their
high-level understanding and the precise physical execution required for
real-world manipulation. To bridge this "semantic-to-physical" gap, we
introduce GRACE, a novel framework that grounds VLM-based reasoning through
executable analytic concepts (EAC)-mathematically defined blueprints that
encode object affordances, geometric constraints, and semantics of
manipulation. Our approach integrates a structured policy scaffolding pipeline
that turn natural language instructions and visual information into an
instantiated EAC, from which we derive grasp poses, force directions and plan
physically feasible motion trajectory for robot execution. GRACE thus provides
a unified and interpretable interface between high-level instruction
understanding and low-level robot control, effectively enabling precise and
generalizable manipulation through semantic-physical grounding. Extensive
experiments demonstrate that GRACE achieves strong zero-shot generalization
across a variety of articulated objects in both simulated and real-world
environments, without requiring task-specific training.

</details>


### [16] [Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints](https://arxiv.org/abs/2510.07986)
*Gaofeng Li,Peisen Xu,Ruize Wang,Qi Ye,Jiming Chen,Dezhen Song,Yanlong Huang*

Main category: cs.RO

TL;DR: 提出基于角度-轴空间的方向表示方法，通过加权平均机制在SO(3)流形上融合多个考虑不同局部约束的轨迹，解决非欧几何带来的畸变问题，实现多局部约束的同时整合。


<details>
  <summary>Details</summary>
Motivation: 旋转群SO(3)是黎曼流形，其非欧几何特性导致局部约束整合困难，特别是同时整合多个局部约束时存在畸变问题。

Method: 基于角度-轴表示法提出SO(3)上的加权平均机制，在不同基点考虑不同局部约束生成多个轨迹，然后融合这些轨迹生成平滑轨迹。

Result: 仿真和实验验证表明，该方法不仅能适应任意期望路径点的方向并处理角加速度约束，还能同时整合多个局部约束获得额外收益，如实现更小的加速度成本。

Conclusion: 该方法解决了SO(3)流形的畸变问题，使现成的欧几里得学习算法在非欧空间中重新适用，有效实现了多局部约束的同时整合。

Abstract: Orientation learning plays a pivotal role in many tasks. However, the
rotation group SO(3) is a Riemannian manifold. As a result, the distortion
caused by non-Euclidean geometric nature introduces difficulties to the
incorporation of local constraints, especially for the simultaneous
incorporation of multiple local constraints. To address this issue, we propose
the Angle-Axis Space-based orientation representation method to solve several
orientation learning problems, including orientation adaptation and
minimization of angular acceleration. Specifically, we propose a weighted
average mechanism in SO(3) based on the angle-axis representation method. Our
main idea is to generate multiple trajectories by considering different local
constraints at different basepoints. Then these multiple trajectories are fused
to generate a smooth trajectory by our proposed weighted average mechanism,
achieving the goal to incorporate multiple local constraints simultaneously.
Compared with existing solution, ours can address the distortion issue and make
the off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean
space. Simulation and Experimental evaluations validate that our solution can
not only adapt orientations towards arbitrary desired via-points and cope with
angular acceleration constraints, but also incorporate multiple local
constraints simultaneously to achieve extra benefits, e.g., achieving smaller
acceleration costs.

</details>


### [17] [FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset](https://arxiv.org/abs/2510.08022)
*Kehui Liu,Zhongjie Jia,Yang Li,Zhaxizhuoma,Pengan Chen,Song Liu,Xin Liu,Pingrui Zhang,Haoming Song,Xinyi Ye,Nieqing Cao,Zhigang Wang,Jia Zeng,Dong Wang,Yan Ding,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: FastUMI-100K是一个大规模UMI风格的多模态演示数据集，包含超过10万条轨迹，涵盖54个任务和数百种物体类型，用于解决机器人操作学习中的数据限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类遥操作机器人收集的数据集在可扩展性、轨迹平滑性和不同机器人形态的适用性方面存在限制，无法满足现实世界复杂操作任务的需求。

Method: 使用FastUMI机器人系统收集数据，该系统采用模块化、硬件解耦的机械设计和集成轻量级跟踪系统，包含末端执行器状态、多视角鱼眼图像和文本注释等多模态数据流。

Result: 实验结果表明，FastUMI-100K在各种基线算法上都能实现高策略成功率，证明了其鲁棒性、适应性和现实世界适用性。

Conclusion: FastUMI-100K提供了一个更可扩展、灵活和适应性强的解决方案，能够满足现实世界机器人演示数据的多样化需求，解决了复杂动态操作挑战。

Abstract: Data-driven robotic manipulation learning depends on large-scale,
high-quality expert demonstration datasets. However, existing datasets, which
primarily rely on human teleoperated robot collection, are limited in terms of
scalability, trajectory smoothness, and applicability across different robotic
embodiments in real-world environments. In this paper, we present FastUMI-100K,
a large-scale UMI-style multimodal demonstration dataset, designed to overcome
these limitations and meet the growing complexity of real-world manipulation
tasks. Collected by FastUMI, a novel robotic system featuring a modular,
hardware-decoupled mechanical design and an integrated lightweight tracking
system, FastUMI-100K offers a more scalable, flexible, and adaptable solution
to fulfill the diverse requirements of real-world robot demonstration data.
Specifically, FastUMI-100K contains over 100K+ demonstration trajectories
collected across representative household environments, covering 54 tasks and
hundreds of object types. Our dataset integrates multimodal streams, including
end-effector states, multi-view wrist-mounted fisheye images and textual
annotations. Each trajectory has a length ranging from 120 to 500 frames.
Experimental results demonstrate that FastUMI-100K enables high policy success
rates across various baseline algorithms, confirming its robustness,
adaptability, and real-world applicability for solving complex, dynamic
manipulation challenges. The source code and dataset will be released in this
link https://github.com/MrKeee/FastUMI-100K.

</details>


### [18] [Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)](https://arxiv.org/abs/2510.08118)
*Massimiliano de Leoni,Faizan Ahmed Khan,Simone Agostinelli*

Main category: cs.RO

TL;DR: 本文提出了一种基于聚类的技术，用于从用户界面日志中提取常规日志，特别针对存在执行不一致性（噪声）的场景，实验证明该方法比现有技术能提取更准确的常规日志。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多不直接关注模型发现，仅提取常规操作集合，且未在存在执行不一致性（噪声）的场景下进行评估，这反映了人类执行的自然变异性和偶然错误。

Method: 采用基于聚类的技术从用户界面日志中提取常规日志，并在九个具有不同噪声水平的UI日志上进行实验。

Result: 与现有技术相比，该方法能够提取更准确的常规日志，特别是在存在噪声的情况下，通过标准最先进指标评估显示效果更好。

Conclusion: 提出的聚类技术能够有效提取常规日志，尤其在噪声环境下表现优于现有方法，为机器人流程自动化提供了更好的模型发现能力。

Abstract: Robotic Process Mining focuses on the identification of the routine types
performed by human resources through a User Interface. The ultimate goal is to
discover routine-type models to enable robotic process automation. The
discovery of routine-type models requires the provision of a routine log.
Unfortunately, the vast majority of existing works do not directly focus on
enabling the model discovery, limiting themselves to extracting the set of
actions that are part of the routines. They were also not evaluated in
scenarios characterized by inconsistent routine execution, hereafter referred
to as noise, which reflects natural variability and occasional errors in human
performance. This paper presents a clustering-based technique that aims to
extract routine logs. Experiments were conducted on nine UI logs from the
literature with different levels of injected noise. Our technique was compared
with existing techniques, most of which are not meant to discover routine logs
but were adapted for the purpose. The results were evaluated through standard
state-of-the-art metrics, showing that we can extract more accurate routine
logs than what the state of the art could, especially in the presence of noise.

</details>


### [19] [Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots](https://arxiv.org/abs/2510.08270)
*Damir Nurtdinov,Aliaksei Korshuk,Alexei Kornaev,Alexander Maloletov*

Main category: cs.RO

TL;DR: 本文比较了经典PID控制器与现代强化学习算法在欠约束线驱动并联机器人控制中的性能，发现TRPO在多种轨迹下表现最佳，具有最低的RMS误差和对更大控制更新间隔的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估经典和现代控制方法在真实世界线驱动并联机器人中的性能，特别关注欠约束系统和有限时间离散化条件下的控制效果。

Method: 对经典PID控制器和现代强化学习算法（包括DDPG、PPO和TRPO）进行对比分析，评估它们在各种轨迹下的控制性能。

Result: TRPO在所有方法中表现最优，实现了最低的RMS误差，并且在更大控制更新间隔下仍保持鲁棒性，能够平衡探索与利用，在噪声环境中实现稳定控制。

Conclusion: TRPO作为复杂机器人控制任务的鲁棒解决方案具有巨大潜力，对动态环境和未来传感器融合或混合控制策略应用具有重要意义。

Abstract: This study evaluates the performance of classical and modern control methods
for real-world Cable-Driven Parallel Robots (CDPRs), focusing on
underconstrained systems with limited time discretization. A comparative
analysis is conducted between classical PID controllers and modern
reinforcement learning algorithms, including Deep Deterministic Policy Gradient
(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy
Optimization (TRPO). The results demonstrate that TRPO outperforms other
methods, achieving the lowest root mean square (RMS) errors across various
trajectories and exhibiting robustness to larger time intervals between control
updates. TRPO's ability to balance exploration and exploitation enables stable
control in noisy, real-world environments, reducing reliance on high-frequency
sensor feedback and computational demands. These findings highlight TRPO's
potential as a robust solution for complex robotic control tasks, with
implications for dynamic environments and future applications in sensor fusion
or hybrid control strategies.

</details>


### [20] [Airy: Reading Robot Intent through Height and Sky](https://arxiv.org/abs/2510.08381)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: Airy是一个艺术装置，通过两个强化学习训练的机械臂竞争将床单抛向空中的表演，让观众直观理解复杂多智能体AI的决策过程。


<details>
  <summary>Details</summary>
Motivation: 随着工业机器人进入共享人类空间，其不透明的决策过程威胁着安全、信任和公众监督。该作品旨在探索复杂多智能体AI是否能够变得直观可理解。

Method: 基于三个设计原则：竞争作为清晰指标（谁抛得更高）、具身熟悉度（观众能识别布料抛掷动作）、传感器到感知映射（通过森林和天气投影显示机器人合作或竞争关系）。

Result: 在五个国际展览中的观察表明，观众能够实时解读机器人的策略、冲突和合作，情感反应与系统内部状态相符。

Conclusion: 该项目展示了感官隐喻如何将黑盒系统转变为公共接口，使复杂AI决策变得可感知和理解。

Abstract: As industrial robots move into shared human spaces, their opaque decision
making threatens safety, trust, and public oversight. This artwork, Airy, asks
whether complex multi agent AI can become intuitively understandable by staging
a competition between two reinforcement trained robot arms that snap a bedsheet
skyward. Building on three design principles, competition as a clear metric
(who lifts higher), embodied familiarity (audiences recognize fabric snapping),
and sensor to sense mapping (robot cooperation or rivalry shown through forest
and weather projections), the installation gives viewers a visceral way to read
machine intent. Observations from five international exhibitions indicate that
audiences consistently read the robots' strategies, conflict, and cooperation
in real time, with emotional reactions that mirror the system's internal state.
The project shows how sensory metaphors can turn a black box into a public
interface.

</details>


### [21] [Reliability of Single-Level Equality-Constrained Inverse Optimal Control](https://arxiv.org/abs/2510.08406)
*Filip Bečanović,Kosta Jovanović,Vincent Bonnet*

Main category: cs.RO

TL;DR: 本文提出了一种基于单级重构的逆最优控制方法，相比传统的双层方法，计算速度提升15倍，同时对噪声具有更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的逆最优控制方法要么基于缓慢的双层过程，要么基于快速但对噪声敏感的优化条件违反最小化方法。本文旨在开发一种既快速又鲁棒的IOC方法。

Method: 采用单级重构方法替代传统的双层IOC方法，基于等式约束的最优控制模型来重构问题。

Result: 在平面到达任务的数值实验中，该方法对非常大的噪声水平表现出弹性，计算时间比经典双层实现减少了15倍。

Conclusion: 单级重构方法在保持结果等效性的同时，显著提高了计算效率和噪声鲁棒性，为逆最优控制提供了一种更实用的解决方案。

Abstract: Inverse optimal control (IOC) allows the retrieval of optimal cost function
weights, or behavioral parameters, from human motion. The literature on IOC
uses methods that are either based on a slow bilevel process or a fast but
noise-sensitive minimization of optimality condition violation. Assuming
equality-constrained optimal control models of human motion, this article
presents a faster but robust approach to solving IOC using a single-level
reformulation of the bilevel method and yields equivalent results. Through
numerical experiments in simulation, we analyze the robustness to noise of the
proposed single-level reformulation to the bilevel IOC formulation with a
human-like planar reaching task that is used across recent studies. The
approach shows resilience to very large levels of noise and reduces the
computation time of the IOC on this task by a factor of 15 when compared to a
classical bilevel implementation.

</details>


### [22] [Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software](https://arxiv.org/abs/2510.08408)
*Bibekananda Patra,Rajeevlochana G. Chittawadigi,Sandipan Bandyopadhyay*

Main category: cs.RO

TL;DR: 提出了一种使用CAD软件API验证6-6 Stewart-Gough平台机械臂最大无碰撞球体尺寸的方法，通过自动化更新移动平台位置来检测腿部之间的碰撞。


<details>
  <summary>Details</summary>
Motivation: 需要验证并联机械臂在给定移动平台方向下的最大无碰撞工作空间，确保操作安全性。

Method: 利用CAD软件API自动化更新移动平台位置，在无碰撞球体表面的壳层内采样，检测每对腿部之间的相互碰撞。

Result: 该方法能够验证预计算的无碰撞球体安全性，并估计任何空间并联机械臂的无碰撞工作空间。

Conclusion: 所提出的基于CAD API的验证方法为并联机械臂的无碰撞工作空间分析提供了一种有效的自动化解决方案。

Abstract: This paper presents a method of validation of the size of the largest
collision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)
for a given orientation of its moving platform (MP) using the Application
Programming Interface (API) of a CAD software. The position of the MP is
updated via the API in an automated manner over a set of samples within a shell
enclosing the surface of the CFS. For each pose of the manipulator, each pair
of legs is investigated for mutual collisions. The CFS is considered safe or
validated iff none of the points falling inside the CFS lead to a collision
between any pair of legs. This approach can not only validate the safety of a
precomputed CFS, but also estimate the same for any spatial parallel
manipulator.

</details>


### [23] [Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered](https://arxiv.org/abs/2510.08464)
*Jason Jabbour,Dong-Ki Kim,Max Smith,Jay Patrikar,Radhika Ghosal,Youhui Wang,Ali Agha,Vijay Janapa Reddi,Shayegan Omidshafiei*

Main category: cs.RO

TL;DR: GLUESTICK是一种后剪枝恢复方法，用于解决视觉-语言-动作模型剪枝后性能急剧下降和安全违规增加的问题，通过权重空间插值恢复模型功能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在资源受限硬件上部署困难，现有剪枝方法会导致模型性能严重下降和安全违规增加，需要一种有效的恢复方法。

Method: 在权重空间中对密集模型和剪枝模型进行一次插值计算校正项，在推理时由每个剪枝层使用该校正项恢复丢失的能力，引入单个超参数控制效率与准确性的权衡。

Result: 在多种VLA架构和操作导航任务中，GLUESTICK实现了有竞争力的内存效率，同时显著恢复了成功率并减少了安全违规。

Conclusion: GLUESTICK提供了一种无需额外训练、与剪枝算法无关的有效方法，能够在保持稀疏性优势的同时恢复剪枝VLA模型的功能。

Abstract: Vision-Language-Action (VLA) models have advanced robotic capabilities but
remain challenging to deploy on resource-limited hardware. Pruning has enabled
efficient compression of large language models (LLMs), yet it is largely
understudied in robotics. Surprisingly, we observe that pruning VLA models
leads to drastic degradation and increased safety violations. We introduce
GLUESTICK, a post-pruning recovery method that restores much of the original
model's functionality while retaining sparsity benefits. Our method performs a
one-time interpolation between the dense and pruned models in weight-space to
compute a corrective term. This correction is used during inference by each
pruned layer to recover lost capabilities with minimal overhead. GLUESTICK
requires no additional training, is agnostic to the pruning algorithm, and
introduces a single hyperparameter that controls the tradeoff between
efficiency and accuracy. Across diverse VLA architectures and tasks in
manipulation and navigation, GLUESTICK achieves competitive memory efficiency
while substantially recovering success rates and reducing safety violations.
Additional material can be found at: https://gluestick-vla.github.io/.

</details>


### [24] [DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos](https://arxiv.org/abs/2510.08475)
*Jhen Hsieh,Kuan-Hsun Tu,Kuo-Han Hung,Tsung-Wei Ke*

Main category: cs.RO

TL;DR: DexMan是一个自动化框架，可将人类视觉演示转换为仿真中人形机器人的双手灵巧操作技能，无需相机标定、深度传感器或3D对象资产。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法仅考虑简化浮动手的问题，直接控制人形机器人，并利用基于接触的奖励从野外视频的噪声手-物体姿态估计中改进策略学习。

Method: 直接在人类操作刚性物体的第三人称视频上操作，使用基于接触的奖励来改进策略学习，无需手动数据收集和昂贵的动作捕捉。

Result: 在TACO基准测试中物体姿态估计达到最先进性能，ADD-S和VSD分别提升0.08和0.12；在OakInk-v2上成功率比先前方法提高19%。

Conclusion: DexMan能够从真实和合成视频生成技能，无需手动数据收集，为训练通用灵巧操作创建大规模多样化数据集。

Abstract: We present DexMan, an automated framework that converts human visual
demonstrations into bimanual dexterous manipulation skills for humanoid robots
in simulation. Operating directly on third-person videos of humans manipulating
rigid objects, DexMan eliminates the need for camera calibration, depth
sensors, scanned 3D object assets, or ground-truth hand and object motion
annotations. Unlike prior approaches that consider only simplified floating
hands, it directly controls a humanoid robot and leverages novel contact-based
rewards to improve policy learning from noisy hand-object poses estimated from
in-the-wild videos.
  DexMan achieves state-of-the-art performance in object pose estimation on the
TACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.
Meanwhile, its reinforcement learning policy surpasses previous methods by 19%
in success rate on OakInk-v2. Furthermore, DexMan can generate skills from both
real and synthetic videos, without the need for manual data collection and
costly motion capture, and enabling the creation of large-scale, diverse
datasets for training generalist dexterous manipulation.

</details>


### [25] [R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation](https://arxiv.org/abs/2510.08547)
*Xiuwei Xu,Angyuan Ma,Hankun Li,Bingyao Yu,Zheng Zhu,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出R2RGen框架，通过真实到真实的3D数据生成增强点云观测-动作对，无需模拟器和渲染，实现高效即插即用的空间泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中的空间泛化问题，传统方法需要大量人工演示覆盖不同空间配置，现有数据生成方法存在显著的模拟到真实差距，且受限于固定基座场景和预定义相机视角。

Method: 提出R2RGen框架，包括：1）细粒度场景和轨迹解析的标注机制；2）处理复杂多对象组合和多样化任务约束的分组增强策略；3）与真实世界3D传感器分布对齐的相机感知处理。

Result: R2RGen在大量实验中显著提高了数据效率，并展示了在移动操作中扩展和应用的强大潜力。

Conclusion: R2RGen是一个无需模拟器和渲染的真实到真实3D数据生成框架，能够高效生成空间多样化的真实世界数据，有效提升机器人操作策略的空间泛化能力。

Abstract: Towards the aim of generalized robotic manipulation, spatial generalization
is the most fundamental capability that requires the policy to work robustly
under different spatial distribution of objects, environment and agent itself.
To achieve this, substantial human demonstrations need to be collected to cover
different spatial configurations for training a generalized visuomotor policy
via imitation learning. Prior works explore a promising direction that
leverages data generation to acquire abundant spatially diverse data from
minimal source demonstrations. However, most approaches face significant
sim-to-real gap and are often limited to constrained settings, such as
fixed-base scenarios and predefined camera viewpoints. In this paper, we
propose a real-to-real 3D data generation framework (R2RGen) that directly
augments the pointcloud observation-action pairs to generate real-world data.
R2RGen is simulator- and rendering-free, thus being efficient and
plug-and-play. Specifically, given a single source demonstration, we introduce
an annotation mechanism for fine-grained parsing of scene and trajectory. A
group-wise augmentation strategy is proposed to handle complex multi-object
compositions and diverse task constraints. We further present camera-aware
processing to align the distribution of generated data with real-world 3D
sensor. Empirically, R2RGen substantially enhances data efficiency on extensive
experiments and demonstrates strong potential for scaling and application on
mobile manipulation.

</details>


### [26] [DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model](https://arxiv.org/abs/2510.08556)
*Xueyi Liu,He Wang,Li Yi*

Main category: cs.RO

TL;DR: 提出一种新颖的sim-to-real框架，通过联合动力学模型有效弥合现实差距，使单一模拟训练策略能够在真实世界中泛化处理各种物体的手内旋转任务。


<details>
  <summary>Details</summary>
Motivation: 解决手内物体旋转任务中sim-to-real的挑战，克服现有方法在物体几何形状、尺寸、手腕姿态等方面的限制，实现更广泛的实际应用。

Method: 使用联合动力学模型学习弥合现实差距，通过因子化关节动力学、压缩系统影响为低维变量，并结合自主数据收集策略获取多样化真实交互数据。

Result: 单一策略成功旋转复杂形状物体（如动物模型）、高宽比物体（达5.33）和小尺寸物体，同时处理多样化手腕方向和旋转轴，展现出前所未有的泛化能力。

Conclusion: 该方法通过高效的数据利用和跨关节动力学建模，有效解决了手内旋转任务的sim-to-real挑战，为复杂操作任务提供了稳健的解决方案。

Abstract: Achieving generalized in-hand object rotation remains a significant challenge
in robotics, largely due to the difficulty of transferring policies from
simulation to the real world. The complex, contact-rich dynamics of dexterous
manipulation create a "reality gap" that has limited prior work to constrained
scenarios involving simple geometries, limited object sizes and aspect ratios,
constrained wrist poses, or customized hands. We address this sim-to-real
challenge with a novel framework that enables a single policy, trained in
simulation, to generalize to a wide variety of objects and conditions in the
real world. The core of our method is a joint-wise dynamics model that learns
to bridge the reality gap by effectively fitting limited amount of real-world
collected data and then adapting the sim policy's actions accordingly. The
model is highly data-efficient and generalizable across different whole-hand
interaction distributions by factorizing dynamics across joints, compressing
system-wide influences into low-dimensional variables, and learning each
joint's evolution from its own dynamic profile, implicitly capturing these net
effects. We pair this with a fully autonomous data collection strategy that
gathers diverse, real-world interaction data with minimal human intervention.
Our complete pipeline demonstrates unprecedented generality: a single policy
successfully rotates challenging objects with complex shapes (e.g., animals),
high aspect ratios (up to 5.33), and small sizes, all while handling diverse
wrist orientations and rotation axes. Comprehensive real-world evaluations and
a teleoperation application for complex tasks validate the effectiveness and
robustness of our approach. Website: https://meowuu7.github.io/DexNDM/

</details>


### [27] [NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos](https://arxiv.org/abs/2510.08568)
*Hongyu Li,Lingfeng Sun,Yafei Hu,Duy Ta,Jennifer Barry,George Konidaris,Jiahui Fu*

Main category: cs.RO

TL;DR: NovaFlow是一个零样本机器人操作框架，能够将任务描述转换为可执行计划，无需演示或特定平台训练，支持刚性、关节和可变形物体的操作。


<details>
  <summary>Details</summary>
Motivation: 实现机器人零样本执行新操作任务是机器人学的核心目标，现有方法通常需要任务分布内数据或特定平台微调，限制了跨平台迁移能力。

Method: 通过视频生成模型将任务描述合成为视频，使用现成感知模块提取3D可操作物体流，对刚性物体计算相对位姿并转化为机器人动作，对可变形物体使用基于粒子的动力学模型进行规划。

Result: 在桌面Franka机械臂和Spot四足移动机器人上验证了刚性、关节和可变形物体操作任务的有效零样本执行，无需演示或特定平台训练。

Conclusion: 通过将任务理解与底层控制解耦，NovaFlow实现了自然的跨平台迁移能力，为机器人零样本操作提供了有效解决方案。

Abstract: Enabling robots to execute novel manipulation tasks zero-shot is a central
goal in robotics. Most existing methods assume in-distribution tasks or rely on
fine-tuning with embodiment-matched data, limiting transfer across platforms.
We present NovaFlow, an autonomous manipulation framework that converts a task
description into an actionable plan for a target robot without any
demonstrations. Given a task description, NovaFlow synthesizes a video using a
video generation model and distills it into 3D actionable object flow using
off-the-shelf perception modules. From the object flow, it computes relative
poses for rigid objects and realizes them as robot actions via grasp proposals
and trajectory optimization. For deformable objects, this flow serves as a
tracking objective for model-based planning with a particle-based dynamics
model. By decoupling task understanding from low-level control, NovaFlow
naturally transfers across embodiments. We validate on rigid, articulated, and
deformable object manipulation tasks using a table-top Franka arm and a Spot
quadrupedal mobile robot, and achieve effective zero-shot execution without
demonstrations or embodiment-specific training. Project website:
https://novaflow.lhy.xyz/.

</details>


### [28] [Scalable Offline Metrics for Autonomous Driving](https://arxiv.org/abs/2510.08571)
*Animikh Aich,Adwait Kulkarni,Eshed Ohn-Bar*

Main category: cs.RO

TL;DR: 本文研究了感知规划模型在自动驾驶中的离线评估与在线性能之间的相关性，发现两者相关性比先前研究更差，并提出基于认知不确定性的离线指标来改善这种相关性。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统的离线评估难以准确预测在线性能，微小的错误可能在测试时累积导致事故，这种离线与在线评估之间的关系在复杂城市场景中缺乏深入研究。

Method: 通过大量模拟实验分析离线与在线评估的相关性，提出基于认知不确定性的离线指标，并在模拟和真实世界环境中验证其有效性。

Result: 发现离线与在线评估的相关性比先前研究更差，提出的基于认知不确定性的离线指标相比先前指标相关性提高了13%以上，在真实世界环境中效果更显著。

Conclusion: 当前自动驾驶策略评估实践存在局限性，基于认知不确定性的离线指标能有效缩小离线与在线评估之间的差距，为更可靠的策略评估提供了新方法。

Abstract: Real-World evaluation of perception-based planning models for robotic
systems, such as autonomous vehicles, can be safely and inexpensively conducted
offline, i.e., by computing model prediction error over a pre-collected
validation dataset with ground-truth annotations. However, extrapolating from
offline model performance to online settings remains a challenge. In these
settings, seemingly minor errors can compound and result in test-time
infractions or collisions. This relationship is understudied, particularly
across diverse closed-loop metrics and complex urban maneuvers. In this work,
we revisit this undervalued question in policy evaluation through an extensive
set of experiments across diverse conditions and metrics. Based on analysis in
simulation, we find an even worse correlation between offline and online
settings than reported by prior studies, casting doubts on the validity of
current evaluation practices and metrics for driving policies. Next, we bridge
the gap between offline and online evaluation. We investigate an offline metric
based on epistemic uncertainty, which aims to capture events that are likely to
cause errors in closed-loop settings. The resulting metric achieves over 13%
improvement in correlation compared to previous offline metrics. We further
validate the generalization of our findings beyond the simulation environment
in real-world settings, where even greater gains are observed.

</details>


### [29] [BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation](https://arxiv.org/abs/2510.08572)
*Rocktim Jyoti Das,Harsh Singh,Diana Turmakhan,Muhammad Abdullah Sohail,Mingfei Han,Preslav Nakov,Fabio Pizzati,Ivan Laptev*

Main category: cs.RO

TL;DR: BLAZER框架通过自动生成训练数据来学习机器人操作策略，利用LLM规划器的零样本能力在模拟环境中自动生成多样化操作任务的演示，成功示例用于微调LLM以提升规划能力，无需人工监督。


<details>
  <summary>Details</summary>
Motivation: 机器人领域缺乏互联网规模的多任务演示数据，现有数据集受限于手动收集和整理，限制了模型的可扩展性和泛化能力。

Method: 基于LLM规划器的零样本能力，在模拟环境中自动生成多样化操作任务的演示，使用成功示例微调LLM以提升规划能力，无需人工监督。

Result: BLAZER显著改善了模拟和真实环境中的零样本操作能力，在训练池外任务上也有提升，并支持LLM模型的下采样。

Conclusion: BLAZER框架通过自动数据生成有效解决了机器人领域数据稀缺问题，提升了操作策略的泛化能力和鲁棒性。

Abstract: Scaling data and models has played a pivotal role in the remarkable progress
of computer vision and language. Inspired by these domains, recent efforts in
robotics have similarly focused on scaling both data and model size to develop
more generalizable and robust policies. However, unlike vision and language,
robotics lacks access to internet-scale demonstrations across diverse robotic
tasks and environments. As a result, the scale of existing datasets typically
suffers from the need for manual data collection and curation. To address this
problem, here we propose BLAZER, a framework that learns manipulation policies
from automatically generated training data. We build on the zero-shot
capabilities of LLM planners and automatically generate demonstrations for
diverse manipulation tasks in simulation. Successful examples are then used to
finetune an LLM and to improve its planning capabilities without human
supervision. Notably, while BLAZER training requires access to the simulator's
state, we demonstrate direct transfer of acquired skills to sensor-based
manipulation. Through extensive experiments, we show BLAZER to significantly
improve zero-shot manipulation in both simulated and real environments.
Moreover, BLAZER improves on tasks outside of its training pool and enables
downscaling of LLM models. Our code and data will be made publicly available on
the project page.

</details>
