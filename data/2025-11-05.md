<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 16]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [TRACE: Textual Reasoning for Affordance Coordinate Extraction](https://arxiv.org/abs/2511.01999)
*Sangyun Park,Jin Kim,Yuchen Cui,Matthew S. Brown*

Main category: cs.RO

TL;DR: TRACE方法通过将文本推理链集成到affordance预测中，显著提升了视觉语言模型在机器人操作任务中的空间推理精度和性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在将高级指令转换为机器人操作所需精确空间affordance方面存在困难，现有视觉思维链方法计算成本高。

Method: 提出TRACE方法，通过自主管道创建包含指令和显式文本推理的大规模数据集，并在该数据上微调视觉语言模型，使其在行动前外部化空间推理。

Result: 在Where2Place基准测试中达到48.1%准确率（相对提升9.6%），在更难的W2P(h)子集达到55.0%。消融研究表明性能与推理数据量直接相关。

Conclusion: 训练视觉语言模型生成文本推理链是提升VLM机器人控制精度、可靠性和可解释性的有效策略。

Abstract: Vision-Language Models (VLMs) struggle to translate high-level instructions
into the precise spatial affordances required for robotic manipulation. While
visual Chain-of-Thought (CoT) methods exist, they are often computationally
intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance
Coordinate Extraction), a novel methodology that integrates a textual Chain of
Reasoning (CoR) into the affordance prediction process. We use this methodology
to create the TRACE dataset, a large-scale collection created via an autonomous
pipeline that pairs instructions with explicit textual rationales. By
fine-tuning a VLM on this data, our model learns to externalize its spatial
reasoning before acting. Our experiments show that our TRACE-tuned model
achieves state-of-the-art performance, reaching 48.1% accuracy on the primary
Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more
challenging W2P(h) subset. Crucially, an ablation study demonstrates that
performance scales directly with the amount of reasoning data used, confirming
the CoR's effectiveness. Furthermore, analysis of the model's attention maps
reveals an interpretable reasoning process where focus shifts dynamically
across reasoning steps. This work shows that training VLMs to generate a
textual CoR is an effective and robust strategy for enhancing the precision,
reliability, and interpretability of VLM-based robot control. Our dataset and
code are available at https://github.com/jink-ucla/TRACE

</details>


### [2] [Stein-based Optimization of Sampling Distributions in Model Predictive Path Integral Control](https://arxiv.org/abs/2511.02015)
*Jace Aldrich,Odest Chadwicke Jenkins*

Main category: cs.RO

TL;DR: 提出了一种结合模型预测路径积分控制与Stein变分梯度下降的新方法SOPPI，通过动态优化采样分布来改进轨迹生成，在多个系统中展示了优于标准MPPI的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MPPI依赖随机采样轨迹，常导致样本不足和次优结果，需要改进采样策略以提高控制性能。

Method: 在MPPI环境步骤之间引入SVGD更新，动态调整噪声分布以形成更优的轨迹表示，称为Stein优化路径积分推理方法。

Result: 在Cart-Pole和二维双足行走任务中验证了方法的有效性，在多种超参数下均优于标准MPPI，且在较低粒子数下仍可行。

Conclusion: 该方法适用于更高自由度系统，并有潜力推动可微分模拟器的新发展。

Abstract: This paper presents a novel method for Model Predictive Path Integral (MPPI)
control that optimizes sample generation towards an optimal trajectory through
Stein Variational Gradient Descent (SVGD). MPPI is traditionally reliant on
randomly sampled trajectories, often by a Gaussian distribution. The result can
lead to sample deprivation, under-representing the space of possible
trajectories, and yield suboptimal results. Through introducing SVGD updates in
between MPPI environment steps, we present Stein-Optimized Path-Integral
Inference (SOPPI), an MPPI/SVGD algorithm that can dynamically update noise
distributions at runtime to shape a more optimal representation without an
excessive increase in computational requirements. We demonstrate the efficacy
of our method systems ranging from a Cart-Pole to a two-dimensional bipedal
walking task, indicating improved performance above standard MPPI across a
range of hyper-parameters and demonstrate feasibility at lower particle counts.
We discuss the applicability of this MPPI/SVGD method to higher
degree-of-freedom systems, as well as its potential to new developments in
state-of-the-art differentiable simulators.

</details>


### [3] [TurboMap: GPU-Accelerated Local Mapping for Visual SLAM](https://arxiv.org/abs/2511.02036)
*Parsa Hosseininejad,Kimia Khabiri,Shishir Gopinath,Soudabeh Mohammadhashemi,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: TurboMap是一个GPU加速和CPU优化的视觉SLAM局部建图模块，通过GPU和CPU优化解决局部建图性能瓶颈，在ORB-SLAM3基础上实现，在EuRoC和TUM-VI数据集上分别获得1.3倍和1.6倍的加速。


<details>
  <summary>Details</summary>
Motivation: 识别视觉SLAM系统中局部建图过程的关键性能瓶颈，通过针对性的GPU和CPU优化来提升建图效率。

Method: 将地图点三角化和融合卸载到GPU，在CPU上加速冗余关键帧剔除，集成GPU加速求解器来加速局部束调整，基于ORB-SLAM3并使用CUDA进行GPU编程。

Result: 在EuRoC数据集上平均加速1.3倍，在TUM-VI数据集上平均加速1.6倍，在桌面和嵌入式平台上均有效，同时保持原始系统的精度。

Conclusion: TurboMap成功解决了视觉SLAM局部建图的性能瓶颈，通过GPU和CPU协同优化实现了显著的加速效果，同时保持了系统精度。

Abstract: This paper presents TurboMap, a GPU-accelerated and CPU-optimized local
mapping module for visual SLAM systems. We identify key performance bottlenecks
in the local mapping process for visual SLAM and address them through targeted
GPU and CPU optimizations. Specifically, we offload map point triangulation and
fusion to the GPU, accelerate redundant keyframe culling on the CPU, and
integrate a GPU-accelerated solver to speed up local bundle adjustment. Our
implementation is built on top of ORB-SLAM3 and leverages CUDA for GPU
programming. The experimental results show that TurboMap achieves an average
speedup of 1.3x in the EuRoC dataset and 1.6x in the TUM-VI dataset in the
local mapping module, on both desktop and embedded platforms, while maintaining
the accuracy of the original system.

</details>


### [4] [TACO: Trajectory-Aware Controller Optimization for Quadrotors](https://arxiv.org/abs/2511.02060)
*Hersh Sanghvi,Spencer Folk,Vijay Kumar,Camillo Jose Taylor*

Main category: cs.RO

TL;DR: TACO是一个实时优化四旋翼控制器参数的框架，通过预测模型和轻量级优化方案，根据参考轨迹和当前状态在线调整控制器增益，显著提升轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼控制器参数通常是固定手动调优的，牺牲了任务特定性能，需要一种能够根据轨迹动态调整参数的方案。

Method: 使用学习预测模型和轻量级优化方案实时优化控制器增益，支持轨迹适应以提高动态可行性，并开发并行化模拟器进行大规模训练。

Result: 实验表明TACO在多种轨迹类型上优于传统静态参数调优，运行速度比黑盒优化基线快几个数量级，在物理四旋翼上实现实时部署，轨迹适应显著降低跟踪误差。

Conclusion: TACO框架能够有效提升四旋翼轨迹跟踪性能，通过实时参数优化和轨迹适应实现更好的控制效果。

Abstract: Controller performance in quadrotor trajectory tracking depends heavily on
parameter tuning, yet standard approaches often rely on fixed, manually tuned
parameters that sacrifice task-specific performance. We present
Trajectory-Aware Controller Optimization (TACO), a framework that adapts
controller parameters online based on the upcoming reference trajectory and
current quadrotor state. TACO employs a learned predictive model and a
lightweight optimization scheme to optimize controller gains in real time with
respect to a broad class of trajectories, and can also be used to adapt
trajectories to improve dynamic feasibility while respecting smoothness
constraints. To enable large-scale training, we also introduce a parallelized
quadrotor simulator supporting fast data collection on diverse trajectories.
Experiments on a variety of trajectory types show that TACO outperforms
conventional, static parameter tuning while operating orders of magnitude
faster than black-box optimization baselines, enabling practical real-time
deployment on a physical quadrotor. Furthermore, we show that adapting
trajectories using TACO significantly reduces the tracking error obtained by
the quadrotor.

</details>


### [5] [A Step Toward World Models: A Survey on Robotic Manipulation](https://arxiv.org/abs/2511.02097)
*Peng-Fei Zhang,Ying Cheng,Xiaofan Sun,Shijie Wang,Lei Zhu,Heng Tao Shen*

Main category: cs.RO

TL;DR: 本文通过综述机器人操作领域的方法，分析了世界模型的核心能力、组件和功能，旨在为开发通用实用的机器人世界模型制定路线图。


<details>
  <summary>Details</summary>
Motivation: 自主智能体需要在复杂动态环境中执行任务，这要求它们理解世界机制和动态，而不仅仅是反应控制或状态复制，因此需要开发能够编码环境状态、捕捉动态并支持预测、规划和推理的世界模型。

Method: 通过综述机器人操作领域的方法，分析具有世界模型核心能力的方法，考察它们在感知、预测和控制中的作用，识别关键挑战和解决方案。

Result: 提炼了真实世界模型应具备的核心组件、能力和功能，澄清了世界模型的定义、范围、架构和基本能力。

Conclusion: 基于分析为开发通用实用的机器人世界模型制定了路线图，强调了世界模型在实现自主智能体复杂任务能力中的关键作用。

Abstract: Autonomous agents are increasingly expected to operate in complex, dynamic,
and uncertain environments, performing tasks such as manipulation, navigation,
and decision-making. Achieving these capabilities requires agents to understand
the underlying mechanisms and dynamics of the world, moving beyond purely
reactive control or simple replication of observed states. This motivates the
development of world models as internal representations that encode
environmental states, capture dynamics, and enable prediction, planning, and
reasoning. Despite growing interest, the definition, scope, architectures, and
essential capabilities of world models remain ambiguous. In this survey, rather
than directly imposing a fixed definition and limiting our scope to methods
explicitly labeled as world models, we examine approaches that exhibit the core
capabilities of world models through a review of methods in robotic
manipulation. We analyze their roles across perception, prediction, and
control, identify key challenges and solutions, and distill the core
components, capabilities, and functions that a real world model should possess.
Building on this analysis, we aim to outline a roadmap for developing
generalizable and practical world models for robotics.

</details>


### [6] [Census-Based Population Autonomy For Distributed Robotic Teaming](https://arxiv.org/abs/2511.02147)
*Tyler M. Paine,Anastasia Bizyaeva,Michael R. Benjamin*

Main category: cs.RO

TL;DR: 本文提出了一种多机器人自主性的分层模型，结合了基于邻居输入加权计数的集体决策和基于多目标行为优化的个体决策，用于提高机器人团队的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统在海洋环境中具有高效完成任务和增强鲁棒性的潜力，但如何建模、分析和设计这些系统以实现协作的全部效益是一个挑战。

Method: 采用分层模型，集体决策使用非线性意见动态模型进行加权计数，个体决策使用区间规划进行多目标行为优化，并引入了子群分配的分布式优化方法。

Result: 该模型能够恢复分布式优化和控制的基础算法，同时支持新的集体行为类型，在自适应采样、高价值单元保护和夺旗游戏三个实验中得到了验证。

Conclusion: 提出的分层模型为多机器人系统提供了有效的建模和分析框架，能够实现复杂的协作行为，在真实场景中具有实用价值。

Abstract: Collaborating teams of robots show promise due in their ability to complete
missions more efficiently and with improved robustness, attributes that are
particularly useful for systems operating in marine environments. A key issue
is how to model, analyze, and design these multi-robot systems to realize the
full benefits of collaboration, a challenging task since the domain of
multi-robot autonomy encompasses both collective and individual behaviors. This
paper introduces a layered model of multi-robot autonomy that uses the
principle of census, or a weighted count of the inputs from neighbors, for
collective decision-making about teaming, coupled with multi-objective behavior
optimization for individual decision-making about actions. The census component
is expressed as a nonlinear opinion dynamics model and the multi-objective
behavior optimization is accomplished using interval programming. This model
can be reduced to recover foundational algorithms in distributed optimization
and control, while the full model enables new types of collective behaviors
that are useful in real-world scenarios. To illustrate these points, a new
method for distributed optimization of subgroup allocation is introduced where
robots use a gradient descent algorithm to minimize portions of the cost
functions that are locally known, while being influenced by the opinion states
from neighbors to account for the unobserved costs. With this method the group
can collectively use the information contained in the Hessian matrix of the
total global cost. The utility of this model is experimentally validated in
three categorically different experiments with fleets of autonomous surface
vehicles: an adaptive sampling scenario, a high value unit protection scenario,
and a competitive game of capture the flag.

</details>


### [7] [Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models](https://arxiv.org/abs/2511.02162)
*Alexander Htet Kyaw,Richa Gupta,Dhruv Shah,Anoop Sinha,Kory Mathewson,Stefanie Pender,Sachin Chitta,Yotto Koga,Faez Ahmed,Lawrence Sass,Randall Davis*

Main category: cs.RO

TL;DR: 该论文提出了一种结合3D生成AI和视觉语言模型（VLM）的管道，用于从自然语言生成多组件3D模型并实现机器人组装。VLM通过零样本多模态推理来分解AI生成的网格，确定结构组件和面板组件的分配。


<details>
  <summary>Details</summary>
Motivation: 解决3D生成AI在创建多组件对象时的挑战，实现从文本提示到物理对象机器人组装的全流程自动化。

Method: 集成3D生成AI与VLM，利用VLM进行零样本多模态几何和功能推理，将AI生成的网格分解为多组件3D模型，使用预定义的结构和面板组件。

Result: 评估显示用户90.6%的情况下偏好VLM生成的组件分配，而规则方法为59.4%，随机分配为2.5%。系统还支持通过对话反馈来优化组件分配。

Conclusion: 该系统通过VLM实现了多组件对象的智能分解和分配，并通过对话反馈机制增强了用户在生成AI和机器人制造过程中的控制权。

Abstract: Advances in 3D generative AI have enabled the creation of physical objects
from text prompts, but challenges remain in creating objects involving multiple
component types. We present a pipeline that integrates 3D generative AI with
vision-language models (VLMs) to enable the robotic assembly of multi-component
objects from natural language. Our method leverages VLMs for zero-shot,
multi-modal reasoning about geometry and functionality to decompose
AI-generated meshes into multi-component 3D models using predefined structural
and panel components. We demonstrate that a VLM is capable of determining which
mesh regions need panel components in addition to structural components, based
on object functionality. Evaluation across test objects shows that users
preferred the VLM-generated assignments 90.6% of the time, compared to 59.4%
for rule-based and 2.5% for random assignment. Lastly, the system allows users
to refine component assignments through conversational feedback, enabling
greater human control and agency in making physical objects with generative AI
and robotics.

</details>


### [8] [Kinematic and Ergonomic Design of a Robotic Arm for Precision Laparoscopic Surgery](https://arxiv.org/abs/2511.02167)
*Tian Hao,Tong Lu,Che Chan*

Main category: cs.RO

TL;DR: 提出一种用于腹腔镜手术的7自由度机器人臂系统，通过远程运动中心和人体工程学设计提高手术精度和外科医生舒适度，实验显示目标精度提高50%以上并减少肌肉劳损。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助微创手术能提高手术精度并减少外科医生疲劳，需要研究运动学和人体工程学设计原则来优化腹腔镜手术机器人臂的性能。

Method: 设计7自由度机器人臂系统，集成远程运动中心和人体工程学考虑，在通用机器人平台上实现，通过模拟手术任务评估目标精度、任务效率和外科医生舒适度。

Result: 优化后的机器人设计显著提高了目标精度（误差减少超过50%），缩短了任务完成时间，同时大幅降低了操作者肌肉劳损和不适感。

Conclusion: 运动学优化和以人为本的人体工程学设计对提升机器人辅助手术性能至关重要，这些见解可指导下一代手术机器人的开发，改善手术结果和手术团队的人体工程学条件。

Abstract: Robotic assistance in minimally invasive surgery can greatly enhance surgical
precision and reduce surgeon fatigue. This paper presents a focused
investigation on the kinematic and ergonomic design principles for a
laparoscopic surgical robotic arm aimed at high-precision tasks. We propose a
7-degree-of-freedom (7-DOF) robotic arm system that incorporates a remote
center of motion (RCM) at the instrument insertion point and ergonomic
considerations to improve surgeon interaction. The design is implemented on a
general-purpose robotic platform, and a series of simulated surgical tasks were
performed to evaluate targeting accuracy, task efficiency, and surgeon comfort
compared to conventional manual laparoscopy. Experimental results demonstrate
that the optimized robotic design achieves significantly improved targeting
accuracy (error reduced by over 50%) and shorter task completion times, while
substantially lowering operator muscle strain and discomfort. These findings
validate the importance of kinematic optimization (such as added articulations
and tremor filtering) and human-centered ergonomic design in enhancing the
performance of robot-assisted surgery. The insights from this work can guide
the development of next-generation surgical robots that improve surgical
outcomes and ergonomics for the operating team.

</details>


### [9] [A Quantitative Comparison of Centralised and Distributed Reinforcement Learning-Based Control for Soft Robotic Arms](https://arxiv.org/abs/2511.02192)
*Linxin Hou,Qirui Wu,Zhihang Qin,Neil Banerjee,Yongxin Guo,Cecilia Laschi*

Main category: cs.RO

TL;DR: 该论文比较了集中式和分布式多智能体强化学习在软体机器人臂控制中的性能，发现当控制段数n≤4时分布式策略无显著优势，当n>4时分布式策略在样本效率、成功率和鲁棒性方面表现更好，但集中式策略训练时间效率更高。


<details>
  <summary>Details</summary>
Motivation: 比较集中式和分布式多智能体强化学习架构在软体机器人臂控制中的性能差异，为软体机器人系统的控制器设计提供指导。

Method: 使用PyElastica和OpenAI Gym接口，在相同预算下训练全局PPO控制器和多智能体PPO，系统性地改变控制段数n，在三种场景下评估性能：基线条件、外部干扰恢复和驱动器故障适应。

Result: 当n≤4时分布式策略无显著优势；n≤2时集中式策略表现更好；4<n≤12时分布式策略样本效率高、成功率和鲁棒性更强；但集中式策略训练时间效率更高。

Conclusion: 集中式和分布式策略在软体机器人强化学习控制中存在权衡，为未来软体杆状机械臂的仿真到真实转移提供了可操作的设计指导。

Abstract: This paper presents a quantitative comparison between centralised and
distributed multi-agent reinforcement learning (MARL) architectures for
controlling a soft robotic arm modelled as a Cosserat rod in simulation. Using
PyElastica and the OpenAI Gym interface, we train both a global Proximal Policy
Optimisation (PPO) controller and a Multi-Agent PPO (MAPPO) under identical
budgets. Both approaches are based on the arm having $n$ number of controlled
sections. The study systematically varies $n$ and evaluates the performance of
the arm to reach a fixed target in three scenarios: default baseline condition,
recovery from external disturbance, and adaptation to actuator failure.
Quantitative metrics used for the evaluation are mean action magnitude, mean
final distance, mean episode length, and success rate. The results show that
there are no significant benefits of the distributed policy when the number of
controlled sections $n\le4$. In very simple systems, when $n\le2$, the
centralised policy outperforms the distributed one. When $n$ increases to $4<
n\le 12$, the distributed policy shows a high sample efficiency. In these
systems, distributed policy promotes a stronger success rate, resilience, and
robustness under local observability and yields faster convergence given the
same sample size. However, centralised policies achieve much higher time
efficiency during training as it takes much less time to train the same size of
samples. These findings highlight the trade-offs between centralised and
distributed policy in reinforcement learning-based control for soft robotic
systems and provide actionable design guidance for future sim-to-real transfer
in soft rod-like manipulators.

</details>


### [10] [LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation](https://arxiv.org/abs/2511.02239)
*Youngjin Hong,Houjian Yu,Mingen Li,Changhyun Choi*

Main category: cs.RO

TL;DR: LACY是一个统一的语言-动作循环框架，通过联合训练语言到动作(L2A)、动作到语言(A2L)和语言一致性验证(L2C)三个任务，实现双向映射学习，并通过主动增强策略自主生成训练数据，显著提升了机器人操作任务的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言指令到动作(L2A)的大规模模型虽然能执行任务，但缺乏对上下文的深度理解，限制了泛化能力和行为解释能力。作者认为动作到语言(A2L)的互补技能对于发展更全面的基础至关重要。

Method: LACY框架在单一视觉语言模型中联合训练三个协同任务：从语言生成参数化动作(L2A)、用语言解释观察到的动作(A2L)、验证两个语言描述之间的语义一致性(L2C)。采用主动增强策略针对低置信度案例自主生成和过滤训练数据。

Result: 在模拟和真实世界的拾取放置任务实验中，LACY平均提高了56.46%的任务成功率，并产生了更鲁棒的语言-动作基础。

Conclusion: 双向语言-动作映射学习能够形成更丰富的内部表示，为机器人操作解锁新的自监督学习范式，证明了LACY框架在提高任务性能和语言-动作基础方面的有效性。

Abstract: Learning generalizable policies for robotic manipulation increasingly relies
on large-scale models that map language instructions to actions (L2A). However,
this one-way paradigm often produces policies that execute tasks without deeper
contextual understanding, limiting their ability to generalize or explain their
behavior. We argue that the complementary skill of mapping actions back to
language (A2L) is essential for developing more holistic grounding. An agent
capable of both acting and explaining its actions can form richer internal
representations and unlock new paradigms for self-supervised learning. We
introduce LACY (Language-Action Cycle), a unified framework that learns such
bidirectional mappings within a single vision-language model. LACY is jointly
trained on three synergistic tasks: generating parameterized actions from
language (L2A), explaining observed actions in language (A2L), and verifying
semantic consistency between two language descriptions (L2C). This enables a
self-improving cycle that autonomously generates and filters new training data
through an active augmentation strategy targeting low-confidence cases, thereby
improving the model without additional human labels. Experiments on
pick-and-place tasks in both simulation and the real world show that LACY
improves task success rates by 56.46% on average and yields more robust
language-action grounding for robotic manipulation. Project page:
https://vla2026.github.io/LACY/

</details>


### [11] [SuckTac: Camera-based Tactile Sucker for Unstructured Surface Perception and Interaction](https://arxiv.org/abs/2511.02294)
*Ruiyong Yuan,Jieji Ren,Zhanxuan Peng,Feifei Chen,Guoying Gu*

Main category: cs.RO

TL;DR: 本文提出了一种名为SuckTac的新型智能吸盘，将基于摄像头的触觉传感器集成到优化结构中，提供高密度感知和鲁棒吸附能力，灵感来自头足类动物的自适应结构和感官能力。


<details>
  <summary>Details</summary>
Motivation: 现有吸盘缺乏高保真感知和触觉传感能力，无法识别目标表面的精细几何特征和交互状态，限制了在复杂非结构化环境中的鲁棒性能。

Method: 通过联合结构设计和优化，基于多材料集成铸造技术，将摄像头和光源嵌入吸盘内部，实现原位高密度感知；通过优化吸盘轮廓、添加柔性唇边和表面微结构来增强鲁棒性和适应性。

Result: 在机器人布料操作和软体移动机器人检测等挑战性任务中的广泛实验证明了该系统的优越性能和广泛适用性。

Conclusion: SuckTac智能吸盘通过集成触觉感知和结构优化，显著提升了在复杂环境中的吸附性能和适应性。

Abstract: Suckers are significant for robots in picking, transferring, manipulation and
locomotion on diverse surfaces. However, most of the existing suckers lack
high-fidelity perceptual and tactile sensing, which impedes them from resolving
the fine-grained geometric features and interaction status of the target
surface. This limits their robust performance with irregular objects and in
complex, unstructured environments. Inspired by the adaptive structure and
high-performance sensory capabilities of cephalopod suckers, in this paper, we
propose a novel, intelligent sucker, named SuckTac, that integrates a
camera-based tactile sensor directly within its optimized structure to provide
high-density perception and robust suction. Specifically, through joint
structure design and optimization and based on a multi-material integrated
casting technique, a camera and light source are embedded into the sucker,
which enables in-situ, high-density perception of fine details like surface
shape, texture and roughness. To further enhance robustness and adaptability,
the sucker's mechanical design is also optimized by refining its profile,
adding a compliant lip, and incorporating surface microstructure. Extensive
experiments, including challenging tasks such as robotic cloth manipulation and
soft mobile robot inspection, demonstrate the superior performance and broad
applicability of the proposed system.

</details>


### [12] [ZJUNlict Extended Team Description Paper 2025](https://arxiv.org/abs/2511.02315)
*Zifei Wu,Lijie Wang,Zhe Yang,Shijie Yang,Liang Wang,Haoran Fu,Yinliang Cai,Rong Xiong*

Main category: cs.RO

TL;DR: ZJUNlict团队在过去一年中在硬件和软件方面都取得了进展，硬件上为v2023机器人集成了IMU以提升姿态精度和角速度规划，软件上优化了策略和CUDA模块，显著提升了决策效率、球追踪预测和控球预测能力。


<details>
  <summary>Details</summary>
Motivation: 为了适应高节奏的比赛动态，需要提升机器人的姿态精度和决策效率，因此团队在硬件和软件两方面都进行了优化。

Method: 硬件方面为v2023机器人集成IMU传感器；软件方面优化策略模块和CUDA模块，改进决策算法、球追踪预测和控球预测。

Result: 实现了机器人姿态精度的提升、角速度规划的改进，以及决策效率、球追踪预测和控球预测能力的显著增强。

Conclusion: 通过硬件和软件的综合优化，ZJUNlict团队成功提升了机器人在高节奏比赛中的适应能力和整体性能。

Abstract: This paper presents the ZJUNlict team's work over the past year, covering
both hardware and software advancements. In the hardware domain, the
integration of an IMU into the v2023 robot was completed to enhance posture
accuracy and angular velocity planning. On the software side, key modules were
optimized, including the strategy and CUDA modules, with significant
improvements in decision making efficiency, ball pursuit prediction, and ball
possession prediction to adapt to high-tempo game dynamics.

</details>


### [13] [Dexterous Robotic Piano Playing at Scale](https://arxiv.org/abs/2511.02504)
*Le Chen,Yi Zhao,Jan Schneider,Quankai Gao,Simon Guist,Cheng Qian,Juho Kannala,Bernhard Schölkopf,Joni Pajarinen,Dieter Büchler*

Main category: cs.RO

TL;DR: OmniPianist是首个能够通过可扩展、无需人类演示的学习方式演奏近千首音乐作品的机器人系统，实现了大规模灵巧机器人钢琴演奏。


<details>
  <summary>Details</summary>
Motivation: 赋予机器人手人类水平的灵巧性一直是机器人学的长期目标。双手机器人钢琴演奏是一个特别具有挑战性的任务：高维度、接触丰富且需要快速精确的控制。

Method: 方法基于三个核心组件：1）基于最优传输的自动指法策略，让智能体能够自主发现高效的钢琴演奏策略；2）通过训练2000多个专门针对不同音乐作品的智能体进行大规模强化学习，并将经验聚合为包含100多万条轨迹的RP1M++数据集；3）使用流匹配变换器通过大规模模仿学习利用RP1M++数据集。

Result: 开发出了能够演奏广泛音乐作品的OmniPianist智能体。广泛的实验和消融研究证明了该方法的有效性和可扩展性。

Conclusion: 该方法推动了大规模灵巧机器人钢琴演奏的发展，展示了无需人类演示即可实现复杂技能学习的可行性。

Abstract: Endowing robot hands with human-level dexterity has been a long-standing goal
in robotics. Bimanual robotic piano playing represents a particularly
challenging task: it is high-dimensional, contact-rich, and requires fast,
precise control. We present OmniPianist, the first agent capable of performing
nearly one thousand music pieces via scalable, human-demonstration-free
learning. Our approach is built on three core components. First, we introduce
an automatic fingering strategy based on Optimal Transport (OT), allowing the
agent to autonomously discover efficient piano-playing strategies from scratch
without demonstrations. Second, we conduct large-scale Reinforcement Learning
(RL) by training more than 2,000 agents, each specialized in distinct music
pieces, and aggregate their experience into a dataset named RP1M++, consisting
of over one million trajectories for robotic piano playing. Finally, we employ
a Flow Matching Transformer to leverage RP1M++ through large-scale imitation
learning, resulting in the OmniPianist agent capable of performing a wide range
of musical pieces. Extensive experiments and ablation studies highlight the
effectiveness and scalability of our approach, advancing dexterous robotic
piano playing at scale.

</details>


### [14] [Non-Contact Manipulation of Induced Magnetic Dipoles](https://arxiv.org/abs/2511.02761)
*Seth Stewart,Joseph Pawelski,Steve Ward,Andrew J. Petruska*

Main category: cs.RO

TL;DR: 该论文展示了通过振荡磁场对导电非磁性物体进行闭环位置控制的方法，特别关注空间碎片回收应用。


<details>
  <summary>Details</summary>
Motivation: 将磁操纵扩展到导电非磁性物体，为空间碎片回收等应用开辟新途径，利用感应涡流产生的磁矩进行控制。

Method: 利用感应涡流产生的反向磁偶极矩，在实验室测试中对半浮铝球实现闭环位置控制，并探索了多种力反演方法的有效性。

Result: 成功实现了对感应磁偶极子的3自由度闭环位置控制，这是迈向更广泛应用的关键第一步。

Conclusion: 闭环控制方法代表了向感应磁偶极子3自由度位置控制更广泛应用迈出的重要步骤，特别适用于空间环境中的低力应用场景。

Abstract: Extending the field of magnetic manipulation to conductive, non-magnetic
objects opens the door for a wide array of applications previously limited to
hard or soft magnetic materials. Of particular interest is the recycling of
space debris through the use of oscillating magnetic fields, which represent a
cache of raw materials in an environment particularly suited to the low forces
generated from inductive magnetic manipulation. Building upon previous work
that demonstrated 3D open-loop position control by leveraging the opposing
dipole moment created from induced eddy currents, this work demonstrates
closed-loop position control of a semi-buoyant aluminum sphere in lab tests,
and the efficacy of varying methods for force inversion is explored. The
closed-loop methods represent a critical first step towards wider applications
for 3-DOF position control of induced magnetic dipoles.

</details>


### [15] [XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations](https://arxiv.org/abs/2511.02776)
*Shichao Fan,Kun Wu,Zhengping Che,Xinhua Wang,Di Wu,Fei Liao,Ning Liu,Yixue Zhang,Zhen Zhao,Zhiyuan Xu,Meng Li,Qingjie Liu,Shanghang Zhang,Min Wan,Jian Tang*

Main category: cs.RO

TL;DR: XR-1提出了一种新颖的视觉-语言-动作模型框架，通过统一视觉-运动编码解决现有VLA模型在精确低级动作生成和跨异构数据源领域差距方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型面临两个基本挑战：(i)从高维观察中产生精确的低级动作，(ii)跨越异构数据源的领域差距，包括不同的机器人体现和人类演示。现有方法未能充分利用大规模异构数据集中存在的互补多模态知识。

Method: XR-1引入了统一视觉-运动编码(UVMC)，通过双分支VQ-VAE联合编码视觉动态和机器人运动。采用三阶段训练范式：自监督UVMC学习、UVMC引导的大规模跨体现机器人数据集预训练、任务特定后训练。

Result: 在6种不同机器人体现上的超过14,000次真实世界实验验证，涵盖120多个多样化操作任务。XR-1持续优于最先进的基线方法，并展示了对新物体、背景变化、干扰物和光照变化的强泛化能力。

Conclusion: XR-1提供了一个多功能且可扩展的VLA学习框架，通过UVMC表示有效解决了视觉-语言-动作模型中的关键挑战，在多样化机器人、任务和环境中表现出卓越性能。

Abstract: Recent progress in large-scale robotic datasets and vision-language models
(VLMs) has advanced research on vision-language-action (VLA) models. However,
existing VLA models still face two fundamental challenges: (i) producing
precise low-level actions from high-dimensional observations, (ii) bridging
domain gaps across heterogeneous data sources, including diverse robot
embodiments and human demonstrations. Existing methods often encode latent
variables from either visual dynamics or robotic actions to guide policy
learning, but they fail to fully exploit the complementary multi-modal
knowledge present in large-scale, heterogeneous datasets. In this work, we
present X Robotic Model 1 (XR-1), a novel framework for versatile and scalable
VLA learning across diverse robots, tasks, and environments. XR-1 introduces
the \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation
learned via a dual-branch VQ-VAE that jointly encodes visual dynamics and
robotic motion. UVMC addresses these challenges by (i) serving as an
intermediate representation between the observations and actions, and (ii)
aligning multimodal dynamic information from heterogeneous data sources to
capture complementary knowledge. To effectively exploit UVMC, we propose a
three-stage training paradigm: (i) self-supervised UVMC learning, (ii)
UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and
(iii) task-specific post-training. We validate XR-1 through extensive
real-world experiments with more than 14,000 rollouts on six different robot
embodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently
outperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,
UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel
objects, background variations, distractors, and illumination changes. Our
project is at https://xr-1-vla.github.io/.

</details>


### [16] [TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System](https://arxiv.org/abs/2511.02832)
*Yanjie Ze,Siheng Zhao,Weizhuo Wang,Angjoo Kanazawa,Rocky Duan,Pieter Abbeel,Guanya Shi,Jiajun Wu,C. Karen Liu*

Main category: cs.RO

TL;DR: TWIST2是一个无需动作捕捉的可移植人形机器人遥操作和数据收集系统，通过VR设备实现全身控制，能够高效收集演示数据并训练分层视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 人形机器人领域缺乏有效的数据收集框架，现有遥操作系统要么使用解耦控制，要么依赖昂贵的动作捕捉设备。

Method: 使用PICO4U VR获取实时全身人体运动，配合定制的2自由度机器人颈部实现自我中心视觉，实现整体人-人形机器人控制。

Result: 系统能在15分钟内收集100个演示，成功率接近100%；训练的分层视觉运动策略能够自主控制完整人形机器人身体，完成全身灵巧操作和动态踢球任务。

Conclusion: TWIST2提供了一个完全可复现的开源系统，为人形机器人研究提供了高效的数据收集和策略学习框架。

Abstract: Large-scale data has driven breakthroughs in robotics, from language models
to vision-language-action models in bimanual manipulation. However, humanoid
robotics lacks equally effective data collection frameworks. Existing humanoid
teleoperation systems either use decoupled control or depend on expensive
motion capture setups. We introduce TWIST2, a portable, mocap-free humanoid
teleoperation and data collection system that preserves full whole-body control
while advancing scalability. Our system leverages PICO4U VR for obtaining
real-time whole-body human motions, with a custom 2-DoF robot neck (cost around
$250) for egocentric vision, enabling holistic human-to-humanoid control. We
demonstrate long-horizon dexterous and mobile humanoid skills and we can
collect 100 demonstrations in 15 minutes with an almost 100% success rate.
Building on this pipeline, we propose a hierarchical visuomotor policy
framework that autonomously controls the full humanoid body based on egocentric
vision. Our visuomotor policy successfully demonstrates whole-body dexterous
manipulation and dynamic kicking tasks. The entire system is fully reproducible
and open-sourced at https://yanjieze.com/TWIST2 . Our collected dataset is also
open-sourced at https://twist-data.github.io .

</details>
