<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 27]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [RoboOmni: Proactive Robot Manipulation in Omni-modal Context](https://arxiv.org/abs/2510.23763)
*Siyin Wang,Jinlan Fu,Feihong Liu,Xinzhe He,Huangxuan Wu,Junhao Shi,Kexin Huang,Zhaoye Fei,Jingjing Gong,Zuxuan Wu,Yugang Jiang,See-Kiong Ng,Tat-Seng Chua,Xipeng Qiu*

Main category: cs.RO

TL;DR: 本文提出RoboOmni框架，基于全模态大语言模型，通过融合听觉和视觉信号进行时空推理，实现机器人从对话、环境声音和视觉线索中主动推断用户意图，而不仅仅依赖显式指令。


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作模型主要依赖显式指令，但真实世界中人机协作需要机器人主动推断用户意图。本文旨在解决从多模态上下文（语音对话、环境声音、视觉线索）中识别意图的新问题。

Method: 提出RoboOmni框架，采用感知者-思考者-说话者-执行者架构，基于端到端全模态大语言模型，统一意图识别、交互确认和动作执行。构建OmniAction数据集（14万条记录）用于训练。

Result: 在仿真和真实环境实验中，RoboOmni在成功率、推理速度、意图识别和主动协助方面均优于基于文本和语音识别的基线方法。

Conclusion: RoboOmni框架有效解决了机器人主动意图识别问题，通过多模态上下文理解实现了更自然的人机协作。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid
progress in Vision-Language-Action (VLA) models for robotic manipulation.
Although effective in many scenarios, current approaches largely rely on
explicit instructions, whereas in real-world interactions, humans rarely issue
instructions directly. Effective collaboration requires robots to infer user
intentions proactively. In this work, we introduce cross-modal contextual
instructions, a new setting where intent is derived from spoken dialogue,
environmental sounds, and visual cues rather than explicit commands. To address
this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor
framework based on end-to-end omni-modal LLMs that unifies intention
recognition, interaction confirmation, and action execution. RoboOmni fuses
auditory and visual signals spatiotemporally for robust intention recognition,
while supporting direct speech interaction. To address the absence of training
data for proactive intention recognition in robotic manipulation, we build
OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640
backgrounds, and six contextual instruction types. Experiments in simulation
and real-world settings show that RoboOmni surpasses text- and ASR-based
baselines in success rate, inference speed, intention recognition, and
proactive assistance.

</details>


### [2] [Motivating Students' Self-study with Goal Reminder and Emotional Support](https://arxiv.org/abs/2510.23860)
*Hyung Chan Cho,Go-Eum Cha,Yanfu Liu,Sooyeon Jeong*

Main category: cs.RO

TL;DR: 本研究探讨社交机器人作为同伴学习伴侣在大学生自主学习中的作用，通过提供目标提醒和积极情感支持，发现这些支持行为能提高学生的专注度、生产力和参与度。


<details>
  <summary>Details</summary>
Motivation: 虽然社交机器人在支持学习任务方面的效果已被广泛研究，但它们在协助学生自主学习方面的潜力尚未得到充分探索。

Method: 采用探索性的Wizard-of-Oz研究，比较目标提醒、情感支持与仅提供物理存在的控制组对学习效果的影响。

Result: 目标提醒和情感支持条件下的参与者报告了更高的易用性，目标提醒组还表现出更高的未来使用意愿。参与者对机器人的满意度与其将机器人视为社会存在的感知相关，这种感知是自主学习任务中目标达成水平的预测因子。

Conclusion: 这些发现突显了社交辅助机器人通过功能和情感参与支持自主学习的潜力。

Abstract: While the efficacy of social robots in supporting people in learning tasks
has been extensively investigated, their potential impact in assisting students
in self-studying contexts has not been investigated much. This study explores
how a social robot can act as a peer study companion for college students
during self-study tasks by delivering task-oriented goal reminder and positive
emotional support. We conducted an exploratory Wizard-of-Oz study to explore
how these robotic support behaviors impacted students' perceived focus,
productivity, and engagement in comparison to a robot that only provided
physical presence (control). Our study results suggest that participants in the
goal reminder and the emotional support conditions reported greater ease of
use, with the goal reminder condition additionally showing a higher willingness
to use the robot in future study sessions. Participants' satisfaction with the
robot was correlated with their perception of the robot as a social other, and
this perception was found to be a predictor for their level of goal achievement
in the self-study task. These findings highlight the potential of socially
assistive robots to support self-study through both functional and emotional
engagement.

</details>


### [3] [Stand, Walk, Navigate: Recovery-Aware Visual Navigation on a Low-Cost Wheeled Quadruped](https://arxiv.org/abs/2510.23902)
*Jans Solano,Diego Quiroz*

Main category: cs.RO

TL;DR: 提出了一种用于低成本轮式四足机器人的恢复感知视觉惯性导航系统，该系统结合深度摄像头和深度强化学习策略，实现鲁棒运动控制和跌倒自主恢复。


<details>
  <summary>Details</summary>
Motivation: 现有轮腿式机器人通常依赖昂贵的执行器和传感器，且很少集成跌倒恢复功能，特别是针对轮腿形态的机器人。

Method: 利用深度摄像头的视觉感知和深度强化学习策略，在低成本轮式四足机器人上实现鲁棒运动控制和自主跌倒恢复。

Result: 仿真实验显示在非规则地形上具有敏捷移动性，并能可靠地从外部扰动和自诱导故障中恢复。在结构化室内空间中实现了目标导向导航。

Conclusion: 该方法降低了在预算受限的机器人平台上部署自主导航和鲁棒运动控制策略的门槛。

Abstract: Wheeled-legged robots combine the efficiency of wheels with the obstacle
negotiation of legs, yet many state-of-the-art systems rely on costly actuators
and sensors, and fall-recovery is seldom integrated, especially for
wheeled-legged morphologies. This work presents a recovery-aware
visual-inertial navigation system on a low-cost wheeled quadruped. The proposed
system leverages vision-based perception from a depth camera and deep
reinforcement learning policies for robust locomotion and autonomous recovery
from falls across diverse terrains. Simulation experiments show agile mobility
with low-torque actuators over irregular terrain and reliably recover from
external perturbations and self-induced failures. We further show goal directed
navigation in structured indoor spaces with low-cost perception. Overall, this
approach lowers the barrier to deploying autonomous navigation and robust
locomotion policies in budget-constrained robotic platforms.

</details>


### [4] [A Comprehensive General Model of Tendon-Actuated Concentric Tube Robots with Multiple Tubes and Tendons](https://arxiv.org/abs/2510.23954)
*Pejman Kheradmand,Behnam Moradkhani,Raghavasimhan Sankaranarayanan,Kent K. Yamamoto,Tanner J. Zachem,Patrick J. Codd,Yash Chitalia,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 提出了基于Cosserat杆的建模框架，用于模拟n个同心管（每个管由m_i个肌腱驱动）的一般情况，实现了尖端预测误差小于机器人总长度的4%。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动同心管机构结合了肌腱驱动连续体机器人和同心管机器人的优点，但缺乏完整通用的力学模型。本研究旨在解决这一开放性问题。

Method: 使用Cosserat杆理论框架建模n个同心管，每个管由m_i个肌腱驱动，允许每个管扭转和伸长，同时强制弯曲共享中心线。

Result: 通过双管和三管组件的实验验证，在各种肌腱布线配置下实现了尖端预测误差小于机器人总长度的4%。应用于现有机器人时，最大尖端偏差保持在总长度的5%左右。

Conclusion: 该模型为精确形状估计和先进肌腱驱动同心管机器人的控制提供了基础。

Abstract: Tendon-actuated concentric tube mechanisms combine the advantages of
tendon-driven continuum robots and concentric tube robots while addressing
their respective limitations. They overcome the restricted degrees of freedom
often seen in tendon-driven designs, and mitigate issues such as snapping
instability associated with concentric tube robots. However, a complete and
general mechanical model for these systems remains an open problem. In this
work, we propose a Cosserat rod-based framework for modeling the general case
of $n$ concentric tubes, each actuated by $m_i$ tendons, where $i = \{1,
\ldots, n\}$. The model allows each tube to twist and elongate while enforcing
a shared centerline for bending. We validate the proposed framework through
experiments with two-tube and three tube assemblies under various tendon
routing configurations, achieving tip prediction errors $<4\%$ of the robot's
total length. We further demonstrate the model's generality by applying it to
existing robots in the field, where maximum tip deviations remain around $5\%$
of the total length. This model provides a foundation for accurate shape
estimation and control of advanced tendon-actuated concentric tube robots.

</details>


### [5] [Adaptive-twist Soft Finger Mechanism for Grasping by Wrapping](https://arxiv.org/abs/2510.23963)
*Hiroki Ishikawa,Kyosuke Ishibashi,Ko Yamamoto*

Main category: cs.RO

TL;DR: 本文提出了一种具有自适应扭转变形能力的软体机器人手指，能够通过包裹方式抓取物体。该手指能够在平面内和平面外方向实现自适应扭转变形，从而深入物体间的狭窄间隙进行抓取。


<details>
  <summary>Details</summary>
Motivation: 为了让软体手能够在密集堆放的多个物体中抓取单个物体，软体手指需要具备在平面内和平面外方向的自适应扭转变形功能，以便深入物体间的有限间隙进行抓取。

Method: 提出了一种可变刚度机制，能够随着压力增加而自适应改变刚度。通过有限元分析确定设计参数，并开发了软体手指进行实验验证。

Result: 通过开发的软体手指进行了基本实验和多种物体的抓取演示，验证了其自适应扭转变形和包裹抓取的能力。

Conclusion: 所提出的可变刚度机制软体手指成功实现了自适应扭转变形和包裹抓取功能，为软体机器人在密集环境中的物体抓取提供了有效解决方案。

Abstract: This paper presents a soft robot finger capable of adaptive-twist deformation
to grasp objects by wrapping them. For a soft hand to grasp and pick-up one
object from densely contained multiple objects, a soft finger requires the
adaptive-twist deformation function in both in-plane and out-of-plane
directions. The function allows the finger to be inserted deeply into a limited
gap among objects. Once inserted, the soft finger requires appropriate control
of grasping force normal to contact surface, thereby maintaining the twisted
deformation. In this paper, we refer to this type of grasping as grasping by
wrapping. To achieve these two functions by a single actuation source, we
propose a variable stiffness mechanism that can adaptively change the stiffness
as the pressure is higher. We conduct a finite element analysis (FEA) on the
proposed mechanism and determine its design parameter based on the FEA result.
Using the developed soft finger, we report basic experimental results and
demonstrations on grasping various objects.

</details>


### [6] [A Survey on Collaborative SLAM with 3D Gaussian Splatting](https://arxiv.org/abs/2510.23988)
*Phuc Nguyen Xuan,Thanh Nguyen Canh,Huu-Hung Nguyen,Nak Young Chong,Xiem HoangVan*

Main category: cs.RO

TL;DR: 本文综述了使用3D高斯泼溅的多机器人协同SLAM技术，分析了集中式和分布式架构，讨论了多智能体一致性、通信效率、高斯表示等核心组件，并指出了终身建图、语义关联等未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅作为显式场景表示方法，能够实现实时高保真渲染，非常适合机器人应用。但在多机器人系统中，保持全局一致性、管理通信和融合异构数据源带来了重大挑战。

Method: 系统性地按架构（集中式和分布式）对方法进行分类，分析多智能体一致性对齐、通信效率、高斯表示、语义蒸馏、融合与位姿优化以及实时可扩展性等核心组件。

Result: 提供了关键数据集和评估指标的总结，以评估性能表现。

Conclusion: 识别了关键开放挑战并规划了未来研究方向，包括终身建图、语义关联与建图、多模型鲁棒性以及弥合Sim2Real差距。

Abstract: This survey comprehensively reviews the evolving field of multi-robot
collaborative Simultaneous Localization and Mapping (SLAM) using 3D Gaussian
Splatting (3DGS). As an explicit scene representation, 3DGS has enabled
unprecedented real-time, high-fidelity render- ing, ideal for robotics.
However, its use in multi-robot systems introduces significant challenges in
maintaining global consistency, managing communication, and fusing data from
heterogeneous sources. We systematically categorize approaches by their
architecture-centralized, distributed- and analyze core components like
multi-agent consistency and alignment, communication- efficient, Gaussian
representation, semantic distillation, fusion and pose optimization, and real-
time scalability. In addition, a summary of critical datasets and evaluation
metrics is provided to contextualize performance. Finally, we identify key open
challenges and chart future research directions, including lifelong mapping,
semantic association and mapping, multi-model for robustness, and bridging the
Sim2Real gap.

</details>


### [7] [VOCALoco: Viability-Optimized Cost-aware Adaptive Locomotion](https://arxiv.org/abs/2510.23997)
*Stanley Wu,Mohamad H. Danesh,Simon Li,Hanna Yurchyk,Amin Abyaneh,Anas El Houssaini,David Meger,Hsiu-Chin Lin*

Main category: cs.RO

TL;DR: VOCALoco是一个模块化技能选择框架，通过动态评估预训练运动策略的安全性和能耗来选择最优策略，在楼梯运动任务中比端到端DRL方法具有更好的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有腿式机器人运动方法大多依赖端到端深度强化学习，在安全性和可解释性方面存在局限，特别是在面对新地形时。需要开发更安全、可解释的框架来适应复杂地形。

Method: 提出VOCALoco模块化技能选择框架，基于感知输入动态评估预训练运动策略的可行性和能耗，通过预测执行安全性和预期运输成本来选择安全且节能的策略。

Result: 在四足机器人楼梯运动任务中，VOCALoco在仿真和真实场景中都表现出比传统端到端DRL策略更好的鲁棒性和安全性。

Conclusion: VOCALoco框架通过模块化技能选择有效解决了端到端DRL方法在安全性和可解释性方面的局限，为腿式机器人在复杂地形上的运动提供了更可靠的解决方案。

Abstract: Recent advancements in legged robot locomotion have facilitated traversal
over increasingly complex terrains. Despite this progress, many existing
approaches rely on end-to-end deep reinforcement learning (DRL), which poses
limitations in terms of safety and interpretability, especially when
generalizing to novel terrains. To overcome these challenges, we introduce
VOCALoco, a modular skill-selection framework that dynamically adapts
locomotion strategies based on perceptual input. Given a set of pre-trained
locomotion policies, VOCALoco evaluates their viability and energy-consumption
by predicting both the safety of execution and the anticipated cost of
transport over a fixed planning horizon. This joint assessment enables the
selection of policies that are both safe and energy-efficient, given the
observed local terrain. We evaluate our approach on staircase locomotion tasks,
demonstrating its performance in both simulated and real-world scenarios using
a quadrupedal robot. Empirical results show that VOCALoco achieves improved
robustness and safety during stair ascent and descent compared to a
conventional end-to-end DRL policy

</details>


### [8] [Improved Accuracy of Robot Localization Using 3-D LiDAR in a Hippocampus-Inspired Model](https://arxiv.org/abs/2510.24029)
*Andrew Gerstenslager,Bekarys Dukenbaev,Ali A. Minai*

Main category: cs.RO

TL;DR: 该论文提出了一种结合垂直角度敏感性的三维边界向量细胞模型，解决了传统二维BVC模型在水平对称环境中容易产生空间模糊的问题，显著提高了生物启发的机器人模型中的空间定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统边界向量细胞模型主要局限于二维环境，在存在水平对称性的环境中容易产生空间模糊，限制了其在真实三维空间中的导航和建图能力。

Method: 将垂直角度敏感性整合到BVC框架中，通过处理LiDAR数据捕捉垂直轮廓，从而在三维空间中实现鲁棒的边界检测。

Result: 在垂直变化最小的环境中，提出的3D模型与2D基线性能相当；但随着3D复杂度的增加，该模型产生显著更独特的场所场，并明显减少了空间混叠。

Conclusion: 在BVC定位中加入垂直维度可以显著增强真实三维空间中的导航和建图能力，同时在较简单的近平面场景中保持性能一致性。

Abstract: Boundary Vector Cells (BVCs) are a class of neurons in the brains of
vertebrates that encode environmental boundaries at specific distances and
allocentric directions, playing a central role in forming place fields in the
hippocampus. Most computational BVC models are restricted to two-dimensional
(2D) environments, making them prone to spatial ambiguities in the presence of
horizontal symmetries in the environment. To address this limitation, we
incorporate vertical angular sensitivity into the BVC framework, thereby
enabling robust boundary detection in three dimensions, and leading to
significantly more accurate spatial localization in a biologically-inspired
robot model.
  The proposed model processes LiDAR data to capture vertical contours, thereby
disambiguating locations that would be indistinguishable under a purely 2D
representation. Experimental results show that in environments with minimal
vertical variation, the proposed 3D model matches the performance of a 2D
baseline; yet, as 3D complexity increases, it yields substantially more
distinct place fields and markedly reduces spatial aliasing. These findings
show that adding a vertical dimension to BVC-based localization can
significantly enhance navigation and mapping in real-world 3D spaces while
retaining performance parity in simpler, near-planar scenarios.

</details>


### [9] [Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation](https://arxiv.org/abs/2510.24055)
*Xiucheng Zhang,Yang Jiang,Hongwei Qing,Jiashuo Bai*

Main category: cs.RO

TL;DR: 提出LCVR和LMoE-DP框架解决模仿学习中感知模糊和任务冲突问题，LCVR通过语言指令增强视觉特征区分相似任务，LMoE-DP使用稀疏专家架构专门处理多模态动作分布，在真实机器人基准测试中显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 感知模糊和任务冲突限制了通过模仿学习实现多任务机器人操作的能力，需要解决视觉相似任务的区分问题和多任务间的冲突问题。

Method: 结合语言条件视觉表示(LCVR)模块和语言条件混合专家密度策略(LMoE-DP)，LCVR通过语言指令增强视觉特征，LMoE-DP使用稀疏专家架构处理多模态动作分布并采用梯度调制稳定训练。

Result: 在真实机器人基准测试中，LCVR将ACT和DP的成功率分别提升33.75%和25%，完整框架达到79%平均成功率，比先进基线提升21%。

Conclusion: 结合语义基础和专家专业化能够实现鲁棒高效的多任务操作。

Abstract: Perceptual ambiguity and task conflict limit multitask robotic manipulation
via imitation learning. We propose a framework combining a Language-Conditioned
Visual Representation (LCVR) module and a Language-conditioned
Mixture-ofExperts Density Policy (LMoE-DP). LCVR resolves perceptual
ambiguities by grounding visual features with language instructions, enabling
differentiation between visually similar tasks. To mitigate task conflict,
LMoE-DP uses a sparse expert architecture to specialize in distinct, multimodal
action distributions, stabilized by gradient modulation. On real-robot
benchmarks, LCVR boosts Action Chunking with Transformers (ACT) and Diffusion
Policy (DP) success rates by 33.75% and 25%, respectively. The full framework
achieves a 79% average success, outperforming the advanced baseline by 21%. Our
work shows that combining semantic grounding and expert specialization enables
robust, efficient multi-task manipulation

</details>


### [10] [Balanced Collaborative Exploration via Distributed Topological Graph Voronoi Partition](https://arxiv.org/abs/2510.24067)
*Tianyi Ding,Ronghao Zheng,Senlin Zhang,Meiqin Liu*

Main category: cs.RO

TL;DR: 提出了一种分布式多机器人自主在线探索方法，通过拓扑图Voronoi算法实现平衡的区域划分和任务分配，在障碍密集的非凸环境中提高探索效率和负载均衡。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在障碍密集非凸环境中的协作探索问题，特别关注动态平衡的探索区域划分和任务分配，以提高整体探索效率和团队负载均衡。

Method: 使用增量更新的拓扑图结构表征空间连通性和全局探索完整性；提出分布式加权拓扑图Voronoi算法实现平衡的图空间划分；局部规划器优化探索目标访问序列并生成安全平滑的运动轨迹。

Result: 与最先进方法相比，在探索效率、完整性和机器人团队工作负载平衡方面显示出显著改进。

Conclusion: 该方法能够有效解决多机器人在复杂环境中的协作探索问题，提供了理论保证的分布式共识收敛和均衡图空间划分，显著提升了探索性能。

Abstract: This work addresses the collaborative multi-robot autonomous online
exploration problem, particularly focusing on distributed exploration planning
for dynamically balanced exploration area partition and task allocation among a
team of mobile robots operating in obstacle-dense non-convex environments.
  We present a novel topological map structure that simultaneously
characterizes both spatial connectivity and global exploration completeness of
the environment. The topological map is updated incrementally to utilize known
spatial information for updating reachable spaces, while exploration targets
are planned in a receding horizon fashion under global coverage guidance.
  A distributed weighted topological graph Voronoi algorithm is introduced
implementing balanced graph space partitions of the fused topological maps.
Theoretical guarantees are provided for distributed consensus convergence and
equitable graph space partitions with constant bounds.
  A local planner optimizes the visitation sequence of exploration targets
within the balanced partitioned graph space to minimize travel distance, while
generating safe, smooth, and dynamically feasible motion trajectories.
  Comprehensive benchmarking against state-of-the-art methods demonstrates
significant improvements in exploration efficiency, completeness, and workload
balance across the robot team.

</details>


### [11] [Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition](https://arxiv.org/abs/2510.24069)
*Sangmin Kim,Hajun Kim,Gijeong Kim,Min-Gyu Kim,Hae-Won Park*

Main category: cs.RO

TL;DR: 本文提出了一种基于相位的轨迹优化方法，用于生成足式机器人的可靠运动，通过解耦接触点动力学和利用贝塞尔多项式特性，确保整个轨迹中平移动力学和摩擦锥约束的可行性。


<details>
  <summary>Details</summary>
Motivation: 为了通过轨迹优化生成足式机器人的可靠运动，需要同时计算机器人的路径和接触序列，并在问题表述中准确考虑动力学。现有方法在确保平移动力学和摩擦锥约束的可行性方面存在挑战。

Method: 利用线性微分方程的叠加特性解耦每个接触点的平移动力学；使用贝塞尔多项式的微分矩阵推导位置与力之间的解析关系；利用贝塞尔多项式的凸包特性确保摩擦锥约束的满足。

Result: 所提出的轨迹优化框架能够为足式机器人生成具有各种步态序列的动态可靠运动。通过四足机器人模型验证了动力学和运动生成的可行性。

Conclusion: 该方法能够有效生成满足平移动力学和摩擦锥约束的可靠足式机器人运动，为各种步态序列提供了可行的轨迹优化解决方案。

Abstract: To generate reliable motion for legged robots through trajectory
optimization, it is crucial to simultaneously compute the robot's path and
contact sequence, as well as accurately consider the dynamics in the problem
formulation. In this paper, we present a phase-based trajectory optimization
that ensures the feasibility of translational dynamics and friction cone
constraints throughout the entire trajectory. Specifically, our approach
leverages the superposition properties of linear differential equations to
decouple the translational dynamics for each contact point, which operates
under different phase sequences. Furthermore, we utilize the differentiation
matrix of B{\'e}zier polynomials to derive an analytical relationship between
the robot's position and force, thereby ensuring the consistent satisfaction of
translational dynamics. Additionally, by exploiting the convex closure property
of B{\'e}zier polynomials, our method ensures compliance with friction cone
constraints. Using the aforementioned approach, the proposed trajectory
optimization framework can generate dynamically reliable motions with various
gait sequences for legged robots. We validate our framework using a quadruped
robot model, focusing on the feasibility of dynamics and motion generation.

</details>


### [12] [ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring](https://arxiv.org/abs/2510.24108)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Jingde Chen,Nadine Chang,Maying Shen,Jingyu Song,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: ZTRS是一种端到端自动驾驶框架，通过强化学习直接从原始传感器数据学习驾驶策略，无需模仿学习，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶框架主要依赖模仿学习，但受限于专家演示质量和部署时的协变量偏移问题；强化学习虽然能利用仿真扩展，但通常限于低维符号输入，无法实现真正的端到端学习。

Method: 提出ZTRS框架，结合离线强化学习和提出的穷举策略优化(EPO)方法，直接在可枚举动作和奖励空间上进行策略梯度优化，从原始高维传感器数据学习驾驶策略。

Result: 在三个基准测试中表现优异：Navtest（通用现实世界开环规划）、Navhard（挑战性现实世界和合成场景开环规划）和HUGSIM（仿真闭环驾驶），在Navhard上达到最先进水平，在HUGSIM上优于基于模仿学习的基线方法。

Conclusion: ZTRS是第一个完全消除模仿学习、仅从奖励学习并直接处理高维传感器数据的框架，证明了强化学习在端到端自动驾驶中的有效性。

Abstract: End-to-end autonomous driving maps raw sensor inputs directly into
ego-vehicle trajectories to avoid cascading errors from perception modules and
to leverage rich semantic cues. Existing frameworks largely rely on Imitation
Learning (IL), which can be limited by sub-optimal expert demonstrations and
covariate shift during deployment. On the other hand, Reinforcement Learning
(RL) has recently shown potential in scaling up with simulations, but is
typically confined to low-dimensional symbolic inputs (e.g. 3D objects and
maps), falling short of full end-to-end learning from raw sensor data. We
introduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory
Scoring), a framework that combines the strengths of both worlds: sensor inputs
without losing information and RL training for robust planning. To the best of
our knowledge, ZTRS is the first framework that eliminates IL entirely by only
learning from rewards while operating directly on high-dimensional sensor data.
ZTRS utilizes offline reinforcement learning with our proposed Exhaustive
Policy Optimization (EPO), a variant of policy gradient tailored for enumerable
actions and rewards. ZTRS demonstrates strong performance across three
benchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop
planning in challenging real-world and synthetic scenarios), and HUGSIM
(simulated closed-loop driving). Specifically, ZTRS achieves the
state-of-the-art result on Navhard and outperforms IL-based baselines on
HUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.

</details>


### [13] [PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI](https://arxiv.org/abs/2510.24109)
*Wenbin Ding,Jun Chen,Mingjia Chen,Fei Xie,Qi Mao,Philip Dames*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉语言模型的新型机器人具身智能体框架，包含人机语音交互、视觉语言智能体和动作执行三大模块，在模拟和真实环境中比纯LLM+CLIP方法平均任务成功率提高了28%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的快速发展为以人为中心的人工智能开辟了新途径，但现有基于LLM的具身智能体缺乏在线规划和执行复杂自然语言控制任务的能力，需要提升机器人在自然语言交互、复杂任务规划和执行方面的智能水平。

Method: 提出包含人机语音交互模块、视觉语言智能体模块和动作执行模块的机器人具身智能体框架。视觉语言智能体本身包括基于视觉的任务规划器、自然语言指令转换器和任务性能反馈评估器。

Result: 实验结果表明，该智能体在模拟和真实环境中比仅依赖LLM+CLIP的方法实现了28%更高的平均任务成功率，显著提高了高级自然语言指令任务的执行成功率。

Conclusion: 基于视觉语言模型的智能机器人操作智能体框架能够有效提升复杂自然语言控制任务的在线规划和执行能力，为实现以人为中心的人工智能提供了可行方案。

Abstract: The rapid advancement of Large Language Models (LLMs) has marked a
significant breakthrough in Artificial Intelligence (AI), ushering in a new era
of Human-centered Artificial Intelligence (HAI). HAI aims to better serve human
welfare and needs, thereby placing higher demands on the intelligence level of
robots, particularly in aspects such as natural language interaction, complex
task planning, and execution. Intelligent agents powered by LLMs have opened up
new pathways for realizing HAI. However, existing LLM-based embodied agents
often lack the ability to plan and execute complex natural language control
tasks online. This paper explores the implementation of intelligent robotic
manipulating agents based on Vision-Language Models (VLMs) in the physical
world. We propose a novel embodied agent framework for robots, which comprises
a human-robot voice interaction module, a vision-language agent module and an
action execution module. The vision-language agent itself includes a
vision-based task planner, a natural language instruction converter, and a task
performance feedback evaluator. Experimental results demonstrate that our agent
achieves a 28\% higher average task success rate in both simulated and real
environments compared to approaches relying solely on LLM+CLIP, significantly
improving the execution success rate of high-level natural language instruction
tasks.

</details>


### [14] [LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation](https://arxiv.org/abs/2510.24118)
*Haotian Zhou,Xiaole Wang,He Li,Fusheng Sun,Shengyu Guo,Guolei Qi,Jianghuan Xu,Huijing Zhao*

Main category: cs.RO

TL;DR: LagMemo是一个基于语言3D高斯泼溅记忆的视觉导航系统，支持多模态、开放词汇查询和多目标导航，通过构建统一的3D语言记忆和局部感知验证机制实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 解决传统视觉导航方法局限于单目标、单模态和封闭集目标设置的不足，满足实际应用中多模态、开放词汇目标查询和多目标视觉导航的需求。

Method: 在探索阶段构建统一的3D语言记忆；接收任务目标时查询记忆并预测候选目标位置；集成基于局部感知的验证机制，在导航过程中动态匹配和验证目标。

Result: 实验结果表明LagMemo的记忆模块能够有效实现多模态开放词汇目标定位，并且在多目标视觉导航任务中优于现有最先进方法。

Conclusion: LagMemo系统通过语言3D高斯泼溅记忆和动态验证机制，成功解决了多模态开放词汇多目标视觉导航的挑战，展现了优越的导航性能。

Abstract: Navigating to a designated goal using visual information is a fundamental
capability for intelligent robots. Most classical visual navigation methods are
restricted to single-goal, single-modality, and closed set goal settings. To
address the practical demands of multi-modal, open-vocabulary goal queries and
multi-goal visual navigation, we propose LagMemo, a navigation system that
leverages a language 3D Gaussian Splatting memory. During exploration, LagMemo
constructs a unified 3D language memory. With incoming task goals, the system
queries the memory, predicts candidate goal locations, and integrates a local
perception-based verification mechanism to dynamically match and validate goals
during navigation. For fair and rigorous evaluation, we curate GOAT-Core, a
high-quality core split distilled from GOAT-Bench tailored to multi-modal
open-vocabulary multi-goal visual navigation. Experimental results show that
LagMemo's memory module enables effective multi-modal open-vocabulary goal
localization, and that LagMemo outperforms state-of-the-art methods in
multi-goal visual navigation. Project page:
https://weekgoodday.github.io/lagmemo

</details>


### [15] [Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames](https://arxiv.org/abs/2510.24194)
*Ev Zisselman,Mirco Mutti,Shelly Francis-Meretzki,Elisei Shafer,Aviv Tamar*

Main category: cs.RO

TL;DR: 该论文提出了一种"蒙眼专家"的行为克隆方法，通过隐藏部分任务信息迫使专家进行非平凡探索，从而在未见任务上实现更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统行为克隆需要人类专家在完全了解任务信息的情况下提供最优行为演示，但这种方法在需要泛化到多种任务时效果有限。作者希望通过限制专家可获得的信息来提升模型的泛化能力。

Method: 提出"蒙眼专家"概念，隐藏部分任务信息迫使专家进行探索性行为，然后克隆这种探索性行为。在真实机器人插入任务和Procgen基准视频游戏中进行了实验验证。

Result: 实验表明，克隆蒙眼专家的行为比克隆完全知情专家的行为在未见任务上泛化效果更好。理论分析证实泛化误差与√(I/m)成正比，其中I是专家可获得的任务信息量，m是演示任务数量。

Conclusion: 克隆蒙眼专家能够用更少的演示任务实现更好的泛化性能，为物理世界基础模型的行为克隆提供了新的有效方法。

Abstract: Behavioral cloning is a simple yet effective technique for learning
sequential decision-making from demonstrations. Recently, it has gained
prominence as the core of foundation models for the physical world, where
achieving generalization requires countless demonstrations of a multitude of
tasks. Typically, a human expert with full information on the task demonstrates
a (nearly) optimal behavior. In this paper, we propose to hide some of the
task's information from the demonstrator. This ``blindfolded'' expert is
compelled to employ non-trivial exploration to solve the task. We show that
cloning the blindfolded expert generalizes better to unseen tasks than its
fully-informed counterpart. We conduct experiments of real-world robot peg
insertion tasks with (limited) human demonstrations, alongside videogames from
the Procgen benchmark. Additionally, we support our findings with theoretical
analysis, which confirms that the generalization error scales with
$\sqrt{I/m}$, where $I$ measures the amount of task information available to
the demonstrator, and $m$ is the number of demonstrated tasks. Both theory and
practice indicate that cloning blindfolded experts generalizes better with
fewer demonstrated tasks. Project page with videos and code:
https://sites.google.com/view/blindfoldedexperts/home

</details>


### [16] [DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation](https://arxiv.org/abs/2510.24261)
*Jingyi Tian,Le Wang,Sanping Zhou,Sen Wang,Jiayi Li,Gang Hua*

Main category: cs.RO

TL;DR: DynaRend是一个通过可微分体积渲染学习3D感知和动态感知的三平面特征的表示学习框架，用于解决机器人操作中真实世界训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法主要依赖2D视觉预训练范式或大规模视频预测模型，无法联合学习几何、语义和动态信息，限制了机器人操作策略的有效性。

Method: 通过掩码重建和未来预测，使用可微分体积渲染在多视角RGB-D视频数据上预训练，学习统一的三平面表示。

Result: 在RLBench和Colosseum基准测试以及真实世界机器人实验中，DynaRend显著提高了策略成功率、对环境扰动的泛化能力以及跨任务的实际应用性。

Conclusion: DynaRend能够有效学习包含空间几何、未来动态和任务语义的统一表示，显著提升了机器人操作任务的性能。

Abstract: Learning generalizable robotic manipulation policies remains a key challenge
due to the scarcity of diverse real-world training data. While recent
approaches have attempted to mitigate this through self-supervised
representation learning, most either rely on 2D vision pretraining paradigms
such as masked image modeling, which primarily focus on static semantics or
scene geometry, or utilize large-scale video prediction models that emphasize
2D dynamics, thus failing to jointly learn the geometry, semantics, and
dynamics required for effective manipulation. In this paper, we present
DynaRend, a representation learning framework that learns 3D-aware and
dynamics-informed triplane features via masked reconstruction and future
prediction using differentiable volumetric rendering. By pretraining on
multi-view RGB-D video data, DynaRend jointly captures spatial geometry, future
dynamics, and task semantics in a unified triplane representation. The learned
representations can be effectively transferred to downstream robotic
manipulation tasks via action value map prediction. We evaluate DynaRend on two
challenging benchmarks, RLBench and Colosseum, as well as in real-world robotic
experiments, demonstrating substantial improvements in policy success rate,
generalization to environmental perturbations, and real-world applicability
across diverse manipulation tasks.

</details>


### [17] [Global-State-Free Obstacle Avoidance for Quadrotor Control in Air-Ground Cooperation](https://arxiv.org/abs/2510.24315)
*Baozhe Zhang,Xinwei Chen,Qingcheng Chen,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出CoNi-OA算法，专门用于无人机-地面车协同场景中的障碍物避让，无需全局状态估计或障碍物预测，仅使用单帧LiDAR数据实时生成避障轨迹。


<details>
  <summary>Details</summary>
Motivation: CoNi-MPC框架虽然能有效进行无人机控制，但缺乏环境信息导致障碍物避让面临挑战，需要一种不依赖全局状态估计的避障解决方案。

Method: 利用无人机单帧原始LiDAR数据生成调制矩阵，直接调整四旋翼飞行器速度实现避障，在非惯性框架内实时生成无碰撞轨迹。

Result: 算法计算效率高（每次迭代小于5毫秒），在动态和不可预测环境中保持安全性，适用于静态和动态环境。

Conclusion: CoNi-OA算法成功解决了无人机-地面车协同任务中的障碍物避让问题，无需全局状态估计或障碍物建模，具有实时性和适应性强的特点。

Abstract: CoNi-MPC provides an efficient framework for UAV control in air-ground
cooperative tasks by relying exclusively on relative states, eliminating the
need for global state estimation. However, its lack of environmental
information poses significant challenges for obstacle avoidance. To address
this issue, we propose a novel obstacle avoidance algorithm, Cooperative
Non-inertial frame-based Obstacle Avoidance (CoNi-OA), designed explicitly for
UAV-UGV cooperative scenarios without reliance on global state estimation or
obstacle prediction. CoNi-OA uniquely utilizes a single frame of raw LiDAR data
from the UAV to generate a modulation matrix, which directly adjusts the
quadrotor's velocity to achieve obstacle avoidance. This modulation-based
method enables real-time generation of collision-free trajectories within the
UGV's non-inertial frame, significantly reducing computational demands (less
than 5 ms per iteration) while maintaining safety in dynamic and unpredictable
environments. The key contributions of this work include: (1) a
modulation-based obstacle avoidance algorithm specifically tailored for UAV-UGV
cooperation in non-inertial frames without global states; (2) rapid, real-time
trajectory generation based solely on single-frame LiDAR data, removing the
need for obstacle modeling or prediction; and (3) adaptability to both static
and dynamic environments, thus extending applicability to featureless or
unknown scenarios.

</details>


### [18] [NVSim: Novel View Synthesis Simulator for Large Scale Indoor Navigation](https://arxiv.org/abs/2510.24335)
*Mingyu Jeong,Eunsung Kim,Sehun Park,Andrew Jaeyong Choi*

Main category: cs.RO

TL;DR: NVSim是一个从普通图像序列自动构建大规模可导航室内模拟器的框架，解决了传统3D扫描的成本和可扩展性限制。该方法通过改进3D高斯泼溅技术来处理稀疏观测地面上的视觉伪影，并引入无网格可穿越性检查算法来构建拓扑图。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描方法在构建大规模室内模拟器时面临成本和可扩展性限制，而机器人遍历数据通常存在地面稀疏观测导致的视觉伪影问题。

Method: 采用改进的3D高斯泼溅技术（Floor-Aware Gaussian Splatting）确保清洁可导航的地面平面，并提出基于渲染视图直接分析的无网格可穿越性检查算法来构建拓扑图。

Result: 系统能够从真实世界数据生成有效的大规模导航图，展示了其实际应用能力。

Conclusion: NVSim框架成功克服了传统方法的限制，为大规模室内导航模拟提供了高效可行的解决方案。

Abstract: We present NVSim, a framework that automatically constructs large-scale,
navigable indoor simulators from only common image sequences, overcoming the
cost and scalability limitations of traditional 3D scanning. Our approach
adapts 3D Gaussian Splatting to address visual artifacts on sparsely observed
floors a common issue in robotic traversal data. We introduce Floor-Aware
Gaussian Splatting to ensure a clean, navigable ground plane, and a novel
mesh-free traversability checking algorithm that constructs a topological graph
by directly analyzing rendered views. We demonstrate our system's ability to
generate valid, large-scale navigation graphs from real-world data. A video
demonstration is avilable at https://youtu.be/tTiIQt6nXC8

</details>


### [19] [Flatness-based trajectory planning for 3D overhead cranes with friction compensation and collision avoidance](https://arxiv.org/abs/2510.24457)
*Jorge Vicente-Martinez,Edgar Ramirez-Laboreo*

Main category: cs.RO

TL;DR: 本文提出了一种基于微分平坦性的3D桥式起重机最优轨迹生成方法，能够直接处理非线性摩擦和碰撞避免等复杂物理约束，实现仅约束最终点载荷摆动的激进运动。


<details>
  <summary>Details</summary>
Motivation: 现有的起重机轨迹生成方法难以有效处理复杂的物理和动态约束，如非线性摩擦和碰撞避免，这限制了起重机的运动速度和安全性。

Method: 利用微分平坦性框架，直接包含非线性摩擦和载荷、绳索碰撞避免等复杂物理约束，仅约束最终点的载荷摆动以实现激进运动。

Result: 对比仿真研究表明，忽略干摩擦会导致执行器饱和和碰撞，而摩擦建模对于快速安全的起重机轨迹是基本要求。

Conclusion: 摩擦建模是实现快速安全起重机轨迹的基本要求，所提出的方法能够有效处理复杂物理约束，实现激进且安全的运动。

Abstract: This paper presents an optimal trajectory generation method for 3D overhead
cranes by leveraging differential flatness. This framework enables the direct
inclusion of complex physical and dynamic constraints, such as nonlinear
friction and collision avoidance for both payload and rope. Our approach allows
for aggressive movements by constraining payload swing only at the final point.
A comparative simulation study validates our approach, demonstrating that
neglecting dry friction leads to actuator saturation and collisions. The
results show that friction modeling is a fundamental requirement for fast and
safe crane trajectories.

</details>


### [20] [Supervisory Measurement-Guided Noise Covariance Estimation](https://arxiv.org/abs/2510.24508)
*Haoying Li,Yifan Peng,Junfeng Wu*

Main category: cs.RO

TL;DR: 提出了一种双层优化方法，将噪声协方差估计表述为贝叶斯问题，通过因子化分解实现高效并行计算，在合成和真实数据集上比现有基线方法更高效。


<details>
  <summary>Details</summary>
Motivation: 实际应用中传感器噪声协方差难以准确确定，因为受到环境变化、前端预处理等因素影响，这会影响状态估计的可靠性。

Method: 将噪声协方差估计构建为双层优化问题：下层使用不变扩展卡尔曼滤波进行轨迹估计，上层通过梯度更新优化协方差；通过因子化分解将嵌套贝叶斯依赖转换为链式结构，实现并行计算。

Result: 在合成和真实数据集上的实验表明，该方法比现有基线方法具有更高的效率。

Conclusion: 提出的双层优化框架能够有效估计噪声协方差，平衡信息利用与计算效率，在状态估计任务中表现出优越性能。

Abstract: Reliable state estimation hinges on accurate specification of sensor noise
covariances, which weigh heterogeneous measurements. In practice, these
covariances are difficult to identify due to environmental variability,
front-end preprocessing, and other reasons. We address this by formulating
noise covariance estimation as a bilevel optimization that, from a Bayesian
perspective, factorizes the joint likelihood of so-called odometry and
supervisory measurements, thereby balancing information utilization with
computational efficiency. The factorization converts the nested Bayesian
dependency into a chain structure, enabling efficient parallel computation: at
the lower level, an invariant extended Kalman filter with state augmentation
estimates trajectories, while a derivative filter computes analytical gradients
in parallel for upper-level gradient updates. The upper level refines the
covariance to guide the lower-level estimation. Experiments on synthetic and
real-world datasets show that our method achieves higher efficiency over
existing baselines.

</details>


### [21] [GeVI-SLAM: Gravity-Enhanced Stereo Visua Inertial SLAM for Underwater Robots](https://arxiv.org/abs/2510.24533)
*Yuan Shen,Yuze Hong,Guangyang Zeng,Tengfei Zhang,Pui Yi Chui,Ziyang Hong,Junfeng Wu*

Main category: cs.RO

TL;DR: GeVI-SLAM是一种重力增强的立体视觉惯性SLAM系统，通过利用立体相机深度估计、重力增强和4自由度PnP求解器，解决了水下机器人视觉退化和IMU运动激励不足的问题，实现了更高的精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 水下机器人视觉惯性SLAM面临视觉频繁退化和IMU运动激励不足的挑战，需要开发更稳定和准确的定位与建图系统。

Method: 利用立体相机直接深度估计消除IMU初始化中的尺度估计需求；通过精确重力初始化将俯仰和横滚从位姿估计中解耦，使用4自由度PnP问题求解；提出可证明一致性的偏置消除4-DOF PnP估计器；处理动态运动时联合估计IMU协方差并自适应调整重力先验权重。

Result: 在模拟和真实世界数据上的广泛实验表明，GeVI-SLAM相比最先进方法实现了更高的精度和更好的稳定性。

Conclusion: GeVI-SLAM通过重力增强和4自由度PnP求解器有效解决了水下SLAM的挑战，为水下机器人提供了更可靠的定位与建图解决方案。

Abstract: Accurate visual inertial simultaneous localization and mapping (VI SLAM) for
underwater robots remains a significant challenge due to frequent visual
degeneracy and insufficient inertial measurement unit (IMU) motion excitation.
In this paper, we present GeVI-SLAM, a gravity-enhanced stereo VI SLAM system
designed to address these issues. By leveraging the stereo camera's direct
depth estimation ability, we eliminate the need to estimate scale during IMU
initialization, enabling stable operation even under low acceleration dynamics.
With precise gravity initialization, we decouple the pitch and roll from the
pose estimation and solve a 4 degrees of freedom (DOF) Perspective-n-Point
(PnP) problem for pose tracking. This allows the use of a minimal 3-point
solver, which significantly reduces computational time to reject outliers
within a Random Sample Consensus framework. We further propose a
bias-eliminated 4-DOF PnP estimator with provable consistency, ensuring the
relative pose converges to the true value as the feature number increases. To
handle dynamic motion, we refine the full 6-DOF pose while jointly estimating
the IMU covariance, enabling adaptive weighting of the gravity prior. Extensive
experiments on simulated and real-world data demonstrate that GeVI-SLAM
achieves higher accuracy and greater stability compared to state-of-the-art
methods.

</details>


### [22] [An Adaptive Inspection Planning Approach Towards Routine Monitoring in Uncertain Environments](https://arxiv.org/abs/2510.24554)
*Vignesh Kottayam Viswanathan,Yifan Bai,Scott Fredriksson,Sumeet Satpute,Christoforos Kanellakis,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 提出分层框架解决机器人巡检中的环境不确定性，通过全局视图规划和局部视图重规划来适应环境变化，在真实地下矿井中验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖已知环境模型规划巡检路线，但模型与实际环境差异（自然或人为活动导致）会改变表面形态或引入路径障碍，需要适应环境不确定性的解决方案。

Method: 分层框架将巡检任务分为：(a)基于历史地图生成感兴趣区域的初始全局视图规划；(b)局部视图重规划以适应当前检查场景的表面形态。

Result: 该层次结构保持全局覆盖目标，同时实现对局部表面形态的响应式适应，使局部自治在环境不确定性下保持鲁棒性并完成巡检任务。

Conclusion: 通过在地下矿井中使用四足机器人的实际部署验证了该方法的有效性。

Abstract: In this work, we present a hierarchical framework designed to support robotic
inspection under environment uncertainty. By leveraging a known environment
model, existing methods plan and safely track inspection routes to visit points
of interest. However, discrepancies between the model and actual site
conditions, caused by either natural or human activities, can alter the surface
morphology or introduce path obstructions. To address this challenge, the
proposed framework divides the inspection task into: (a) generating the initial
global view-plan for region of interests based on a historical map and (b)
local view replanning to adapt to the current morphology of the inspection
scene. The proposed hierarchy preserves global coverage objectives while
enabling reactive adaptation to the local surface morphology. This enables the
local autonomy to remain robust against environment uncertainty and complete
the inspection tasks. We validate the approach through deployments in
real-world subterranean mines using quadrupedal robot.

</details>


### [23] [Spatiotemporal Calibration of Doppler Velocity Logs for Underwater Robots](https://arxiv.org/abs/2510.24571)
*Hongxu Zhao,Guangyang Zeng,Yunling Shao,Tengfei Zhang,Junfeng Wu*

Main category: cs.RO

TL;DR: 提出统一迭代校准(UIC)框架，用于多传感器系统的外参和时间偏移联合估计，特别针对水下SLAM中的DVL传感器校准问题。


<details>
  <summary>Details</summary>
Motivation: 现有DVL校准方法要么局限于特定传感器配置，要么依赖过度简化的假设，且没有同时估计平移外参和时间偏移的方法。

Method: 采用最大后验概率估计框架，结合高斯过程运动先验进行高保真运动插值，通过交替进行GP运动状态更新和梯度校准变量更新，并设计统计一致的序列初始化方案。

Result: UIC框架可应用于IMU、相机等多种传感器，并发布了开源DVL-相机校准工具箱。仿真和真实世界测试验证了方法的有效性。

Conclusion: 该框架不仅适用于水下应用，其GP先验集成和可靠初始化程序等设计也可广泛应用于其他多传感器校准问题。

Abstract: The calibration of extrinsic parameters and clock offsets between sensors for
high-accuracy performance in underwater SLAM systems remains insufficiently
explored. Existing methods for Doppler Velocity Log (DVL) calibration are
either constrained to specific sensor configurations or rely on oversimplified
assumptions, and none jointly estimate translational extrinsics and time
offsets. We propose a Unified Iterative Calibration (UIC) framework for general
DVL sensor setups, formulated as a Maximum A Posteriori (MAP) estimation with a
Gaussian Process (GP) motion prior for high-fidelity motion interpolation. UIC
alternates between efficient GP-based motion state updates and gradient-based
calibration variable updates, supported by a provably statistically consistent
sequential initialization scheme. The proposed UIC can be applied to IMU,
cameras and other modalities as co-sensors. We release an open-source
DVL-camera calibration toolbox. Beyond underwater applications, several aspects
of UIC-such as the integration of GP priors for MAP-based calibration and the
design of provably reliable initialization procedures-are broadly applicable to
other multi-sensor calibration problems. Finally, simulations and real-world
tests validate our approach.

</details>


### [24] [Towards Quadrupedal Jumping and Walking for Dynamic Locomotion using Reinforcement Learning](https://arxiv.org/abs/2510.24584)
*Jørgen Anker Olsen,Lars Rønhaug Pettersen,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种基于课程学习的强化学习框架，用于训练机器人'Olympus'的精确高性能跳跃策略。该方法通过分离垂直和水平跳跃策略，利用弹道运动定律稠密化稀疏奖励，并采用参考状态初始化方案加速探索。结合行走策略，实现了多功能动态运动能力，实验验证了在多种地形上的行走和超越先前工作的跳跃性能。


<details>
  <summary>Details</summary>
Motivation: 开发能够实现精确高性能跳跃的机器人策略，解决稀疏奖励问题和动态跳跃行为的探索挑战，同时实现行走与跳跃的协同运动能力。

Method: 采用基于课程学习的强化学习框架，分离垂直和水平跳跃策略；利用弹道运动定律稠密化稀疏奖励；使用参考状态初始化方案加速动态跳跃行为探索；结合行走策略实现多功能运动。

Result: 实验验证了在多种地形上的行走能力，水平跳跃可达1.25米且具有厘米级精度，垂直跳跃可达1.0米。仅需少量修改即可学习全向跳跃，有效跨越Sim2Real差距。

Conclusion: 提出的方法成功训练出精确高性能的跳跃策略，结合行走策略实现了多功能动态运动能力，在真实环境中验证了卓越的跳跃性能，为机器人动态运动控制提供了有效解决方案。

Abstract: This paper presents a curriculum-based reinforcement learning framework for
training precise and high-performance jumping policies for the robot `Olympus'.
Separate policies are developed for vertical and horizontal jumps, leveraging a
simple yet effective strategy. First, we densify the inherently sparse jumping
reward using the laws of projectile motion. Next, a reference state
initialization scheme is employed to accelerate the exploration of dynamic
jumping behaviors without reliance on reference trajectories. We also present a
walking policy that, when combined with the jumping policies, unlocks versatile
and dynamic locomotion capabilities. Comprehensive testing validates walking on
varied terrain surfaces and jumping performance that exceeds previous works,
effectively crossing the Sim2Real gap. Experimental validation demonstrates
horizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to
1.0 m. Additionally, we show that with only minor modifications, the proposed
method can be used to learn omnidirectional jumping.

</details>


### [25] [Fare: Failure Resilience in Learned Visual Navigation Control](https://arxiv.org/abs/2510.24680)
*Zishuo Wang,Joel Loo,David Hsu*

Main category: cs.RO

TL;DR: Fare框架构建了故障恢复的模仿学习策略，通过OOD检测和识别实现故障自动恢复，无需使用显式故障数据。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在分布外场景中容易发生不可预测的故障，需要能够检测故障并自动恢复的故障恢复策略。

Method: 提出Fare框架，在策略中嵌入OOD检测和识别功能，无需显式故障数据，并配备恢复启发式方法。

Result: 真实世界实验表明，Fare能够在两种不同策略架构上实现故障恢复，在复杂环境中实现鲁棒的长距离导航。

Conclusion: Fare框架成功构建了故障恢复的模仿学习策略，显著提升了视觉导航在分布外场景中的鲁棒性。

Abstract: While imitation learning (IL) enables effective visual navigation, IL
policies are prone to unpredictable failures in out-of-distribution (OOD)
scenarios. We advance the notion of failure-resilient policies, which not only
detect failures but also recover from them automatically. Failure recognition
that identifies the factors causing failure is key to informing recovery: e.g.
pinpointing image regions triggering failure detections can provide cues to
guide recovery. We present Fare, a framework to construct failure-resilient IL
policies, embedding OOD-detection and recognition in them without using
explicit failure data, and pairing them with recovery heuristics. Real-world
experiments show that Fare enables failure recovery across two different policy
architectures, enabling robust long-range navigation in complex environments.

</details>


### [26] [A Framework for the Systematic Evaluation of Obstacle Avoidance and Object-Aware Controllers](https://arxiv.org/abs/2510.24683)
*Caleb Escobedo,Nataliya Nechyporenko,Shreyas Kadekodi,Alessandro Roncone*

Main category: cs.RO

TL;DR: 提出了一个分析物体感知控制器的框架，重点关注运动学、运动曲线和虚拟约束三个设计考虑因素，并通过机器人-障碍物实验场景验证机器人行为。


<details>
  <summary>Details</summary>
Motivation: 实时控制是机器人在动态物体环境中安全操作的关键方面，需要开发能够预测和避免碰撞的物体感知控制器。

Method: 建立分析框架，重点关注运动学、运动曲线和虚拟约束三个设计因素，使用基础机器人-障碍物实验场景进行行为验证，并比较三个代表性物体感知控制器。

Result: 分析发现物体感知控制器的设计通常缺乏运动学考虑、控制点连续性和运动曲线稳定性。

Conclusion: 该框架可用于未来设计、比较和基准测试避障方法。

Abstract: Real-time control is an essential aspect of safe robot operation in the real
world with dynamic objects. We present a framework for the analysis of
object-aware controllers, methods for altering a robot's motion to anticipate
and avoid possible collisions. This framework is focused on three design
considerations: kinematics, motion profiles, and virtual constraints.
Additionally, the analysis in this work relies on verification of robot
behaviors using fundamental robot-obstacle experimental scenarios. To showcase
the effectiveness of our method we compare three representative object-aware
controllers. The comparison uses metrics originating from the design
considerations. From the analysis, we find that the design of object-aware
controllers often lacks kinematic considerations, continuity of control points,
and stability in movement profiles. We conclude that this framework can be used
in the future to design, compare, and benchmark obstacle avoidance methods.

</details>


### [27] [Embodying Physical Computing into Soft Robots](https://arxiv.org/abs/2510.24692)
*Jun Wang,Ziyang Zhou,Ardalan Kahak,Suyi Li*

Main category: cs.RO

TL;DR: 本文提出将物理计算嵌入软体机器人的框架，探讨了三种实现策略：模拟振荡器、物理储层计算和物理算法计算，使软体机器人能够执行复杂行为而无需传统电子元件。


<details>
  <summary>Details</summary>
Motivation: 软化和集成计算机与控制器是软体机器人实现日常使用鲁棒性和智能化的关键挑战，物理计算为实现这一目标提供了有前景的途径。

Method: 提出物理计算框架，通过三种具体策略：1) 模拟振荡器；2) 物理储层计算；3) 物理算法计算，将输入编码到机械计算内核并利用内部相互作用计算输出。

Result: 这些嵌入式计算机使软体机器人能够执行复杂行为，包括带障碍物规避的协调运动、有效载荷重量和方向分类，以及基于逻辑规则的可编程操作。

Conclusion: 本文详细阐述了这些嵌入式物理计算方法的工作原理，综述了当前最新进展，并为未来发展提供了展望。

Abstract: Softening and onboarding computers and controllers is one of the final
frontiers in soft robotics towards their robustness and intelligence for
everyday use. In this regard, embodying soft and physical computing presents
exciting potential. Physical computing seeks to encode inputs into a mechanical
computing kernel and leverage the internal interactions among this kernel's
constituent elements to compute the output. Moreover, such input-to-output
evolution can be re-programmable. This perspective paper proposes a framework
for embodying physical computing into soft robots and discusses three unique
strategies in the literature: analog oscillators, physical reservoir computing,
and physical algorithmic computing. These embodied computers enable the soft
robot to perform complex behaviors that would otherwise require CMOS-based
electronics -- including coordinated locomotion with obstacle avoidance,
payload weight and orientation classification, and programmable operation based
on logical rules. This paper will detail the working principles of these
embodied physical computing methods, survey the current state-of-the-art, and
present a perspective for future development.

</details>
