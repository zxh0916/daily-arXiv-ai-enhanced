<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 19]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Toward an Agricultural Operational Design Domain: A Framework](https://arxiv.org/abs/2511.02937)
*Mirco Felske,Jannik Redenius,Georg Happich,Julius Schöning*

Main category: cs.RO

TL;DR: 本文提出了农业ODD框架，用于描述和验证自主农业系统的操作边界，解决了现有ODD概念在农业应用中的不足。


<details>
  <summary>Details</summary>
Motivation: 农业自动化系统在复杂多变的环境中运行，需要处理驾驶和工作过程的双重约束，现有ODD概念无法满足农业应用的独特需求。

Method: 提出农业ODD框架，包含三个核心要素：结构化描述概念、扩展的7层模型（包含过程层）、迭代验证流程。

Result: 该框架能够创建明确且可验证的农业ODD，支持自主农业系统环境描述的标准化和可扩展性。

Conclusion: 农业ODD框架为自主农业系统提供了统一的环境描述和验证方法，有助于提高开发效率和系统可靠性。

Abstract: The agricultural sector increasingly relies on autonomous systems that
operate in complex and variable environments. Unlike on-road applications,
agricultural automation integrates driving and working processes, each of which
imposes distinct operational constraints. Handling this complexity and ensuring
consistency throughout the development and validation processes requires a
structured, transparent, and verified description of the environment. However,
existing Operational Design Domain (ODD) concepts do not yet address the unique
challenges of agricultural applications.
  Therefore, this work introduces the Agricultural ODD (Ag-ODD) Framework,
which can be used to describe and verify the operational boundaries of
autonomous agricultural systems. The Ag-ODD Framework consists of three core
elements. First, the Ag-ODD description concept, which provides a structured
method for unambiguously defining environmental and operational parameters
using concepts from ASAM Open ODD and CityGML. Second, the 7-Layer Model
derived from the PEGASUS 6-Layer Model, has been extended to include a process
layer to capture dynamic agricultural operations. Third, the iterative
verification process verifies the Ag-ODD against its corresponding logical
scenarios, derived from the 7-Layer Model, to ensure the Ag-ODD's completeness
and consistency.
  Together, these elements provide a consistent approach for creating
unambiguous and verifiable Ag-ODD. Demonstrative use cases show how the Ag-ODD
Framework can support the standardization and scalability of environmental
descriptions for autonomous agricultural systems.

</details>


### [2] [Comprehensive Assessment of LiDAR Evaluation Metrics: A Comparative Study Using Simulated and Real Data](https://arxiv.org/abs/2511.02994)
*Syed Mostaquim Ali,Taufiq Rahman,Ghazal Farhani,Mohamed H. Zaki,Benoit Anctil,Dominique Charlebois*

Main category: cs.RO

TL;DR: 本文探索了用于比较真实世界和模拟LiDAR扫描的评估指标，发现密度感知Chamfer距离（DCD）在所有情况下表现最佳，并构建了虚拟测试环境来验证模拟和真实LiDAR扫描的几何相似性和感知差异。


<details>
  <summary>Details</summary>
Motivation: 由于传统物理测试成本高且存在安全风险，需要采用虚拟测试环境（VTE）来开发安全的自动驾驶系统，而比较VTE生成的传感器输出与真实世界对应物可以验证VTE的准确性。

Method: 采用综合实验方法测试不同评估指标的敏感性和准确性，包括噪声、密度、失真、传感器方向和通道设置；构建虚拟测试环境，使用配备LiDAR、IMU和摄像头的仪器车辆在受控环境中收集数据，生成模拟LiDAR扫描。

Result: 密度感知Chamfer距离（DCD）在所有情况下表现最佳；模拟和真实LiDAR扫描在语义分割输出上相似（mIoU为21%），平均DCD为0.63，表明几何属性略有差异但模型输出存在显著差异。

Conclusion: 密度感知Chamfer距离是与感知方法最相关的评估指标，可用于有效比较真实和模拟LiDAR扫描，为自动驾驶系统的虚拟测试提供可靠评估工具。

Abstract: For developing safe Autonomous Driving Systems (ADS), rigorous testing is
required before they are deemed safe for road deployments. Since comprehensive
conventional physical testing is impractical due to cost and safety concerns,
Virtual Testing Environments (VTE) can be adopted as an alternative. Comparing
VTE-generated sensor outputs against their real-world analogues can be a strong
indication that the VTE accurately represents reality. Correspondingly, this
work explores a comprehensive experimental approach to finding evaluation
metrics suitable for comparing real-world and simulated LiDAR scans. The
metrics were tested in terms of sensitivity and accuracy with different noise,
density, distortion, sensor orientation, and channel settings. From comparing
the metrics, we found that Density Aware Chamfer Distance (DCD) works best
across all cases. In the second step of the research, a Virtual Testing
Environment was generated using real LiDAR scan data. The data was collected in
a controlled environment with only static objects using an instrumented vehicle
equipped with LiDAR, IMU and cameras. Simulated LiDAR scans were generated from
the VTEs using the same pose as real LiDAR scans. The simulated and LiDAR scans
were compared in terms of model perception and geometric similarity. Actual and
simulated LiDAR scans have a similar semantic segmentation output with a mIoU
of 21\% with corrected intensity and an average density aware chamfer distance
(DCD) of 0.63. This indicates a slight difference in the geometric properties
of simulated and real LiDAR scans and a significant difference between model
outputs. During the comparison, density-aware chamfer distance was found to be
the most correlated among the metrics with perception methods.

</details>


### [3] [WorldPlanner: Monte Carlo Tree Search and MPC with Action-Conditioned Visual World Models](https://arxiv.org/abs/2511.03077)
*R. Khorrambakht,Joaquim Ortiz-Haro,Joseph Amigo,Omar Mostafa,Daniel Dugas,Franziska Meier,Ludovic Righetti*

Main category: cs.RO

TL;DR: 该论文提出了一种基于模型的方法，使用易于收集的非结构化游戏数据学习视觉世界模型、扩散动作采样器和可选奖励模型，然后通过蒙特卡洛树搜索规划器优化动作序列，在真实机器人任务中显著优于行为克隆基线。


<details>
  <summary>Details</summary>
Motivation: 解决行为克隆方法难以迁移到新任务、训练数据收集困难的问题，通过模型规划方法实现更好的泛化能力和任务适应性。

Method: 收集非结构化游戏数据学习动作条件视觉世界模型、扩散动作采样器和奖励模型，使用蒙特卡洛树搜索规划器优化长动作序列，通过零阶模型预测控制器执行计划。

Result: 在3个真实世界机器人任务上验证了方法的有效性，规划方法相比行为克隆基线在标准操作测试环境中显著提升性能。

Conclusion: 基于模型的规划方法能够有效缓解世界模型在规划过程中的幻觉问题，在复杂机器人任务中展现出比行为克隆更好的性能。

Abstract: Robots must understand their environment from raw sensory inputs and reason
about the consequences of their actions in it to solve complex tasks. Behavior
Cloning (BC) leverages task-specific human demonstrations to learn this
knowledge as end-to-end policies. However, these policies are difficult to
transfer to new tasks, and generating training data is challenging because it
requires careful demonstrations and frequent environment resets. In contrast to
such policy-based view, in this paper we take a model-based approach where we
collect a few hours of unstructured easy-to-collect play data to learn an
action-conditioned visual world model, a diffusion-based action sampler, and
optionally a reward model. The world model -- in combination with the action
sampler and a reward model -- is then used to optimize long sequences of
actions with a Monte Carlo Tree Search (MCTS) planner. The resulting plans are
executed on the robot via a zeroth-order Model Predictive Controller (MPC). We
show that the action sampler mitigates hallucinations of the world model during
planning and validate our approach on 3 real-world robotic tasks with varying
levels of planning and modeling complexity. Our experiments support the
hypothesis that planning leads to a significant improvement over BC baselines
on a standard manipulation test environment.

</details>


### [4] [Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.03167)
*Xin Liu,Jinze Wu,Yinghui Li,Chenkun Qi,Yufei Xue,Feng Gao*

Main category: cs.RO

TL;DR: 本文提出了一种基于运动先验的方法，成功将深度强化学习算法应用于真实六足机器人，使其能够在复杂地形中生成自然步态并展现出色鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多足机器人在复杂地形中具有更好的稳定性，但如何在大动作探索空间中有效协调多条腿以生成自然且鲁棒的运动是一个关键问题。

Method: 生成优化运动先验数据集，基于先验训练对抗判别器来引导六足机器人学习自然步态，并将学习到的策略成功转移到真实六足机器人上。

Result: 学习到的策略在真实六足机器人上展示了自然步态模式，在复杂地形中无需视觉信息即表现出显著的鲁棒性。

Conclusion: 这是首次将强化学习控制器成功用于实现真实六足机器人在复杂地形上的行走。

Abstract: Multi-legged robots offer enhanced stability to navigate complex terrains
with their multiple legs interacting with the environment. However, how to
effectively coordinate the multiple legs in a larger action exploration space
to generate natural and robust movements is a key issue. In this paper, we
introduce a motion prior-based approach, successfully applying deep
reinforcement learning algorithms to a real hexapod robot. We generate a
dataset of optimized motion priors, and train an adversarial discriminator
based on the priors to guide the hexapod robot to learn natural gaits. The
learned policy is then successfully transferred to a real hexapod robot, and
demonstrate natural gait patterns and remarkable robustness without visual
information in complex terrains. This is the first time that a reinforcement
learning controller has been used to achieve complex terrain walking on a real
hexapod robot.

</details>


### [5] [Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control](https://arxiv.org/abs/2511.03181)
*Rewida Ali,Cristian C. Beltran-Hernandez,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一个基于学习的框架，结合LLM高级任务规划器和混合模仿学习与强化学习的低级策略，用于解决礼品包装等长时程变形物体操作问题，在真实世界任务中达到97%的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中变形物体操作的挑战，特别是礼品包装这类需要精确折叠、控制折痕和安全固定的长时程操作任务。

Method: 集成LLM高级任务规划器和混合IL/RL低级策略，核心是子任务感知机器人变换器(START)，通过子任务ID提供显式时间定位，学习统一策略。

Result: 在真实世界包装任务中达到97%的成功率，统一变换器策略减少了对专用模型的需求，支持可控的人类监督。

Conclusion: 该框架有效连接高级意图与细粒度力控制，为变形物体操作提供了稳健的解决方案。

Abstract: Human-robot cooperation is essential in environments such as warehouses and
retail stores, where workers frequently handle deformable objects like paper,
bags, and fabrics. Coordinating robotic actions with human assistance remains
difficult due to the unpredictable dynamics of deformable materials and the
need for adaptive force control. To explore this challenge, we focus on the
task of gift wrapping, which exemplifies a long-horizon manipulation problem
involving precise folding, controlled creasing, and secure fixation of paper.
Success is achieved when the robot completes the sequence to produce a neatly
wrapped package with clean folds and no tears.
  We propose a learning-based framework that integrates a high-level task
planner powered by a large language model (LLM) with a low-level hybrid
imitation learning (IL) and reinforcement learning (RL) policy. At its core is
a Sub-task Aware Robotic Transformer (START) that learns a unified policy from
human demonstrations. The key novelty lies in capturing long-range temporal
dependencies across the full wrapping sequence within a single model. Unlike
vanilla Action Chunking with Transformer (ACT), typically applied to short
tasks, our method introduces sub-task IDs that provide explicit temporal
grounding. This enables robust performance across the entire wrapping process
and supports flexible execution, as the policy learns sub-goals rather than
merely replicating motion sequences.
  Our framework achieves a 97% success rate on real-world wrapping tasks. We
show that the unified transformer-based policy reduces the need for specialized
models, allows controlled human supervision, and effectively bridges high-level
intent with the fine-grained force control required for deformable object
manipulation.

</details>


### [6] [Collaborative Assembly Policy Learning of a Sightless Robot](https://arxiv.org/abs/2511.03189)
*Zeqing Zhang,Weifeng Lu,Lei Yang,Wei Jing,Bowei Tang,Jia Pan*

Main category: cs.RO

TL;DR: 提出一种结合导纳控制和强化学习的新方法，用于盲机器人协助人类完成板插入框架的物理人机协作任务，相比传统导纳控制提高了成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 传统导纳控制在物理人机协作中难以准确测量人力/力矩来估计人类意图，而纯强化学习方法因安全约束和稀疏奖励不适合板插入任务。

Method: 使用人类设计的导纳控制器来促进更主动的机器人行为，结合强化学习方法减少人类操作负担。

Result: 仿真和真实实验表明，该方法在成功率和任务完成时间上优于导纳控制，且测量到的力/力矩显著减少。

Conclusion: 提出的强化学习方法能有效提升盲机器人在物理人机协作任务中的性能，减少人类操作负担。

Abstract: This paper explores a physical human-robot collaboration (pHRC) task
involving the joint insertion of a board into a frame by a sightless robot and
a human operator. While admittance control is commonly used in pHRC tasks, it
can be challenging to measure the force/torque applied by the human for
accurate human intent estimation, limiting the robot's ability to assist in the
collaborative task. Other methods that attempt to solve pHRC tasks using
reinforcement learning (RL) are also unsuitable for the board-insertion task
due to its safety constraints and sparse rewards. Therefore, we propose a novel
RL approach that utilizes a human-designed admittance controller to facilitate
more active robot behavior and reduce human effort. Through simulation and
real-world experiments, we demonstrate that our approach outperforms admittance
control in terms of success rate and task completion time. Additionally, we
observed a significant reduction in measured force/torque when using our
proposed approach compared to admittance control. The video of the experiments
is available at https://youtu.be/va07Gw6YIog.

</details>


### [7] [GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement](https://arxiv.org/abs/2511.03400)
*Minquan Gao,Xinyi Li,Qing Yan,Xiaojian Sun,Xiaopan Zhang,Chien-Ming Huang,Jiachen Li*

Main category: cs.RO

TL;DR: GUIDES是一个轻量级框架，通过基础模型的语义指导增强预训练机器人策略，无需架构重新设计，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 预训练机器人策略缺乏基础模型的语义感知能力，但完全替换成本高昂且会丢失积累的知识。

Method: 使用微调的视觉语言模型生成上下文指令，通过辅助模块编码为引导嵌入，注入策略的潜在空间，并进行简短的目标微调。推理时使用基于大语言模型的反射器监控置信度并优化动作。

Result: 在RoboCasa仿真环境中验证，不同策略架构的任务成功率均有显著提升。UR5机器人实际部署显示提高了关键子任务（如抓取）的运动精度。

Conclusion: GUIDES提供了一种实用且资源高效的途径来升级而非替换经过验证的机器人策略。

Abstract: Pre-trained robot policies serve as the foundation of many validated robotic
systems, which encapsulate extensive embodied knowledge. However, they often
lack the semantic awareness characteristic of foundation models, and replacing
them entirely is impractical in many situations due to high costs and the loss
of accumulated knowledge. To address this gap, we introduce GUIDES, a
lightweight framework that augments pre-trained policies with semantic guidance
from foundation models without requiring architectural redesign. GUIDES employs
a fine-tuned vision-language model (Instructor) to generate contextual
instructions, which are encoded by an auxiliary module into guidance
embeddings. These embeddings are injected into the policy's latent space,
allowing the legacy model to adapt to this new semantic input through brief,
targeted fine-tuning. For inference-time robustness, a large language
model-based Reflector monitors the Instructor's confidence and, when confidence
is low, initiates a reasoning loop that analyzes execution history, retrieves
relevant examples, and augments the VLM's context to refine subsequent actions.
Extensive validation in the RoboCasa simulation environment across diverse
policy architectures shows consistent and substantial improvements in task
success rates. Real-world deployment on a UR5 robot further demonstrates that
GUIDES enhances motion precision for critical sub-tasks such as grasping.
Overall, GUIDES offers a practical and resource-efficient pathway to upgrade,
rather than replace, validated robot policies.

</details>


### [8] [Value Elicitation for a Socially Assistive Robot Addressing Social Anxiety: A Participatory Design Approach](https://arxiv.org/abs/2511.03444)
*Vesna Poprcova,Iulia Lefter,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: 本研究通过参与式设计工作坊，探索了社交辅助机器人支持社交焦虑的价值导向设计，揭示了适应性、接受度和有效性等核心设计价值观。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑是普遍的心理健康问题，但现有支持不足。社交机器人技术为补充传统心理健康干预提供了新机会，需要理解哪些价值观能塑造有意义、可接受且有效的设计。

Method: 采用参与式设计工作坊，与心理健康学术研究人员合作，通过创意、反思和展望活动，系统性地探索情景和设计可能性，引出相关价值观、期望、需求和偏好。

Result: 研究发现丰富的设计相关价值观，包括适应性、接受度和有效性，这些是支持社交焦虑个体的核心要素。

Conclusion: 研究强调了以研究为主导的价值引出方法的重要性，在社交辅助机器人开发中应注重以用户为中心和情境感知的设计考量。

Abstract: Social anxiety is a prevalent mental health condition that can significantly
impact overall well-being and quality of life. Despite its widespread effects,
adequate support or treatment for social anxiety is often insufficient.
Advances in technology, particularly in social robotics, offer promising
opportunities to complement traditional mental health. As an initial step
toward developing effective solutions, it is essential to understand the values
that shape what is considered meaningful, acceptable, and helpful. In this
study, a participatory design workshop was conducted with mental health
academic researchers to elicit the underlying values that should inform the
design of socially assistive robots for social anxiety support. Through
creative, reflective, and envisioning activities, participants explored
scenarios and design possibilities, allowing for systematic elicitation of
values, expectations, needs, and preferences related to robot-supported
interventions. The findings reveal rich insights into design-relevant
values-including adaptivity, acceptance, and efficacy-that are core to support
for individuals with social anxiety. This study highlights the significance of
a research-led approach to value elicitation, emphasising user-centred and
context-aware design considerations in the development of socially assistive
robots.

</details>


### [9] [Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control](https://arxiv.org/abs/2511.03481)
*Jianbo Yuan,Haohua Zhu,Jing Dai,Sheng Yi*

Main category: cs.RO

TL;DR: Dex-Hand 021是一款高性能的电缆驱动五指机器人手，具有12个主动和7个被动自由度，总自由度19，重量仅1公斤。采用基于本体感觉力传感的导纳控制方法，在单指负载能力、重复精度和力估计精度方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类手在日常生活和工业应用中至关重要，但复制其多功能能力（包括运动、感知和协调操作）仍然是一个巨大挑战。需要平衡类人敏捷性与工程约束（如复杂性、尺寸重量比、耐久性和力传感性能）。

Method: 开发了电缆驱动的五指机器人手，采用基于本体感觉力传感的导纳控制方法来增强操作能力。

Result: 实验结果显示：单指负载能力超过10N，指尖重复精度低于0.001m，力估计误差低于0.2N。与PID控制相比，多物体抓取中的关节扭矩减少了31.19%，显著提高了力传感能力并防止碰撞过载。成功执行了33种GRASP分类动作和复杂操作任务。

Conclusion: 这项工作推进了轻量级工业级灵巧手的设计，增强了本体感觉控制，为机器人操作和智能制造做出了贡献。

Abstract: The human hand plays a vital role in daily life and industrial applications,
yet replicating its multifunctional capabilities-including motion, sensing, and
coordinated manipulation-with robotic systems remains a formidable challenge.
Developing a dexterous robotic hand requires balancing human-like agility with
engineering constraints such as complexity, size-to-weight ratio, durability,
and force-sensing performance. This letter presents Dex-Hand 021, a
high-performance, cable-driven five-finger robotic hand with 12 active and 7
passive degrees of freedom (DoFs), achieving 19 DoFs dexterity in a lightweight
1 kg design. We propose a proprioceptive force-sensing-based admittance control
method to enhance manipulation. Experimental results demonstrate its superior
performance: a single-finger load capacity exceeding 10 N, fingertip
repeatability under 0.001 m, and force estimation errors below 0.2 N. Compared
to PID control, joint torques in multi-object grasping are reduced by 31.19%,
significantly improves force-sensing capability while preventing overload
during collisions. The hand excels in both power and precision grasps,
successfully executing 33 GRASP taxonomy motions and complex manipulation
tasks. This work advances the design of lightweight, industrial-grade dexterous
hands and enhances proprioceptive control, contributing to robotic manipulation
and intelligent manufacturing.

</details>


### [10] [ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications](https://arxiv.org/abs/2511.03497)
*Lei Fu,Sahar Salimpour,Leonardo Militano,Harry Edelman,Jorge Peña Queralta,Giovanni Toffetti*

Main category: cs.RO

TL;DR: 本文提出了一个用于分析ROS和ROS 2数据包的MCP服务器，通过LLMs和VLMs实现机器人数据的自然语言分析、可视化和处理，并评估了不同语言模型的工具调用能力。


<details>
  <summary>Details</summary>
Motivation: Agentic AI系统和物理或具身AI系统是人工智能和机器人学的前沿研究方向，但这两个领域的交叉研究仍然稀缺。本文旨在填补这一空白，通过MCP协议实现具身智能体的机器人数据分析。

Method: 构建了一个专门用于ROS/ROS 2数据包分析的MCP服务器，集成了机器人领域知识的工具，支持轨迹、激光扫描数据、变换和时间序列数据的分析，并提供轻量级UI进行不同LLM模型的基准测试。

Result: 实验评估了8种最先进的LLM/VLM模型的工具调用能力，发现Kimi K2和Claude Sonnet 4表现出明显优越的性能，同时识别出影响成功率的多个因素，包括工具描述模式、参数数量和可用工具数量。

Conclusion: 该工作为Agentic Embodied AI领域提供了重要工具，证明了MCP在机器人数据分析中的有效性，并揭示了不同语言模型在工具调用能力上的显著差异。

Abstract: Agentic AI systems and Physical or Embodied AI systems have been two key
research verticals at the forefront of Artificial Intelligence and Robotics,
with Model Context Protocol (MCP) increasingly becoming a key component and
enabler of agentic applications. However, the literature at the intersection of
these verticals, i.e., Agentic Embodied AI, remains scarce. This paper
introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for
analyzing, visualizing and processing robot data with natural language through
LLMs and VLMs. We describe specific tooling built with robotics domain
knowledge, with our initial release focused on mobile robotics and supporting
natively the analysis of trajectories, laser scan data, transforms, or time
series data. This is in addition to providing an interface to standard ROS 2
CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to
filter bags with a subset of topics or trimmed in time. Coupled with the MCP
server, we provide a lightweight UI that allows the benchmarking of the tooling
with different LLMs, both proprietary (Anthropic, OpenAI) and open-source
(through Groq). Our experimental results include the analysis of tool calling
capabilities of eight different state-of-the-art LLM/VLM models, both
proprietary and open-source, large and small. Our experiments indicate that
there is a large divide in tool calling capabilities, with Kimi K2 and Claude
Sonnet 4 demonstrating clearly superior performance. We also conclude that
there are multiple factors affecting the success rates, from the tool
description schema to the number of arguments, as well as the number of tools
available to the models. The code is available with a permissive license at
https://github.com/binabik-ai/mcp-rosbags.

</details>


### [11] [Indicating Robot Vision Capabilities with Augmented Reality](https://arxiv.org/abs/2511.03550)
*Hong Wang,Ridhima Phatak,James Ocampo,Zhao Han*

Main category: cs.RO

TL;DR: 本研究提出四种增强现实视野指示器，帮助人类准确理解机器人的视野范围，解决人类错误假设机器人视野与人类相同的问题。


<details>
  <summary>Details</summary>
Motivation: 人类经常错误地假设机器人与人类具有相同的视野范围，这种不准确的心理模型可能导致人机协作任务失败，特别是当机器人被要求完成关于视野外物体的不可能任务时。

Method: 在增强现实中设计了四种视野指示器，涵盖从自我中心（机器人眼部和头部空间）到异我中心（任务空间）的频谱，并通过用户实验（N=41）评估其准确性、信心、任务效率和认知负荷。

Result: 结果显示，任务空间的异我中心块状指示器具有最高的准确性，但解读机器人视野时存在延迟；自我中心的深眼窝指示器也能提高准确性；所有指示器下参与者信心高且认知负荷低。

Conclusion: 研究为实践者提供了六条指南，以应用增强现实指示器或物理改造来使人类心理模型与机器人视觉能力保持一致。

Abstract: Research indicates that humans can mistakenly assume that robots and humans
have the same field of view (FoV), possessing an inaccurate mental model of
robots. This misperception may lead to failures during human-robot
collaboration tasks where robots might be asked to complete impossible tasks
about out-of-view objects. The issue is more severe when robots do not have a
chance to scan the scene to update their world model while focusing on assigned
tasks. To help align humans' mental models of robots' vision capabilities, we
propose four FoV indicators in augmented reality (AR) and conducted a user
human-subjects experiment (N=41) to evaluate them in terms of accuracy,
confidence, task efficiency, and workload. These indicators span a spectrum
from egocentric (robot's eye and head space) to allocentric (task space).
Results showed that the allocentric blocks at the task space had the highest
accuracy with a delay in interpreting the robot's FoV. The egocentric indicator
of deeper eye sockets, possible for physical alteration, also increased
accuracy. In all indicators, participants' confidence was high while cognitive
load remained low. Finally, we contribute six guidelines for practitioners to
apply our AR indicators or physical alterations to align humans' mental models
with robots' vision capabilities.

</details>


### [12] [OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera](https://arxiv.org/abs/2511.03571)
*Hao Shi,Ze Wang,Shangwei Guo,Mengfei Duan,Song Wang,Teng Chen,Kailun Yang,Lin Wang,Kaiwei Wang*

Main category: cs.RO

TL;DR: OneOcc是一个专为腿式/人形机器人设计的全景语义场景补全框架，通过双投影融合、双网格体素化、轻量级解码器和步态位移补偿等技术，实现了360度连续感知，在两个新发布的全景占用基准上达到了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语义场景补全系统主要面向轮式平台和前向传感器，而腿式/人形机器人需要应对步态引起的身体抖动和360度连续感知需求。

Method: 结合双投影融合(DP-ER)利用环形全景和等距柱面展开，双网格体素化(BGV)在笛卡尔和柱极坐标系中推理，轻量级解码器与分层AMoE-3D进行动态多尺度融合，以及即插即用的步态位移补偿(GDC)学习特征级运动校正。

Result: 在QuadOcc基准上超越了强大的视觉基线和流行的LiDAR方法；在H3O基准上分别获得+3.83 mIoU(城内)和+8.08(跨城)的提升。

Conclusion: OneOcc模块轻量级，可为腿式/人形机器人提供可部署的全方位感知能力，发布的两个全景占用基准将推动相关研究发展。

Abstract: Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most
semantic scene completion (SSC) systems target wheeled platforms with
forward-facing sensors. We present OneOcc, a vision-only panoramic SSC
framework designed for gait-introduced body jitter and 360{\deg} continuity.
OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular
panorama and its equirectangular unfolding, preserving 360{\deg} continuity and
grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and
cylindrical-polar spaces, reducing discretization bias and sharpening
free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D
for dynamic multi-scale fusion and better long-range/occlusion reasoning; and
(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level
motion correction without extra sensors. We also release two panoramic
occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and
Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic
occupancy; standardized within-/cross-city splits). OneOcc sets new
state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and
popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08
(cross-city). Modules are lightweight, enabling deployable full-surround
perception for legged/humanoid robots. Datasets and code will be publicly
available at https://github.com/MasterHow/OneOcc.

</details>


### [13] [Multi-User Personalisation in Human-Robot Interaction: Using Quantitative Bipolar Argumentation Frameworks for Preferences Conflict Resolution](https://arxiv.org/abs/2511.03576)
*Aniol Civit,Antonio Andriella,Carles Sierra,Guillem Alenyà*

Main category: cs.RO

TL;DR: 本文提出了MUP-QBAF框架，基于定量双极论证框架解决多用户个性化中的偏好冲突问题，特别针对机器人环境中的动态观察和适应性需求。


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互个性化方法主要关注单用户适应，忽略了多用户场景中可能存在的偏好冲突问题。

Method: 采用定量双极论证框架，将用户偏好表示为参数，结合机器人的动态环境观察，迭代重新计算参数强度。

Result: 通过一个辅助机器人在衰弱评估任务中调解护理人员和被护理者偏好冲突的现实案例研究，验证了框架的有效性，并进行了参数基础分数的敏感性分析。

Conclusion: 该工作为多用户人机交互领域提供了透明、结构化且上下文敏感的方法来解决竞争性用户偏好，为数据驱动方法提供了原则性替代方案。

Abstract: While personalisation in Human-Robot Interaction (HRI) has advanced
significantly, most existing approaches focus on single-user adaptation,
overlooking scenarios involving multiple stakeholders with potentially
conflicting preferences. To address this, we propose the Multi-User Preferences
Quantitative Bipolar Argumentation Framework (MUP-QBAF), a novel multi-user
personalisation framework based on Quantitative Bipolar Argumentation
Frameworks (QBAFs) that explicitly models and resolves multi-user preference
conflicts. Unlike prior work in Argumentation Frameworks, which typically
assumes static inputs, our approach is tailored to robotics: it incorporates
both users' arguments and the robot's dynamic observations of the environment,
allowing the system to adapt over time and respond to changing contexts.
Preferences, both positive and negative, are represented as arguments whose
strength is recalculated iteratively based on new information. The framework's
properties and capabilities are presented and validated through a realistic
case study, where an assistive robot mediates between the conflicting
preferences of a caregiver and a care recipient during a frailty assessment
task. This evaluation further includes a sensitivity analysis of argument base
scores, demonstrating how preference outcomes can be shaped by user input and
contextual observations. By offering a transparent, structured, and
context-sensitive approach to resolving competing user preferences, this work
advances the field of multi-user HRI. It provides a principled alternative to
data-driven methods, enabling robots to navigate conflicts in real-world
environments.

</details>


### [14] [Manifold-constrained Hamilton-Jacobi Reachability Learning for Decentralized Multi-Agent Motion Planning](https://arxiv.org/abs/2511.03591)
*Qingyi Chen,Ruiqi Ni,Jun Kim,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本文提出了一种流形约束的Hamilton-Jacobi可达性学习框架，用于解决多智能体运动规划中考虑任务诱导约束的安全导航问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中许多场景要求机器人在动态环境中导航时遵守任务施加的流形约束，例如服务机器人必须保持杯子直立同时避免与其他机器人或人类碰撞。现有的去中心化多智能体运动规划方法难以有效整合流形约束。

Method: 通过求解流形约束下的Hamilton-Jacobi可达性问题来捕捉任务感知的安全条件，然后将这些条件集成到去中心化轨迹优化规划器中，使机器人能够生成既安全又任务可行的运动规划。

Result: 该方法能够泛化到不同的流形约束任务，并有效扩展到高维多智能体操作问题。实验表明该方法优于现有的约束运动规划器，且运行速度适合实际应用。

Conclusion: 所提出的流形约束Hamilton-Jacobi可达性学习框架为多智能体运动规划提供了一种有效的解决方案，能够在复杂约束条件下实现安全且任务可行的运动规划。

Abstract: Safe multi-agent motion planning (MAMP) under task-induced constraints is a
critical challenge in robotics. Many real-world scenarios require robots to
navigate dynamic environments while adhering to manifold constraints imposed by
tasks. For example, service robots must carry cups upright while avoiding
collisions with humans or other robots. Despite recent advances in
decentralized MAMP for high-dimensional systems, incorporating manifold
constraints remains difficult. To address this, we propose a
manifold-constrained Hamilton-Jacobi reachability (HJR) learning framework for
decentralized MAMP. Our method solves HJR problems under manifold constraints
to capture task-aware safety conditions, which are then integrated into a
decentralized trajectory optimization planner. This enables robots to generate
motion plans that are both safe and task-feasible without requiring assumptions
about other agents' policies. Our approach generalizes across diverse
manifold-constrained tasks and scales effectively to high-dimensional
multi-agent manipulation problems. Experiments show that our method outperforms
existing constrained motion planners and operates at speeds suitable for
real-world applications. Video demonstrations are available at
https://youtu.be/RYcEHMnPTH8 .

</details>


### [15] [Multi-robot searching with limited sensing range for static and mobile intruders](https://arxiv.org/abs/2511.03622)
*Swadhin Agrawal,Sujoy Bhore,Joseph S. B. Mitchell,P. B. Sujit,Aayush Gohil*

Main category: cs.RO

TL;DR: 研究多机器人搜索几何域中入侵者的问题，针对静态和移动入侵者，发现该问题是NP难的，因此开发了基于空间填充曲线、随机搜索和协作随机搜索的高效鲁棒算法，并评估了机器人数量与搜索时间之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决在几何域中使用多个搜索机器人寻找入侵者的问题，特别是针对具有有限感知能力的机器人在正交多边形域中的搜索需求。

Method: 提出了三种算法：基于空间填充曲线的搜索、随机搜索以及协作随机搜索，考虑了静态和移动入侵者两种情况。

Result: 发现寻找入侵者的问题是NP难的，即使对于静态入侵者也是如此。开发的算法能够有效处理这一复杂问题。

Conclusion: 通过多种搜索策略的开发，为多机器人在几何域中搜索入侵者提供了有效的解决方案，并分析了机器人数量与搜索效率之间的权衡关系。

Abstract: We consider the problem of searching for an intruder in a geometric domain by
utilizing multiple search robots. The domain is a simply connected orthogonal
polygon with edges parallel to the cartesian coordinate axes. Each robot has a
limited sensing capability. We study the problem for both static and mobile
intruders. It turns out that the problem of finding an intruder is NP-hard,
even for a stationary intruder. Given this intractability, we turn our
attention towards developing efficient and robust algorithms, namely methods
based on space-filling curves, random search, and cooperative random search.
Moreover, for each proposed algorithm, we evaluate the trade-off between the
number of search robots and the time required for the robots to complete the
search process while considering the geometric properties of the connected
orthogonal search area.

</details>


### [16] [Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural](https://arxiv.org/abs/2511.03651)
*Andrei A. Korigodskii,Oleg D. Kalachev,Artem E. Vasiunik,Matvei V. Urvantsev,Georgii E. Bondar*

Main category: cs.RO

TL;DR: 本文介绍了一种创新的自主无人机系统，成功完成了世界上最大的无人机壁画创作。系统结合红外运动捕捉相机和LiDAR技术，在恶劣户外条件下实现精确导航和绘画，采用独特的控制架构和轨迹规划算法，确保艺术精度和操作可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决在恶劣户外条件（如大风和阳光直射）下保持艺术精度和操作可靠性的双重挑战，扩展机器人在创意领域的应用范围。

Method: 采用红外运动捕捉相机与LiDAR技术结合的新型导航系统，使用切向和法向不同调节的控制架构，开发轨迹规划和路径优化算法，设计定制喷漆机制以应对螺旋桨产生的湍流。

Result: 实验结果表明系统在各种条件下都具有鲁棒性和精确性，成功完成了世界最大无人机壁画创作，展示了在自主大规模艺术创作中的潜力。

Conclusion: 该系统证明了无人机在创意领域的应用潜力，为自主大规模艺术创作提供了可靠的技术方案，扩展了机器人在艺术创作中的功能应用。

Abstract: This paper presents the innovative design and successful deployment of a
pioneering autonomous unmanned aerial system developed for executing the
world's largest mural painted by a drone. Addressing the dual challenges of
maintaining artistic precision and operational reliability under adverse
outdoor conditions such as wind and direct sunlight, our work introduces a
robust system capable of navigating and painting outdoors with unprecedented
accuracy. Key to our approach is a novel navigation system that combines an
infrared (IR) motion capture camera and LiDAR technology, enabling precise
location tracking tailored specifically for largescale artistic applications.
We employ a unique control architecture that uses different regulation in
tangential and normal directions relative to the planned path, enabling precise
trajectory tracking and stable line rendering. We also present algorithms for
trajectory planning and path optimization, allowing for complex curve drawing
and area filling. The system includes a custom-designed paint spraying
mechanism, specifically engineered to function effectively amidst the turbulent
airflow generated by the drone's propellers, which also protects the drone's
critical components from paint-related damage, ensuring longevity and
consistent performance. Experimental results demonstrate the system's
robustness and precision in varied conditions, showcasing its potential for
autonomous large-scale art creation and expanding the functional applications
of robotics in creative fields.

</details>


### [17] [Motion Planning Under Temporal Logic Specifications In Semantically Unknown Environments](https://arxiv.org/abs/2511.03652)
*Azizollah Taheri,Derya Aksaray*

Main category: cs.RO

TL;DR: 提出一种针对不确定环境中时空逻辑任务（scLTL规范）的运动规划方法，通过构建特殊乘积自动机来捕捉语义标签的不确定性，并使用值迭代进行在线重规划。


<details>
  <summary>Details</summary>
Motivation: 解决在环境语义标签具有概率性不确定性的情况下，如何实现时空逻辑任务规划的问题。例如任务要求"先到区域1再到区域2"，但区域1和2的确切位置未知，只有概率信念可用。

Method: 构建特殊乘积自动机来捕捉语义标签的不确定性，为自动机的每条边设计奖励函数，并利用值迭代算法进行在线重规划。

Result: 展示了理论结果，并通过仿真/实验验证了所提方法的有效性。

Conclusion: 提出的自动机理论方法能够有效处理环境语义标签不确定性下的时空逻辑任务规划问题。

Abstract: This paper addresses a motion planning problem to achieve
spatio-temporal-logical tasks, expressed by syntactically co-safe linear
temporal logic specifications (scLTL\next), in uncertain environments. Here,
the uncertainty is modeled as some probabilistic knowledge on the semantic
labels of the environment. For example, the task is "first go to region 1, then
go to region 2"; however, the exact locations of regions 1 and 2 are not known
a priori, instead a probabilistic belief is available. We propose a novel
automata-theoretic approach, where a special product automaton is constructed
to capture the uncertainty related to semantic labels, and a reward function is
designed for each edge of this product automaton. The proposed algorithm
utilizes value iteration for online replanning. We show some theoretical
results and present some simulations/experiments to demonstrate the efficacy of
the proposed approach.

</details>


### [18] [Unconscious and Intentional Human Motion Cues for Expressive Robot-Arm Motion Design](https://arxiv.org/abs/2511.03676)
*Taito Tashiro,Tomoko Yonezawa,Hirotake Yamazoe*

Main category: cs.RO

TL;DR: 本研究探讨如何利用人类运动线索设计富有表现力的机器人手臂运动，通过分析Geister游戏中人类棋子移动的自然和有意表达，创建了基于速度和停止时长的特定阶段机器人运动，并评估了物理机器人与视频展示的效果差异。


<details>
  <summary>Details</summary>
Motivation: 研究人类运动线索在机器人运动设计中的应用，探索如何通过分析人类在游戏中的自然和有意运动模式来设计更富有表现力的机器人动作。

Method: 使用Geister游戏分析人类棋子移动的两种类型：自然游戏（无意识倾向）和指导表达（有意线索），基于这些发现创建了通过改变运动速度和停止时长来设计的特定阶段机器人运动，并在物理机器人和录制视频两种展示模式下评估观察者的印象。

Result: 结果表明，后期运动时机（特别是撤回阶段）在印象形成中起重要作用，物理体现增强了运动线索的可解释性。

Conclusion: 这些发现为基于人类时机行为设计富有表现力的机器人运动提供了见解，强调了运动时机和物理体现在机器人表达性运动设计中的重要性。

Abstract: This study investigates how human motion cues can be used to design
expressive robot-arm movements. Using the imperfect-information game Geister,
we analyzed two types of human piece-moving motions: natural gameplay
(unconscious tendencies) and instructed expressions (intentional cues). Based
on these findings, we created phase-specific robot motions by varying movement
speed and stop duration, and evaluated observer impressions under two
presentation modalities: a physical robot and a recorded video. Results
indicate that late-phase motion timing, particularly during withdrawal, plays
an important role in impression formation and that physical embodiment enhances
the interpretability of motion cues. These findings provide insights for
designing expressive robot motions based on human timing behavior.

</details>


### [19] [Source-Free Bistable Fluidic Gripper for Size-Selective and Stiffness-Adaptive Grasping](https://arxiv.org/abs/2511.03691)
*Zhihang Qin,Yueheng Zhang,Wan Su,Linxin Hou,Shenghao Zhou,Zhijun Chen,Yu Jun Tan,Cecilia Laschi*

Main category: cs.RO

TL;DR: 本文提出了一种自包含的软体抓取器，通过内部液体在三个互连的双稳态快速切换腔室中重新分配来实现操作，无需外部能源，能够实现稳定、尺寸选择性的抓取和被动适应物体刚度的抓取压力。


<details>
  <summary>Details</summary>
Motivation: 传统的流体驱动软体抓取器通常依赖外部能源，这限制了便携性和长期自主性。本文旨在开发一种无需外部能源的自包含软体抓取器。

Method: 设计了一个固定尺寸的自包含软体抓取器，通过三个互连的双稳态快速切换腔室实现内部液体重新分配。当顶部传感腔室在接触时变形，被置换的液体触发抓取腔室的快速切换扩张。

Result: 该抓取器能够实现稳定和尺寸选择性的抓取，无需持续能量输入，并通过内部液压反馈被动适应抓取压力到物体刚度。

Conclusion: 这种无源紧凑设计为软体机器人中的轻量级、刚度自适应流体驱动操作开辟了新可能性，为水下和野外环境中的目标尺寸特定采样和操作提供了可行方法。

Abstract: Conventional fluid-driven soft grippers typically depend on external sources,
which limit portability and long-term autonomy. This work introduces a
self-contained soft gripper with fixed size that operates solely through
internal liquid redistribution among three interconnected bistable snap-through
chambers. When the top sensing chamber deforms upon contact, the displaced
liquid triggers snap-through expansion of the grasping chambers, enabling
stable and size-selective grasping without continuous energy input. The
internal hydraulic feedback further allows passive adaptation of gripping
pressure to object stiffness. This source-free and compact design opens new
possibilities for lightweight, stiffness-adaptive fluid-driven manipulation in
soft robotics, providing a feasible approach for targeted size-specific
sampling and operation in underwater and field environments.

</details>
