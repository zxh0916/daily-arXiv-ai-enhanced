<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields](https://arxiv.org/abs/2510.25913)
*Gilbert Bahati,Ryan M. Bena,Meg Wilkinson,Pol Mestres,Ryan K. Cosner,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本文提出了一种风险感知的安全过滤器方法，通过泊松方程和拉普拉斯引导场来编码环境理解和风险，确保机器人系统在导航时优先避开高风险障碍物。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人系统需要对其环境进行语义理解，以正确确定安全动作。本文旨在开发这种表示的数学基础，特别是开发风险感知的安全过滤器。

Method: 采用两步法：首先通过泊松方程的狄利克雷问题生成安全函数，将系统安全编码为其0-超水平集；然后通过拉普拉斯方程的狄利克雷问题合成安全引导场，通过可调通量边界条件编码障碍物周围的可变谨慎级别。

Result: 安全函数和引导场结合定义安全约束，并用于合成风险感知安全过滤器，在给定环境语义理解和相关风险级别的情况下保证安全，同时优先避开高风险障碍物。

Conclusion: 该方法在仿真中得到验证，展示了如何将先验的障碍物风险理解直接纳入安全过滤器，以生成风险感知的安全行为。

Abstract: Robotic systems navigating in real-world settings require a semantic
understanding of their environment to properly determine safe actions. This
work aims to develop the mathematical underpinnings of such a
representation--specifically, the goal is to develop safety filters that are
risk-aware. To this end, we take a two step approach: encoding an understanding
of the environment via Poisson's equation, and associated risk via Laplace
guidance fields. That is, we first solve a Dirichlet problem for Poisson's
equation to generate a safety function that encodes system safety as its
0-superlevel set. We then separately solve a Dirichlet problem for Laplace's
equation to synthesize a safe \textit{guidance field} that encodes variable
levels of caution around obstacles -- by enforcing a tunable flux boundary
condition. The safety function and guidance fields are then combined to define
a safety constraint and used to synthesize a risk-aware safety filter which,
given a semantic understanding of an environment with associated risk levels of
environmental features, guarantees safety while prioritizing avoidance of
higher risk obstacles. We demonstrate this method in simulation and discuss how
\textit{a priori} understandings of obstacle risk can be directly incorporated
into the safety filter to generate safe behaviors that are risk-aware.

</details>


### [2] [A New Type of Axis-Angle Attitude Control Law for Rotational Systems: Synthesis, Analysis, and Experiments](https://arxiv.org/abs/2510.25985)
*Francisco M. F. R. Gonçalves,Ryan M. Bena,Néstor O. Pérez-Arancibia*

Main category: cs.RO

TL;DR: 提出了一种基于姿态误差欧拉轴角信息的新型姿态控制律，相比传统的四元数控制方法，能够保证闭环系统具有唯一的平衡点，并提供更灵活的比例控制效果。


<details>
  <summary>Details</summary>
Motivation: 传统四元数姿态控制方法存在两个主要问题：1) 不保证闭环系统具有唯一的平衡点；2) 当姿态误差欧拉轴旋转角度超过π弧度时，比例控制效果会减弱。

Method: 开发了基于姿态误差欧拉轴角信息的新型控制律，通过构建严格的Lyapunov函数来证明闭环旋转系统的唯一平衡点是均匀渐近稳定的。

Result: 数值仿真和实时翻滚恢复机动实验表明，所提出的轴角方法在稳定时间方面优于高性能四元数控制器。

Conclusion: 基于轴角的方法在姿态控制中表现出优越性能，特别是在稳定时间方面优于传统四元数控制方法。

Abstract: Over the past few decades, continuous quaternion-based attitude control has
been proven highly effective for driving rotational systems that can be modeled
as rigid bodies, such as satellites and drones. However, methods rooted in this
approach do not enforce the existence of a unique closed-loop (CL) equilibrium
attitude-error quaternion (AEQ); and, for rotational errors about the
attitude-error Euler axis larger than {\pi}rad, their proportional-control
effect diminishes as the system state moves away from the stable equilibrium of
the CL rotational dynamics. In this paper, we introduce a new type of attitude
control law that more effectively leverages the attitude-error Euler axis-angle
information to guarantee a unique CL equilibrium AEQ and to provide greater
flexibility in the use of proportional-control efforts. Furthermore, using two
different control laws as examples-through the construction of a strict
Lyapunov function for the CL dynamics-we demonstrate that the resulting unique
equilibrium of the CL rotational system can be enforced to be uniformly
asymptotically stable. To assess and demonstrate the functionality and
performance of the proposed approach, we performed numerical simulations and
executed dozens of real-time tumble-recovery maneuvers using a small quadrotor.
These simulations and flight tests compellingly demonstrate that the proposed
axis-angle-based method achieves superior flight performance-compared with that
obtained using a high-performance quaternion-based controller-in terms of
stabilization time.

</details>


### [3] [DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System](https://arxiv.org/abs/2510.26004)
*Bai Li,Achilleas Kourtellis,Rong Cao,Joseph Post,Brian Porter,Yu Zhang*

Main category: cs.RO

TL;DR: DARTS是一个基于无人机和AI的实时交通事件检测系统，通过无人机的高机动性和热成像技术，结合轻量级深度学习框架，实现了99%的检测准确率，在实地测试中比传统方法提前12分钟检测到追尾事故。


<details>
  <summary>Details</summary>
Motivation: 传统交通事件检测方法存在检测与验证分离、灵活性有限、需要密集基础设施等问题，限制了系统的适应性和可扩展性。

Method: DARTS整合了无人机的高机动性和空中视角进行自适应监控，使用热成像技术提升低能见度性能和隐私保护，采用轻量级深度学习框架进行实时车辆轨迹提取和事件检测。

Result: 系统在自收集数据集上达到99%的检测准确率，支持同时在线视觉验证、严重性评估和事件引发拥堵传播监控。在佛罗里达州75号州际公路的实地测试中，比当地交通管理中心提前12分钟检测到追尾事故。

Conclusion: DARTS展示了更灵活和集成的实时交通事件检测系统的潜力，具有显著提高现代交通管理运营效率和响应能力的意义，特别是在偏远地区和资源受限环境中的可扩展性和成本效益。

Abstract: Rapid and reliable incident detection is critical for reducing crash-related
fatalities, injuries, and congestion. However, conventional methods, such as
closed-circuit television, dashcam footage, and sensor-based detection,
separate detection from verification, suffer from limited flexibility, and
require dense infrastructure or high penetration rates, restricting
adaptability and scalability to shifting incident hotspots. To overcome these
challenges, we developed DARTS, a drone-based, AI-powered real-time traffic
incident detection system. DARTS integrates drones' high mobility and aerial
perspective for adaptive surveillance, thermal imaging for better
low-visibility performance and privacy protection, and a lightweight deep
learning framework for real-time vehicle trajectory extraction and incident
detection. The system achieved 99% detection accuracy on a self-collected
dataset and supports simultaneous online visual verification, severity
assessment, and incident-induced congestion propagation monitoring via a
web-based interface. In a field test on Interstate 75 in Florida, DARTS
detected and verified a rear-end collision 12 minutes earlier than the local
transportation management center and monitored incident-induced congestion
propagation, suggesting potential to support faster emergency response and
enable proactive traffic control to reduce congestion and secondary crash risk.
Crucially, DARTS's flexible deployment architecture reduces dependence on
frequent physical patrols, indicating potential scalability and
cost-effectiveness for use in remote areas and resource-constrained settings.
This study presents a promising step toward a more flexible and integrated
real-time traffic incident detection system, with significant implications for
the operational efficiency and responsiveness of modern transportation
management.

</details>


### [4] [RADRON: Cooperative Localization of Ionizing Radiation Sources by MAVs with Compton Cameras](https://arxiv.org/abs/2510.26018)
*Petr Stibinger,Tomas Baca,Daniela Doubravova,Jan Rusnak,Jaroslav Solc,Jan Jakubek,Petr Stepan,Martin Saska*

Main category: cs.RO

TL;DR: 提出了一种使用微型无人机群合作定位放射性材料的新方法，采用单探测器康普顿相机作为轻量级辐射探测器，通过融合测量数据实时估计辐射源位置。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够通过合作微型无人机群快速定位和跟踪放射性材料的方法，利用轻量级康普顿相机的高灵敏度特性。

Method: 使用40克重的单探测器康普顿相机作为辐射探测器，在无人机上直接进行数据读取和处理，通过动态反馈控制无人机运动，采用紧密合作的蜂群策略最大化信息获取。

Result: 实现了从极其稀疏的测量数据中实时估计辐射源位置，能够快速定位静态辐射源并跟踪移动辐射源。

Conclusion: 该方法展示了轻量级康普顿相机与微型无人机群合作在放射性材料定位中的有效性，为辐射检测开辟了新可能性。

Abstract: We present a novel approach to localizing radioactive material by cooperating
Micro Aerial Vehicles (MAVs). Our approach utilizes a state-of-the-art
single-detector Compton camera as a highly sensitive, yet miniature detector of
ionizing radiation. The detector's exceptionally low weight (40 g) opens up new
possibilities of radiation detection by a team of cooperating agile MAVs. We
propose a new fundamental concept of fusing the Compton camera measurements to
estimate the position of the radiation source in real time even from extremely
sparse measurements. The data readout and processing are performed directly
onboard and the results are used in a dynamic feedback to drive the motion of
the vehicles. The MAVs are stabilized in a tightly cooperating swarm to
maximize the information gained by the Compton cameras, rapidly locate the
radiation source, and even track a moving radiation source.

</details>


### [5] [Accelerating Real-World Overtaking in F1TENTH Racing Employing Reinforcement Learning Methods](https://arxiv.org/abs/2510.26040)
*Emily Steiner,Daniel van der Spuy,Futian Zhou,Afereti Pama,Minas Liarokapis,Henry Williams*

Main category: cs.RO

TL;DR: 本文提出了一种能够在仿真和现实中可靠导航赛道并超越对手的新型赛车和超车智能体，在F1Tenth车辆上部署并与其他竞争算法对抗，实现了87%的超车成功率。


<details>
  <summary>Details</summary>
Motivation: 虽然自主赛车在计时赛场景中取得了显著进展，但轮对轮自主赛车和超车仍然严重受限，特别是在真实驾驶场景中，现有算法难以安全可靠地完成超车操作。

Method: 开发了一种新型赛车和超车智能体，能够在F1Tenth平台上学习可靠导航和超车，并在仿真和现实中部署测试，与运行不同竞争算法的对手进行对抗。

Result: 该智能体在与对手的训练中实现了87%的超车成功率，而仅接受赛车训练的智能体超车成功率仅为56%。

Conclusion: 通过与对手进行训练，智能体能够实现有意识的超车行为，显著提高了在轮对轮自主赛车中的超车能力。

Abstract: While autonomous racing performance in Time-Trial scenarios has seen
significant progress and development, autonomous wheel-to-wheel racing and
overtaking are still severely limited. These limitations are particularly
apparent in real-life driving scenarios where state-of-the-art algorithms
struggle to safely or reliably complete overtaking manoeuvres. This is
important, as reliable navigation around other vehicles is vital for safe
autonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful
opportunity for developing wheel-to-wheel racing algorithms on a standardised
physical platform. The competition format makes it possible to evaluate
overtaking and wheel-to-wheel racing algorithms against the state-of-the-art.
This research presents a novel racing and overtaking agent capable of learning
to reliably navigate a track and overtake opponents in both simulation and
reality. The agent was deployed on an F1Tenth vehicle and competed against
opponents running varying competitive algorithms in the real world. The results
demonstrate that the agent's training against opponents enables deliberate
overtaking behaviours with an overtaking rate of 87% compared 56% for an agent
trained just to race.

</details>


### [6] [Beyond the Uncanny Valley: A Mixed-Method Investigation of Anthropomorphism in Protective Responses to Robot Abuse](https://arxiv.org/abs/2510.26082)
*Fan Yang,Lingyao Li,Yaxin Hu,Michael Rodgers,Renkai Ma*

Main category: cs.RO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Robots with anthropomorphic features are increasingly shaping how humans
perceive and morally engage with them. Our research investigates how different
levels of anthropomorphism influence protective responses to robot abuse,
extending the Computers as Social Actors (CASA) and uncanny valley theories
into a moral domain. In an experiment, we invite 201 participants to view
videos depicting abuse toward a robot with low (Spider), moderate (Two-Foot),
or high (Humanoid) anthropomorphism. To provide a comprehensive analysis, we
triangulate three modalities: self-report surveys measuring emotions and
uncanniness, physiological data from automated facial expression analysis, and
qualitative reflections. Findings indicate that protective responses are not
linear. The moderately anthropomorphic Two-Foot robot, rated highest in
eeriness and "spine-tingling" sensations consistent with the uncanny valley,
elicited the strongest physiological anger expressions. Self-reported anger and
guilt are significantly higher for both the Two-Foot and Humanoid robots
compared to the Spider. Qualitative findings further reveal that as
anthropomorphism increases, moral reasoning shifts from technical assessments
of property damage to condemnation of the abuser's character, while governance
proposals expand from property law to calls for quasi-animal rights and broader
societal responsibility. These results suggest that the uncanny valley does not
dampen moral concern but paradoxically heightens protective impulses, offering
critical implications for robot design, policy, and future legal frameworks.

</details>


### [7] [Kinodynamic Task and Motion Planning using VLM-guided and Interleaved Sampling](https://arxiv.org/abs/2510.26139)
*Minseo Kwon,Young J. Kim*

Main category: cs.RO

TL;DR: 提出了一种基于混合状态树的运动动力学TAMP框架，将符号和数值状态统一表示，结合视觉语言模型引导搜索，显著提高了长时域规划问题的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有TAMP方法在长时域问题中由于过度运动采样而成本高昂，而LLM虽然提供常识先验但缺乏3D空间推理能力，无法确保几何或动力学可行性。

Method: 使用混合状态树统一表示符号和数值状态，通过现成的运动规划器和物理模拟器验证运动动力学约束，利用VLM基于状态视觉渲染引导TAMP解探索和回溯搜索。

Result: 在模拟和真实世界实验中，相比传统和基于LLM的TAMP规划器，平均成功率提高了32.14%-1166.67%，复杂问题规划时间减少，消融实验进一步验证了VLM引导的益处。

Conclusion: 该框架通过统一表示和VLM引导有效解决了TAMP中的运动采样成本和空间推理问题，显著提升了规划性能和效率。

Abstract: Task and Motion Planning (TAMP) integrates high-level task planning with
low-level motion feasibility, but existing methods are costly in long-horizon
problems due to excessive motion sampling. While LLMs provide commonsense
priors, they lack 3D spatial reasoning and cannot ensure geometric or dynamic
feasibility. We propose a kinodynamic TAMP framework based on a hybrid state
tree that uniformly represents symbolic and numeric states during planning,
enabling task and motion decisions to be jointly decided. Kinodynamic
constraints embedded in the TAMP problem are verified by an off-the-shelf
motion planner and physics simulator, and a VLM guides exploring a TAMP
solution and backtracks the search based on visual rendering of the states.
Experiments on the simulated domains and in the real world show 32.14% -
1166.67% increased average success rates compared to traditional and LLM-based
TAMP planners and reduced planning time on complex problems, with ablations
further highlighting the benefits of VLM guidance.

</details>


### [8] [Adaptive Trajectory Refinement for Optimization-based Local Planning in Narrow Passages](https://arxiv.org/abs/2510.26142)
*Hahjin Lee,Young J. Kim*

Main category: cs.RO

TL;DR: 提出自适应轨迹优化算法，通过分段保守碰撞检测和位姿修正，解决移动机器人在狭窄通道中的轨迹规划问题，提高成功率和规划速度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在狭窄通道环境中经常失败或生成次优路径，需要解决移动机器人在拥挤环境中的轨迹规划挑战。

Method: 采用两阶段方法：1) 分段保守碰撞测试，递归细分风险轨迹段消除碰撞风险；2) 基于穿透方向和线搜索的位姿修正，确保每个位姿无碰撞且远离障碍物。

Result: 仿真结果显示，相比最先进方法，成功率提高1.69倍，规划时间加快3.79倍。真实实验验证机器人能安全通过狭窄通道并保持快速规划性能。

Conclusion: 所提自适应轨迹优化算法能有效解决狭窄通道中的轨迹规划问题，显著提高成功率和规划效率。

Abstract: Trajectory planning for mobile robots in cluttered environments remains a
major challenge due to narrow passages, where conventional methods often fail
or generate suboptimal paths. To address this issue, we propose the adaptive
trajectory refinement algorithm, which consists of two main stages. First, to
ensure safety at the path-segment level, a segment-wise conservative collision
test is applied, where risk-prone trajectory path segments are recursively
subdivided until collision risks are eliminated. Second, to guarantee
pose-level safety, pose correction based on penetration direction and line
search is applied, ensuring that each pose in the trajectory is collision-free
and maximally clear from obstacles. Simulation results demonstrate that the
proposed method achieves up to 1.69x higher success rates and up to 3.79x
faster planning times than state-of-the-art approaches. Furthermore, real-world
experiments confirm that the robot can safely pass through narrow passages
while maintaining rapid planning performance.

</details>


### [9] [Self-localization on a 3D map by fusing global and local features from a monocular camera](https://arxiv.org/abs/2510.26170)
*Satoshi Kikuch,Masaya Kato,Tsuyoshi Tasaki*

Main category: cs.RO

TL;DR: 提出了一种结合CNN和Vision Transformer的新方法，用于在存在动态障碍物的3D地图上进行单目相机自定位，相比现有最优方法在精度上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 使用廉价单目相机在3D地图上实现自定位是自动驾驶的关键需求。现有基于CNN的方法在处理动态障碍物时效果不佳，因为CNN主要提取局部特征。

Method: 将CNN与Vision Transformer相结合，CNN擅长提取局部特征，而Vision Transformer擅长提取全局特征，能够处理图像中各个patch之间的关系。

Result: 实验结果显示：在包含动态障碍物的CG数据集上，精度提升率比无动态障碍物时高1.5倍；在公共数据集上，自定位误差比现有最优方法减少20.1%；机器人使用该方法平均定位误差为7.51cm，优于现有最优方法。

Conclusion: 结合CNN和Vision Transformer的方法能够有效处理动态障碍物，显著提升自定位精度，为自动驾驶应用提供了更可靠的解决方案。

Abstract: Self-localization on a 3D map by using an inexpensive monocular camera is
required to realize autonomous driving. Self-localization based on a camera
often uses a convolutional neural network (CNN) that can extract local features
that are calculated by nearby pixels. However, when dynamic obstacles, such as
people, are present, CNN does not work well. This study proposes a new method
combining CNN with Vision Transformer, which excels at extracting global
features that show the relationship of patches on whole image. Experimental
results showed that, compared to the state-of-the-art method (SOTA), the
accuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times
higher than that without dynamic obstacles. Moreover, the self-localization
error of our method is 20.1% smaller than that of SOTA on public datasets.
Additionally, our robot using our method can localize itself with 7.51cm error
on average, which is more accurate than SOTA.

</details>


### [10] [PHUMA: Physically-Grounded Humanoid Locomotion Dataset](https://arxiv.org/abs/2510.26236)
*Kyungmin Lee,Sibeen Kim,Minho Park,Hyunseung Kim,Dongyoon Hwang,Hojoon Lee,Jaegul Choo*

Main category: cs.RO

TL;DR: PHUMA是一个基于物理约束的人形机器人运动数据集，通过大规模人类视频数据生成，解决了现有方法中的物理伪影问题，在运动模仿和路径跟随任务中表现优于Humanoid-X和AMASS。


<details>
  <summary>Details</summary>
Motivation: 现有运动模仿方法依赖稀缺昂贵的运动捕捉数据集（如AMASS），而基于互联网视频的方法（如Humanoid-X）存在物理伪影问题，限制了可扩展性和多样性。

Method: 通过精心数据筛选和物理约束重定向技术，强制执行关节限制、确保地面接触、消除脚部滑动，生成大规模且物理可靠的运动数据。

Result: 在未见运动模仿和骨盆引导路径跟随两种条件下，PHUMA训练的策略均优于Humanoid-X和AMASS，在多样化运动模仿方面取得显著提升。

Conclusion: PHUMA成功解决了基于视频的运动数据生成中的物理伪影问题，为大规模人形机器人运动模仿提供了可靠的数据基础。

Abstract: Motion imitation is a promising approach for humanoid locomotion, enabling
agents to acquire humanlike behaviors. Existing methods typically rely on
high-quality motion capture datasets such as AMASS, but these are scarce and
expensive, limiting scalability and diversity. Recent studies attempt to scale
data collection by converting large-scale internet videos, exemplified by
Humanoid-X. However, they often introduce physical artifacts such as floating,
penetration, and foot skating, which hinder stable imitation. In response, we
introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that
leverages human video at scale, while addressing physical artifacts through
careful data curation and physics-constrained retargeting. PHUMA enforces joint
limits, ensures ground contact, and eliminates foot skating, producing motions
that are both large-scale and physically reliable. We evaluated PHUMA in two
sets of conditions: (i) imitation of unseen motion from self-recorded test
videos and (ii) path following with pelvis-only guidance. In both cases,
PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant
gains in imitating diverse motions. The code is available at
https://davian-robotics.github.io/PHUMA.

</details>


### [11] [Thor: Towards Human-Level Whole-Body Reactions for Intense Contact-Rich Environments](https://arxiv.org/abs/2510.26280)
*Gangyang Li,Qing Shi,Youhao Hu,Jincheng Hu,Zhongyuan Wang,Xinlong Wang,Shaqi Luo*

Main category: cs.RO

TL;DR: 提出Thor人形机器人框架，用于在接触丰富的环境中实现类人水平的全身反应，通过力自适应躯干倾斜奖励函数和分层强化学习架构，在Unitree G1机器人上显著提升了力交互能力。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在服务、工业和救援应用中，需要在保持全身稳定性的同时执行强烈、接触丰富的环境交互的挑战，使机器人能够产生类人的自适应响应。

Method: 基于机器人受力分析设计力自适应躯干倾斜奖励函数；引入将上半身、腰部和下半身解耦的强化学习架构，各组件共享全身全局观测并联合更新参数。

Result: 在Unitree G1机器人上部署Thor，向后移动时峰值拉力达到167.7N（约G1体重的48%），向前移动时145.5N，相比最佳基线分别提升68.9%和74.7%；能够拉动130N负载的货架和单手打开60N的防火门。

Conclusion: Thor框架有效增强了人形机器人的力交互能力，在接触丰富的环境中实现了类人水平的全身反应。

Abstract: Humanoids hold great potential for service, industrial, and rescue
applications, in which robots must sustain whole-body stability while
performing intense, contact-rich interactions with the environment. However,
enabling humanoids to generate human-like, adaptive responses under such
conditions remains a major challenge. To address this, we propose Thor, a
humanoid framework for human-level whole-body reactions in contact-rich
environments. Based on the robot's force analysis, we design a force-adaptive
torso-tilt (FAT2) reward function to encourage humanoids to exhibit human-like
responses during force-interaction tasks. To mitigate the high-dimensional
challenges of humanoid control, Thor introduces a reinforcement learning
architecture that decouples the upper body, waist, and lower body. Each
component shares global observations of the whole body and jointly updates its
parameters. Finally, we deploy Thor on the Unitree G1, and it substantially
outperforms baselines in force-interaction tasks. Specifically, the robot
achieves a peak pulling force of 167.7 N (approximately 48% of the G1's body
weight) when moving backward and 145.5 N when moving forward, representing
improvements of 68.9% and 74.7%, respectively, compared with the
best-performing baseline. Moreover, Thor is capable of pulling a loaded rack
(130 N) and opening a fire door with one hand (60 N). These results highlight
Thor's effectiveness in enhancing humanoid force-interaction capabilities.

</details>


### [12] [AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM](https://arxiv.org/abs/2510.26358)
*Mirko Usuelli,David Rapado-Rincon,Gert Kootstra,Matteo Matteucci*

Main category: cs.RO

TL;DR: AgriGS-SLAM是一个视觉-LiDAR SLAM框架，结合直接LiDAR里程计和闭环检测与多相机3D高斯泼溅渲染，用于果园环境下的实时3D场景理解。


<details>
  <summary>Details</summary>
Motivation: 果园环境下的自主机器人需要实时3D场景理解，但面临重复行几何、季节性外观变化和风驱动叶片运动等挑战。

Method: 通过批量光栅化互补视角恢复被遮挡的果园结构，使用统一的梯度驱动地图生命周期在关键帧之间执行，通过概率LiDAR深度一致性项指导姿态优化。

Result: 在苹果和梨园的多季节测试中，相比现有3DGS-SLAM基线方法，AgriGS-SLAM提供了更清晰、更稳定的重建和更平稳的轨迹，同时保持实时性能。

Conclusion: 该方法在果园监测中表现出色，也可应用于其他需要鲁棒多模态感知的户外领域。

Abstract: Autonomous robots in orchards require real-time 3D scene understanding
despite repetitive row geometry, seasonal appearance changes, and wind-driven
foliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that
couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian
Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints
recovers orchard structure under occlusions, while a unified gradient-driven
map lifecycle executed between keyframes preserves fine details and bounds
memory. Pose refinement is guided by a probabilistic LiDAR-based depth
consistency term, back-propagated through the camera projection to tighten
geometry-appearance coupling. We deploy the system on a field platform in apple
and pear orchards across dormancy, flowering, and harvesting, using a
standardized trajectory protocol that evaluates both training-view and
novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons
and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and
steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while
maintaining real-time performance on-tractor. While demonstrated in orchard
monitoring, the approach can be applied to other outdoor domains requiring
robust multimodal perception.

</details>


### [13] [Cooperative Task Spaces for Multi-Arm Manipulation Control based on Similarity Transformations](https://arxiv.org/abs/2510.26362)
*Tobias Löw,Cem Bilaloglu,Sylvain Calinon*

Main category: cs.RO

TL;DR: 本文基于共形几何代数提出多臂机器人系统的协作任务空间理论框架，将复杂系统抽象为类似单臂系统，便于集成到经典控制方法中。


<details>
  <summary>Details</summary>
Motivation: 人类环境中许多任务需要多个运动链协作，如搬运大物体或灵巧操作，但这些高自由度系统的运动协调建模困难。

Method: 使用共形几何代数定义几何基元，基于其相似变换推导协作几何基元，获得解析和几何雅可比矩阵，集成到操作空间控制中。

Result: 在双手机器人、人形机器人和多指手的实验中验证了方法，包括最优控制达到期望几何基元和微分运动学遥操作。

Conclusion: 该工作为协作操作控制框架奠定了理论基础，几何基元自然嵌入零空间结构可用于次级控制目标，为未来应用提供指导。

Abstract: Many tasks in human environments require collaborative behavior between
multiple kinematic chains, either to provide additional support for carrying
big and bulky objects or to enable the dexterity that is required for in-hand
manipulation. Since these complex systems often have a very high number of
degrees of freedom coordinating their movements is notoriously difficult to
model. In this article, we present the derivation of the theoretical
foundations for cooperative task spaces of multi-arm robotic systems based on
geometric primitives defined using conformal geometric algebra. Based on the
similarity transformations of these cooperative geometric primitives, we derive
an abstraction of complex robotic systems that enables representing these
systems in a way that directly corresponds to single-arm systems. By deriving
the associated analytic and geometric Jacobian matrices, we then show the
straightforward integration of our approach into classical control techniques
rooted in operational space control. We demonstrate this using bimanual
manipulators, humanoids and multi-fingered hands in optimal control experiments
for reaching desired geometric primitives and in teleoperation experiments
using differential kinematics control. We then discuss how the geometric
primitives naturally embed nullspace structures into the controllers that can
be exploited for introducing secondary control objectives. This work,
represents the theoretical foundations of this cooperative manipulation control
framework, and thus the experiments are presented in an abstract way, while
giving pointers towards potential future applications.

</details>


### [14] [Human-in-the-loop Online Rejection Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.26406)
*Guanxing Lu,Rui Zhao,Haitao Lin,He Zhang,Yansong Tang*

Main category: cs.RO

TL;DR: Hi-ORS是一种简单有效的后训练方法，通过拒绝采样实现训练稳定性和高鲁棒性。该方法在在线微调中过滤负奖励样本来稳定价值估计，并采用奖励加权的监督训练目标提供密集的中间步骤监督。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）用于生成鲁棒的机器人操作策略，但微调视觉-语言-动作（VLA）模型时可能因不准确的价值估计和稀疏的中间步骤监督而不稳定。模仿学习（IL）易于训练但性能较差。

Method: 提出Hi-ORS方法，使用拒绝采样过滤负奖励样本稳定价值估计，采用奖励加权的监督训练目标提供密集监督。开发异步推理-训练框架支持灵活的在线人机交互校正。

Result: 在三个真实世界任务和两个实体上，Hi-ORS仅用1.5小时真实世界训练就能微调pi-base策略掌握接触丰富的操作，在效果和效率上大幅优于RL和IL基线。

Conclusion: Hi-ORS方法实现了训练稳定性和高鲁棒性，微调后的策略展现出强大的测试时扩展性，能够可靠执行复杂的错误恢复行为以获得更好性能。

Abstract: Reinforcement learning (RL) is widely used to produce robust robotic
manipulation policies, but fine-tuning vision-language-action (VLA) models with
RL can be unstable due to inaccurate value estimates and sparse supervision at
intermediate steps. In contrast, imitation learning (IL) is easy to train but
often underperforms due to its offline nature. In this paper, we propose
Hi-ORS, a simple yet effective post-training method that utilizes rejection
sampling to achieve both training stability and high robustness. Hi-ORS
stabilizes value estimation by filtering out negatively rewarded samples during
online fine-tuning, and adopts a reward-weighted supervised training objective
to provide dense intermediate-step supervision. For systematic study, we
develop an asynchronous inference-training framework that supports flexible
online human-in-the-loop corrections, which serve as explicit guidance for
learning error-recovery behaviors. Across three real-world tasks and two
embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich
manipulation in just 1.5 hours of real-world training, outperforming RL and IL
baselines by a substantial margin in both effectiveness and efficiency.
Notably, the fine-tuned policy exhibits strong test-time scalability by
reliably executing complex error-recovery behaviors to achieve better
performance.

</details>


### [15] [RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration](https://arxiv.org/abs/2510.26536)
*Huajie Tan,Cheng Chi,Xiansheng Chen,Yuheng Ji,Zhongxia Zhao,Xiaoshuai Hao,Yaoxu Lyu,Mingyu Cao,Junkai Zhao,Huaihai Lyu,Enshen Zhou,Ning Chen,Yankai Fu,Cheng Peng,Wei Guo,Dong Liang,Zhuo Chen,Mengsi Lyu,Chenrui He,Yulong Ao,Yonghua Lin,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出RoboOS-NeXT框架，通过统一的时空-具身记忆(STEM)实现多机器人系统的终身适应、可扩展协调和鲁棒调度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的单智能体记忆，无法实现长期学习、异构团队扩展或故障恢复，需要统一的记忆表示来解决多机器人协作中的核心挑战。

Method: 引入RoboOS-NeXT框架，核心是时空-具身记忆(STEM)，集成空间场景几何、时间事件历史和具身配置文件。采用大脑-小脑架构，高层大脑模型进行全局规划，低层控制器本地执行动作。

Result: 在餐厅、超市和家庭等复杂协调任务中的实验表明，RoboOS-NeXT在异构具身机器人上实现了优越性能。

Conclusion: RoboOS-NeXT通过记忆中心设计实现了终身、可扩展和鲁棒的多机器人协作，验证了其有效性。

Abstract: The proliferation of collaborative robots across diverse tasks and
embodiments presents a central challenge: achieving lifelong adaptability,
scalable coordination, and robust scheduling in multi-agent systems. Existing
approaches, from vision-language-action (VLA) models to hierarchical
frameworks, fall short due to their reliance on limited or dividual-agent
memory. This fundamentally constrains their ability to learn over long
horizons, scale to heterogeneous teams, or recover from failures, highlighting
the need for a unified memory representation. To address these limitations, we
introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable,
and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel
Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene
geometry, temporal event history, and embodiment profiles into a shared
representation. This memory-centric design is integrated into a
brain-cerebellum framework, where a high-level brain model performs global
planning by retrieving and updating STEM, while low-level controllers execute
actions locally. This closed loop between cognition, memory, and execution
enables dynamic task allocation, fault-tolerant collaboration, and consistent
state synchronization. We conduct extensive experiments spanning complex
coordination tasks in restaurants, supermarkets, and households. Our results
demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous
embodiments, validating its effectiveness in enabling lifelong, scalable, and
robust multi-robot collaboration. Project website:
https://flagopen.github.io/RoboOS/

</details>


### [16] [Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool Manipulation in Robotics](https://arxiv.org/abs/2510.26551)
*Prathamesh Kothavale,Sravani Boddepalli*

Main category: cs.RO

TL;DR: 提出了一种扩展机器人逆运动学求解器的新框架，使机器人能够学习使用不同长度工具的顺序动作库，并通过仿真到现实的技能迁移实现精确的工具操作。


<details>
  <summary>Details</summary>
Motivation: 传统机器人对自身运动学理解有限，只能执行预编程任务，无法有效利用工具。需要解决工具使用的四个关键要素：理解期望结果、选择合适工具、确定最佳工具方向、执行精确操作。

Method: 扩展机器人逆运动学求解器，集成仿真学习的动作轨迹与工具，实现从仿真到现实场景的技能迁移。

Result: 扩展逆运动学求解器误差小于1cm，训练策略在仿真中平均误差8cm，使用两种不同长度工具时性能几乎无差异。

Conclusion: 该研究为探索工具使用的四个基本方面提供了潜在进展，使机器人能够掌握跨不同任务的复杂工具操作艺术。

Abstract: Conventional robots possess a limited understanding of their kinematics and
are confined to preprogrammed tasks, hindering their ability to leverage tools
efficiently. Driven by the essential components of tool usage - grasping the
desired outcome, selecting the most suitable tool, determining optimal tool
orientation, and executing precise manipulations - we introduce a pioneering
framework. Our novel approach expands the capabilities of the robot's inverse
kinematics solver, empowering it to acquire a sequential repertoire of actions
using tools of varying lengths. By integrating a simulation-learned action
trajectory with the tool, we showcase the practicality of transferring acquired
skills from simulation to real-world scenarios through comprehensive
experimentation. Remarkably, our extended inverse kinematics solver
demonstrates an impressive error rate of less than 1 cm. Furthermore, our
trained policy achieves a mean error of 8 cm in simulation. Noteworthy, our
model achieves virtually indistinguishable performance when employing two
distinct tools of different lengths. This research provides an indication of
potential advances in the exploration of all four fundamental aspects of tool
usage, enabling robots to master the intricate art of tool manipulation across
diverse tasks.

</details>


### [17] [A Sliding-Window Filter for Online Continuous-Time Continuum Robot State Estimation](https://arxiv.org/abs/2510.26623)
*Spencer Teetaert,Sven Lilge,Jessica Burgner-Kahrs,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 提出了一种用于连续体机器人连续时间状态估计的滑动窗口滤波器，在保持滤波器方法精度的同时实现在线操作，且运行速度快于实时速度。


<details>
  <summary>Details</summary>
Motivation: 现有连续体机器人随机状态估计方法难以平衡精度和计算效率，滑动窗口方法仅限于简化离散时间近似且缺乏随机表示，而随机滤波器方法受限于测量速度运行，连续时间估计技术目前仅限于离线操作。

Method: 开发了专门针对连续体机器人的随机滑动窗口滤波器，用于连续时间状态估计，结合了滑动窗口和连续时间方法的优势。

Result: 该方法在保持滤波器方法精度的同时，使连续时间方法能够在线运行，且运行速度快于实时速度。

Conclusion: 这是首个专门为连续体机器人设计的随机滑动窗口滤波器，为该领域未来研究提供了有前景的方向。

Abstract: Stochastic state estimation methods for continuum robots (CRs) often struggle
to balance accuracy and computational efficiency. While several recent works
have explored sliding-window formulations for CRs, these methods are limited to
simplified, discrete-time approximations and do not provide stochastic
representations. In contrast, current stochastic filter methods must run at the
speed of measurements, limiting their full potential. Recent works in
continuous-time estimation techniques for CRs show a principled approach to
addressing this runtime constraint, but are currently restricted to offline
operation. In this work, we present a sliding-window filter (SWF) for
continuous-time state estimation of CRs that improves upon the accuracy of a
filter approach while enabling continuous-time methods to operate online, all
while running at faster-than-real-time speeds. This represents the first
stochastic SWF specifically designed for CRs, providing a promising direction
for future research in this area.

</details>


### [18] [Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments](https://arxiv.org/abs/2510.26646)
*Xiaoyi He,Danggui Chen,Zhenshuo Zhang,Zimeng Bai*

Main category: cs.RO

TL;DR: 提出了一种分层路径规划与控制框架，结合高层DQN进行离散子目标选择与底层TD3控制器进行连续执行，在动态和部分可观测环境中实现了更好的成功率和样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决单一算法在复杂环境中路径规划的局限性，通过分层结构结合离散决策和连续控制的优势，提高在动态和部分可观测环境中的导航性能。

Method: 使用高层Deep Q-Network进行行为选择和子目标决策，底层Twin Delayed Deep Deterministic Policy Gradient控制器执行连续速度命令，设计了包含方向、距离、避障、动作平滑度等要素的奖励机制，并集成了LiDAR安全门防止不安全运动。

Result: 在ROS + Gazebo环境中使用PathBench指标评估，相比单一算法基准（单独DQN或TD3）和基于规则的规划器，显示出更高的成功率、更好的样本效率，对未见过的障碍物配置具有更好的泛化能力，并减少了突然的控制变化。

Conclusion: 分层强化学习框架在复杂导航任务中优于单一算法方法，结合了离散决策和连续控制的优势，为动态和部分可观测环境中的自主导航提供了有效解决方案。

Abstract: This paper presents a hierarchical path-planning and control framework that
combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with
a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller
for continuous actuation. The high-level module selects behaviors and
sub-goals; the low-level module executes smooth velocity commands. We design a
practical reward shaping scheme (direction, distance, obstacle avoidance,
action smoothness, collision penalty, time penalty, and progress), together
with a LiDAR-based safety gate that prevents unsafe motions. The system is
implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,
including success rate, collision rate, path efficiency, and re-planning
efficiency, in dynamic and partially observable environments. Experiments show
improved success rate and sample efficiency over single-algorithm baselines
(DQN or TD3 alone) and rule-based planners, with better generalization to
unseen obstacle configurations and reduced abrupt control changes. Code and
evaluation scripts are available at the project repository.

</details>


### [19] [Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and Real-Time Efficiency in Robotic Manipulation](https://arxiv.org/abs/2510.26670)
*Qianyou Zhao,Yuliang Shen,Xuanran Zhai,Ce Hao,Duidi Wu,Jin Qi,Jie Hu,Qiaojun Yu*

Main category: cs.RO

TL;DR: 提出了混合一致性策略（HCP），通过结合随机前缀和一步一致性跳跃，在保持多模态行为的同时显著加速扩散策略的推理速度。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的模仿学习方法在捕捉多样化行为方面表现优异，但传统方法难以同时实现快速采样和强多模态特性。

Method: HCP运行一个短随机前缀到自适应切换时间，然后应用一步一致性跳跃生成最终动作，采用时变一致性蒸馏结合轨迹一致性目标和去噪匹配目标。

Result: 在仿真和真实机器人上，HCP仅需25步SDE加一次跳跃即可接近80步DDPM教师的准确性和模态覆盖度，同时显著降低延迟。

Conclusion: 多模态不需要慢推理，切换时间将模态保持与速度解耦，为机器人策略提供了实用的准确性与效率权衡。

Abstract: In visuomotor policy learning, diffusion-based imitation learning has become
widely adopted for its ability to capture diverse behaviors. However,
approaches built on ordinary and stochastic denoising processes struggle to
jointly achieve fast sampling and strong multi-modality. To address these
challenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short
stochastic prefix up to an adaptive switch time, and then applies a one-step
consistency jump to produce the final action. To align this one-jump
generation, HCP performs time-varying consistency distillation that combines a
trajectory-consistency objective to keep neighboring predictions coherent and a
denoising-matching objective to improve local fidelity. In both simulation and
on a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step
DDPM teacher in accuracy and mode coverage while significantly reducing
latency. These results show that multi-modality does not require slow
inference, and a switch time decouples mode retention from speed. It yields a
practical accuracy efficiency trade-off for robot policies.

</details>


### [20] [Running VLAs at Real-time Speed](https://arxiv.org/abs/2510.26742)
*Yunchao Ma,Yizhuang Zhou,Yunhuan Yang,Tiancai Wang,Haoqiang Fan*

Main category: cs.RO

TL;DR: 本文展示了如何在单个消费级GPU上以30Hz帧率和最高480Hz轨迹频率运行pi0级多视角VLA，实现了之前认为大型VLA模型无法完成的动态实时任务。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言动作模型在实时机器人控制中的性能瓶颈，使动态实时任务成为可能。

Method: 引入一系列策略来消除模型推理中的开销，并提出了完整的流式推理框架。

Result: 在抓取下落笔的任务中，采用本文策略的pi0策略实现了100%的成功率。

Conclusion: 所提出的方法使VLA模型能够在消费级硬件上实现实时机器人控制，为动态任务开辟了新途径。

Abstract: In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate
and at most 480Hz trajectory frequency using a single consumer GPU. This
enables dynamic and real-time tasks that were previously believed to be
unattainable by large VLA models. To achieve it, we introduce a bag of
strategies to eliminate the overheads in model inference. The real-world
experiment shows that the pi0 policy with our strategy achieves a 100% success
rate in grasping a falling pen task. Based on the results, we further propose a
full streaming inference framework for real-time robot control of VLA. Code is
available at https://github.com/Dexmal/realtime-vla.

</details>
