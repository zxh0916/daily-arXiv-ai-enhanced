{"id": "2510.07417", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07417", "abs": "https://arxiv.org/abs/2510.07417", "authors": ["Corban Rivera", "Grayson Byrd", "Meghan Booker", "Bethany Kemp", "Allison Gaines", "Emma Holmes", "James Uplinger", "Celso M de Melo", "David Handelman"], "title": "FLEET: Formal Language-Grounded Scheduling for Heterogeneous Robot Teams", "comment": null, "summary": "Coordinating heterogeneous robot teams from free-form natural-language\ninstructions is hard. Language-only planners struggle with long-horizon\ncoordination and hallucination, while purely formal methods require\nclosed-world models. We present FLEET, a hybrid decentralized framework that\nturns language into optimized multi-robot schedules. An LLM front-end produces\n(i) a task graph with durations and precedence and (ii) a capability-aware\nrobot--task fitness matrix; a formal back-end solves a makespan-minimization\nproblem while the underlying robots execute their free-form subtasks with\nagentic closed-loop control. Across multiple free-form language-guided autonomy\ncoordination benchmarks, FLEET improves success over state of the art\ngenerative planners on two-agent teams across heterogeneous tasks. Ablations\nshow that mixed integer linear programming (MILP) primarily improves temporal\nstructure, while LLM-derived fitness is decisive for capability-coupled tasks;\ntogether they deliver the highest overall performance. We demonstrate the\ntranslation to real world challenges with hardware trials using a pair of\nquadruped robots with disjoint capabilities.", "AI": {"tldr": "FLEET\u662f\u4e00\u4e2a\u6df7\u5408\u5206\u6563\u5f0f\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u4f18\u5316\u7684\u591a\u673a\u5668\u4eba\u8c03\u5ea6\u65b9\u6848\uff0c\u901a\u8fc7LLM\u524d\u7aef\u751f\u6210\u4efb\u52a1\u56fe\u548c\u673a\u5668\u4eba-\u4efb\u52a1\u9002\u914d\u77e9\u9635\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u540e\u7aef\u8fdb\u884c\u6700\u5c0f\u5316\u5b8c\u6210\u65f6\u95f4\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u4ece\u81ea\u7531\u5f62\u5f0f\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u534f\u8c03\u7684\u56f0\u96be\uff0c\u514b\u670d\u7eaf\u8bed\u8a00\u89c4\u5212\u5668\u7684\u957f\u65f6\u7a0b\u534f\u8c03\u95ee\u9898\u548c\u5e7b\u89c9\uff0c\u4ee5\u53ca\u7eaf\u5f62\u5f0f\u5316\u65b9\u6cd5\u9700\u8981\u5c01\u95ed\u4e16\u754c\u6a21\u578b\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528LLM\u524d\u7aef\u751f\u6210\u4efb\u52a1\u56fe\uff08\u5305\u542b\u6301\u7eed\u65f6\u95f4\u548c\u4f18\u5148\u7ea7\uff09\u548c\u673a\u5668\u4eba-\u4efb\u52a1\u9002\u914d\u77e9\u9635\uff0c\u5f62\u5f0f\u5316\u540e\u7aef\u89e3\u51b3\u6700\u5c0f\u5316\u5b8c\u6210\u65f6\u95f4\u95ee\u9898\uff0c\u5e95\u5c42\u673a\u5668\u4eba\u901a\u8fc7\u667a\u80fd\u95ed\u73af\u63a7\u5236\u6267\u884c\u81ea\u7531\u5f62\u5f0f\u5b50\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u4e3b\u534f\u8c03\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFLEET\u5728\u5f02\u6784\u4efb\u52a1\u7684\u4e24\u667a\u80fd\u4f53\u56e2\u961f\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u751f\u6210\u89c4\u5212\u5668\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793aMILP\u4e3b\u8981\u6539\u5584\u65f6\u95f4\u7ed3\u6784\uff0cLLM\u5bfc\u51fa\u7684\u9002\u914d\u6027\u5bf9\u80fd\u529b\u8026\u5408\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "FLEET\u6846\u67b6\u6210\u529f\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u4f18\u5316\u7684\u591a\u673a\u5668\u4eba\u8c03\u5ea6\uff0c\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u65b9\u6cd5\u5728\u5f02\u6784\u673a\u5668\u4eba\u534f\u8c03\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.07447", "categories": ["cs.RO", "cs.LG", "math.DS"], "pdf": "https://arxiv.org/pdf/2510.07447", "abs": "https://arxiv.org/abs/2510.07447", "authors": ["Girolamo Oddo", "Roberto Nuca", "Matteo Parsani"], "title": "VeMo: A Lightweight Data-Driven Approach to Model Vehicle Dynamics", "comment": null, "summary": "Developing a dynamic model for a high-performance vehicle is a complex\nproblem that requires extensive structural information about the system under\nanalysis. This information is often unavailable to those who did not design the\nvehicle and represents a typical issue in autonomous driving applications,\nwhich are frequently developed on top of existing vehicles; therefore, vehicle\nmodels are developed under conditions of information scarcity. This paper\nproposes a lightweight encoder-decoder model based on Gate Recurrent Unit\nlayers to correlate the vehicle's future state with its past states, measured\nonboard, and control actions the driver performs. The results demonstrate that\nthe model achieves a maximum mean relative error below 2.6% in extreme dynamic\nconditions. It also shows good robustness when subject to noisy input data\nacross the interested frequency components. Furthermore, being entirely\ndata-driven and free from physical constraints, the model exhibits physical\nconsistency in the output signals, such as longitudinal and lateral\naccelerations, yaw rate, and the vehicle's longitudinal velocity.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u95e8\u63a7\u5faa\u73af\u5355\u5143\u7684\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u4fe1\u606f\u7a00\u7f3a\u6761\u4ef6\u4e0b\u9884\u6d4b\u9ad8\u6027\u80fd\u8f66\u8f86\u7684\u672a\u6765\u72b6\u6001\uff0c\u6700\u5927\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4f4e\u4e8e2.6%\uff0c\u5177\u6709\u826f\u597d\u7684\u566a\u58f0\u9c81\u68d2\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u9ad8\u6027\u80fd\u8f66\u8f86\u7684\u52a8\u6001\u5efa\u6a21\u9700\u8981\u8be6\u7ec6\u7684\u7cfb\u7edf\u7ed3\u6784\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u4fe1\u606f\u901a\u5e38\u5bf9\u975e\u8bbe\u8ba1\u8005\u4e0d\u53ef\u5f97\uff0c\u8fd9\u662f\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u5178\u578b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u95e8\u63a7\u5faa\u73af\u5355\u5143(GRU)\u7684\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u8f66\u8f86\u5386\u53f2\u72b6\u6001\u6d4b\u91cf\u503c\u548c\u9a7e\u9a76\u5458\u63a7\u5236\u52a8\u4f5c\u6765\u5173\u8054\u672a\u6765\u72b6\u6001\u3002", "result": "\u5728\u6781\u7aef\u52a8\u6001\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u6700\u5927\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4f4e\u4e8e2.6%\uff1b\u5bf9\u611f\u5174\u8da3\u9891\u7387\u8303\u56f4\u5185\u7684\u566a\u58f0\u8f93\u5165\u6570\u636e\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\uff1b\u8f93\u51fa\u4fe1\u53f7\uff08\u7eb5\u5411\u548c\u6a2a\u5411\u52a0\u901f\u5ea6\u3001\u6a2a\u6446\u7387\u3001\u7eb5\u5411\u901f\u5ea6\uff09\u8868\u73b0\u51fa\u7269\u7406\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5728\u4fe1\u606f\u7a00\u7f3a\u6761\u4ef6\u4e0b\u6709\u6548\u9884\u6d4b\u8f66\u8f86\u52a8\u6001\uff0c\u65e0\u9700\u7269\u7406\u7ea6\u675f\u5373\u53ef\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u3002"}}
{"id": "2510.07514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07514", "abs": "https://arxiv.org/abs/2510.07514", "authors": ["Cael Yasutake", "Zachary Kingston", "Brian Plancher"], "title": "HJCD-IK: GPU-Accelerated Inverse Kinematics through Batched Hybrid Jacobian Coordinate Descent", "comment": null, "summary": "Inverse Kinematics (IK) is a core problem in robotics, in which joint\nconfigurations are found to achieve a desired end-effector pose. Although\nanalytical solvers are fast and efficient, they are limited to systems with low\ndegrees-of-freedom and specific topological structures. Numerical\noptimization-based approaches are more general, but suffer from high\ncomputational costs and frequent convergence to spurious local minima. Recent\nefforts have explored the use of GPUs to combine sampling and optimization to\nenhance both the accuracy and speed of IK solvers. We build on this recent\nliterature and introduce HJCD-IK, a GPU-accelerated, sampling-based hybrid\nsolver that combines an orientation-aware greedy coordinate descent\ninitialization scheme with a Jacobian-based polishing routine. This design\nenables our solver to improve both convergence speed and overall accuracy as\ncompared to the state-of-the-art, consistently finding solutions along the\naccuracy-latency Pareto frontier and often achieving order-of-magnitude gains.\nIn addition, our method produces a broad distribution of high-quality samples,\nyielding the lowest maximum mean discrepancy. We release our code open-source\nfor the benefit of the community.", "AI": {"tldr": "HJCD-IK\u662f\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u91c7\u6837\u6df7\u5408\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\uff0c\u7ed3\u5408\u4e86\u65b9\u5411\u611f\u77e5\u7684\u8d2a\u5a6a\u5750\u6807\u4e0b\u964d\u521d\u59cb\u5316\u65b9\u6848\u548c\u57fa\u4e8e\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u4f18\u5316\u4f8b\u7a0b\uff0c\u5728\u7cbe\u5ea6\u548c\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u89e3\u6790\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\u53d7\u9650\u4e8e\u4f4e\u81ea\u7531\u5ea6\u548c\u7279\u5b9a\u62d3\u6251\u7ed3\u6784\uff0c\u800c\u6570\u503c\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u3002\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6c42\u89e3\u5668\u3002", "method": "\u4f7f\u7528GPU\u52a0\u901f\u7684\u91c7\u6837\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u65b9\u5411\u611f\u77e5\u8d2a\u5a6a\u5750\u6807\u4e0b\u964d\u521d\u59cb\u5316\u548c\u96c5\u53ef\u6bd4\u77e9\u9635\u4f18\u5316\u629b\u5149\u7a0b\u5e8f\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0c\u8be5\u6c42\u89e3\u5668\u5728\u6536\u655b\u901f\u5ea6\u548c\u6574\u4f53\u7cbe\u5ea6\u65b9\u9762\u5747\u6709\u63d0\u5347\uff0c\u5728\u7cbe\u5ea6-\u5ef6\u8fdf\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u627e\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u5e38\u5b9e\u73b0\u6570\u91cf\u7ea7\u589e\u76ca\uff0c\u5e76\u4ea7\u751f\u9ad8\u8d28\u91cf\u6837\u672c\u7684\u5e7f\u6cdb\u5206\u5e03\u3002", "conclusion": "HJCD-IK\u5728\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2510.07548", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07548", "abs": "https://arxiv.org/abs/2510.07548", "authors": ["Adam Hung", "Fan Yang", "Abhinav Kumar", "Sergio Aguilera Marinovic", "Soshi Iba", "Rana Soltani Zarrin", "Dmitry Berenson"], "title": "AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation", "comment": null, "summary": "Dexterous manipulation tasks often require switching between different\ncontact modes, such as rolling, sliding, sticking, or non-contact contact\nmodes. When formulating dexterous manipulation tasks as a trajectory\noptimization problem, a common approach is to decompose these tasks into\nsub-tasks for each contact mode, which are each solved independently.\nOptimizing each sub-task independently can limit performance, as optimizing\ncontact points, contact forces, or other variables without information about\nfuture sub-tasks can place the system in a state from which it is challenging\nto make progress on subsequent sub-tasks. Further, optimizing these sub-tasks\nis very computationally expensive. To address these challenges, we propose\nAmortized Value Optimization (AVO), which introduces a learned value function\nthat predicts the total future task performance. By incorporating this value\nfunction into the cost of the trajectory optimization at each planning step,\nthe value function gradients guide the optimizer toward states that minimize\nthe cost in future sub-tasks. This effectively bridges separately optimized\nsub-tasks, and accelerates the optimization by reducing the amount of online\ncomputation needed. We validate AVO on a screwdriver grasping and turning task\nin both simulation and real world experiments, and show improved performance\neven with 50% less computational budget compared to trajectory optimization\nwithout the value function.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u644a\u9500\u4ef7\u503c\u4f18\u5316\uff08AVO\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5b66\u4e60\u5230\u7684\u4ef7\u503c\u51fd\u6570\u6765\u9884\u6d4b\u672a\u6765\u4efb\u52a1\u6027\u80fd\uff0c\u6307\u5bfc\u8f68\u8ff9\u4f18\u5316\u5668\u671d\u5411\u6709\u5229\u4e8e\u540e\u7eed\u5b50\u4efb\u52a1\u7684\u72b6\u6001\uff0c\u4ece\u800c\u89e3\u51b3\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u72ec\u7acb\u4f18\u5316\u5b50\u4efb\u52a1\u5bfc\u81f4\u7684\u6027\u80fd\u9650\u5236\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u5728\u4e0d\u540c\u63a5\u89e6\u6a21\u5f0f\u95f4\u5207\u6362\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u72ec\u7acb\u4f18\u5316\u7684\u5b50\u4efb\u52a1\uff0c\u8fd9\u9650\u5236\u4e86\u6027\u80fd\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u5bf9\u672a\u6765\u5b50\u4efb\u52a1\u7684\u4fe1\u606f\u4f1a\u5bfc\u81f4\u7cfb\u7edf\u9677\u5165\u96be\u4ee5\u7ee7\u7eed\u540e\u7eed\u4efb\u52a1\u7684\u72b6\u6001\u3002", "method": "\u63d0\u51fa\u644a\u9500\u4ef7\u503c\u4f18\u5316\uff08AVO\uff09\uff0c\u5f15\u5165\u5b66\u4e60\u5230\u7684\u4ef7\u503c\u51fd\u6570\u9884\u6d4b\u603b\u672a\u6765\u4efb\u52a1\u6027\u80fd\uff0c\u5c06\u8be5\u4ef7\u503c\u51fd\u6570\u7eb3\u5165\u6bcf\u4e2a\u89c4\u5212\u6b65\u9aa4\u7684\u8f68\u8ff9\u4f18\u5316\u6210\u672c\u4e2d\uff0c\u901a\u8fc7\u4ef7\u503c\u51fd\u6570\u68af\u5ea6\u5f15\u5bfc\u4f18\u5316\u5668\u671d\u5411\u6700\u5c0f\u5316\u672a\u6765\u5b50\u4efb\u52a1\u6210\u672c\u7684\u72b6\u6001\u3002", "result": "\u5728\u87ba\u4e1d\u5200\u6293\u53d6\u548c\u8f6c\u52a8\u4efb\u52a1\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86AVO\u7684\u6709\u6548\u6027\uff0c\u5373\u4f7f\u8ba1\u7b97\u9884\u7b97\u51cf\u5c1150%\uff0c\u76f8\u6bd4\u6ca1\u6709\u4ef7\u503c\u51fd\u6570\u7684\u8f68\u8ff9\u4f18\u5316\u4ecd\u80fd\u83b7\u5f97\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "AVO\u65b9\u6cd5\u6709\u6548\u6865\u63a5\u4e86\u72ec\u7acb\u4f18\u5316\u7684\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u51cf\u5c11\u5728\u7ebf\u8ba1\u7b97\u9700\u6c42\u52a0\u901f\u4e86\u4f18\u5316\u8fc7\u7a0b\uff0c\u5728\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.07611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07611", "abs": "https://arxiv.org/abs/2510.07611", "authors": ["Jingyang You", "Hanna Kurniawati", "Lashika Medagoda"], "title": "Inspection Planning Primitives with Implicit Models", "comment": null, "summary": "The aging and increasing complexity of infrastructures make efficient\ninspection planning more critical in ensuring safety. Thanks to sampling-based\nmotion planning, many inspection planners are fast. However, they often require\nhuge memory. This is particularly true when the structure under inspection is\nlarge and complex, consisting of many struts and pillars of various geometry\nand sizes. Such structures can be represented efficiently using implicit\nmodels, such as neural Signed Distance Functions (SDFs). However, most\nprimitive computations used in sampling-based inspection planner have been\ndesigned to work efficiently with explicit environment models, which in turn\nrequires the planner to use explicit environment models or performs frequent\ntransformations between implicit and explicit environment models during\nplanning. This paper proposes a set of primitive computations, called\nInspection Planning Primitives with Implicit Models (IPIM), that enable\nsampling-based inspection planners to entirely use neural SDFs representation\nduring planning. Evaluation on three scenarios, including inspection of a\ncomplex real-world structure with over 92M triangular mesh faces, indicates\nthat even a rudimentary sampling-based planner with IPIM can generate\ninspection trajectories of similar quality to those generated by the\nstate-of-the-art planner, while using up to 70x less memory than the\nstate-of-the-art inspection planner.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u5957\u540d\u4e3aIPIM\u7684\u539f\u59cb\u8ba1\u7b97\u5de5\u5177\uff0c\u4f7f\u57fa\u4e8e\u91c7\u6837\u7684\u68c0\u6d4b\u89c4\u5212\u5668\u80fd\u591f\u5b8c\u5168\u4f7f\u7528\u795e\u7ecfSDF\u8868\u793a\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u8f68\u8ff9\u8d28\u91cf\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u57fa\u7840\u8bbe\u65bd\u8001\u5316\u4e0e\u590d\u6742\u6027\u589e\u52a0\u4f7f\u5f97\u9ad8\u6548\u68c0\u6d4b\u89c4\u5212\u66f4\u4e3a\u5173\u952e\u3002\u73b0\u6709\u57fa\u4e8e\u91c7\u6837\u7684\u68c0\u6d4b\u89c4\u5212\u5668\u867d\u7136\u5feb\u901f\u4f46\u5185\u5b58\u9700\u6c42\u5927\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u578b\u590d\u6742\u7ed3\u6784\u3002\u9690\u5f0f\u6a21\u578b\uff08\u5982\u795e\u7ecfSDF\uff09\u80fd\u9ad8\u6548\u8868\u793a\u8fd9\u4e9b\u7ed3\u6784\uff0c\u4f46\u73b0\u6709\u89c4\u5212\u5668\u4e3b\u8981\u8bbe\u8ba1\u7528\u4e8e\u663e\u5f0f\u73af\u5883\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86IPIM\uff08\u57fa\u4e8e\u9690\u5f0f\u6a21\u578b\u7684\u68c0\u6d4b\u89c4\u5212\u539f\u8bed\uff09\u5de5\u5177\u96c6\uff0c\u4f7f\u57fa\u4e8e\u91c7\u6837\u7684\u68c0\u6d4b\u89c4\u5212\u5668\u80fd\u591f\u5b8c\u5168\u4f7f\u7528\u795e\u7ecfSDF\u8868\u793a\uff0c\u65e0\u9700\u5728\u9690\u5f0f\u548c\u663e\u5f0f\u6a21\u578b\u95f4\u9891\u7e41\u8f6c\u6362\u3002", "result": "\u5728\u4e09\u4e2a\u573a\u666f\uff08\u5305\u62ec\u5177\u67099200\u4e07\u4e2a\u4e09\u89d2\u7f51\u683c\u9762\u7684\u590d\u6742\u771f\u5b9e\u7ed3\u6784\uff09\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5373\u4f7f\u4f7f\u7528\u57fa\u672c\u7684\u57fa\u4e8e\u91c7\u6837\u89c4\u5212\u5668\u914d\u5408IPIM\uff0c\u4e5f\u80fd\u751f\u6210\u4e0e\u6700\u5148\u8fdb\u89c4\u5212\u5668\u8d28\u91cf\u76f8\u5f53\u7684\u68c0\u6d4b\u8f68\u8ff9\uff0c\u540c\u65f6\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u9ad8\u8fbe70\u500d\u3002", "conclusion": "IPIM\u4f7f\u5f97\u57fa\u4e8e\u91c7\u6837\u7684\u68c0\u6d4b\u89c4\u5212\u5668\u80fd\u591f\u6709\u6548\u5229\u7528\u795e\u7ecfSDF\u8868\u793a\uff0c\u5728\u4fdd\u6301\u68c0\u6d4b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u4e3a\u5927\u578b\u590d\u6742\u57fa\u7840\u8bbe\u65bd\u7684\u68c0\u6d4b\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07625", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.07625", "abs": "https://arxiv.org/abs/2510.07625", "authors": ["Alexander Du", "Emre Adabag", "Gabriel Bravo", "Brian Plancher"], "title": "GATO: GPU-Accelerated and Batched Trajectory Optimization for Scalable Edge Model Predictive Control", "comment": null, "summary": "While Model Predictive Control (MPC) delivers strong performance across\nrobotics applications, solving the underlying (batches of) nonlinear trajectory\noptimization (TO) problems online remains computationally demanding. Existing\nGPU-accelerated approaches typically (i) parallelize a single solve to meet\nreal-time deadlines, (ii) scale to very large batches at slower-than-real-time\nrates, or (iii) achieve speed by restricting model generality (e.g., point-mass\ndynamics or a single linearization). This leaves a large gap in solver\nperformance for many state-of-the-art MPC applications that require real-time\nbatches of tens to low-hundreds of solves. As such, we present GATO, an open\nsource, GPU-accelerated, batched TO solver co-designed across algorithm,\nsoftware, and computational hardware to deliver real-time throughput for these\nmoderate batch size regimes. Our approach leverages a combination of block-,\nwarp-, and thread-level parallelism within and across solves for ultra-high\nperformance. We demonstrate the effectiveness of our approach through a\ncombination of: simulated benchmarks showing speedups of 18-21x over CPU\nbaselines and 1.4-16x over GPU baselines as batch size increases; case studies\nhighlighting improved disturbance rejection and convergence behavior; and\nfinally a validation on hardware using an industrial manipulator. We open\nsource GATO to support reproducibility and adoption.", "AI": {"tldr": "GATO\u662f\u4e00\u4e2a\u5f00\u6e90\u3001GPU\u52a0\u901f\u7684\u6279\u91cf\u8f68\u8ff9\u4f18\u5316\u6c42\u89e3\u5668\uff0c\u4e13\u95e8\u4e3a\u4e2d\u7b49\u6279\u91cf\u89c4\u6a21\uff08\u51e0\u5341\u5230\u51e0\u767e\u4e2a\u6c42\u89e3\uff09\u7684\u5b9e\u65f6MPC\u5e94\u7528\u8bbe\u8ba1\uff0c\u76f8\u6bd4CPU\u57fa\u51c6\u5b9e\u73b018-21\u500d\u52a0\u901f\uff0c\u76f8\u6bd4GPU\u57fa\u51c6\u5b9e\u73b01.4-16\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709GPU\u52a0\u901f\u65b9\u6cd5\u8981\u4e48\u5e76\u884c\u5316\u5355\u4e2a\u6c42\u89e3\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u8981\u6c42\uff0c\u8981\u4e48\u4ee5\u4f4e\u4e8e\u5b9e\u65f6\u7684\u901f\u7387\u6269\u5c55\u5230\u975e\u5e38\u5927\u7684\u6279\u91cf\uff0c\u8981\u4e48\u901a\u8fc7\u9650\u5236\u6a21\u578b\u901a\u7528\u6027\u6765\u5b9e\u73b0\u901f\u5ea6\u3002\u8fd9\u4e3a\u8bb8\u591a\u9700\u8981\u5b9e\u65f6\u6279\u91cf\u6c42\u89e3\u7684\u6700\u5148\u8fdbMPC\u5e94\u7528\u7559\u4e0b\u4e86\u5de8\u5927\u7684\u6c42\u89e3\u5668\u6027\u80fd\u5dee\u8ddd\u3002", "method": "GATO\u5728\u7b97\u6cd5\u3001\u8f6f\u4ef6\u548c\u8ba1\u7b97\u786c\u4ef6\u4e0a\u534f\u540c\u8bbe\u8ba1\uff0c\u5229\u7528\u5757\u7ea7\u3001warp\u7ea7\u548c\u7ebf\u7a0b\u7ea7\u5e76\u884c\u6027\u5728\u6c42\u89e3\u5185\u90e8\u548c\u6c42\u89e3\u4e4b\u95f4\u5b9e\u73b0\u8d85\u9ad8\u6027\u80fd\u3002", "result": "\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u968f\u7740\u6279\u91cf\u5927\u5c0f\u589e\u52a0\uff0c\u76f8\u6bd4CPU\u57fa\u51c6\u5b9e\u73b018-21\u500d\u52a0\u901f\uff0c\u76f8\u6bd4GPU\u57fa\u51c6\u5b9e\u73b01.4-16\u500d\u52a0\u901f\uff1b\u6848\u4f8b\u7814\u7a76\u663e\u793a\u6539\u8fdb\u4e86\u6270\u52a8\u6291\u5236\u548c\u6536\u655b\u884c\u4e3a\uff1b\u5728\u5de5\u4e1a\u673a\u68b0\u81c2\u4e0a\u8fdb\u884c\u4e86\u786c\u4ef6\u9a8c\u8bc1\u3002", "conclusion": "GATO\u4e3a\u4e2d\u7b49\u6279\u91cf\u89c4\u6a21\u7684\u5b9e\u65f6MPC\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u6027\u548c\u91c7\u7528\u3002"}}
{"id": "2510.07700", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.07700", "abs": "https://arxiv.org/abs/2510.07700", "authors": ["Raghav Mishra", "Ian R. Manchester"], "title": "EB-MBD: Emerging-Barrier Model-Based Diffusion for Safe Trajectory Optimization in Highly Constrained Environments", "comment": null, "summary": "We propose enforcing constraints on Model-Based Diffusion by introducing\nemerging barrier functions inspired by interior point methods. We show that\nconstraints on Model-Based Diffusion can lead to catastrophic performance\ndegradation, even on simple 2D systems due to sample inefficiency in the Monte\nCarlo approximation of the score function. We introduce Emerging-Barrier\nModel-Based Diffusion (EB-MBD) which uses progressively introduced barrier\nconstraints to avoid these problems, significantly improving solution quality,\nwithout the need for computationally expensive operations such as projections.\nWe analyze the sampling liveliness of samples each iteration to inform barrier\nparameter scheduling choice. We demonstrate results for 2D collision avoidance\nand a 3D underwater manipulator system and show that our method achieves lower\ncost solutions than Model-Based Diffusion, and requires orders of magnitude\nless computation time than projection based methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u65b0\u5174\u969c\u788d\u51fd\u6570\u7684\u6a21\u578b\u6269\u6563\u7ea6\u675f\u65b9\u6cd5\uff08EB-MBD\uff09\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f15\u5165\u969c\u788d\u7ea6\u675f\u89e3\u51b3\u6a21\u578b\u6269\u6563\u4e2d\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u89e3\u7684\u8d28\u91cf\uff0c\u65e0\u9700\u8ba1\u7b97\u6602\u8d35\u7684\u6295\u5f71\u64cd\u4f5c\u3002", "motivation": "\u6a21\u578b\u6269\u6563\u4e2d\u7684\u7ea6\u675f\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u7684\u6027\u80fd\u9000\u5316\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u76842D\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u8499\u7279\u5361\u6d1b\u8fd1\u4f3c\u5f97\u5206\u51fd\u6570\u7684\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5f15\u5165\u65b0\u5174\u969c\u788d\u6a21\u578b\u6269\u6563\uff08EB-MBD\uff09\uff0c\u4f7f\u7528\u6e10\u8fdb\u5f15\u5165\u7684\u969c\u788d\u7ea6\u675f\uff0c\u5206\u6790\u6bcf\u8f6e\u8fed\u4ee3\u7684\u91c7\u6837\u6d3b\u8dc3\u5ea6\u6765\u6307\u5bfc\u969c\u788d\u53c2\u6570\u8c03\u5ea6\u9009\u62e9\u3002", "result": "\u57282D\u78b0\u649e\u907f\u514d\u548c3D\u6c34\u4e0b\u673a\u68b0\u81c2\u7cfb\u7edf\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6a21\u578b\u6269\u6563\u83b7\u5f97\u66f4\u4f4e\u6210\u672c\u7684\u89e3\uff0c\u6bd4\u57fa\u4e8e\u6295\u5f71\u7684\u65b9\u6cd5\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u6570\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "EB-MBD\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u6269\u6563\u4e2d\u7684\u7ea6\u675f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2510.07725", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07725", "abs": "https://arxiv.org/abs/2510.07725", "authors": ["Kasidit Muenprasitivej", "Ye Zhao", "Glen Chou"], "title": "Probabilistically-Safe Bipedal Navigation over Uncertain Terrain via Conformal Prediction and Contraction Analysis", "comment": "9 pages, 4 figures", "summary": "We address the challenge of enabling bipedal robots to traverse rough terrain\nby developing probabilistically safe planning and control strategies that\nensure dynamic feasibility and centroidal robustness under terrain uncertainty.\nSpecifically, we propose a high-level Model Predictive Control (MPC) navigation\nframework for a bipedal robot with a specified confidence level of safety that\n(i) enables safe traversal toward a desired goal location across a terrain map\nwith uncertain elevations, and (ii) formally incorporates uncertainty bounds\ninto the centroidal dynamics of locomotion control. To model the rough terrain,\nwe employ Gaussian Process (GP) regression to estimate elevation maps and\nleverage Conformal Prediction (CP) to construct calibrated confidence intervals\nthat capture the true terrain elevation. Building on this, we formulate\ncontraction-based reachable tubes that explicitly account for terrain\nuncertainty, ensuring state convergence and tube invariance. In addition, we\nintroduce a contraction-based flywheel torque control law for the reduced-order\nLinear Inverted Pendulum Model (LIPM), which stabilizes the angular momentum\nabout the center-of-mass (CoM). This formulation provides both probabilistic\nsafety and goal reachability guarantees. For a given confidence level, we\nestablish the forward invariance of the proposed torque control law by\ndemonstrating exponential stabilization of the actual CoM phase-space\ntrajectory and the desired trajectory prescribed by the high-level planner.\nFinally, we evaluate the effectiveness of our planning framework through\nphysics-based simulations of the Digit bipedal robot in MuJoCo.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u5728\u7c97\u7cd9\u5730\u5f62\u4e0a\u5bfc\u822a\u7684\u6982\u7387\u5b89\u5168\u89c4\u5212\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u5730\u5f62\u4f30\u8ba1\u548c\u4fdd\u5f62\u9884\u6d4b\uff0c\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\u548c\u8d28\u5fc3\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u5730\u5f62\u73af\u5883\u4e0b\u5b89\u5168\u5bfc\u822a\u7684\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u5730\u5f62\u4e0d\u786e\u5b9a\u6027\u548c\u52a8\u6001\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u4f30\u8ba1\u5730\u5f62\u9ad8\u7a0b\uff0c\u5229\u7528\u4fdd\u5f62\u9884\u6d4b\u6784\u5efa\u6821\u51c6\u7f6e\u4fe1\u533a\u95f4\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u6536\u7f29\u7684\u53ef\u8fbe\u7ba1\u548c\u98de\u8f6e\u626d\u77e9\u63a7\u5236\u5f8b\u6765\u7a33\u5b9a\u8d28\u5fc3\u89d2\u52a8\u91cf\u3002", "result": "\u901a\u8fc7MuJoCo\u7269\u7406\u4eff\u771f\u9a8c\u8bc1\u4e86Digit\u53cc\u8db3\u673a\u5668\u4eba\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u6982\u7387\u5b89\u5168\u6027\u548c\u76ee\u6807\u53ef\u8fbe\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u786e\u5b9a\u5730\u5f62\u4e0a\u7684\u5b89\u5168\u5bfc\u822a\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.07749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07749", "abs": "https://arxiv.org/abs/2510.07749", "authors": ["Alexandre Moreira Nascimento", "Gabriel Kenji Godoy Shimanuki", "L\u00facio Flavio Vismari", "Jo\u00e3o Batista Camargo Jr", "Jorge Rady de Almeida Jr", "Paulo Sergio Cugnasca", "Anna Carolina Muller Queiroz", "Jeremy Noah Bailenson"], "title": "Injecting Hallucinations in Autonomous Vehicles: A Component-Agnostic Safety Evaluation Framework", "comment": "22 pages, 15 figures, 21 tables", "summary": "Perception failures in autonomous vehicles (AV) remain a major safety concern\nbecause they are the basis for many accidents. To study how these failures\naffect safety, researchers typically inject artificial faults into hardware or\nsoftware components and observe the outcomes. However, existing fault injection\nstudies often target a single sensor or machine perception (MP) module,\nresulting in siloed frameworks that are difficult to generalize or integrate\ninto unified simulation environments. This work addresses that limitation by\nreframing perception failures as hallucinations, false perceptions that distort\nan AV situational awareness and may trigger unsafe control actions. Since\nhallucinations describe only observable effects, this abstraction enables\nanalysis independent of specific sensors or algorithms, focusing instead on how\ntheir faults manifest along the MP pipeline. Building on this concept, we\npropose a configurable, component-agnostic hallucination injection framework\nthat induces six plausible hallucination types in an iterative open-source\nsimulator. More than 18,350 simulations were executed in which hallucinations\nwere injected while AVs crossed an unsignalized transverse street with traffic.\nThe results statistically validate the framework and quantify the impact of\neach hallucination type on collisions and near misses. Certain hallucinations,\nsuch as perceptual latency and drift, significantly increase the risk of\ncollision in the scenario tested, validating the proposed paradigm can stress\nthe AV system safety. The framework offers a scalable, statistically validated,\ncomponent agnostic, and fully interoperable toolset that simplifies and\naccelerates AV safety validations, even those with novel MP architectures and\ncomponents. It can potentially reduce the time-to-market of AV and lay the\nfoundation for future research on fault tolerance, and resilient AV design.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u914d\u7f6e\u3001\u7ec4\u4ef6\u65e0\u5173\u7684\u5e7b\u89c9\u6ce8\u5165\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u611f\u77e5\u6545\u969c\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e7b\u89c9\uff0c\u5728\u5f00\u6e90\u6a21\u62df\u5668\u4e2d\u6ce8\u5165\u516d\u79cd\u53ef\u4fe1\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u8bc4\u4f30\u5176\u5bf9\u5b89\u5168\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u611f\u77e5\u6545\u969c\u662f\u5bfc\u81f4\u4e8b\u6545\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f46\u73b0\u6709\u6545\u969c\u6ce8\u5165\u7814\u7a76\u901a\u5e38\u9488\u5bf9\u5355\u4e00\u4f20\u611f\u5668\u6216\u611f\u77e5\u6a21\u5757\uff0c\u96be\u4ee5\u63a8\u5e7f\u6216\u96c6\u6210\u5230\u7edf\u4e00\u6a21\u62df\u73af\u5883\u4e2d\u3002", "method": "\u5c06\u611f\u77e5\u6545\u969c\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e7b\u89c9\uff0c\u63d0\u51fa\u53ef\u914d\u7f6e\u3001\u7ec4\u4ef6\u65e0\u5173\u7684\u5e7b\u89c9\u6ce8\u5165\u6846\u67b6\uff0c\u5728\u5f00\u6e90\u6a21\u62df\u5668\u4e2d\u6ce8\u5165\u516d\u79cd\u5e7b\u89c9\u7c7b\u578b\uff0c\u6267\u884c\u8d85\u8fc718,350\u6b21\u6a21\u62df\u6d4b\u8bd5\u3002", "result": "\u67d0\u4e9b\u5e7b\u89c9\u7c7b\u578b\uff08\u5982\u611f\u77e5\u5ef6\u8fdf\u548c\u6f02\u79fb\uff09\u5728\u6d4b\u8bd5\u573a\u666f\u4e2d\u663e\u8457\u589e\u52a0\u78b0\u649e\u98ce\u9669\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7edf\u8ba1\u9a8c\u8bc1\u3001\u7ec4\u4ef6\u65e0\u5173\u4e14\u5b8c\u5168\u4e92\u64cd\u4f5c\u7684\u5de5\u5177\u96c6\uff0c\u53ef\u7b80\u5316\u548c\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u9a8c\u8bc1\uff0c\u4e3a\u5bb9\u9519\u548c\u5f39\u6027\u81ea\u52a8\u9a7e\u9a76\u8bbe\u8ba1\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.07773", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07773", "abs": "https://arxiv.org/abs/2510.07773", "authors": ["YuHang Tang", "Yixuan Lou", "Pengfei Han", "Haoming Song", "Xinyi Ye", "Dong Wang", "Bin Zhao"], "title": "Trajectory Conditioned Cross-embodiment Skill Transfer", "comment": null, "summary": "Learning manipulation skills from human demonstration videos presents a\npromising yet challenging problem, primarily due to the significant embodiment\ngap between human body and robot manipulators. Existing methods rely on paired\ndatasets or hand-crafted rewards, which limit scalability and generalization.\nWe propose TrajSkill, a framework for Trajectory Conditioned Cross-embodiment\nSkill Transfer, enabling robots to acquire manipulation skills directly from\nhuman demonstration videos. Our key insight is to represent human motions as\nsparse optical flow trajectories, which serve as embodiment-agnostic motion\ncues by removing morphological variations while preserving essential dynamics.\nConditioned on these trajectories together with visual and textual inputs,\nTrajSkill jointly synthesizes temporally consistent robot manipulation videos\nand translates them into executable actions, thereby achieving cross-embodiment\nskill transfer. Extensive experiments are conducted, and the results on\nsimulation data (MetaWorld) show that TrajSkill reduces FVD by 39.6\\% and KVD\nby 36.6\\% compared with the state-of-the-art, and improves cross-embodiment\nsuccess rate by up to 16.7\\%. Real-robot experiments in kitchen manipulation\ntasks further validate the effectiveness of our approach, demonstrating\npractical human-to-robot skill transfer across embodiments.", "AI": {"tldr": "TrajSkill\u662f\u4e00\u4e2a\u4ece\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u76f4\u63a5\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u8fd0\u52a8\u8868\u793a\u4e3a\u7a00\u758f\u5149\u6d41\u8f68\u8ff9\u6765\u5b9e\u73b0\u8de8\u5f62\u6001\u6280\u80fd\u8fc1\u79fb\uff0c\u65e0\u9700\u914d\u5bf9\u6570\u636e\u96c6\u6216\u624b\u5de5\u5956\u52b1\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u4e4b\u95f4\u7684\u5f62\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u914d\u5bf9\u6570\u636e\u96c6\u6216\u624b\u5de5\u5956\u52b1\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u4eba\u7c7b\u8fd0\u52a8\u8868\u793a\u4e3a\u7a00\u758f\u5149\u6d41\u8f68\u8ff9\u4f5c\u4e3a\u5f62\u6001\u65e0\u5173\u7684\u8fd0\u52a8\u7ebf\u7d22\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u8054\u5408\u5408\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u5e76\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u52a8\u4f5c\u3002", "result": "\u5728MetaWorld\u4eff\u771f\u6570\u636e\u4e0a\uff0cFVD\u964d\u4f4e39.6%\uff0cKVD\u964d\u4f4e36.6%\uff0c\u8de8\u5f62\u6001\u6210\u529f\u7387\u63d0\u534716.7%\uff1b\u771f\u5b9e\u673a\u5668\u4eba\u53a8\u623f\u64cd\u4f5c\u4efb\u52a1\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "TrajSkill\u5b9e\u73b0\u4e86\u4ece\u4eba\u7c7b\u6f14\u793a\u5230\u673a\u5668\u4eba\u7684\u8de8\u5f62\u6001\u6280\u80fd\u8fc1\u79fb\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.07778", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.07778", "abs": "https://arxiv.org/abs/2510.07778", "authors": ["Yandu Chen", "Kefan Gu", "Yuqing Wen", "Yucheng Zhao", "Tiancai Wang", "Liqiang Nie"], "title": "IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction", "comment": null, "summary": "Vision-Language-Action (VLA) models leverage pretrained vision-language\nmodels (VLMs) to couple perception with robotic control, offering a promising\npath toward general-purpose embodied intelligence. However, current SOTA VLAs\nare primarily pretrained on multimodal tasks with limited relevance to embodied\nscenarios, and then finetuned to map explicit instructions to actions.\nConsequently, due to the lack of reasoning-intensive pretraining and\nreasoning-guided manipulation, these models are unable to perform implicit\nhuman intention reasoning required for complex, real-world interactions. To\novercome these limitations, we propose \\textbf{IntentionVLA}, a VLA framework\nwith a curriculum training paradigm and an efficient inference mechanism. Our\nproposed method first leverages carefully designed reasoning data that combine\nintention inference, spatial grounding, and compact embodied reasoning,\nendowing the model with both reasoning and perception capabilities. In the\nfollowing finetuning stage, IntentionVLA employs the compact reasoning outputs\nas contextual guidance for action generation, enabling fast inference under\nindirect instructions. Experimental results show that IntentionVLA\nsubstantially outperforms $\\pi_0$, achieving 18\\% higher success rates with\ndirect instructions and 28\\% higher than ECoT under intention instructions. On\nout-of-distribution intention tasks, IntentionVLA achieves over twice the\nsuccess rate of all baselines, and further enables zero-shot human-robot\ninteraction with 40\\% success rate. These results highlight IntentionVLA as a\npromising paradigm for next-generation human-robot interaction (HRI) systems.", "AI": {"tldr": "IntentionVLA\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8bfe\u7a0b\u8bad\u7ec3\u8303\u5f0f\u8d4b\u4e88\u6a21\u578b\u63a8\u7406\u548c\u611f\u77e5\u80fd\u529b\uff0c\u5728\u95f4\u63a5\u6307\u4ee4\u4e0b\u5b9e\u73b0\u5feb\u901f\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u7684\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u9884\u8bad\u7ec3\uff0c\u4f46\u7f3a\u4e4f\u63a8\u7406\u5bc6\u96c6\u578b\u9884\u8bad\u7ec3\u548c\u63a8\u7406\u5f15\u5bfc\u7684\u64cd\u4f5c\uff0c\u65e0\u6cd5\u6267\u884c\u590d\u6742\u73b0\u5b9e\u4ea4\u4e92\u6240\u9700\u7684\u9690\u5f0f\u4eba\u7c7b\u610f\u56fe\u63a8\u7406\u3002", "method": "\u9996\u5148\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63a8\u7406\u6570\u636e\uff08\u7ed3\u5408\u610f\u56fe\u63a8\u7406\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u7d27\u51d1\u4f53\u73b0\u63a8\u7406\uff09\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u7d27\u51d1\u63a8\u7406\u8f93\u51fa\u4f5c\u4e3a\u52a8\u4f5c\u751f\u6210\u7684\u4e0a\u4e0b\u6587\u6307\u5bfc\u3002", "result": "IntentionVLA\u663e\u8457\u4f18\u4e8e\u03c00\uff0c\u76f4\u63a5\u6307\u4ee4\u4e0b\u6210\u529f\u7387\u63d0\u9ad818%\uff0c\u610f\u56fe\u6307\u4ee4\u4e0b\u6bd4ECoT\u63d0\u9ad828%\u3002\u5728\u5206\u5e03\u5916\u610f\u56fe\u4efb\u52a1\u4e0a\uff0c\u6210\u529f\u7387\u662f\u57fa\u7ebf\u7684\u4e24\u500d\u4ee5\u4e0a\uff0c\u96f6\u6837\u672c\u4eba\u673a\u4ea4\u4e92\u6210\u529f\u738740%\u3002", "conclusion": "IntentionVLA\u662f\u4e0b\u4e00\u4ee3\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u7684\u6709\u524d\u666f\u8303\u5f0f\uff0c\u901a\u8fc7\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u4e86VLA\u6a21\u578b\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2510.07807", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07807", "abs": "https://arxiv.org/abs/2510.07807", "authors": ["Grace Cai", "Nithin Parepally", "Laura Zheng", "Ming C. Lin"], "title": "GM3: A General Physical Model for Micro-Mobility Vehicles", "comment": null, "summary": "Modeling the dynamics of micro-mobility vehicles (MMV) is becoming\nincreasingly important for training autonomous vehicle systems and building\nurban traffic simulations. However, mainstream tools rely on variants of the\nKinematic Bicycle Model (KBM) or mode-specific physics that miss tire slip,\nload transfer, and rider/vehicle lean. To our knowledge, no unified,\nphysics-based model captures these dynamics across the full range of common\nMMVs and wheel layouts. We propose the \"Generalized Micro-mobility Model\"\n(GM3), a tire-level formulation based on the tire brush representation that\nsupports arbitrary wheel configurations, including single/double track and\nmulti-wheel platforms. We introduce an interactive model-agnostic simulation\nframework that decouples vehicle/layout specification from dynamics to compare\nthe GM3 with the KBM and other models, consisting of fixed step RK4\nintegration, human-in-the-loop and scripted control, real-time trajectory\ntraces and logging for analysis. We also empirically validate the GM3 on the\nStanford Drone Dataset's deathCircle (roundabout) scene for biker, skater, and\ncart classes.", "AI": {"tldr": "\u63d0\u51fa\u4e86GM3\uff08\u5e7f\u4e49\u5fae\u79fb\u52a8\u6a21\u578b\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u8f6e\u80ce\u5237\u8868\u793a\u7684\u7edf\u4e00\u7269\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u5404\u79cd\u5fae\u79fb\u52a8\u8f66\u8f86\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5305\u62ec\u8f6e\u80ce\u6ed1\u79fb\u3001\u8f7d\u8377\u8f6c\u79fb\u548c\u9a91\u624b/\u8f66\u8f86\u503e\u659c\u7b49\u88ab\u4f20\u7edf\u6a21\u578b\u5ffd\u7565\u7684\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709\u7684\u5fae\u79fb\u52a8\u8f66\u8f86\u52a8\u529b\u5b66\u5efa\u6a21\u5de5\u5177\u4e3b\u8981\u4f9d\u8d56\u8fd0\u52a8\u5b66\u81ea\u884c\u8f66\u6a21\u578b\u6216\u5176\u53d8\u4f53\uff0c\u8fd9\u4e9b\u6a21\u578b\u5ffd\u7565\u4e86\u8f6e\u80ce\u6ed1\u79fb\u3001\u8f7d\u8377\u8f6c\u79fb\u548c\u9a91\u624b/\u8f66\u8f86\u503e\u659c\u7b49\u91cd\u8981\u7269\u7406\u6548\u5e94\uff0c\u4e14\u7f3a\u4e4f\u7edf\u4e00\u7684\u8de8\u5e73\u53f0\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u8f6e\u80ce\u5237\u8868\u793a\u7684\u8f6e\u80ce\u7ea7\u516c\u5f0f\u5316GM3\u6a21\u578b\uff0c\u652f\u6301\u4efb\u610f\u8f6e\u5f0f\u914d\u7f6e\uff1b\u6784\u5efa\u4e86\u4ea4\u4e92\u5f0f\u6a21\u578b\u65e0\u5173\u4eff\u771f\u6846\u67b6\uff0c\u5305\u542b\u56fa\u5b9a\u6b65\u957fRK4\u79ef\u5206\u3001\u4eba\u5728\u56de\u8def\u548c\u811a\u672c\u63a7\u5236\u3001\u5b9e\u65f6\u8f68\u8ff9\u8ddf\u8e2a\u548c\u5206\u6790\u65e5\u5fd7\u8bb0\u5f55\u3002", "result": "\u5728\u65af\u5766\u798f\u65e0\u4eba\u673a\u6570\u636e\u96c6\u7684\u6b7b\u4ea1\u5706\u73af\uff08\u73af\u5f62\u4ea4\u53c9\u53e3\uff09\u573a\u666f\u4e2d\uff0c\u5bf9\u81ea\u884c\u8f66\u3001\u6ed1\u677f\u8f66\u548c\u624b\u63a8\u8f66\u7c7b\u522b\u8fdb\u884c\u4e86GM3\u6a21\u578b\u7684\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "conclusion": "GM3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7269\u7406\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u5404\u79cd\u5fae\u79fb\u52a8\u8f66\u8f86\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u5efa\u6a21\u5de5\u5177\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.07869", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07869", "abs": "https://arxiv.org/abs/2510.07869", "authors": ["Junwen Gu", "Zhiheng wu", "Pengxuan Si", "Shuang Qiu", "Yukai Feng", "Luoyang Sun", "Laien Luo", "Lianyi Yu", "Jian Wang", "Zhengxing Wu"], "title": "USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots", "comment": null, "summary": "Underwater environments present unique challenges for robotic operation,\nincluding complex hydrodynamics, limited visibility, and constrained\ncommunication. Although data-driven approaches have advanced embodied\nintelligence in terrestrial robots and enabled task-specific autonomous\nunderwater robots, developing underwater intelligence capable of autonomously\nperforming multiple tasks remains highly challenging, as large-scale,\nhigh-quality underwater datasets are still scarce. To address these\nlimitations, we introduce USIM, a simulation-based multi-task\nVision-Language-Action (VLA) dataset for underwater robots. USIM comprises over\n561K frames from 1,852 trajectories, totaling approximately 15.6 hours of\nBlueROV2 interactions across 20 tasks in 9 diverse scenarios, ranging from\nvisual navigation to mobile manipulation. Building upon this dataset, we\npropose U0, a VLA model for general underwater robots, which integrates\nbinocular vision and other sensor modalities through multimodal fusion, and\nfurther incorporates a convolution-attention-based perception focus enhancement\nmodule (CAP) to improve spatial understanding and mobile manipulation. Across\ntasks such as inspection, obstacle avoidance, scanning, and dynamic tracking,\nthe framework achieves a success rate of 80%, while in challenging mobile\nmanipulation tasks, it reduces the distance to the target by 21.2% compared\nwith baseline methods, demonstrating its effectiveness. USIM and U0 show that\nVLA models can be effectively applied to underwater robotic applications,\nproviding a foundation for scalable dataset construction, improved task\nautonomy, and the practical realization of intelligent general underwater\nrobots.", "AI": {"tldr": "USIM\u662f\u4e00\u4e2a\u57fa\u4e8e\u4eff\u771f\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u591a\u4efb\u52a1\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc756.1\u4e07\u5e27\u6570\u636e\uff0c\u8986\u76d620\u4e2a\u4efb\u52a1\u548c9\u79cd\u573a\u666f\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u7684U0\u6a21\u578b\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u611f\u77e5\u805a\u7126\u589e\u5f3a\u6a21\u5757\uff0c\u5728\u5404\u9879\u4efb\u52a1\u4e2d\u8fbe\u523080%\u7684\u6210\u529f\u7387\uff0c\u5728\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c1121.2%\u7684\u76ee\u6807\u8ddd\u79bb\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u5e26\u6765\u72ec\u7279\u6311\u6218\uff0c\u5305\u62ec\u590d\u6742\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u6709\u9650\u80fd\u89c1\u5ea6\u548c\u53d7\u9650\u901a\u4fe1\u3002\u867d\u7136\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u9646\u5730\u673a\u5668\u4eba\u4e2d\u63a8\u52a8\u4e86\u5177\u8eab\u667a\u80fd\u53d1\u5c55\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6c34\u4e0b\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u591a\u4efb\u52a1\u7684\u6c34\u4e0b\u667a\u80fd\u7cfb\u7edf\u4ecd\u7136\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faUSIM\u4eff\u771f\u6570\u636e\u96c6\u548cU0\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002U0\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u6574\u5408\u53cc\u76ee\u89c6\u89c9\u548c\u5176\u4ed6\u4f20\u611f\u5668\u6a21\u6001\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5377\u79ef\u6ce8\u610f\u529b\u7684\u611f\u77e5\u805a\u7126\u589e\u5f3a\u6a21\u5757\u6765\u63d0\u5347\u7a7a\u95f4\u7406\u89e3\u548c\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b\u3002", "result": "\u5728\u68c0\u67e5\u3001\u907f\u969c\u3001\u626b\u63cf\u548c\u52a8\u6001\u8ddf\u8e2a\u7b49\u4efb\u52a1\u4e2d\uff0c\u6846\u67b6\u8fbe\u523080%\u7684\u6210\u529f\u7387\u3002\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5230\u76ee\u6807\u7684\u8ddd\u79bb\u51cf\u5c11\u4e8621.2%\u3002", "conclusion": "USIM\u548cU0\u8868\u660e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u6c34\u4e0b\u673a\u5668\u4eba\u5e94\u7528\uff0c\u4e3a\u53ef\u6269\u5c55\u6570\u636e\u96c6\u6784\u5efa\u3001\u6539\u8fdb\u4efb\u52a1\u81ea\u4e3b\u6027\u4ee5\u53ca\u667a\u80fd\u901a\u7528\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u5b9e\u9645\u5b9e\u73b0\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2510.07871", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.07871", "abs": "https://arxiv.org/abs/2510.07871", "authors": ["Erjia Xiao", "Lingfeng Zhang", "Yingbo Tang", "Hao Cheng", "Renjing Xu", "Wenbo Ding", "Lei Zhou", "Long Chen", "Hangjun Ye", "Xiaoshuai Hao"], "title": "Team Xiaomi EV-AD VLA: Learning to Navigate Socially Through Proactive Risk Perception -- Technical Report for IROS 2025 RoboSense Challenge Social Navigation Track", "comment": null, "summary": "In this report, we describe the technical details of our submission to the\nIROS 2025 RoboSense Challenge Social Navigation Track. This track focuses on\ndeveloping RGBD-based perception and navigation systems that enable autonomous\nagents to navigate safely, efficiently, and socially compliantly in dynamic\nhuman-populated indoor environments. The challenge requires agents to operate\nfrom an egocentric perspective using only onboard sensors including RGB-D\nobservations and odometry, without access to global maps or privileged\ninformation, while maintaining social norm compliance such as safe distances\nand collision avoidance. Building upon the Falcon model, we introduce a\nProactive Risk Perception Module to enhance social navigation performance. Our\napproach augments Falcon with collision risk understanding that learns to\npredict distance-based collision risk scores for surrounding humans, which\nenables the agent to develop more robust spatial awareness and proactive\ncollision avoidance behaviors. The evaluation on the Social-HM3D benchmark\ndemonstrates that our method improves the agent's ability to maintain personal\nspace compliance while navigating toward goals in crowded indoor scenes with\ndynamic human agents, achieving 2nd place among 16 participating teams in the\nchallenge.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u53c2\u52a0IROS 2025 RoboSense\u6311\u6218\u8d5b\u793e\u4f1a\u5bfc\u822a\u8d5b\u9053\u7684\u6280\u672f\u65b9\u6848\uff0c\u901a\u8fc7\u5728Falcon\u6a21\u578b\u57fa\u7840\u4e0a\u589e\u52a0\u4e3b\u52a8\u98ce\u9669\u611f\u77e5\u6a21\u5757\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u52a8\u6001\u4eba\u7fa4\u73af\u5883\u4e2d\u7684\u793e\u4f1a\u5408\u89c4\u5bfc\u822a\u80fd\u529b\uff0c\u572816\u652f\u53c2\u8d5b\u961f\u4f0d\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u52a8\u6001\u4eba\u7fa4\u73af\u5883\u4e2d\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u4ec5\u4f7f\u7528\u673a\u8f7dRGB-D\u4f20\u611f\u5668\u548c\u91cc\u7a0b\u8ba1\uff0c\u65e0\u9700\u5168\u5c40\u5730\u56fe\u6216\u7279\u6743\u4fe1\u606f\u3002", "method": "\u5728Falcon\u6a21\u578b\u57fa\u7840\u4e0a\u5f15\u5165\u4e3b\u52a8\u98ce\u9669\u611f\u77e5\u6a21\u5757\uff0c\u5b66\u4e60\u9884\u6d4b\u5468\u56f4\u4eba\u7c7b\u57fa\u4e8e\u8ddd\u79bb\u7684\u78b0\u649e\u98ce\u9669\u5206\u6570\uff0c\u589e\u5f3a\u7a7a\u95f4\u611f\u77e5\u548c\u4e3b\u52a8\u907f\u78b0\u884c\u4e3a\u3002", "result": "\u5728Social-HM3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u62e5\u6324\u5ba4\u5185\u573a\u666f\u4e2d\u4fdd\u6301\u4e2a\u4eba\u7a7a\u95f4\u5408\u89c4\u6027\u7684\u80fd\u529b\uff0c\u5728\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e8c\u540d\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e3b\u52a8\u98ce\u9669\u611f\u77e5\u6a21\u5757\u6709\u6548\u63d0\u5347\u4e86\u793e\u4f1a\u5bfc\u822a\u6027\u80fd\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u52a8\u6001\u4eba\u7c7b\u73af\u5883\u4e2d\u66f4\u7a33\u5065\u5730\u5bfc\u822a\u3002"}}
{"id": "2510.07975", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.07975", "abs": "https://arxiv.org/abs/2510.07975", "authors": ["Mingyang Sun", "Jiude Wei", "Qichen He", "Donglin Wang", "Cewu Lu", "Jianhua Sun"], "title": "Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation", "comment": null, "summary": "Enabling robots to perform precise and generalized manipulation in\nunstructured environments remains a fundamental challenge in embodied AI. While\nVision-Language Models (VLMs) have demonstrated remarkable capabilities in\nsemantic reasoning and task planning, a significant gap persists between their\nhigh-level understanding and the precise physical execution required for\nreal-world manipulation. To bridge this \"semantic-to-physical\" gap, we\nintroduce GRACE, a novel framework that grounds VLM-based reasoning through\nexecutable analytic concepts (EAC)-mathematically defined blueprints that\nencode object affordances, geometric constraints, and semantics of\nmanipulation. Our approach integrates a structured policy scaffolding pipeline\nthat turn natural language instructions and visual information into an\ninstantiated EAC, from which we derive grasp poses, force directions and plan\nphysically feasible motion trajectory for robot execution. GRACE thus provides\na unified and interpretable interface between high-level instruction\nunderstanding and low-level robot control, effectively enabling precise and\ngeneralizable manipulation through semantic-physical grounding. Extensive\nexperiments demonstrate that GRACE achieves strong zero-shot generalization\nacross a variety of articulated objects in both simulated and real-world\nenvironments, without requiring task-specific training.", "AI": {"tldr": "GRACE\u6846\u67b6\u901a\u8fc7\u53ef\u6267\u884c\u5206\u6790\u6982\u5ff5\uff08EAC\uff09\u5f25\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u63a8\u7406\u4e0e\u673a\u5668\u4eba\u7269\u7406\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u7cbe\u786e\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u63a8\u7406\u548c\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u7269\u7406\u6267\u884c\u4e4b\u95f4\u7684\"\u8bed\u4e49\u5230\u7269\u7406\"\u5dee\u8ddd\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u7cbe\u786e\u548c\u901a\u7528\u7684\u64cd\u4f5c\u3002", "method": "\u5f15\u5165GRACE\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u6267\u884c\u5206\u6790\u6982\u5ff5\uff08EAC\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u548c\u89c6\u89c9\u4fe1\u606f\u8f6c\u5316\u4e3a\u5b9e\u4f8b\u5316\u7684EAC\u84dd\u56fe\uff0c\u4ece\u4e2d\u63a8\u5bfc\u6293\u53d6\u59ff\u6001\u3001\u529b\u65b9\u5411\u548c\u7269\u7406\u53ef\u884c\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5bf9\u591a\u79cd\u94f0\u63a5\u7269\u4f53\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "conclusion": "GRACE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u63a5\u53e3\uff0c\u901a\u8fc7\u8bed\u4e49-\u7269\u7406\u57fa\u7840\u6709\u6548\u5b9e\u73b0\u4e86\u7cbe\u786e\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2510.07986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.07986", "abs": "https://arxiv.org/abs/2510.07986", "authors": ["Gaofeng Li", "Peisen Xu", "Ruize Wang", "Qi Ye", "Jiming Chen", "Dezhen Song", "Yanlong Huang"], "title": "Orientation Learning and Adaptation towards Simultaneous Incorporation of Multiple Local Constraints", "comment": null, "summary": "Orientation learning plays a pivotal role in many tasks. However, the\nrotation group SO(3) is a Riemannian manifold. As a result, the distortion\ncaused by non-Euclidean geometric nature introduces difficulties to the\nincorporation of local constraints, especially for the simultaneous\nincorporation of multiple local constraints. To address this issue, we propose\nthe Angle-Axis Space-based orientation representation method to solve several\norientation learning problems, including orientation adaptation and\nminimization of angular acceleration. Specifically, we propose a weighted\naverage mechanism in SO(3) based on the angle-axis representation method. Our\nmain idea is to generate multiple trajectories by considering different local\nconstraints at different basepoints. Then these multiple trajectories are fused\nto generate a smooth trajectory by our proposed weighted average mechanism,\nachieving the goal to incorporate multiple local constraints simultaneously.\nCompared with existing solution, ours can address the distortion issue and make\nthe off-theshelf Euclidean learning algorithm be re-applicable in non-Euclidean\nspace. Simulation and Experimental evaluations validate that our solution can\nnot only adapt orientations towards arbitrary desired via-points and cope with\nangular acceleration constraints, but also incorporate multiple local\nconstraints simultaneously to achieve extra benefits, e.g., achieving smaller\nacceleration costs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89d2\u5ea6-\u8f74\u7a7a\u95f4\u7684\u65b9\u5411\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u673a\u5236\u5728SO(3)\u6d41\u5f62\u4e0a\u878d\u5408\u591a\u4e2a\u8003\u8651\u4e0d\u540c\u5c40\u90e8\u7ea6\u675f\u7684\u8f68\u8ff9\uff0c\u89e3\u51b3\u975e\u6b27\u51e0\u4f55\u5e26\u6765\u7684\u7578\u53d8\u95ee\u9898\uff0c\u5b9e\u73b0\u591a\u5c40\u90e8\u7ea6\u675f\u7684\u540c\u65f6\u6574\u5408\u3002", "motivation": "\u65cb\u8f6c\u7fa4SO(3)\u662f\u9ece\u66fc\u6d41\u5f62\uff0c\u5176\u975e\u6b27\u51e0\u4f55\u7279\u6027\u5bfc\u81f4\u5c40\u90e8\u7ea6\u675f\u6574\u5408\u56f0\u96be\uff0c\u7279\u522b\u662f\u540c\u65f6\u6574\u5408\u591a\u4e2a\u5c40\u90e8\u7ea6\u675f\u65f6\u5b58\u5728\u7578\u53d8\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u89d2\u5ea6-\u8f74\u8868\u793a\u6cd5\u63d0\u51faSO(3)\u4e0a\u7684\u52a0\u6743\u5e73\u5747\u673a\u5236\uff0c\u5728\u4e0d\u540c\u57fa\u70b9\u8003\u8651\u4e0d\u540c\u5c40\u90e8\u7ea6\u675f\u751f\u6210\u591a\u4e2a\u8f68\u8ff9\uff0c\u7136\u540e\u878d\u5408\u8fd9\u4e9b\u8f68\u8ff9\u751f\u6210\u5e73\u6ed1\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u9002\u5e94\u4efb\u610f\u671f\u671b\u8def\u5f84\u70b9\u7684\u65b9\u5411\u5e76\u5904\u7406\u89d2\u52a0\u901f\u5ea6\u7ea6\u675f\uff0c\u8fd8\u80fd\u540c\u65f6\u6574\u5408\u591a\u4e2a\u5c40\u90e8\u7ea6\u675f\u83b7\u5f97\u989d\u5916\u6536\u76ca\uff0c\u5982\u5b9e\u73b0\u66f4\u5c0f\u7684\u52a0\u901f\u5ea6\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86SO(3)\u6d41\u5f62\u7684\u7578\u53d8\u95ee\u9898\uff0c\u4f7f\u73b0\u6210\u7684\u6b27\u51e0\u91cc\u5f97\u5b66\u4e60\u7b97\u6cd5\u5728\u975e\u6b27\u7a7a\u95f4\u4e2d\u91cd\u65b0\u9002\u7528\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u591a\u5c40\u90e8\u7ea6\u675f\u7684\u540c\u65f6\u6574\u5408\u3002"}}
{"id": "2510.08022", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08022", "abs": "https://arxiv.org/abs/2510.08022", "authors": ["Kehui Liu", "Zhongjie Jia", "Yang Li", "Zhaxizhuoma", "Pengan Chen", "Song Liu", "Xin Liu", "Pingrui Zhang", "Haoming Song", "Xinyi Ye", "Nieqing Cao", "Zhigang Wang", "Jia Zeng", "Dong Wang", "Yan Ding", "Bin Zhao", "Xuelong Li"], "title": "FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset", "comment": null, "summary": "Data-driven robotic manipulation learning depends on large-scale,\nhigh-quality expert demonstration datasets. However, existing datasets, which\nprimarily rely on human teleoperated robot collection, are limited in terms of\nscalability, trajectory smoothness, and applicability across different robotic\nembodiments in real-world environments. In this paper, we present FastUMI-100K,\na large-scale UMI-style multimodal demonstration dataset, designed to overcome\nthese limitations and meet the growing complexity of real-world manipulation\ntasks. Collected by FastUMI, a novel robotic system featuring a modular,\nhardware-decoupled mechanical design and an integrated lightweight tracking\nsystem, FastUMI-100K offers a more scalable, flexible, and adaptable solution\nto fulfill the diverse requirements of real-world robot demonstration data.\nSpecifically, FastUMI-100K contains over 100K+ demonstration trajectories\ncollected across representative household environments, covering 54 tasks and\nhundreds of object types. Our dataset integrates multimodal streams, including\nend-effector states, multi-view wrist-mounted fisheye images and textual\nannotations. Each trajectory has a length ranging from 120 to 500 frames.\nExperimental results demonstrate that FastUMI-100K enables high policy success\nrates across various baseline algorithms, confirming its robustness,\nadaptability, and real-world applicability for solving complex, dynamic\nmanipulation challenges. The source code and dataset will be released in this\nlink https://github.com/MrKeee/FastUMI-100K.", "AI": {"tldr": "FastUMI-100K\u662f\u4e00\u4e2a\u5927\u89c4\u6a21UMI\u98ce\u683c\u7684\u591a\u6a21\u6001\u6f14\u793a\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc710\u4e07\u6761\u8f68\u8ff9\uff0c\u6db5\u76d654\u4e2a\u4efb\u52a1\u548c\u6570\u767e\u79cd\u7269\u4f53\u7c7b\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u7c7b\u9065\u64cd\u4f5c\u673a\u5668\u4eba\u6536\u96c6\u7684\u6570\u636e\u96c6\u5728\u53ef\u6269\u5c55\u6027\u3001\u8f68\u8ff9\u5e73\u6ed1\u6027\u548c\u4e0d\u540c\u673a\u5668\u4eba\u5f62\u6001\u7684\u9002\u7528\u6027\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528FastUMI\u673a\u5668\u4eba\u7cfb\u7edf\u6536\u96c6\u6570\u636e\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528\u6a21\u5757\u5316\u3001\u786c\u4ef6\u89e3\u8026\u7684\u673a\u68b0\u8bbe\u8ba1\u548c\u96c6\u6210\u8f7b\u91cf\u7ea7\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u5305\u542b\u672b\u7aef\u6267\u884c\u5668\u72b6\u6001\u3001\u591a\u89c6\u89d2\u9c7c\u773c\u56fe\u50cf\u548c\u6587\u672c\u6ce8\u91ca\u7b49\u591a\u6a21\u6001\u6570\u636e\u6d41\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFastUMI-100K\u5728\u5404\u79cd\u57fa\u7ebf\u7b97\u6cd5\u4e0a\u90fd\u80fd\u5b9e\u73b0\u9ad8\u7b56\u7565\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u548c\u73b0\u5b9e\u4e16\u754c\u9002\u7528\u6027\u3002", "conclusion": "FastUMI-100K\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u548c\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u6001\u64cd\u4f5c\u6311\u6218\u3002"}}
{"id": "2510.08118", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.08118", "abs": "https://arxiv.org/abs/2510.08118", "authors": ["Massimiliano de Leoni", "Faizan Ahmed Khan", "Simone Agostinelli"], "title": "Accurate and Noise-Tolerant Extraction of Routine Logs in Robotic Process Automation (Extended Version)", "comment": "16 pages, 5 figures", "summary": "Robotic Process Mining focuses on the identification of the routine types\nperformed by human resources through a User Interface. The ultimate goal is to\ndiscover routine-type models to enable robotic process automation. The\ndiscovery of routine-type models requires the provision of a routine log.\nUnfortunately, the vast majority of existing works do not directly focus on\nenabling the model discovery, limiting themselves to extracting the set of\nactions that are part of the routines. They were also not evaluated in\nscenarios characterized by inconsistent routine execution, hereafter referred\nto as noise, which reflects natural variability and occasional errors in human\nperformance. This paper presents a clustering-based technique that aims to\nextract routine logs. Experiments were conducted on nine UI logs from the\nliterature with different levels of injected noise. Our technique was compared\nwith existing techniques, most of which are not meant to discover routine logs\nbut were adapted for the purpose. The results were evaluated through standard\nstate-of-the-art metrics, showing that we can extract more accurate routine\nlogs than what the state of the art could, especially in the presence of noise.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6280\u672f\uff0c\u7528\u4e8e\u4ece\u7528\u6237\u754c\u9762\u65e5\u5fd7\u4e2d\u63d0\u53d6\u5e38\u89c4\u65e5\u5fd7\uff0c\u7279\u522b\u9488\u5bf9\u5b58\u5728\u6267\u884c\u4e0d\u4e00\u81f4\u6027\uff08\u566a\u58f0\uff09\u7684\u573a\u666f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u80fd\u63d0\u53d6\u66f4\u51c6\u786e\u7684\u5e38\u89c4\u65e5\u5fd7\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u5927\u591a\u4e0d\u76f4\u63a5\u5173\u6ce8\u6a21\u578b\u53d1\u73b0\uff0c\u4ec5\u63d0\u53d6\u5e38\u89c4\u64cd\u4f5c\u96c6\u5408\uff0c\u4e14\u672a\u5728\u5b58\u5728\u6267\u884c\u4e0d\u4e00\u81f4\u6027\uff08\u566a\u58f0\uff09\u7684\u573a\u666f\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u53cd\u6620\u4e86\u4eba\u7c7b\u6267\u884c\u7684\u81ea\u7136\u53d8\u5f02\u6027\u548c\u5076\u7136\u9519\u8bef\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u6280\u672f\u4ece\u7528\u6237\u754c\u9762\u65e5\u5fd7\u4e2d\u63d0\u53d6\u5e38\u89c4\u65e5\u5fd7\uff0c\u5e76\u5728\u4e5d\u4e2a\u5177\u6709\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u7684UI\u65e5\u5fd7\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u53d6\u66f4\u51c6\u786e\u7684\u5e38\u89c4\u65e5\u5fd7\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u6807\u51c6\u6700\u5148\u8fdb\u6307\u6807\u8bc4\u4f30\u663e\u793a\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u805a\u7c7b\u6280\u672f\u80fd\u591f\u6709\u6548\u63d0\u53d6\u5e38\u89c4\u65e5\u5fd7\uff0c\u5c24\u5176\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6a21\u578b\u53d1\u73b0\u80fd\u529b\u3002"}}
{"id": "2510.08270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08270", "abs": "https://arxiv.org/abs/2510.08270", "authors": ["Damir Nurtdinov", "Aliaksei Korshuk", "Alexei Kornaev", "Alexander Maloletov"], "title": "Evaluation of a Robust Control System in Real-World Cable-Driven Parallel Robots", "comment": null, "summary": "This study evaluates the performance of classical and modern control methods\nfor real-world Cable-Driven Parallel Robots (CDPRs), focusing on\nunderconstrained systems with limited time discretization. A comparative\nanalysis is conducted between classical PID controllers and modern\nreinforcement learning algorithms, including Deep Deterministic Policy Gradient\n(DDPG), Proximal Policy Optimization (PPO), and Trust Region Policy\nOptimization (TRPO). The results demonstrate that TRPO outperforms other\nmethods, achieving the lowest root mean square (RMS) errors across various\ntrajectories and exhibiting robustness to larger time intervals between control\nupdates. TRPO's ability to balance exploration and exploitation enables stable\ncontrol in noisy, real-world environments, reducing reliance on high-frequency\nsensor feedback and computational demands. These findings highlight TRPO's\npotential as a robust solution for complex robotic control tasks, with\nimplications for dynamic environments and future applications in sensor fusion\nor hybrid control strategies.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u7ecf\u5178PID\u63a7\u5236\u5668\u4e0e\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u6b20\u7ea6\u675f\u7ebf\u9a71\u52a8\u5e76\u8054\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0TRPO\u5728\u591a\u79cd\u8f68\u8ff9\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u5177\u6709\u6700\u4f4e\u7684RMS\u8bef\u5dee\u548c\u5bf9\u66f4\u5927\u63a7\u5236\u66f4\u65b0\u95f4\u9694\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u8bc4\u4f30\u7ecf\u5178\u548c\u73b0\u4ee3\u63a7\u5236\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7ebf\u9a71\u52a8\u5e76\u8054\u673a\u5668\u4eba\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u5173\u6ce8\u6b20\u7ea6\u675f\u7cfb\u7edf\u548c\u6709\u9650\u65f6\u95f4\u79bb\u6563\u5316\u6761\u4ef6\u4e0b\u7684\u63a7\u5236\u6548\u679c\u3002", "method": "\u5bf9\u7ecf\u5178PID\u63a7\u5236\u5668\u548c\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5305\u62ecDDPG\u3001PPO\u548cTRPO\uff09\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u5404\u79cd\u8f68\u8ff9\u4e0b\u7684\u63a7\u5236\u6027\u80fd\u3002", "result": "TRPO\u5728\u6240\u6709\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684RMS\u8bef\u5dee\uff0c\u5e76\u4e14\u5728\u66f4\u5927\u63a7\u5236\u66f4\u65b0\u95f4\u9694\u4e0b\u4ecd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u63a7\u5236\u3002", "conclusion": "TRPO\u4f5c\u4e3a\u590d\u6742\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u5bf9\u52a8\u6001\u73af\u5883\u548c\u672a\u6765\u4f20\u611f\u5668\u878d\u5408\u6216\u6df7\u5408\u63a7\u5236\u7b56\u7565\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2510.08381", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08381", "abs": "https://arxiv.org/abs/2510.08381", "authors": ["Baoyang Chen", "Xian Xu", "Huamin Qu"], "title": "Airy: Reading Robot Intent through Height and Sky", "comment": null, "summary": "As industrial robots move into shared human spaces, their opaque decision\nmaking threatens safety, trust, and public oversight. This artwork, Airy, asks\nwhether complex multi agent AI can become intuitively understandable by staging\na competition between two reinforcement trained robot arms that snap a bedsheet\nskyward. Building on three design principles, competition as a clear metric\n(who lifts higher), embodied familiarity (audiences recognize fabric snapping),\nand sensor to sense mapping (robot cooperation or rivalry shown through forest\nand weather projections), the installation gives viewers a visceral way to read\nmachine intent. Observations from five international exhibitions indicate that\naudiences consistently read the robots' strategies, conflict, and cooperation\nin real time, with emotional reactions that mirror the system's internal state.\nThe project shows how sensory metaphors can turn a black box into a public\ninterface.", "AI": {"tldr": "Airy\u662f\u4e00\u4e2a\u827a\u672f\u88c5\u7f6e\uff0c\u901a\u8fc7\u4e24\u4e2a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u673a\u68b0\u81c2\u7ade\u4e89\u5c06\u5e8a\u5355\u629b\u5411\u7a7a\u4e2d\u7684\u8868\u6f14\uff0c\u8ba9\u89c2\u4f17\u76f4\u89c2\u7406\u89e3\u590d\u6742\u591a\u667a\u80fd\u4f53AI\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u968f\u7740\u5de5\u4e1a\u673a\u5668\u4eba\u8fdb\u5165\u5171\u4eab\u4eba\u7c7b\u7a7a\u95f4\uff0c\u5176\u4e0d\u900f\u660e\u7684\u51b3\u7b56\u8fc7\u7a0b\u5a01\u80c1\u7740\u5b89\u5168\u3001\u4fe1\u4efb\u548c\u516c\u4f17\u76d1\u7763\u3002\u8be5\u4f5c\u54c1\u65e8\u5728\u63a2\u7d22\u590d\u6742\u591a\u667a\u80fd\u4f53AI\u662f\u5426\u80fd\u591f\u53d8\u5f97\u76f4\u89c2\u53ef\u7406\u89e3\u3002", "method": "\u57fa\u4e8e\u4e09\u4e2a\u8bbe\u8ba1\u539f\u5219\uff1a\u7ade\u4e89\u4f5c\u4e3a\u6e05\u6670\u6307\u6807\uff08\u8c01\u629b\u5f97\u66f4\u9ad8\uff09\u3001\u5177\u8eab\u719f\u6089\u5ea6\uff08\u89c2\u4f17\u80fd\u8bc6\u522b\u5e03\u6599\u629b\u63b7\u52a8\u4f5c\uff09\u3001\u4f20\u611f\u5668\u5230\u611f\u77e5\u6620\u5c04\uff08\u901a\u8fc7\u68ee\u6797\u548c\u5929\u6c14\u6295\u5f71\u663e\u793a\u673a\u5668\u4eba\u5408\u4f5c\u6216\u7ade\u4e89\u5173\u7cfb\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u56fd\u9645\u5c55\u89c8\u4e2d\u7684\u89c2\u5bdf\u8868\u660e\uff0c\u89c2\u4f17\u80fd\u591f\u5b9e\u65f6\u89e3\u8bfb\u673a\u5668\u4eba\u7684\u7b56\u7565\u3001\u51b2\u7a81\u548c\u5408\u4f5c\uff0c\u60c5\u611f\u53cd\u5e94\u4e0e\u7cfb\u7edf\u5185\u90e8\u72b6\u6001\u76f8\u7b26\u3002", "conclusion": "\u8be5\u9879\u76ee\u5c55\u793a\u4e86\u611f\u5b98\u9690\u55bb\u5982\u4f55\u5c06\u9ed1\u76d2\u7cfb\u7edf\u8f6c\u53d8\u4e3a\u516c\u5171\u63a5\u53e3\uff0c\u4f7f\u590d\u6742AI\u51b3\u7b56\u53d8\u5f97\u53ef\u611f\u77e5\u548c\u7406\u89e3\u3002"}}
{"id": "2510.08406", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC", "I.2.9; G.1.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.08406", "abs": "https://arxiv.org/abs/2510.08406", "authors": ["Filip Be\u010danovi\u0107", "Kosta Jovanovi\u0107", "Vincent Bonnet"], "title": "Reliability of Single-Level Equality-Constrained Inverse Optimal Control", "comment": "8 pages, 3 figures", "summary": "Inverse optimal control (IOC) allows the retrieval of optimal cost function\nweights, or behavioral parameters, from human motion. The literature on IOC\nuses methods that are either based on a slow bilevel process or a fast but\nnoise-sensitive minimization of optimality condition violation. Assuming\nequality-constrained optimal control models of human motion, this article\npresents a faster but robust approach to solving IOC using a single-level\nreformulation of the bilevel method and yields equivalent results. Through\nnumerical experiments in simulation, we analyze the robustness to noise of the\nproposed single-level reformulation to the bilevel IOC formulation with a\nhuman-like planar reaching task that is used across recent studies. The\napproach shows resilience to very large levels of noise and reduces the\ncomputation time of the IOC on this task by a factor of 15 when compared to a\nclassical bilevel implementation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u7ea7\u91cd\u6784\u7684\u9006\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u53cc\u5c42\u65b9\u6cd5\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u534715\u500d\uff0c\u540c\u65f6\u5bf9\u566a\u58f0\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u9006\u6700\u4f18\u63a7\u5236\u65b9\u6cd5\u8981\u4e48\u57fa\u4e8e\u7f13\u6162\u7684\u53cc\u5c42\u8fc7\u7a0b\uff0c\u8981\u4e48\u57fa\u4e8e\u5feb\u901f\u4f46\u5bf9\u566a\u58f0\u654f\u611f\u7684\u4f18\u5316\u6761\u4ef6\u8fdd\u53cd\u6700\u5c0f\u5316\u65b9\u6cd5\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e2\u5feb\u901f\u53c8\u9c81\u68d2\u7684IOC\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5355\u7ea7\u91cd\u6784\u65b9\u6cd5\u66ff\u4ee3\u4f20\u7edf\u7684\u53cc\u5c42IOC\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7b49\u5f0f\u7ea6\u675f\u7684\u6700\u4f18\u63a7\u5236\u6a21\u578b\u6765\u91cd\u6784\u95ee\u9898\u3002", "result": "\u5728\u5e73\u9762\u5230\u8fbe\u4efb\u52a1\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5bf9\u975e\u5e38\u5927\u7684\u566a\u58f0\u6c34\u5e73\u8868\u73b0\u51fa\u5f39\u6027\uff0c\u8ba1\u7b97\u65f6\u95f4\u6bd4\u7ecf\u5178\u53cc\u5c42\u5b9e\u73b0\u51cf\u5c11\u4e8615\u500d\u3002", "conclusion": "\u5355\u7ea7\u91cd\u6784\u65b9\u6cd5\u5728\u4fdd\u6301\u7ed3\u679c\u7b49\u6548\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u4e3a\u9006\u6700\u4f18\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08408", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08408", "abs": "https://arxiv.org/abs/2510.08408", "authors": ["Bibekananda Patra", "Rajeevlochana G. Chittawadigi", "Sandipan Bandyopadhyay"], "title": "Validation of collision-free spheres of Stewart-Gough platforms for constant orientations using the Application Programming Interface of a CAD software", "comment": null, "summary": "This paper presents a method of validation of the size of the largest\ncollision-free sphere (CFS) of a 6-6 Stewart-Gough platform manipulator (SGPM)\nfor a given orientation of its moving platform (MP) using the Application\nProgramming Interface (API) of a CAD software. The position of the MP is\nupdated via the API in an automated manner over a set of samples within a shell\nenclosing the surface of the CFS. For each pose of the manipulator, each pair\nof legs is investigated for mutual collisions. The CFS is considered safe or\nvalidated iff none of the points falling inside the CFS lead to a collision\nbetween any pair of legs. This approach can not only validate the safety of a\nprecomputed CFS, but also estimate the same for any spatial parallel\nmanipulator.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528CAD\u8f6f\u4ef6API\u9a8c\u8bc16-6 Stewart-Gough\u5e73\u53f0\u673a\u68b0\u81c2\u6700\u5927\u65e0\u78b0\u649e\u7403\u4f53\u5c3a\u5bf8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u66f4\u65b0\u79fb\u52a8\u5e73\u53f0\u4f4d\u7f6e\u6765\u68c0\u6d4b\u817f\u90e8\u4e4b\u95f4\u7684\u78b0\u649e\u3002", "motivation": "\u9700\u8981\u9a8c\u8bc1\u5e76\u8054\u673a\u68b0\u81c2\u5728\u7ed9\u5b9a\u79fb\u52a8\u5e73\u53f0\u65b9\u5411\u4e0b\u7684\u6700\u5927\u65e0\u78b0\u649e\u5de5\u4f5c\u7a7a\u95f4\uff0c\u786e\u4fdd\u64cd\u4f5c\u5b89\u5168\u6027\u3002", "method": "\u5229\u7528CAD\u8f6f\u4ef6API\u81ea\u52a8\u5316\u66f4\u65b0\u79fb\u52a8\u5e73\u53f0\u4f4d\u7f6e\uff0c\u5728\u65e0\u78b0\u649e\u7403\u4f53\u8868\u9762\u7684\u58f3\u5c42\u5185\u91c7\u6837\uff0c\u68c0\u6d4b\u6bcf\u5bf9\u817f\u90e8\u4e4b\u95f4\u7684\u76f8\u4e92\u78b0\u649e\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9a8c\u8bc1\u9884\u8ba1\u7b97\u7684\u65e0\u78b0\u649e\u7403\u4f53\u5b89\u5168\u6027\uff0c\u5e76\u4f30\u8ba1\u4efb\u4f55\u7a7a\u95f4\u5e76\u8054\u673a\u68b0\u81c2\u7684\u65e0\u78b0\u649e\u5de5\u4f5c\u7a7a\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eCAD API\u7684\u9a8c\u8bc1\u65b9\u6cd5\u4e3a\u5e76\u8054\u673a\u68b0\u81c2\u7684\u65e0\u78b0\u649e\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08464", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08464", "abs": "https://arxiv.org/abs/2510.08464", "authors": ["Jason Jabbour", "Dong-Ki Kim", "Max Smith", "Jay Patrikar", "Radhika Ghosal", "Youhui Wang", "Ali Agha", "Vijay Janapa Reddi", "Shayegan Omidshafiei"], "title": "Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered", "comment": null, "summary": "Vision-Language-Action (VLA) models have advanced robotic capabilities but\nremain challenging to deploy on resource-limited hardware. Pruning has enabled\nefficient compression of large language models (LLMs), yet it is largely\nunderstudied in robotics. Surprisingly, we observe that pruning VLA models\nleads to drastic degradation and increased safety violations. We introduce\nGLUESTICK, a post-pruning recovery method that restores much of the original\nmodel's functionality while retaining sparsity benefits. Our method performs a\none-time interpolation between the dense and pruned models in weight-space to\ncompute a corrective term. This correction is used during inference by each\npruned layer to recover lost capabilities with minimal overhead. GLUESTICK\nrequires no additional training, is agnostic to the pruning algorithm, and\nintroduces a single hyperparameter that controls the tradeoff between\nefficiency and accuracy. Across diverse VLA architectures and tasks in\nmanipulation and navigation, GLUESTICK achieves competitive memory efficiency\nwhile substantially recovering success rates and reducing safety violations.\nAdditional material can be found at: https://gluestick-vla.github.io/.", "AI": {"tldr": "GLUESTICK\u662f\u4e00\u79cd\u540e\u526a\u679d\u6062\u590d\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u526a\u679d\u540e\u6027\u80fd\u6025\u5267\u4e0b\u964d\u548c\u5b89\u5168\u8fdd\u89c4\u589e\u52a0\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6743\u91cd\u7a7a\u95f4\u63d2\u503c\u6062\u590d\u6a21\u578b\u529f\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u548c\u5b89\u5168\u8fdd\u89c4\u589e\u52a0\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6062\u590d\u65b9\u6cd5\u3002", "method": "\u5728\u6743\u91cd\u7a7a\u95f4\u4e2d\u5bf9\u5bc6\u96c6\u6a21\u578b\u548c\u526a\u679d\u6a21\u578b\u8fdb\u884c\u4e00\u6b21\u63d2\u503c\u8ba1\u7b97\u6821\u6b63\u9879\uff0c\u5728\u63a8\u7406\u65f6\u7531\u6bcf\u4e2a\u526a\u679d\u5c42\u4f7f\u7528\u8be5\u6821\u6b63\u9879\u6062\u590d\u4e22\u5931\u7684\u80fd\u529b\uff0c\u5f15\u5165\u5355\u4e2a\u8d85\u53c2\u6570\u63a7\u5236\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u3002", "result": "\u5728\u591a\u79cdVLA\u67b6\u6784\u548c\u64cd\u4f5c\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cGLUESTICK\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5185\u5b58\u6548\u7387\uff0c\u540c\u65f6\u663e\u8457\u6062\u590d\u4e86\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u5b89\u5168\u8fdd\u89c4\u3002", "conclusion": "GLUESTICK\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3001\u4e0e\u526a\u679d\u7b97\u6cd5\u65e0\u5173\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u7a00\u758f\u6027\u4f18\u52bf\u7684\u540c\u65f6\u6062\u590d\u526a\u679dVLA\u6a21\u578b\u7684\u529f\u80fd\u3002"}}
{"id": "2510.08475", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08475", "abs": "https://arxiv.org/abs/2510.08475", "authors": ["Jhen Hsieh", "Kuan-Hsun Tu", "Kuo-Han Hung", "Tsung-Wei Ke"], "title": "DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos", "comment": "Video results are available at:\n  https://embodiedai-ntu.github.io/dexman/index.html", "summary": "We present DexMan, an automated framework that converts human visual\ndemonstrations into bimanual dexterous manipulation skills for humanoid robots\nin simulation. Operating directly on third-person videos of humans manipulating\nrigid objects, DexMan eliminates the need for camera calibration, depth\nsensors, scanned 3D object assets, or ground-truth hand and object motion\nannotations. Unlike prior approaches that consider only simplified floating\nhands, it directly controls a humanoid robot and leverages novel contact-based\nrewards to improve policy learning from noisy hand-object poses estimated from\nin-the-wild videos.\n  DexMan achieves state-of-the-art performance in object pose estimation on the\nTACO benchmark, with absolute gains of 0.08 and 0.12 in ADD-S and VSD.\nMeanwhile, its reinforcement learning policy surpasses previous methods by 19%\nin success rate on OakInk-v2. Furthermore, DexMan can generate skills from both\nreal and synthetic videos, without the need for manual data collection and\ncostly motion capture, and enabling the creation of large-scale, diverse\ndatasets for training generalist dexterous manipulation.", "AI": {"tldr": "DexMan\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u53ef\u5c06\u4eba\u7c7b\u89c6\u89c9\u6f14\u793a\u8f6c\u6362\u4e3a\u4eff\u771f\u4e2d\u4eba\u5f62\u673a\u5668\u4eba\u7684\u53cc\u624b\u7075\u5de7\u64cd\u4f5c\u6280\u80fd\uff0c\u65e0\u9700\u76f8\u673a\u6807\u5b9a\u3001\u6df1\u5ea6\u4f20\u611f\u5668\u62163D\u5bf9\u8c61\u8d44\u4ea7\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4ec5\u8003\u8651\u7b80\u5316\u6d6e\u52a8\u624b\u7684\u95ee\u9898\uff0c\u76f4\u63a5\u63a7\u5236\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u63a5\u89e6\u7684\u5956\u52b1\u4ece\u91ce\u5916\u89c6\u9891\u7684\u566a\u58f0\u624b-\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u6539\u8fdb\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u76f4\u63a5\u5728\u4eba\u7c7b\u64cd\u4f5c\u521a\u6027\u7269\u4f53\u7684\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u4e0a\u64cd\u4f5c\uff0c\u4f7f\u7528\u57fa\u4e8e\u63a5\u89e6\u7684\u5956\u52b1\u6765\u6539\u8fdb\u7b56\u7565\u5b66\u4e60\uff0c\u65e0\u9700\u624b\u52a8\u6570\u636e\u6536\u96c6\u548c\u6602\u8d35\u7684\u52a8\u4f5c\u6355\u6349\u3002", "result": "\u5728TACO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0cADD-S\u548cVSD\u5206\u522b\u63d0\u53470.08\u548c0.12\uff1b\u5728OakInk-v2\u4e0a\u6210\u529f\u7387\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad819%\u3002", "conclusion": "DexMan\u80fd\u591f\u4ece\u771f\u5b9e\u548c\u5408\u6210\u89c6\u9891\u751f\u6210\u6280\u80fd\uff0c\u65e0\u9700\u624b\u52a8\u6570\u636e\u6536\u96c6\uff0c\u4e3a\u8bad\u7ec3\u901a\u7528\u7075\u5de7\u64cd\u4f5c\u521b\u5efa\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u3002"}}
{"id": "2510.08547", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08547", "abs": "https://arxiv.org/abs/2510.08547", "authors": ["Xiuwei Xu", "Angyuan Ma", "Hankun Li", "Bingyao Yu", "Zheng Zhu", "Jie Zhou", "Jiwen Lu"], "title": "R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation", "comment": "Project page: https://r2rgen.github.io/", "summary": "Towards the aim of generalized robotic manipulation, spatial generalization\nis the most fundamental capability that requires the policy to work robustly\nunder different spatial distribution of objects, environment and agent itself.\nTo achieve this, substantial human demonstrations need to be collected to cover\ndifferent spatial configurations for training a generalized visuomotor policy\nvia imitation learning. Prior works explore a promising direction that\nleverages data generation to acquire abundant spatially diverse data from\nminimal source demonstrations. However, most approaches face significant\nsim-to-real gap and are often limited to constrained settings, such as\nfixed-base scenarios and predefined camera viewpoints. In this paper, we\npropose a real-to-real 3D data generation framework (R2RGen) that directly\naugments the pointcloud observation-action pairs to generate real-world data.\nR2RGen is simulator- and rendering-free, thus being efficient and\nplug-and-play. Specifically, given a single source demonstration, we introduce\nan annotation mechanism for fine-grained parsing of scene and trajectory. A\ngroup-wise augmentation strategy is proposed to handle complex multi-object\ncompositions and diverse task constraints. We further present camera-aware\nprocessing to align the distribution of generated data with real-world 3D\nsensor. Empirically, R2RGen substantially enhances data efficiency on extensive\nexperiments and demonstrates strong potential for scaling and application on\nmobile manipulation.", "AI": {"tldr": "\u63d0\u51faR2RGen\u6846\u67b6\uff0c\u901a\u8fc7\u771f\u5b9e\u5230\u771f\u5b9e\u76843D\u6570\u636e\u751f\u6210\u589e\u5f3a\u70b9\u4e91\u89c2\u6d4b-\u52a8\u4f5c\u5bf9\uff0c\u65e0\u9700\u6a21\u62df\u5668\u548c\u6e32\u67d3\uff0c\u5b9e\u73b0\u9ad8\u6548\u5373\u63d2\u5373\u7528\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u7a7a\u95f4\u6cdb\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6f14\u793a\u8986\u76d6\u4e0d\u540c\u7a7a\u95f4\u914d\u7f6e\uff0c\u73b0\u6709\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u663e\u8457\u7684\u6a21\u62df\u5230\u771f\u5b9e\u5dee\u8ddd\uff0c\u4e14\u53d7\u9650\u4e8e\u56fa\u5b9a\u57fa\u5ea7\u573a\u666f\u548c\u9884\u5b9a\u4e49\u76f8\u673a\u89c6\u89d2\u3002", "method": "\u63d0\u51faR2RGen\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u7ec6\u7c92\u5ea6\u573a\u666f\u548c\u8f68\u8ff9\u89e3\u6790\u7684\u6807\u6ce8\u673a\u5236\uff1b2\uff09\u5904\u7406\u590d\u6742\u591a\u5bf9\u8c61\u7ec4\u5408\u548c\u591a\u6837\u5316\u4efb\u52a1\u7ea6\u675f\u7684\u5206\u7ec4\u589e\u5f3a\u7b56\u7565\uff1b3\uff09\u4e0e\u771f\u5b9e\u4e16\u754c3D\u4f20\u611f\u5668\u5206\u5e03\u5bf9\u9f50\u7684\u76f8\u673a\u611f\u77e5\u5904\u7406\u3002", "result": "R2RGen\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u79fb\u52a8\u64cd\u4f5c\u4e2d\u6269\u5c55\u548c\u5e94\u7528\u7684\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "R2RGen\u662f\u4e00\u4e2a\u65e0\u9700\u6a21\u62df\u5668\u548c\u6e32\u67d3\u7684\u771f\u5b9e\u5230\u771f\u5b9e3D\u6570\u636e\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u7a7a\u95f4\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.08556", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08556", "abs": "https://arxiv.org/abs/2510.08556", "authors": ["Xueyi Liu", "He Wang", "Li Yi"], "title": "DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model", "comment": "Project Website: https://meowuu7.github.io/DexNDM/ Video:\n  https://youtu.be/tU2Mv8vWftU", "summary": "Achieving generalized in-hand object rotation remains a significant challenge\nin robotics, largely due to the difficulty of transferring policies from\nsimulation to the real world. The complex, contact-rich dynamics of dexterous\nmanipulation create a \"reality gap\" that has limited prior work to constrained\nscenarios involving simple geometries, limited object sizes and aspect ratios,\nconstrained wrist poses, or customized hands. We address this sim-to-real\nchallenge with a novel framework that enables a single policy, trained in\nsimulation, to generalize to a wide variety of objects and conditions in the\nreal world. The core of our method is a joint-wise dynamics model that learns\nto bridge the reality gap by effectively fitting limited amount of real-world\ncollected data and then adapting the sim policy's actions accordingly. The\nmodel is highly data-efficient and generalizable across different whole-hand\ninteraction distributions by factorizing dynamics across joints, compressing\nsystem-wide influences into low-dimensional variables, and learning each\njoint's evolution from its own dynamic profile, implicitly capturing these net\neffects. We pair this with a fully autonomous data collection strategy that\ngathers diverse, real-world interaction data with minimal human intervention.\nOur complete pipeline demonstrates unprecedented generality: a single policy\nsuccessfully rotates challenging objects with complex shapes (e.g., animals),\nhigh aspect ratios (up to 5.33), and small sizes, all while handling diverse\nwrist orientations and rotation axes. Comprehensive real-world evaluations and\na teleoperation application for complex tasks validate the effectiveness and\nrobustness of our approach. Website: https://meowuu7.github.io/DexNDM/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684sim-to-real\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u52a8\u529b\u5b66\u6a21\u578b\u6709\u6548\u5f25\u5408\u73b0\u5b9e\u5dee\u8ddd\uff0c\u4f7f\u5355\u4e00\u6a21\u62df\u8bad\u7ec3\u7b56\u7565\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6cdb\u5316\u5904\u7406\u5404\u79cd\u7269\u4f53\u7684\u624b\u5185\u65cb\u8f6c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u624b\u5185\u7269\u4f53\u65cb\u8f6c\u4efb\u52a1\u4e2dsim-to-real\u7684\u6311\u6218\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u3001\u5c3a\u5bf8\u3001\u624b\u8155\u59ff\u6001\u7b49\u65b9\u9762\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u8054\u5408\u52a8\u529b\u5b66\u6a21\u578b\u5b66\u4e60\u5f25\u5408\u73b0\u5b9e\u5dee\u8ddd\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u5173\u8282\u52a8\u529b\u5b66\u3001\u538b\u7f29\u7cfb\u7edf\u5f71\u54cd\u4e3a\u4f4e\u7ef4\u53d8\u91cf\uff0c\u5e76\u7ed3\u5408\u81ea\u4e3b\u6570\u636e\u6536\u96c6\u7b56\u7565\u83b7\u53d6\u591a\u6837\u5316\u771f\u5b9e\u4ea4\u4e92\u6570\u636e\u3002", "result": "\u5355\u4e00\u7b56\u7565\u6210\u529f\u65cb\u8f6c\u590d\u6742\u5f62\u72b6\u7269\u4f53\uff08\u5982\u52a8\u7269\u6a21\u578b\uff09\u3001\u9ad8\u5bbd\u6bd4\u7269\u4f53\uff08\u8fbe5.33\uff09\u548c\u5c0f\u5c3a\u5bf8\u7269\u4f53\uff0c\u540c\u65f6\u5904\u7406\u591a\u6837\u5316\u624b\u8155\u65b9\u5411\u548c\u65cb\u8f6c\u8f74\uff0c\u5c55\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u548c\u8de8\u5173\u8282\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u624b\u5185\u65cb\u8f6c\u4efb\u52a1\u7684sim-to-real\u6311\u6218\uff0c\u4e3a\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08568", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08568", "abs": "https://arxiv.org/abs/2510.08568", "authors": ["Hongyu Li", "Lingfeng Sun", "Yafei Hu", "Duy Ta", "Jennifer Barry", "George Konidaris", "Jiahui Fu"], "title": "NovaFlow: Zero-Shot Manipulation via Actionable Flow from Generated Videos", "comment": null, "summary": "Enabling robots to execute novel manipulation tasks zero-shot is a central\ngoal in robotics. Most existing methods assume in-distribution tasks or rely on\nfine-tuning with embodiment-matched data, limiting transfer across platforms.\nWe present NovaFlow, an autonomous manipulation framework that converts a task\ndescription into an actionable plan for a target robot without any\ndemonstrations. Given a task description, NovaFlow synthesizes a video using a\nvideo generation model and distills it into 3D actionable object flow using\noff-the-shelf perception modules. From the object flow, it computes relative\nposes for rigid objects and realizes them as robot actions via grasp proposals\nand trajectory optimization. For deformable objects, this flow serves as a\ntracking objective for model-based planning with a particle-based dynamics\nmodel. By decoupling task understanding from low-level control, NovaFlow\nnaturally transfers across embodiments. We validate on rigid, articulated, and\ndeformable object manipulation tasks using a table-top Franka arm and a Spot\nquadrupedal mobile robot, and achieve effective zero-shot execution without\ndemonstrations or embodiment-specific training. Project website:\nhttps://novaflow.lhy.xyz/.", "AI": {"tldr": "NovaFlow\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u8ba1\u5212\uff0c\u65e0\u9700\u6f14\u793a\u6216\u7279\u5b9a\u5e73\u53f0\u8bad\u7ec3\uff0c\u652f\u6301\u521a\u6027\u3001\u5173\u8282\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u64cd\u4f5c\u3002", "motivation": "\u5b9e\u73b0\u673a\u5668\u4eba\u96f6\u6837\u672c\u6267\u884c\u65b0\u64cd\u4f5c\u4efb\u52a1\u662f\u673a\u5668\u4eba\u5b66\u7684\u6838\u5fc3\u76ee\u6807\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4efb\u52a1\u5206\u5e03\u5185\u6570\u636e\u6216\u7279\u5b9a\u5e73\u53f0\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u8de8\u5e73\u53f0\u8fc1\u79fb\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u89c6\u9891\u751f\u6210\u6a21\u578b\u5c06\u4efb\u52a1\u63cf\u8ff0\u5408\u6210\u4e3a\u89c6\u9891\uff0c\u4f7f\u7528\u73b0\u6210\u611f\u77e5\u6a21\u5757\u63d0\u53d63D\u53ef\u64cd\u4f5c\u7269\u4f53\u6d41\uff0c\u5bf9\u521a\u6027\u7269\u4f53\u8ba1\u7b97\u76f8\u5bf9\u4f4d\u59ff\u5e76\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u5bf9\u53ef\u53d8\u5f62\u7269\u4f53\u4f7f\u7528\u57fa\u4e8e\u7c92\u5b50\u7684\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5728\u684c\u9762Franka\u673a\u68b0\u81c2\u548cSpot\u56db\u8db3\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u521a\u6027\u3001\u5173\u8282\u548c\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u7684\u6709\u6548\u96f6\u6837\u672c\u6267\u884c\uff0c\u65e0\u9700\u6f14\u793a\u6216\u7279\u5b9a\u5e73\u53f0\u8bad\u7ec3\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4efb\u52a1\u7406\u89e3\u4e0e\u5e95\u5c42\u63a7\u5236\u89e3\u8026\uff0cNovaFlow\u5b9e\u73b0\u4e86\u81ea\u7136\u7684\u8de8\u5e73\u53f0\u8fc1\u79fb\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u96f6\u6837\u672c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08571", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.08571", "abs": "https://arxiv.org/abs/2510.08571", "authors": ["Animikh Aich", "Adwait Kulkarni", "Eshed Ohn-Bar"], "title": "Scalable Offline Metrics for Autonomous Driving", "comment": "Accepted at IROS 2025 (IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)", "summary": "Real-World evaluation of perception-based planning models for robotic\nsystems, such as autonomous vehicles, can be safely and inexpensively conducted\noffline, i.e., by computing model prediction error over a pre-collected\nvalidation dataset with ground-truth annotations. However, extrapolating from\noffline model performance to online settings remains a challenge. In these\nsettings, seemingly minor errors can compound and result in test-time\ninfractions or collisions. This relationship is understudied, particularly\nacross diverse closed-loop metrics and complex urban maneuvers. In this work,\nwe revisit this undervalued question in policy evaluation through an extensive\nset of experiments across diverse conditions and metrics. Based on analysis in\nsimulation, we find an even worse correlation between offline and online\nsettings than reported by prior studies, casting doubts on the validity of\ncurrent evaluation practices and metrics for driving policies. Next, we bridge\nthe gap between offline and online evaluation. We investigate an offline metric\nbased on epistemic uncertainty, which aims to capture events that are likely to\ncause errors in closed-loop settings. The resulting metric achieves over 13%\nimprovement in correlation compared to previous offline metrics. We further\nvalidate the generalization of our findings beyond the simulation environment\nin real-world settings, where even greater gains are observed.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u611f\u77e5\u89c4\u5212\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u79bb\u7ebf\u8bc4\u4f30\u4e0e\u5728\u7ebf\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u4e24\u8005\u76f8\u5173\u6027\u6bd4\u5148\u524d\u7814\u7a76\u66f4\u5dee\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u79bb\u7ebf\u6307\u6807\u6765\u6539\u5584\u8fd9\u79cd\u76f8\u5173\u6027\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u79bb\u7ebf\u8bc4\u4f30\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u5728\u7ebf\u6027\u80fd\uff0c\u5fae\u5c0f\u7684\u9519\u8bef\u53ef\u80fd\u5728\u6d4b\u8bd5\u65f6\u7d2f\u79ef\u5bfc\u81f4\u4e8b\u6545\uff0c\u8fd9\u79cd\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u4e4b\u95f4\u7684\u5173\u7cfb\u5728\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e2d\u7f3a\u4e4f\u6df1\u5165\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5927\u91cf\u6a21\u62df\u5b9e\u9a8c\u5206\u6790\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u7684\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u79bb\u7ebf\u6307\u6807\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u53d1\u73b0\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u6bd4\u5148\u524d\u7814\u7a76\u66f4\u5dee\uff0c\u63d0\u51fa\u7684\u57fa\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u79bb\u7ebf\u6307\u6807\u76f8\u6bd4\u5148\u524d\u6307\u6807\u76f8\u5173\u6027\u63d0\u9ad8\u4e8613%\u4ee5\u4e0a\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u8bc4\u4f30\u5b9e\u8df5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u57fa\u4e8e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u79bb\u7ebf\u6307\u6807\u80fd\u6709\u6548\u7f29\u5c0f\u79bb\u7ebf\u4e0e\u5728\u7ebf\u8bc4\u4f30\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u66f4\u53ef\u9760\u7684\u7b56\u7565\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2510.08572", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08572", "abs": "https://arxiv.org/abs/2510.08572", "authors": ["Rocktim Jyoti Das", "Harsh Singh", "Diana Turmakhan", "Muhammad Abdullah Sohail", "Mingfei Han", "Preslav Nakov", "Fabio Pizzati", "Ivan Laptev"], "title": "BLAZER: Bootstrapping LLM-based Manipulation Agents with Zero-Shot Data Generation", "comment": "11 pages, 8 figures", "summary": "Scaling data and models has played a pivotal role in the remarkable progress\nof computer vision and language. Inspired by these domains, recent efforts in\nrobotics have similarly focused on scaling both data and model size to develop\nmore generalizable and robust policies. However, unlike vision and language,\nrobotics lacks access to internet-scale demonstrations across diverse robotic\ntasks and environments. As a result, the scale of existing datasets typically\nsuffers from the need for manual data collection and curation. To address this\nproblem, here we propose BLAZER, a framework that learns manipulation policies\nfrom automatically generated training data. We build on the zero-shot\ncapabilities of LLM planners and automatically generate demonstrations for\ndiverse manipulation tasks in simulation. Successful examples are then used to\nfinetune an LLM and to improve its planning capabilities without human\nsupervision. Notably, while BLAZER training requires access to the simulator's\nstate, we demonstrate direct transfer of acquired skills to sensor-based\nmanipulation. Through extensive experiments, we show BLAZER to significantly\nimprove zero-shot manipulation in both simulated and real environments.\nMoreover, BLAZER improves on tasks outside of its training pool and enables\ndownscaling of LLM models. Our code and data will be made publicly available on\nthe project page.", "AI": {"tldr": "BLAZER\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u6570\u636e\u6765\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u5229\u7528LLM\u89c4\u5212\u5668\u7684\u96f6\u6837\u672c\u80fd\u529b\u5728\u6a21\u62df\u73af\u5883\u4e2d\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u7684\u6f14\u793a\uff0c\u6210\u529f\u793a\u4f8b\u7528\u4e8e\u5fae\u8c03LLM\u4ee5\u63d0\u5347\u89c4\u5212\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u673a\u5668\u4eba\u9886\u57df\u7f3a\u4e4f\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u591a\u4efb\u52a1\u6f14\u793a\u6570\u636e\uff0c\u73b0\u6709\u6570\u636e\u96c6\u53d7\u9650\u4e8e\u624b\u52a8\u6536\u96c6\u548c\u6574\u7406\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u57fa\u4e8eLLM\u89c4\u5212\u5668\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u7684\u6f14\u793a\uff0c\u4f7f\u7528\u6210\u529f\u793a\u4f8b\u5fae\u8c03LLM\u4ee5\u63d0\u5347\u89c4\u5212\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\u3002", "result": "BLAZER\u663e\u8457\u6539\u5584\u4e86\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u96f6\u6837\u672c\u64cd\u4f5c\u80fd\u529b\uff0c\u5728\u8bad\u7ec3\u6c60\u5916\u4efb\u52a1\u4e0a\u4e5f\u6709\u63d0\u5347\uff0c\u5e76\u652f\u6301LLM\u6a21\u578b\u7684\u4e0b\u91c7\u6837\u3002", "conclusion": "BLAZER\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u751f\u6210\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
