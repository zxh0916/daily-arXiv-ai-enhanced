<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 40]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: VAR-SLAM是一个基于ORB-SLAM3的视觉SLAM系统，结合轻量级语义关键点滤波器处理已知移动物体，并使用Barron自适应鲁棒损失处理未知移动物体，在动态环境中显著提升了轨迹精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有动态环境下的视觉SLAM方法主要依赖语义滤波（仅处理已知物体类别）或固定鲁棒核函数（无法适应未知移动物体），当场景中出现未知移动物体时会导致精度下降。

Method: 结合轻量级语义关键点滤波器处理已知移动物体，使用Barron自适应鲁棒损失处理未知移动物体，通过在线估计鲁棒核的形状参数来自动调整高斯和重尾行为。

Result: 在TUM RGB-D、Bonn RGB-D Dynamic和OpenLORIS数据集上的评估显示，相比最先进的基线方法，VAR-SLAM实现了更高的轨迹精度和鲁棒性，在挑战性序列上比NGD-SLAM的ATE RMSE降低了25%，同时平均性能保持在27 FPS。

Conclusion: VAR-SLAM通过结合语义滤波和自适应鲁棒损失，有效解决了动态环境中已知和未知移动物体的挑战，显著提升了SLAM系统的性能。

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [2] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: DeGrip是一种专为报废电脑台式机拆卸设计的定制化机械手，具有3个自由度，采用线缆驱动机制，能够在受限空间中操作，并在Isaac Sim仿真环境中验证了其拆卸能力。


<details>
  <summary>Details</summary>
Motivation: 智能机器人拆卸报废产品在机器人领域一直是个挑战，现有机器学习技术因缺乏专用硬件而难以在实际场景中应用。

Method: 开发DeGrip定制机械手，提供3个自由度，采用线缆驱动传输机制减小尺寸，设计腕部解耦腕关节和钳口关节的驱动，并在Isaac Sim中建立拆卸环境进行评估。

Result: 评估结果证实DeGrip能够在受限空间中操作，并以任意配置拆卸组件，具备报废台式机拆卸的能力。

Conclusion: DeGrip机械手成功解决了报废产品拆卸中的硬件限制问题，为实际应用提供了有效的解决方案。

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [3] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: 本文介绍了Cosmos-Surg-dVRK，一个基于Cosmos世界基础模型的外科手术微调模型，结合训练的视频分类器，实现了手术策略的自动化在线评估和基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于物理机器人平台（如da Vinci Research Kit）评估手术策略存在高成本、时间消耗大、可重复性差和执行变异性等问题，需要开发更高效的评估方法。

Method: 使用Cosmos世界基础模型进行外科手术微调，开发Cosmos-Surg-dVRK模型，并结合V-JEPA 2衍生的视频分类器构建自动化评估管道。

Result: 在桌面缝合垫任务中，Cosmos-Surg-dVRK的在线推演与真实dVRK平台结果具有强相关性，视频分类器与人工标注者之间也有良好一致性。离体猪胆囊切除术实验显示与真实世界评估的初步对齐。

Conclusion: Cosmos-Surg-dVRK平台为复杂外科手术程序提供了有前景的自动化评估解决方案，能够有效替代昂贵的物理机器人平台测试。

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [4] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 提出了一种无需训练、运行时策略引导的方法，通过采样多个候选动作序列、模拟预测结果，并使用预训练视觉语言模型选择与文本计划最匹配的动作序列，从而提高推理-动作对齐的忠实度。


<details>
  <summary>Details</summary>
Motivation: 现有的推理视觉语言动作模型在生成正确文本计划后，实际执行的动作仍可能偏离预期结果，特别是在分布外场景中。这体现了推理与动作之间缺乏忠实对齐的问题。

Method: 基于推理VLA的中间文本计划，从同一模型采样多个候选动作序列，通过模拟预测结果，使用预训练VLM选择与VLA自身文本计划最匹配的动作序列。

Result: 在行为组合任务上比先前工作性能提升高达15%，对语义和视觉分布外扰动具有更强的鲁棒性，且性能随计算和数据多样性扩展。

Conclusion: 该方法将基础VLA的自然动作多样性从错误来源转变为优势，无需昂贵的重新训练即可实现新颖行为组合，并贡献了推理标注的LIBERO-100扩展版本用于分布外评估。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [5] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: SPOT是一个统一的规划框架，通过障碍物威胁建模将感知目标明确纳入运动优化，解决了单深度相机无人机在动态障碍物避障中的视野限制和盲区问题。


<details>
  <summary>Details</summary>
Motivation: 配备单深度相机的无人机由于视野有限和不可避免的盲区，在动态障碍物避障方面面临重大挑战。现有方法通常将运动规划与感知考虑分开，导致障碍物响应效果较差且延迟。

Method: 提出基于高斯过程的障碍物信念地图，建立已识别和潜在障碍物的统一概率表示。通过碰撞感知推理机制将空间不确定性和轨迹接近度转化为时变观察紧急度地图，并在当前视野内集成紧急度值来定义可微分目标。

Result: 仿真和真实世界实验表明，该方法比基线方法提前2.8秒检测到潜在动态障碍物，动态障碍物可见性提高超过500%，能够在杂乱、遮挡环境中安全导航。

Conclusion: SPOT框架通过将感知目标纳入运动优化，实现了实时、感知感知的轨迹规划，计算时间低于10毫秒，显著提高了动态障碍物检测能力和导航安全性。

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [6] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: 该论文提出了Manual2Skill++框架，将连接器作为装配任务的一等公民，通过视觉语言模型从装配手册中自动提取结构化连接信息，构建层次化图表示来编码装配任务。


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配方法将连接器视为次要考虑，而连接实际上是决定装配成败的关键环节。论文旨在将连接作为装配表示的核心原语。

Method: 基于人类通过分步指导手册学习装配的启发，使用大规模视觉语言模型解析装配手册中的符号图表和注释，构建层次化图表示，其中节点代表零件和子装配体，边显式建模组件间的连接关系。

Result: 构建了包含20多个装配任务的数据集验证表示提取方法，并在仿真环境中评估了四个复杂装配场景的完整任务理解到执行流程。

Conclusion: 将连接作为一等公民的装配表示方法能够有效提取和编码装配知识，为机器人装配提供了更可靠的解决方案。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [7] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 该研究收集了1,893个用户对家用机器人的问题，涵盖12个类别和70个子类别，揭示了用户最关心的问题类型及其重要性排序，为机器人问答系统设计提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和对话界面在人机交互中的广泛应用，机器人回答用户问题的能力变得愈发重要。现有可解释机器人研究主要关注"为什么"类问题，而缺乏对用户实际提问多样性的理解。

Method: 通过创建15个视频刺激和7个文本刺激，描绘机器人执行各种家务任务的场景，在Prolific平台上向100名参与者收集他们在每个情境下会向机器人提出的问题。

Result: 数据集中最常见的问题类别包括任务执行细节（22.5%）、机器人能力（12.7%）和性能评估（11.3%）。虽然关于机器人处理困难场景和确保正确行为的问题较少，但用户认为这些是最重要的问题。新手用户与有经验用户的问题类型存在差异。

Conclusion: 该数据集为识别机器人需要记录和暴露给对话界面的信息、基准测试问答模块以及设计符合用户期望的解释策略提供了宝贵基础。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [8] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: ORAD-3D是目前最大的越野自动驾驶数据集，涵盖多种地形和环境条件，并建立了包含5个核心任务的基准评估体系。


<details>
  <summary>Details</summary>
Motivation: 越野自动驾驶研究面临大规模高质量数据集稀缺的瓶颈，需要填补这一空白。

Method: 构建了ORAD-3D数据集，覆盖林地、农田、草地、河岸、碎石路、水泥路和乡村地区等多种地形，并捕捉不同天气条件和光照水平的环境变化。

Result: 建立了包含2D自由空间检测、3D占用预测、粗略GPS引导路径规划、视觉语言模型驱动自动驾驶和越野环境世界模型等5个任务的综合基准评估套件。

Conclusion: 该数据集和基准为推进挑战性越野场景下的感知和规划提供了统一且强大的资源。

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [9] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SPD机械手是一种新型机器人夹爪，采用线性平行夹持机制，无需调整整体高度即可抓取桌面上的各种尺寸物体，解决了传统工业夹爪需要调整机器人手臂高度的问题。


<details>
  <summary>Details</summary>
Motivation: 传统工业夹爪在平行夹持时指尖呈弧形运动，需要调整整个机器人手臂的高度以避免与桌面碰撞，这限制了抓取效率和适应性。

Method: 设计了具有手掌和两个机械相同、对称排列手指的SPD夹爪，指尖沿线性轨迹运动，可独立驱动或由单个电机驱动，并进行了优化分析理论研究和原型开发。

Result: 实验结果表明，该机械手成功实现了线性平行夹持功能，并展现出良好的适应性。

Conclusion: SPD机械手在体现智能技术发展中，能够帮助各种机器人实现有效抓取，为增强深度学习训练的数据收集奠定基础。

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [10] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: DIV-Nav是一个实时导航系统，能够处理包含空间关系的复杂自由文本查询，通过语义映射分解、交集计算和LVLM验证来实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本物体导航通常设计用于简单查询（如物体名称），但无法处理包含空间关系的复杂自由文本查询（如"在桌子上找遥控器"）。

Method: 通过三个松弛步骤：i) 将复杂空间约束的自然语言指令分解为语义映射上的简单物体级查询，ii) 计算个体语义信念映射的交集以识别所有物体共存的区域，iii) 使用LVLM验证发现的物体是否符合原始复杂空间约束。

Result: 在MultiON基准测试和波士顿动力Spot机器人上的真实世界部署中进行了广泛实验验证。

Conclusion: DIV-Nav系统能够有效处理复杂空间查询，并通过适应前沿探索目标来更有效地指导搜索过程。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [11] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SP-Diff平行夹爪系统采用创新的差动连杆机构和模块化对称双指配置，实现线性平行抓取，通过行星齿轮传动实现同步线性运动和独立手指姿态调整，减少Z轴重新校准需求30%。


<details>
  <summary>Details</summary>
Motivation: 解决智能工业自动化中传统末端执行器适应性有限的问题，开发具有自适应能力的抓取系统。

Method: 采用差动连杆机构、模块化对称双指配置、行星齿轮传动系统、运动学优化的平行四边形连杆和差动机构。

Result: 系统展示了适应各种工业工件和可变形物体（如柑橘类水果）的自适应抓取能力，减少Z轴重新校准需求30%，并具备力/视觉传感器集成接口。

Conclusion: SP-Diff通过其自适应架构推进了机器人末端执行器的智能化，在协作机器人、物流自动化和专业操作场景中具有广阔应用前景。

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [12] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: MoS-VLA是一个视觉-语言-动作模型框架，通过将机器人操作策略表示为有限技能基函数的线性组合，实现跨数据集预训练和快速适应新任务。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在新环境、新机器人或新任务中往往无法直接使用，需要更高效的适应方法。

Method: 在预训练阶段联合学习技能基函数，测试时仅需单次专家演示，通过轻量级凸优化推断技能表示，无需梯度更新。

Result: 在五个未见数据集上获得更低的动作预测误差，在仿真和真实机器人任务中成功完成预训练VLA模型失败的任务。

Conclusion: MoS-VLA通过技能混合表示实现了高效的任务适应，为通用机器人控制提供了新思路。

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [13] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 本研究调查了急救人员在紧急行动中对机器人系统使用语义信息和态势感知的态度。通过对8个国家22名急救人员的问卷调查，发现他们对机器人持积极态度，认为语义信息对构建态势感知很有用，并愿意使用不完美但信息丰富的AI支持工具。


<details>
  <summary>Details</summary>
Motivation: 了解急救人员对语义增强态势感知在机器人系统中应用的态度和需求，填补该领域直接调查急救人员意见的研究空白。

Method: 采用结构化问卷调查法，调查了来自8个国家的22名急救人员，收集了人口统计信息、对机器人的一般态度以及语义增强态势感知的体验。

Result: 大多数急救人员对机器人持积极态度，语义信息对构建态势感知的有用性评分为3.6/5，对预测突发事件的有用性评分为3.9/5。参与者要求语义输出平均74.6%的准确率才能信任，67.8%的准确率才被认为有用。

Conclusion: 研究揭示了语义信息在紧急响应中的价值，发现了实验室机器人能力与现场部署现实之间的关键差距，强调了急救人员与机器人研究人员之间需要更有意义的合作，以开发更符合用户需求和情境感知的机器人系统。

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [14] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文分析了激励设计对刚体纳卫星和微卫星惯性特性识别的影响，比较了最小二乘法和扩展卡尔曼滤波器在不同卫星配置下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究激励设计如何影响刚性纳卫星和微卫星的惯性特性识别，为在轨自适应惯性识别提供实用指导。

Method: 模拟非线性姿态动力学，考虑反作用轮耦合、执行器限制和外部干扰，使用八种不同频谱丰富度的扭矩剖面激励系统，比较批处理最小二乘法和扩展卡尔曼滤波器。

Result: 结果表明，激励频率内容和估计器假设共同决定了估计精度和鲁棒性，为每种方法的最佳性能条件提供了指导。

Conclusion: 激励频率内容和估计器假设对惯性识别性能有显著影响，研究结果为在轨自适应惯性识别提供了实用指导，代码已开源。

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [15] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: 提出自适应不变扩展卡尔曼滤波器，通过在线协方差估计自适应调整接触足模型噪声水平，改进腿式机器人的本体状态估计性能。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人的状态估计直接影响控制性能和运动稳定性，传统方法难以处理微小滑动且过度敏感的滑动拒绝设置可能导致滤波器发散。

Method: 使用自适应不变扩展卡尔曼滤波器，基于在线协方差估计调整接触足模型噪声水平，采用接触检测算法替代接触传感器。

Result: 在四足机器人LeoQuad上的真实实验验证了该方法在动态运动场景中具有增强的状态估计性能。

Conclusion: 该方法能有效处理传统滑动拒绝方法无法解决的小滑动问题，减少对额外硬件的依赖，提高腿式机器人在变化接触条件下的状态估计精度。

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [16] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: T3 Planner是一个基于大语言模型的机器人运动规划框架，通过三个级联模块分解时空任务约束，使用信号时序逻辑验证器自我修正输出，生成满足复杂约束的可行运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域专业知识定制规划器，难以处理时空耦合问题，导致运动不可行或任务规划与运动执行不一致。大语言模型虽然擅长语义推理，但会产生不可行的运动规划。

Method: 通过三个级联模块分解时空任务约束，每个模块激发大语言模型生成候选轨迹序列，并使用信号时序逻辑验证器检查可行性，直到找到满足复杂空间、时间和逻辑约束的轨迹。

Result: 在不同场景下的实验表明，T3 Planner显著优于基线方法，所需的推理能力可以蒸馏到轻量级的Qwen3-4B模型中实现高效部署。

Conclusion: T3 Planner框架成功解决了自然语言指令到可执行运动规划的转换问题，通过形式化方法自我修正大语言模型的输出，实现了可靠的机器人运动规划。

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [17] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: 本文提出将定位导航授时(PNT)从"工具导向"转向"认知驱动"的新视角和路线图，通过结合机器PNT的高精度和脑启发空间认知导航，开发更弹性、节能和认知能力强的通用PNT系统。


<details>
  <summary>Details</summary>
Motivation: 当前复杂环境需要更弹性、节能和认知能力强的PNT系统，目标是开发具有脑启发空间认知导航能力的无人系统，同时利用机器PNT的高精度来推进通用PNT发展。

Method: 提出四层(观测-能力-决策-硬件)融合框架，将数值精度与脑启发智能相结合；对传统PNT、生物脑PNT和脑启发PNT进行多层次差异分析。

Result: 建立了结合机器PNT精度和脑启发智能的融合框架，为PNT系统从工具导向转向认知驱动提供了理论基础和方法路径。

Conclusion: 脑启发PNT代表了PNT发展的新方向，通过融合数值精度和认知智能，能够开发出更适应复杂环境的通用PNT系统，为未来PNT发展提供了前瞻性建议。

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [18] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: 论文提出了一种新的轨迹采样方法C-Free-Uniform，该方法能够根据局部地图信息均匀采样自由配置空间，并将其集成到MPPI控制器中，在复杂环境中以更少的采样预算实现更高的导航成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹采样方法中控制输入分布通常独立于环境，这限制了在复杂环境中的导航性能。需要一种能够根据当前环境信息自适应调整采样策略的方法。

Method: 提出了自由配置空间均匀性(C-Free-Uniform)概念，设计了一种显式依赖于当前局部地图的控制输入分布，并将其集成到新的模型预测路径积分(MPPI)控制器CFU-MPPI中。

Result: 实验表明，CFU-MPPI在复杂多边形环境中的导航任务中，相比现有方法具有更高的成功率，同时所需的采样预算显著减少。

Conclusion: C-Free-Uniform方法通过环境感知的采样策略有效提升了导航性能，证明了在轨迹采样中考虑环境信息的重要性。

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [19] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: RAPID Hand是一款低成本、20自由度的灵巧手原型，采用创新的仿人驱动和传动方案，通过3D打印部件和定制齿轮实现经济性，在灵巧遥操作任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧遥操作中缺乏经济实惠的完全驱动五指手的问题，这对于在'从演示中学习'范式中收集大规模真实机器人数据至关重要。

Method: 采用新型仿人驱动和传动方案，包括非拇指手指的通用指骨传动方案和全向拇指驱动机制，使用3D打印部件和定制齿轮以降低成本并便于更换维修。

Result: 在灵巧遥操作系统中通过定量指标和定性测试评估，在多指抓取、勺子操作和类人钢琴演奏等三个挑战性任务中表现良好。

Conclusion: RAPID Hand的完全驱动20自由度设计在灵巧遥操作方面具有重要潜力。

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [20] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: 提出DINO-CVA多模态目标条件行为克隆框架，用于实现自主导管导航，融合视觉观察和操纵杆运动学信息，在合成血管模型实验中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 传统心脏导管介入手术依赖人工操作，现有机器人系统缺乏智能自主性，导致操作者疲劳、辐射暴露增加和手术结果不一致。

Method: 开发多模态目标条件行为克隆框架，将视觉观察和操纵杆运动学融合到联合嵌入空间，通过自回归方式从专家演示中预测动作，目标条件指导导航到指定目的地。

Result: DINO-CVA在预测动作方面达到高精度，与仅使用运动学的基线性能相当，同时将预测基于解剖环境。

Conclusion: 多模态目标条件架构在导管导航中具有可行性，是减少操作者依赖、提高导管治疗可靠性的重要一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [21] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 提出CEM-RM框架，通过结合交叉熵方法和奖励模型，高效优化肌腱驱动软体机械手设计，相比纯优化方法减少一半以上设计评估次数，同时从预收集的遥操作数据中学习优化设计分布。


<details>
  <summary>Details</summary>
Motivation: 软体机械手虽然能提供柔顺安全的交互，但设计既柔顺又功能多样化的软体手具有挑战性。硬件与控制协同设计虽然能更好耦合形态与行为，但搜索空间高维，仿真评估计算成本高。

Method: 提出CEM-RM框架，基于遥操作控制策略优化肌腱驱动软体机械手，在仿真中实现并行化训练，优化后的设计通过3D打印制造并在真实世界部署。

Result: 在仿真和硬件实验中，优化设计在多样化挑战性物体上的抓取成功率显著优于基线机械手。

Conclusion: CEM-RM框架能有效优化软体机械手设计，显著提升抓取性能，同时大幅减少设计评估成本。

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [22] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 这篇论文系统综述了提高视觉-语言-动作（VLA）模型效率的方法，重点关注减少延迟、内存占用和训练/推理成本，将现有解决方案分为四个维度：模型架构、感知特征、动作生成和训练/推理策略。


<details>
  <summary>Details</summary>
Motivation: VLA模型虽然能够将自然语言指令和视觉观察映射到机器人动作，但其巨大的计算和内存需求与边缘平台（如需要实时性能的移动机械臂）的约束相冲突，这已成为当前研究的核心焦点。

Method: 通过系统性文献综述，将提高VLA效率的方法分类为四个维度：模型架构优化、感知特征处理、动作生成机制以及训练和推理策略，并在每个类别中总结代表性技术。

Result: 论文提供了VLA效率优化技术的全面分类框架，涵盖了从模型设计到部署的各个环节，为研究人员提供了系统性的参考指南。

Conclusion: 论文讨论了未来趋势和开放挑战，强调了推进高效具身智能的发展方向，为VLA系统的实际部署和应用提供了重要指导。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [23] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++是一个增强的自动驾驶框架，通过度量引导的对齐机制，将认知推理与端到端规划相结合，解决了传统E2E模型在长尾场景中泛化能力不足和VLA模型3D推理能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 传统端到端驾驶模型能生成物理上可行的轨迹，但缺乏必要的世界知识来处理长尾场景；而视觉-语言-动作模型虽然能利用世界知识处理复杂情况，但3D推理能力有限可能导致物理上不可行的动作。

Method: 1. 构建直接生成语义基础驾驶轨迹的VLA模块；2. 设计具有密集轨迹词汇表的E2E模块确保物理可行性；3. 引入度量引导的轨迹评分器来对齐VLA和E2E模块的输出。

Result: 在ICCV 2025自动驾驶大挑战排行榜上，DiffVLA++实现了49.12的EPDMS分数。

Conclusion: DiffVLA++通过将认知推理与端到端规划相结合，成功整合了两种方法的互补优势，在自动驾驶任务中取得了优异表现。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [24] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: OmniVIC是一个基于视觉语言模型的通用可变阻抗控制器，通过自改进的检索增强生成和上下文学习技术，在接触丰富的机器人操作任务中提高安全性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统可变阻抗控制器在物理交互中具有优势，但在未见过的复杂非结构化安全交互场景中缺乏泛化能力。需要将高级语义推理与低级合规控制相结合，实现更安全和可泛化的操作。

Method: 使用自改进的检索增强生成从结构化记忆库中检索相关先验经验，结合上下文学习技术，利用视觉语言模型根据当前任务提示生成上下文感知的自适应阻抗参数，并通过实时力/扭矩反馈确保交互力在安全阈值内。

Result: 在模拟和真实机器人任务中，OmniVIC在复杂的接触丰富任务上优于基线方法，平均成功率从27%（基线）提高到61.4%（OmniVIC），同时减少了力违规。

Conclusion: OmniVIC在连接高级语义推理和低级合规控制方面迈出了一步，实现了更安全和更可泛化的机器人操作。

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [25] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: 提出SimpleVSF框架，通过融合视觉语言模型的认知能力和轨迹融合技术来增强端到端自动驾驶规划，在ICCV 2025挑战赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法在复杂场景中决策能力不足，需要更智能的规划策略。

Method: 结合传统评分器和VLM增强评分器，使用权重融合器进行定量聚合和VLM融合器进行情境感知的定性决策。

Result: 在ICCV 2025 NAVSIM v2挑战赛中取得领先性能，在安全性、舒适性和效率方面达到最佳平衡。

Conclusion: SimpleVSF框架通过融合VLM认知能力显著提升了端到端自动驾驶的规划性能。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [26] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 该研究提出了一种基于事件相机的新型自定位系统，通过可见光通信(VLC)获取LED发射器的坐标信息，并通过可见光定位(VLP)估计到每个发射器的距离，实现在GPS失效环境下的车辆定位。


<details>
  <summary>Details</summary>
Motivation: 利用事件相机的高时间分辨率和高动态范围特性，解决在隧道等GPS失效环境下车辆的定位问题，同时实现通信和定位功能。

Method: 在发射端安装多个LED，每个LED分配基于Walsh-Hadamard码的唯一导频序列。事件相机通过相关接收信号与这些码来识别单个LED，实现多输入单输出通信和基于相位相关(POC)的精确距离估计。

Result: 在30 km/h车速下的现场实验显示，距离估计的均方根误差在100米范围内小于0.75米，误码率低于0.01。

Conclusion: 这是首个使用单个事件相机同时实现VLC和VLP功能的车辆载系统，在真实环境中表现出稳健的性能。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [27] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 提出了一种名为"Pole-Image"的混合方法，使用杆状物作为锚点，从周围3D结构生成签名，解决移动机器人长期自主性中自定位和地图维护的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决传统地标方法在高可检测性但低区分度地标（如杆状物）与高区分度但难以稳定检测地标（如局部点云结构）之间的基本权衡问题。

Method: 提出Pole-Image规范表示法，将杆状地标及其周围环境表示为以杆本身为原点的2D极坐标图像，利用杆作为高精度参考点，显式编码稳定杆与可变周围点云之间的相对几何关系。

Result: 通过对比学习训练出视角不变且高区分度的描述符，能够克服感知混淆，实现鲁棒自定位，同时高精度编码支持高灵敏度变化检测，有助于地图维护。

Conclusion: Pole-Image方法成功解决了地标检测与区分度之间的权衡问题，为移动机器人长期自主性提供了有效的自定位和地图维护解决方案。

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [28] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了一种模块化控制框架，结合基于模型的动态控制、在线模型自适应和自适应脚步规划，以解决机器人和地形属性不确定性问题，并在火山实地测试中验证了性能。


<details>
  <summary>Details</summary>
Motivation: 行星探索任务需要能够在极端未知环境中导航的机器人。轮式漫游车在过往任务中占主导地位，但其移动性仅限于可穿越表面。腿式机器人（特别是四足机器人）可以通过处理不平坦、障碍物丰富和可变形地形来克服这些限制。然而，在未知条件下部署此类机器人具有挑战性，因为需要环境特定的控制，当地形和机器人参数不确定时这是不可行的。

Method: 开发了一个模块化控制框架，结合了基于模型的动态控制、在线模型自适应和自适应脚步规划。该框架包括带和不带接触感知的四足机器人状态估计，支持运行时重新配置，并集成到ROS 2中且开源可用。

Result: 该框架在两个四足机器人平台、多种硬件架构上进行了验证，并在火山实地测试中表现良好，机器人行走距离超过700米。

Conclusion: 提出的模块化控制框架能够有效应对机器人和地形属性的不确定性，为行星探索任务中的腿式机器人部署提供了可行的解决方案。

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [29] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出了一种新的视频规划框架，通过在线更新模型参数和过滤失败计划，在部分观测环境中实现隐式状态估计和动态适应。


<details>
  <summary>Details</summary>
Motivation: 现有视频规划框架难以在交互时适应失败情况，无法在部分观测环境中处理不确定性。

Method: 集成交互时数据到规划过程，在线更新模型参数并在生成过程中过滤失败计划，实现隐式状态估计。

Result: 在新模拟操作基准上的广泛实验表明，该方法能够改进重新规划性能。

Conclusion: 该框架提升了视频决策领域的能力，使系统能够动态适应而无需显式建模未知状态变量。

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [30] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: 提出DDBot框架，用于自动化操作颗粒材料（如沙土），通过可微分物理模拟器实现高效系统识别和挖掘技能优化，在真实环境中实现零样本部署。


<details>
  <summary>Details</summary>
Motivation: 现有方法在颗粒材料操作任务中难以实现高效和精确，特别是在物理属性未知的情况下。

Method: 使用GPU加速的可微分物理模拟器，结合可微分技能到动作映射、任务导向演示方法、梯度裁剪和基于线搜索的梯度下降。

Result: DDBot能在5-20分钟内收敛，高效识别未知颗粒材料动力学并优化挖掘技能，在真实环境中实现高精度零样本部署。

Conclusion: DDBot在挖掘任务中表现出鲁棒性和高效性，验证了其在实际应用中的可行性。

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [31] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 提出统一的交互力-阻抗控制（IFIC）框架，通过适应交互功率流确保在接触丰富环境中的轻松安全交互，基于端口哈密顿框架保证系统无源性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在混合或统一力-阻抗控制下与主动人类或非被动环境物理交互时可能失去无源性从而危及安全的问题。

Method: 在端口哈密顿框架内制定控制架构，包含交互和任务控制端口，通过适应交互功率流来保证系统无源性。

Result: IFIC框架能够确保在接触丰富环境中的轻松安全交互。

Conclusion: 所提出的统一交互力-阻抗控制框架有效解决了机器人物理交互中的无源性问题，为接触丰富环境中的安全协作提供了保障。

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [32] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: 本研究将视觉-语言-动作模型部署在软体连续机械臂上，通过结构化微调解决了本体不匹配问题，实现了安全的人机交互。


<details>
  <summary>Details</summary>
Motivation: 机器人系统需要在人类中心的无结构环境中安全运行，现有VLA模型主要应用于传统刚性机械臂，缺乏与环境的软性安全交互能力。

Method: 提出结构化微调和部署流程，评估两种最先进的VLA模型在代表性操作任务中的表现，通过针对性微调解决本体不匹配问题。

Result: 未经微调的策略因本体不匹配而失败，但经过微调后软体机器人的表现与刚性机器人相当。

Conclusion: 微调对于弥合本体差距至关重要，VLA模型与软体机器人结合能够在人类共享环境中实现安全和灵活的具身AI。

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [33] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON是一个新的视觉-语言-动作模型范式，通过将丰富的3D空间token注入动作头来解决现有VLA模型在空间推理方面的局限性，仅使用RGB图像就能获得强大的几何先验，并在模拟和真实世界任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型基于2D编码器在3D真实世界中行动，存在空间推理差距，限制了泛化能力和适应性。现有的3D集成技术要么需要专用传感器且跨模态迁移能力差，要么注入缺乏几何信息的弱线索并损害视觉-语言对齐。

Method: FALCON利用空间基础模型仅从RGB图像提供强大的几何先验，包含一个具身空间模型，可选择性地融合深度或姿态信息以获得更高保真度，无需重新训练或架构更改。为了保持语言推理能力，空间token由空间增强动作头处理，而不是连接到视觉-语言主干中。

Result: 在三个模拟基准和十一个真实世界任务的综合评估中，FALCON实现了最先进的性能，始终超越竞争基线，并在杂乱环境、空间提示条件以及物体尺度和高度变化下保持稳健。

Conclusion: FALCON通过创新的空间token注入方法有效解决了空间表示、模态可迁移性和对齐方面的限制，为VLA模型提供了更强大的3D空间推理能力。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [34] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: HumanMPC是一个用于3D微型飞行器在人群中导航的模型预测控制框架，结合了理论安全保证和数据驱动的人类运动预测模型，能够在保证安全的同时实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注简化的2D人群导航，未能充分考虑人体动力学的复杂性。需要开发能够安全高效地在人群中导航的机器人系统。

Method: 提出了一种新颖的基于可达性的安全公式，仅约束初始控制输入以确保安全，同时在整个规划范围内建模其效果。结合数据驱动的人类运动预测模型。

Result: 在模拟实验和真实世界测试中验证了方法的有效性，涵盖从目标导向导航到视觉伺服跟踪人类等多种任务。方法确保安全而不过度保守，在效率和可靠性方面优于基线方法。

Conclusion: HumanMPC框架为3D微型飞行器在人群中导航提供了安全高效的解决方案，虽然本文应用于微型飞行器，但该方法具有通用性，可适应其他平台。

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [35] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一种基于ADMM和DDP的时空轨迹优化框架D-PDDP，用于解决大规模无人机群轨迹优化问题，通过分布式参数化DDP实现多无人机时空参数共识，并采用自适应惩罚参数调整减少迭代次数。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预先设定无人机最终时间且迭代次数多，限制了大规模无人机群的实际应用，需要开发更高效的分布式优化框架。

Method: 采用两级架构：使用参数化DDP作为单个无人机的轨迹优化器，ADMM用于满足局部约束并实现多无人机时空参数共识，形成分布式参数化DDP算法。

Result: 通过多个仿真实验验证了所提算法的有效性，能够高效解决大规模无人机群轨迹优化问题。

Conclusion: D-PDDP框架成功解决了大规模无人机群轨迹优化中的非线性问题和计算效率问题，为实际应用提供了可行方案。

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [36] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 本文提出了一种意图驱动的规划管道，用于多机器人协作完成复杂操作任务，通过集成感知到文本的场景编码、LLM集合生成候选动作序列、LLM验证器和确定性一致性过滤器，实现从人类简单语言指令到可执行多机器人计划的可靠映射。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人协作完成复杂操作任务的问题，这些机器人具有不同的末端执行器和能力，需要在非结构化场景中规划并执行对任意位置和配置物体的连接动作序列。

Method: 提出意图驱动规划管道，包括：(i)感知到文本场景编码，(ii)基于操作者意图生成候选移除序列的LLM集合，(iii)强制执行格式和优先级约束的LLM验证器，(iv)拒绝幻觉对象的确定性一致性过滤器。

Result: 在200个真实场景和600个操作者提示的评估中，使用完整序列正确性和下一任务正确性指标评估了五个基于LLM的规划器。结果表明，集成验证方法能可靠地将操作者意图映射到安全、可执行的多机器人计划，同时保持较低的用户工作量。

Conclusion: 该管道能够稳健地构建具有不同监督输入程度的动作序列，通过简单语言指令实现多机器人协作任务的可靠规划。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [37] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 本文提出了一种改进的混合专家模型，将TLIO方法扩展到自行车定位，在保持精度的同时显著降低了计算成本和参数数量。


<details>
  <summary>Details</summary>
Motivation: 随着共享单车和多样化骑行应用的快速增长，精确的自行车定位变得至关重要。传统GNSS方法存在多路径效应问题，而现有惯性导航方法依赖精确建模且鲁棒性有限。TLIO方法虽然能实现低位置漂移，但计算成本高限制了在移动设备上的部署。

Method: 将TLIO扩展到自行车定位，并引入改进的混合专家模型来降低训练和推理成本。

Result: 与最先进的LLIO框架相比，该方法在保持相当精度的同时，参数减少了64.7%，计算成本降低了81.8%。

Conclusion: 提出的改进混合专家模型在自行车定位任务中实现了高精度与低计算成本的平衡，为移动设备部署提供了可行方案。

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [38] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [39] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: SoftMimic是一个从示例动作中学习人形机器人柔顺全身控制策略的框架，通过强化学习训练机器人柔顺响应外部力，同时保持平衡和姿态。


<details>
  <summary>Details</summary>
Motivation: 现有方法激励僵硬控制，当机器人遇到意外接触时会导致脆弱和不安全行为，需要开发能够柔顺响应外部力的控制策略。

Method: 利用逆运动学求解器生成可行的柔顺运动增强数据集，训练强化学习策略匹配柔顺响应而非刚性跟踪参考运动。

Result: 通过仿真和真实世界实验验证，展示了与环境的安有效交互，能够吸收干扰并从单个动作片段泛化到各种任务。

Conclusion: SoftMimic框架能够学习柔顺的全身控制策略，使机器人能够安全有效地与环境交互，同时保持平衡和姿态。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [40] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 本文提出了RoboBench基准测试，用于系统评估多模态大语言模型作为具身大脑在机器人操作任务中的认知能力，涵盖5个维度、14种能力、25个任务和6092个问答对。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注执行成功率，或在高层次推理方面存在维度不完整和任务真实性有限的问题，无法全面评估具身大脑的认知能力。

Method: RoboBench定义了五个维度：指令理解、感知推理、泛化规划、功能预测和失败分析，并引入了MLLM作为世界模拟器的评估框架来评估具身可行性。

Result: 对14个MLLM的实验揭示了基本局限性：在隐式指令理解、时空推理、跨场景规划、细粒度功能理解和执行失败诊断方面存在困难。

Conclusion: RoboBench为量化高层次认知提供了全面框架，并指导下一代具身MLLM的发展。

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>
